{
    "id": "SOLR-10619",
    "title": "Optimize using cache for DistributedQueue in case of single-consumer",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "6.6",
            "7.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Right now, for every time childWatcher is kicked off. We refetch all children of DQ's node. It is a wasted in case of single consumer.",
    "attachments": {
        "SOLR-10619-dragonsinth.patch": "https://issues.apache.org/jira/secure/attachment/12867040/SOLR-10619-dragonsinth.patch",
        "SOLR-10619.patch": "https://issues.apache.org/jira/secure/attachment/12866711/SOLR-10619.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-05-06T02:29:55+0000",
            "content": "Initial patch for this ticket. The Overseer.testPerformance time reduced from 2m18s to 31s.\nBefore apply the patch, running time = 2m18s\nOverseer loop finished processing: \n\t avgRequestsPerSecond: 0.007718809350800701\n\t 5minRateRequestsPerSecond: 0.0\n\t 15minRateRequestsPerSecond: 0.0\n\t avgTimePerRequest: 129550895586000000\n\t medianRequestTime: 129550895586000000\n\t 75thPcRequestTime: 129550895586000000\n\t 95thPcRequestTime: 129550895586000000\n\t 99thPcRequestTime: 129550895586000000\n\t 999thPcRequestTime: 129550895586000000\nop: am_i_leader, success: 3, failure: 0\n\t avgRequestsPerSecond: 0.02319808972135658\n\t 5minRateRequestsPerSecond: 0.2681280184142557\n\t 15minRateRequestsPerSecond: 0.35006932761717896\n\t avgTimePerRequest: 279884000000\n\t medianRequestTime: 113274000000\n\t 75thPcRequestTime: 113274000000\n\t 95thPcRequestTime: 1470039000000\n\t 99thPcRequestTime: 1470039000000\n\t 999thPcRequestTime: 1470039000000\nop: update_state, success: 20011, failure: 0\n\t avgRequestsPerSecond: 154.80879143800885\n\t 5minRateRequestsPerSecond: 102.1642410535161\n\t 15minRateRequestsPerSecond: 87.06135680694149\n\t avgTimePerRequest: 226193000000\n\t medianRequestTime: 213152000000\n\t 75thPcRequestTime: 228076000000\n\t 95thPcRequestTime: 258815000000\n\t 99thPcRequestTime: 332711000000\n\t 999thPcRequestTime: 3070637000000\nop: state, success: 20001, failure: 0\n\t avgRequestsPerSecond: 154.95644369471393\n\t 5minRateRequestsPerSecond: 103.31921470642384\n\t 15minRateRequestsPerSecond: 88.36168489656214\n\t avgTimePerRequest: 15785000000\n\t medianRequestTime: 14638000000\n\t 75thPcRequestTime: 19736000000\n\t 95thPcRequestTime: 26959000000\n\t 99thPcRequestTime: 41592000000\n\t 999thPcRequestTime: 82301000000\n\n\nAfter apply the patch, running time = 31s\nOverseer loop finished processing: \n\t avgRequestsPerSecond: 0.04702007081952135\n\t 5minRateRequestsPerSecond: 0.0\n\t 15minRateRequestsPerSecond: 0.0\n\t avgTimePerRequest: 21266388653000000\n\t medianRequestTime: 21266388653000000\n\t 75thPcRequestTime: 21266388653000000\n\t 95thPcRequestTime: 21266388653000000\n\t 99thPcRequestTime: 21266388653000000\n\t 999thPcRequestTime: 21266388653000000\nop: am_i_leader, success: 3, failure: 0\n\t avgRequestsPerSecond: 0.1424634644143451\n\t 5minRateRequestsPerSecond: 0.3804917698002856\n\t 15minRateRequestsPerSecond: 0.393388581528647\n\t avgTimePerRequest: 715047000000\n\t medianRequestTime: 354218000000\n\t 75thPcRequestTime: 1893782000000\n\t 95thPcRequestTime: 1893782000000\n\t 99thPcRequestTime: 1893782000000\n\t 999thPcRequestTime: 1893782000000\nop: update_state, success: 20011, failure: 0\n\t avgRequestsPerSecond: 952.638561665829\n\t 5minRateRequestsPerSecond: 794.828260100909\n\t 15minRateRequestsPerSecond: 786.4717084310993\n\t avgTimePerRequest: 214683000000\n\t medianRequestTime: 197207000000\n\t 75thPcRequestTime: 217411000000\n\t 95thPcRequestTime: 298345000000\n\t 99thPcRequestTime: 363035000000\n\t 999thPcRequestTime: 2295814000000\nop: state, success: 20001, failure: 0\n\t avgRequestsPerSecond: 953.595055990491\n\t 5minRateRequestsPerSecond: 799.4104704273237\n\t 15minRateRequestsPerSecond: 791.1978869728018\n\t avgTimePerRequest: 12457000000\n\t medianRequestTime: 8845000000\n\t 75thPcRequestTime: 14149000000\n\t 95thPcRequestTime: 28728000000\n\t 99thPcRequestTime: 50239000000\n\t 999thPcRequestTime: 79344000000\n ",
            "author": "Cao Manh Dat",
            "id": "comment-15999228"
        },
        {
            "date": "2017-05-06T02:47:04+0000",
            "content": "I think this patch is ready to be committed. Scott Blum Can you take a look at the patch? ",
            "author": "Cao Manh Dat",
            "id": "comment-15999232"
        },
        {
            "date": "2017-05-06T05:30:53+0000",
            "content": "What's the whole \"singleConsumer\" thing?  Can you just give me an overview of what's happening here? ",
            "author": "Scott Blum",
            "id": "comment-15999299"
        },
        {
            "date": "2017-05-06T07:36:11+0000",
            "content": "Scott Blum \"singleConsumer\" ( multiple producer single consumer ) means many thread enqueue data to DQ but only one thread remove ( process ) message from DQ.\nThe scenario here is whenever we call to firstChild() ( peek or poll ) with isDirty state equals true. We always fetchZkChildren even when knowChildren is not empty. This is wasteful in case of Overseer.DQ ( single consumer ), ex: \n\n\tknowChildren already have 20,000 elements\n\tsome other thread offer one more element to the queue,\n\tchildWatcher is kicked off in the Overseer.DQ, so isDirty state is set to true\n\tthe next time we call peek, fetchZkChildren is called to refill knowChildren with 20,001 elements\n\n ",
            "author": "Cao Manh Dat",
            "id": "comment-15999326"
        },
        {
            "date": "2017-05-06T21:20:46+0000",
            "content": "I completely follow your logic.  But then, why do we need a singleConsumer flag?  Couldn't we just do the optimization in all cases?\n\nIn other words, let's say we're dirty but we have children in memory.  Fine, let's optimistically fetch the first child's data based on the node name we have in memory.  If it succeeds, we're done, just return it.\n\nBut if it fails, if there's no node then, and we're in a dirty state, THEN we can refetch the child list from ZK. ",
            "author": "Scott Blum",
            "id": "comment-15999586"
        },
        {
            "date": "2017-05-06T21:30:29+0000",
            "content": "I completely follow your logic. But then, why do we need a singleConsumer flag? Couldn't we just do the optimization in all cases?\nI thought the reason about not doing this one in the first place. The only reason is in case of multiple consumer, the current logic makes sense.\n\nYeah, your idea seems work. Will update the patch in that way ",
            "author": "Cao Manh Dat",
            "id": "comment-15999590"
        },
        {
            "date": "2017-05-06T21:34:54+0000",
            "content": "But instead of refetch the child list from ZK when there's no node. I think we should try to go to next node name we have in memory. ",
            "author": "Cao Manh Dat",
            "id": "comment-15999591"
        },
        {
            "date": "2017-05-07T02:01:39+0000",
            "content": "Good thought.  I was debating the same thing in my head.  In normal Solr operation, it would be an exceptional case, since Overseer is single-consumer, it should almost never be the case that resolving the head node to ZK fails.  But for the sake of generality, I can see both sides:\n\n\n\tIf you continue trying to iterate through in-memory children, you might hit a really long list of failures (many, many round trips to ZK) before hitting the next live child.  Theoretically, re-fetching the child list from ZK would be faster to get a fresh view.\n\n\n\n\n\tBut, in a true multi-consumer use case, re-fetching the child list every time you get a \"miss\" because someone else consumed ahead of you would cause the many consumers to thrash a lot.\n\n\n\nSo I'm a little torn, since option 1 might be slightly better for Overseer in particular, but option 2 is clearly more general.  Thoughts? ",
            "author": "Scott Blum",
            "id": "comment-15999652"
        },
        {
            "date": "2017-05-07T02:10:01+0000",
            "content": "Yeah, that's the reason why I added a flag ( single-consumer ) to the class. It's much easier to optimize if we know how DQ is used. \nBut in case of single-consumer, the event of a node not exist is kinda rare ( right? ), if option 2 is more general, we should go with option 2? ",
            "author": "Cao Manh Dat",
            "id": "comment-15999654"
        },
        {
            "date": "2017-05-07T04:30:20+0000",
            "content": "Sounds good! ",
            "author": "Scott Blum",
            "id": "comment-15999680"
        },
        {
            "date": "2017-05-07T07:28:08+0000",
            "content": "I was under the impression that the DistribuedQueue only set a watch on ZK if knownChildren is empty. It says so in the implementation notes section of the class. Did that change or did that never work at all? ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15999720"
        },
        {
            "date": "2017-05-07T07:59:16+0000",
            "content": "I read the code again and yeah my understanding was wrong. The watcher is set on each fetch from zk which either happens because we are in a dirty state or when we run out of items in the knownChildren so we end up having a watcher all the time. The docs in the implementation notes are also wrong.\n\nI'll lean towards the option that keeps things simple for the use-case we need this queue for \u2013 which is the overseer. There's no harm in explicitly marking/documenting DistributedQueue as a single-consumer queue.\n\nIf the overseer sees a node disappear from ZK it is either because someone mucked around with ZK manually or a new overseer has been elected and the old overseer hasn't realised that yet. In such a case, it might be worth to add a callback which the overseer can use to check if it is still the leader. The result of this callback can be used to abort further calls to ZK.\n\nWow, I'm glad Dat caught this! Thanks Dat! ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15999727"
        },
        {
            "date": "2017-05-08T05:55:03+0000",
            "content": "I'll take another look tomorrow. ",
            "author": "Scott Blum",
            "id": "comment-16000304"
        },
        {
            "date": "2017-05-08T10:00:58+0000",
            "content": "Updated patch for this ticket. In this patch, firstChild() will always return from cache first, if the node is not exist, the callers of firstChild() will clear the cache and recall firstChild() ",
            "author": "Cao Manh Dat",
            "id": "comment-16000532"
        },
        {
            "date": "2017-05-09T00:45:53+0000",
            "content": "Cao Manh Dat patch LGTM.  I would update the main class comment on DistributedQueue to read:\n\n\"A distributed queue.  Optimized for single-consumer, multiple-producer: if there are multiple consumers on the same ZK queue, the results should be correct but inefficient.\"\n\nAnd then where you call \"knownChildren.clear();\" you could add a comment \"Efficient only for single-consumer\" ",
            "author": "Scott Blum",
            "id": "comment-16001872"
        },
        {
            "date": "2017-05-09T03:31:16+0000",
            "content": "The patch make MultiThreadedOCPTest.testFillWorkQueue() fail, because DQ.peekElements() changed its behaviour, trying to find a ways to solve this problem. ",
            "author": "Cao Manh Dat",
            "id": "comment-16001975"
        },
        {
            "date": "2017-05-09T04:13:32+0000",
            "content": "Introduced a flag to firstChild so peekElements can expect the same behaviour as before. We can open a new ticket if we found peekElements is not efficient enough. ",
            "author": "Cao Manh Dat",
            "id": "comment-16001998"
        },
        {
            "date": "2017-05-09T04:35:05+0000",
            "content": "Hmm lemme take a look.  Maybe that test should just have different expectations? ",
            "author": "Scott Blum",
            "id": "comment-16002013"
        },
        {
            "date": "2017-05-09T05:20:11+0000",
            "content": "Cao Manh Dat Okay, I see what's going on here.  Your change looks good.  Although... I think maybe we should change peekElements so that it only forces a refresh if it's going to have to block? ",
            "author": "Scott Blum",
            "id": "comment-16002053"
        },
        {
            "date": "2017-05-09T05:52:54+0000",
            "content": "What do you think about this? ",
            "author": "Scott Blum",
            "id": "comment-16002091"
        },
        {
            "date": "2017-05-09T06:27:25+0000",
            "content": "+1 LGTM for Scott's patch that does not refetch unless we need to block ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16002139"
        },
        {
            "date": "2017-05-09T06:40:55+0000",
            "content": "Scott Blum The patch LGTM, I will commit the patch soon. ",
            "author": "Cao Manh Dat",
            "id": "comment-16002157"
        },
        {
            "date": "2017-05-09T06:57:53+0000",
            "content": "Commit a24fa8d7db5d880723cfc0eaa26d7ae320c4cbeb in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a24fa8d ]\n\nSOLR-10619: Optimize using cache for DistributedQueue in case of single-consumer ",
            "author": "ASF subversion and git services",
            "id": "comment-16002178"
        },
        {
            "date": "2017-05-09T07:07:56+0000",
            "content": "Commit 7b23d96d326100313fda184811e0a0ff434d7d02 in lucene-solr's branch refs/heads/branch_6x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7b23d96 ]\n\nSOLR-10619: Optimize using cache for DistributedQueue in case of single-consumer ",
            "author": "ASF subversion and git services",
            "id": "comment-16002193"
        },
        {
            "date": "2017-05-09T09:10:32+0000",
            "content": "Thanks Scott Blum and Shalin Shekhar Mangar ",
            "author": "Cao Manh Dat",
            "id": "comment-16002346"
        }
    ]
}