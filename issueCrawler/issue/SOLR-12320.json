{
    "id": "SOLR-12320",
    "title": "Not all multi-part post requests should create tmp files.",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "We create tmp files for multi-part posts because often they are uploaded files for Solr cell or something but we also sometimes write params only or params and updates as multi-part post. These should not create any tmp files.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-05-07T06:01:08+0000",
            "content": "Hi Mark,\nthanks for bringing this up! I have thought about this multiple times, especially about cleaning up the temporary files! Because of our discussion around ContentStreams (sorry for this, I was overreacting) I just wanted to share my idea about this:\n\n\n\tYou are right, small files should be stored in maybe byte[] blobs after parsing the multipart request. This is one opportunity (see below). I think you can configure commons-fileupload to do this!? We have to figure out (I looked at this 3 years ago).\n\tLarge files may create temp files. The problem is indeed the cleanup. My idea after the discussion yesterday would be: The ContentStreams of a SolrRequest are all read-once. So my idea would be that all ContentStreams pointing to files from a multipart request, should have a customized \"close()\" method that deletes the file after calling \"super.close()\". This would make the cleanup easier. For the remaining files that may not have been read, the SolrRequestParsers class should call the delete on shutdown of request. Maybe we can add a hook in SolrDispatchFilter so it calls a \"cleanup()\" method in SolrRequestParsers.\n\n\n\nAnother aproach working completely without temp files might be:\n\n\n\tChange the ContentStreams from List to just Iterable or better Iterator. This would change them to be only consumed \"in order\" (and if only Iterator - once).\n\tWhile consuming this iterator, the multipart request parts are extracted from the request on-the fly. The ContentStream implementation just return an InputStream wrapper on top of the underlying ServletInputStream (similar to ZipInputStream for reading zip files). I have no idea if this is doable with commons-fileupload, I hope.\n\tThe underlying ServletInputStream is never closed, the wrapper streams may be closed (a no-op on the underlying stream).\n\tNot sure how contrib-extraction handles it, but TIKA already creates another set of temp files if you pass a stream to it and it requires random access.\n\n\n\nI would prefer the second variant, but this one may require larger changes. ",
            "author": "Uwe Schindler",
            "id": "comment-16465465"
        },
        {
            "date": "2018-05-07T22:08:41+0000",
            "content": "I will have to investigate some to be sure of much, but as far as I know, we currently write to a tmp file due to a File based implementation we pick from commons-fileupload.\n\nIf Tika writes to a different temp file anyway, maybe it's as simple as changing that implementation, I'm not sure yet.\n\nWe do currently clean up using this reaper thread class that we have copied from commons-fileupload due to needing some customizations. I really don't like it though - if we could do cleanup without a reaper thread always running in the background that would be great - or if we didn't need to write to a tmp file here at all. I don't know if we can avoid it entirely or not though. The only thing I could think of that might count on it is uploading very large files for tika to parse with Solr Cell. ",
            "author": "Mark Miller",
            "id": "comment-16466535"
        },
        {
            "date": "2018-05-27T02:26:18+0000",
            "content": "So the disk based impl we use is actually a hybrid impl - it stores in RAM instead for data up to 10kb by default. Probably we just want to\u00a0raise that to something reasonable. ",
            "author": "Mark Miller",
            "id": "comment-16491880"
        }
    ]
}