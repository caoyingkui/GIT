{
    "id": "LUCENE-307",
    "title": "Lock obtain time out errors when opening readers and writers",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/other"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.4",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "The attached Java file shows a locking issue that occurs with Lucene 1.4.2.\n\nOne thread opens and closes an IndexReader.  The other thread opens an\nIndexWriter, adds a document and then closes the IndexWriter.  I would expect\nthat this app should be able to happily run without an issues.\n\nIt fails with:\n  java.io.IOException: Lock obtain timed out\n\nIs this expected?  I thought a Reader could be opened while a Writer is adding a\ndocument.\n\nI am able to get the error in less than 5 minutes when running this on Windows\nXP and Mac OS X.\n\nAny help is appreciated.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--FSLock.java": "https://issues.apache.org/jira/secure/attachment/12312443/ASF.LICENSE.NOT.GRANTED--FSLock.java",
        "ASF.LICENSE.NOT.GRANTED--TestLuceneLocks.java": "https://issues.apache.org/jira/secure/attachment/12312442/ASF.LICENSE.NOT.GRANTED--TestLuceneLocks.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-11-11T09:01:06+0000",
            "content": "Created an attachment (id=13394)\nSimple app that shows how to reproduce the issue ",
            "author": "Reece (YT)",
            "id": "comment-12322026"
        },
        {
            "date": "2004-11-11T09:24:55+0000",
            "content": "I'm suspecting that this may be a CPU starvation issue.  I'm experiencing this\nissue when load is high on the server but want to confirm that its not a Lucene\nissue. ",
            "author": "Reece (YT)",
            "id": "comment-12322027"
        },
        {
            "date": "2004-11-30T10:19:41+0000",
            "content": "For what it's worth, I can definitely reproduce this on my debian linux box...\n\nLinux asimov 2.6.6.hoss1 #1 SMP Tue Jul 6 16:31:01 PDT 2004 i686 GNU/Linux\n\n...it allways crashes within 30 seconds.\n\nThe System.out info seems to indicate that first the IndexReader thread is doing\na few thousand iterations, then the IndexWriter thread is doing a few thousand\niterations, then the IndexReader thread jugs along for many thousands of\niterations without any interuptions, untill the IndexWriter thread generates the\nlock exception...\n\njava.io.IOException: Lock obtain timed out:\nLock@/tmp/lucene-b5cc3e5c3fb36ba171e16cf179c80def-commit.lock\n        at org.apache.lucene.store.Lock.obtain(Lock.java:58)\n        at org.apache.lucene.store.Lock$With.run(Lock.java:108)\n        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)\n        at\norg.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)\n        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)\n        at TestLuceneLocks$2.run(TestLuceneLocks.java:63)\n        at java.lang.Thread.run(Thread.java:534)\n     [java] Total time: 17secs\n\n\nin a dozen tests, the stack trace was always identicle except for the lock filename.\n\n(NOTE: If you modify the test case to use a RAMDirectory, it seems to run\nindefinitely, which would suggest to me that the issue definitely likes in the\ndifference between the Lock instances returned by Directory.makeLock)\n\nI'm not really a concurrency expect, but I'm guessing that the problem has to do\nwith the way Lucene does locking.  It loks like whenever something wants an\nexclusive lock, Lock.obtaion(long) will sleep in 1 second intervals waking up\neach escond to see if \"now\" it can obtain() the lock.  Which means that if some\nother client (in this case the IndexReader thread) is constantly accessing the\nindex, it's very easy for a situation to arise in which the thread scheduler is\nconstantly letting the IndexWritter thread (which is waiting on the lock) \"wake\nup\" while the reader is still blocking it.\n\nI seem to recall that the idiom of \"while (!ready) \n{ sleep() }\n\" is considered\nnot the best aproach, and that using \"wait()\" instead of sleep is considered\nbetter ... but i don't remember exactly why, (or even if i'm right about which\none is better).   So i think that it might be possible to change lucene so that\nit doesn't exhibit this problem \u2013 but i'm not certain whatthat change might be.\n\nIn general, it seems like you might be able to work around this problem, by\nmaking your \"reader\" thread be a little more considerate, and making your writer\nthread retry a couple of times if it gets a lock exception.\n\n ",
            "author": "Hoss Man (Legacy Bugzilla Account)",
            "id": "comment-12322028"
        },
        {
            "date": "2004-12-16T15:09:27+0000",
            "content": "Created an attachment (id=13764)\na lock implementation using java nio FSLock object\n\nI was able to implement this problem very quickly.\n\nI implemented a Lock object using java.nio.channels.FileLock object (FSLock),\nand reimplemented FSDirectory.makeLock by returning the FSLock object. And the\nproblem disappeared.\n\nMaybe lucene should consider using the new java.nio feature of file locking\ninstead of the cumbersome lock file.\n\n-John ",
            "author": "John Wang",
            "id": "comment-12322029"
        },
        {
            "date": "2006-06-23T19:28:54+0000",
            "content": "This looks like a thread starvation issue.  This is suggested in the above comment and I agree.\n\nMeaning, because the IndexWriter lock acquisition is ad-hoc (sleep for 1 second and try again), it's entirely likely this will just eventually time out.\n\nBut, I'm surprised that the FileLock implementation solves this.  The JavaDocs don't seem to indicate that this API makes any effort at scheduling / sequencing lock acquisition to prevent starvation.  Or perhaps this was a platform specific capability that FileLock inherited. ",
            "author": "Michael McCandless",
            "id": "comment-12417477"
        },
        {
            "date": "2006-06-23T23:01:09+0000",
            "content": "I have seen something similar.\n\nWhen the lock file is deleted the return value is not checked.\nI have seen cases where the lock file is left by this call when you would expect it to be deleted.\n\nI suspect that checking the file exists or trying to create the lock file can prevent it from being deleted.\nOther processes could also be preventing the deletion,  but not in my case.\nIt seems to be more likely under heavy lock load.\n\nIt would be simple to retry the delete and report an exception if the lock file could not eventually be deleted by the owner.\n\nThere is also no reason why there can not be a single shared lock of each type for the singe instance of the FSDirectory.\nI suspect there are many use cases where all that is required is in-JVM locking.\n\nI am all for lucene using nio locking and pluggable locking.\n\nFor in-JVM locking with nio you have to use the same file channel instance for locking. In this patch I do not think that is the case as a new RAF and channel instance will be created for each new lock instance.  \n\n ",
            "author": "Andy Hind",
            "id": "comment-12417515"
        },
        {
            "date": "2006-11-02T18:46:12+0000",
            "content": "I think we can close this one?\n\nWe now have locking implementation (*LockFactory) decoupled from directory implementation, so you can do in-JVM locking with an FSDirectory.  We also have a native locking LockFactory implementation (LUCENE-678 ).\n\nI'm still surprised that native locking alone would prevent the starvation issue.  If you try to open/close readers & writers too frequently against a single index there will inevitably be starvation.  Lockless commits (LUCENE-701 ) can help somewhat in that you should not see IOExceptions anymore, but, it will still hit starvation by doing many retries before opening an index.  In any event, I think these open rates are far beyond where Lucene would normally be used so I don't think we should spend time fixing starvation. ",
            "author": "Michael McCandless",
            "id": "comment-12446688"
        }
    ]
}