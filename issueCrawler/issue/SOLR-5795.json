{
    "id": "SOLR-5795",
    "title": "Option to periodically delete docs based on an expiration field -- or ttl specified when indexed.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "A question I get periodically from people is how to automatically remove documents from a collection at a certain time (or after a certain amount of time).  \n\nExcluding from search results using a filter query on a date field is trivial, but you still have to periodically send a deleteByQuery to clean up those older \"expired\" documents.  And in the case where you want all documents to auto-expire some fixed amount of time when they were indexed, you still have to setup a simple UpdateProcessorto set that expiration date.  So i've been thinking it would be nice if there was a simple way to configure solr to do it all for you.",
    "attachments": {
        "SOLR-5795.patch": "https://issues.apache.org/jira/secure/attachment/12632585/SOLR-5795.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-13916125",
            "date": "2014-02-28T18:14:33+0000",
            "content": "Here's the basic design i've been fleshing out in my head...\n\n\n\tA new \"ExpireDocsUpdateProcessorFactory\"\n\t\n\t\tcan compute an expiration field to add to indexed docs based on a \"ttl\" field in the input doc\n\t\t\n\t\t\tperhaps could also fallback to a ttl update request param when bulk adding similar to _version_ ?\n\t\t\tIgnoreFieldUpdateProcessorFactory could be used to remove the ttl if they don't wnat a record in the index of when/why expiration_date was computed\n\t\t\n\t\t\n\t\tCan trigger periodic deleteByQuery on expiration time field\n\t\n\t\n\trough idea for configuration...\n\n<processor class=\"solr.ExpireDocsUpdateProcessorFactory\">\n  <!-- mandatory, must be a date based field in schema.xml -->\n  <str name=\"expiration.fieldName\">expire_at</str>\n  <!-- optional, default is not to auto-expire docs -->\n  <int name=\"deleteIntervalInSeconds\">300</int>\n  <!-- optional, default is not to compute expiration automatically \n       if this field doesn't exist in schema, then IgnoreFieldUpdateProcessorFactory can be configured to remove it.\n    -->\n  <str name=\"ttl.fieldName\">ttl</str>\n</process>\n\n\n\tExpireDocsUpdateProcessorFactory.init() logic:\n\t\n\t\tif ttl.fieldName is specified make a note of it\n\t\tvalidate expiration.fieldName is set & exists in schema\n\t\t\n\t\t\tperhaps in managed schema mode create automatically if it doesn't?\n\t\t\n\t\t\n\t\tif deleteIntervalInSeconds is set:\n\t\t\n\t\t\tspin up a recurring ScheduledThreadPoolExecutor with a recurring AutoExpireDocsCallable\n\t\t\tadd a core Shutdown hook to shutdown the executor when the core shuts down\n\t\t\n\t\t\n\t\n\t\n\tExpireDocsUpdateProcessor.processAdd() logic:\n\t\n\t\tif ttl.fieldName is configured & doc contains that field name:\n\t\t\n\t\t\ttreat value as datemath from NOW and put computed value in expiration.fieldName\n\t\t\n\t\t\n\t\telse: No-Op\n\t\n\t\n\tAutoExpireDocsCallable logic:\n\t\n\t\tif cloud mode, return No-Op unless we are running on the overseer\n\t\tCreate a DeleteUpdateCommand using deleteByQuery of [* TO NOW] using the expiration.fieldName\n\t\t\n\t\t\tthis can be fired directly against the UpdateRequestProcessor returned by the ExpireDocsUpdateProcessorFactory itself using a LocalSolrQueryRequest\n\t\t\t\n\t\t\t\tOr perhaps we make an optional configuration so you can specify any chain name and we fetch it from the SolrCore?\n\t\t\t\n\t\t\t\n\t\t\tthe existing distributed delete logic should ensure it gets distributed cleanly in cloud mode\n\t\t\tNOTE: the executor should run on every node, and only do the overseer check when the executor fires, so even when the overseer changes periodically, whoever the current overseer is every X minutes will fire the delete.\n\t\t\n\t\t\n\t\n\t\n\n\n\nThis, combined with things like DefaultValueUpdateProcessorFactory, IgnoreFieldUpdateProcessorFactory and FirstFieldValueUpdateProcessorFactory on the ttl.fieldName and/or expiration.fieldName should allow all sorts of various usecases:\n\n\n\tevery doc expires after X amount of time no matter what the client says\n\tevery doc defaults to an ttl of X unless it has a doc explicit ttl\n\tevery doc defaults to an ttl of X unless it has a doc explicit expire date\n\tdocs can optional expire after a ttl specified when they were indexed\n\tdocs can optional expire at an explicit time specified when they were indexed\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13916126",
            "date": "2014-02-28T18:14:55+0000",
            "content": "The one hitch with this idea \u2013 which is already a problem if you do the same logic from an external client \u2013 is that ass things stand today, if you do a lot of periodic deleteByQuery commands with auto-commit, every one will cause a new searcher to be opened, even if nothing was actually deleted \u2013 but it looks like we can fix that independently in SOLR-5783.\n\nI'm going to tackle the design i laid out here once I get SOLR-5783 in shape with enough tests that i'm comfortable committing. "
        },
        {
            "author": "Steven Bower",
            "id": "comment-13916185",
            "date": "2014-02-28T18:58:06+0000",
            "content": "The idea makes sense but this seems to me like a horrible feature to add... Content should never be removed without explicit external interactions and this will lead to so many \"where did my content go\" type problems.. Especially since once its gone from the index debugging what went wrong is not going to be easy.. writing a script to send a query delete periodically is really not that complex and then it becomes the responsibility of the content owner/developer to delete content..\n\nI would suggest that if this does go in some sort of \"audit\" output be produced (eg X docs delete automatically or a list of ids)\n\nAlso per this design both the exp and ttl fields must be required if specified in the config else mayhem.. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13917955",
            "date": "2014-03-03T11:10:08+0000",
            "content": "Duplicate of SOLR-3874\n\nApart from that, I think this makes sense to have in a URP like Chris Hostetter (Unused) suggests "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13918203",
            "date": "2014-03-03T16:10:35+0000",
            "content": "Steven Bower Time To Live is a pretty common feature of data platforms.  The \"explicit external interaction\" that you mentioned, in my mind, is the user/application setting up a TTL for a document, it just happens that the event lives in the future.  It is also quite common use case in compliance situations and applications which are searching \"low value\" data and you want to clean up old data periodically.\n\n+1, however, on the audit option. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13919845",
            "date": "2014-03-04T19:51:02+0000",
            "content": "Content should never be removed without explicit external interactions and this will lead to so many \"where did my content go\" type problems.. Especially since once its gone from the index debugging what went wrong is not going to be easy.. writing a script to send a query delete periodically is really not that complex and then it becomes the responsibility of the content owner/developer to delete content..\n\nI'm not sure i follow your reasoning there \u2013 \"where did my content go\" type situations can already exist via any deleteByQuery (not to mention really subtle things like SignatureUpdateProcessorFactory).  If anything the approach i'm suggesting should be more obvious then an external script \u2013 because it would need to be configured right there in the solrconfig.xml where it's obvious and easy to see, as opposed to \"where did my content go? ... time to wade through days of logs looking for deleteByQuey requests that could be coming from anywhere, at any interval of time.\"\n\nThe bottom line, is that someone with the ability to edit solrconfig.xml already has the ability to trump & manipulate & block & mess with content sent from remote clients by content owners/developers \u2013 this would in fact be another way to do that, but i don't think that's a bad thing.  It would just be a simpler, self contained, way for solr admins to say \"I want to have a way to automatically expire content that people put in my index\"\n\nI would suggest that if this does go in some sort of \"audit\" output be produced (eg X docs delete automatically or a list of ids)\n\nthat would be really nice in general with any sort of deleteByQuery \u2013 but it's not currently possible to get that info back from the IndexWriter.  The best we can do is explicitly log when/why we are triggering the automatic deleteByQuery commands\n\n\n\nI'm attaching a patch with a really rough proof of concept for the design outlined above ... still a lot of nocommits & error checking & tests needed, but it gives you something to try out to see what I had in mind.\n\nWith this patch applied, you can startup the example and load docs along the lines of this...\n\n\njava -Durl=\"http://localhost:8983/solr/collection1/update?update.chain=nocommit\" -Ddata=args -jar post.jar '<add><doc><field name=\"id\">EXP</field><field name=\"_expire_at_\">NOW+8MINUTES</field></doc><doc><field name=\"id\">SAFE</field></doc><doc><field name=\"id\">TTL</field><field name=\"_ttl_\">+3MINUTES</field></doc></add>'\n\n\n\n\n\tEvery 5 minutes, a thread will wake up and delete docs\n\tEXP has an explicit value in the _expire_at_ of 8 minutes after it was indexed \u2013 if you index the docs immediatley after starting up Solr, it should be deleted ~10Minutes after startup.\n\tTTL has an implicit  _ttl_ value of 3 minutes after it was indexed, which the processor converts to an absolute value and puts in the _expire_at_ \u2013 if you index the docs immediatley after starting up Solr, it should be deleted ~5Minutes after startup.\n\tSAFE will never be deleted, because nothing gives it a value in the _expire_at_ field.\n\n\n\nOne note where we definitely have to deviate from what i described initially: having hte scheduled task use the factory to access the chain to trigger the delete didn't pan out because i wasn't thinking clearly enough about what that existing API looks like \u2013 the factory doesn't know what chain it's in, or what processor(s) should be \"next\", that's input to getInstance() method on the factory from the chain.  so instead the configuration requires you to specify the name of a chain (can be the same chain you are in) and that chain is used to execute the delete.\n\n(The trickiest part of all of this, will be writing the tests) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13921862",
            "date": "2014-03-06T01:42:44+0000",
            "content": "some baseline tests (including a watcher for the periodic delete commands) and fleshing out some of the config validation.\n\nI tweaked the name of the config args to try and make it more in your face that you were enabling an \"automatic deleting\".  From the test configs...\n\n\n  <updateRequestProcessorChain name=\"convert-ttl\">\n    <processor class=\"solr.processor.DocExpirationUpdateProcessorFactory\">\n      <str name=\"ttlFieldName\">_ttl_</str>\n      <str name=\"expirationFieldName\">_expire_at_</str>\n    </processor>\n    <processor class=\"solr.IgnoreFieldUpdateProcessorFactory\">\n      <str name=\"fieldName\">_ttl_</str>\n    </processor>\n  </updateRequestProcessorChain>\n\n  <updateRequestProcessorChain name=\"scheduled-delete\" default=\"true\">\n    <!-- NOTE: this chain is default so we can see that\n         autoDeleteChainName defaults to the default chain for the SolrCore\n    -->\n    <processor class=\"solr.processor.DocExpirationUpdateProcessorFactory\">\n      <!-- str name=\"autoDeleteChainName\">scheduled-delete</str -->\n      <int name=\"autoDeletePeriodSeconds\">3</int>\n      <str name=\"expirationFieldName\">eXpF</str>\n    </processor>\n    <processor class=\"solr.RecordingUpdateProcessorFactory\" />\n  </updateRequestProcessorChain>\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13923271",
            "date": "2014-03-06T23:53:22+0000",
            "content": "Update patch: Minor improvements to the code, but a whole new cloud based test has been added.  \n\nThe \"run only on overseer\" logic still is the biggest piece of functionality that still needs implemented, because I can't seem to find anyway for code to know if it's the overseer \u2013 i spun that off into blocker SOLR-5823 since it might be meaty in it's own right, and will start looking into that next before i wory too much about polishing what's here. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13947469",
            "date": "2014-03-26T02:20:39+0000",
            "content": "Updated patch:\n\n\n\tjavadocs\n\trefactor some redundent code\n\tadd support for configuring a \"ttlParamName\" that can be used instead of (or as a default to) the \"ttlFieldName\"\n\tadd scafolding for the \"only run on overser\" logic (waiting for SOLR-5823)\n\n\n\nThere's still some TODOs but nothing that I think should be a blocker, just room for improvement and/or additional configuration.\n\n\n\nUnfortunately, when i tried testing this in combination with SOLR-5823 (so only the overseer triggers the periodic deletes) the distrib test failed repeatedly \u2013 it timed out waiting for the doc to be deleted and it never was.  I spent a bit of time looking through the logs, and i can't make sense of it:\n\n\n\tthe overseer logic seemed to be working, periodic deletes were being logged from one node, but other nodes just logged once that they weren't hte overseer and weren't going to manage the deletes\n\tthe deleteByQuery commands seemed to be getting forwarded \u2013 i was seeing deleteByQuery that had TOLEADER and FROMLEADER params getting logged.\n\tlikewise the commit commands also seemed to be getting forwared\n\n\n\n...and yet still, the query loop for the doc that should be expired continously got numFound=1\n\n\nI'll dig in more tomorrow with fresh eyes.\n\nin the meantime: feedback on teh patch \u2013 particularly the javadocs even if folks don't want to wade into the code \u2013 would be appreciated. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13947549",
            "date": "2014-03-26T04:53:15+0000",
            "content": "IMO there should be a default field name for ttl say _ttl even if no field name is specified "
        },
        {
            "author": "Upayavira",
            "id": "comment-13947655",
            "date": "2014-03-26T08:13:01+0000",
            "content": "This looks very useful. Would it be possible, however, to set an active field to false instead of deleting?? Or set an expired field to true. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13947660",
            "date": "2014-03-26T08:17:51+0000",
            "content": "Upayavira There already is a field in the doc which marks the expiry time. So the same field can be used right? An option to not delete the docs (don't run the scheduler) . But at query time you should be able to retrieve the deleted docs with an extra filter/flag  "
        },
        {
            "author": "Upayavira",
            "id": "comment-13947750",
            "date": "2014-03-26T10:01:34+0000",
            "content": "Noble Paul: Yes, if the UpdateProcessor is converting a TTL into an exact time, then it is possible to filter on that exact time, and thus retrieve (un)deleted docs, which is what I was trying to get at, so you are correct, this should be possible already  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13949525",
            "date": "2014-03-27T16:28:52+0000",
            "content": "the deleteByQuery commands seemed to be getting forwarded \u2013 i was seeing deleteByQuery that had TOLEADER and FROMLEADER params getting logged.\n\nI'm not sure what i was looking at before, but after digging into the code a lot more i realized that the only deletes i were seeing happen where happening on the control server \u2013 which it turns out, was acting as the overseer (see SOLR-5919) ... none of the replicas of the test collection were acting sa the overseer, so nothing was doing periodic deletes in the test collection.\n\nbasically, when i laid out my desin for dealing with cloud, i was being silly-stupid...\n\nif cloud mode, return No-Op unless we are running on the overseer\n\n...because there is no garuntee that the overseer node will be hosting a core for every cllection \u2013 you might have 1000 nodes in your cluster, and \"collection47\" might only be using cores on 10 of those nodes \u2013 that's a 1/100 chance that any of the nodes collection47 will be on the overseer.\n\nSo i'm going to need to step back and rethink a way of ensuring that the distributed deletes happen, but don't happen on every node and flood the whole collection with N**2 delete requests.  (possibly by using a micro \"LeaderElection\" just for this purpose? constrained to the existing shard leaders?  or use a best-guess hueristic about the shard leaders? - it's not the end of hte world to have some redundent deletes, we just don't want it to be exponential) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13950194",
            "date": "2014-03-28T00:16:08+0000",
            "content": "Ok - overseer is dead, long live the overseer!\n\nAfter investigating some different options for preventing too many nodes from triggering redundent deletes, what i came up with is this...\n\nIn simple standalone instalations this method always returns true, but in cloud mode it will be true if and only if we are currently the leader of the (active) slice with the first name (lexigraphically).\n\nI outlined the reasoning why I think this is the most straightforward solution in the code...\n\n\n    // This is a lot simpler then doing our own \"leader\" election across all replicas \n    // of all shards since:\n    //   a) we already have a per shard leader\n    //   b) shard names must be unique\n    //   c) ClusterState is already being \"watched\" by ZkController, no additional zk hits\n    //   d) there might be multiple instances of this factory (in multiple chains) per \n    //      collection, so picking an ephemeral node name for our election would be tricky\n\n\n\nWatching the logs when running the tests, things look pretty good, and seem to be operating as designed.  That said: I'd still like to try and come up with some additional black tests to verify only one node is triggering these deletes .. i've got some rough ideas, but nothing concrete \u2013 i'll keep thinking about it.\n\nAnybody see any problems with this approach?\n\n\n\nIMO there should be a default field name for ttl say _ttl even if no field name is specified\n\nI'd deliberately avoided doing that because I'm not a fan of \"magic\" field names and i wanted to ensure we supported the ability to use this processor with out any sort of TTL calculation \u2013 for people who just want to specify their own expiration field values explicitly.\n\nthat said: having a sensible default probably would make the common case more useful \u2013 and we could always document (and test) using <null name=\"ttlFieldName\"/> for people who wnat to disable it.\n\nI'll look into adding that tomorrow. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13951631",
            "date": "2014-03-29T00:11:05+0000",
            "content": "that said: having a sensible default probably would make the common case more useful \u2013 and we could always document (and test) using <null name=\"ttlFieldName\"/> for people who wnat to disable it.\n\nUpdated patch adds support for _ttl_ as a default for both ttlFieldName and ttlParamName. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13956750",
            "date": "2014-04-01T16:55:43+0000",
            "content": "Commit 1583734 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1583734 ]\n\nSOLR-5795: New DocExpirationUpdateProcessorFactory supports computing an expiration date for documents from the TTL expression, as well as automatically deleting expired documents on a periodic basis "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13956795",
            "date": "2014-04-01T17:28:22+0000",
            "content": "Commit 1583741 from hossman@apache.org in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1583741 ]\n\nSOLR-5795: New DocExpirationUpdateProcessorFactory supports computing an expiration date for documents from the TTL expression, as well as automatically deleting expired documents on a periodic basis (merge r1583734) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13957904",
            "date": "2014-04-02T17:20:13+0000",
            "content": "Commit 1584097 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1584097 ]\n\nSOLR-5795: harden leader check to log cleanly (no NPE) in transient situations when there is no leader due to election in progress "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13957911",
            "date": "2014-04-02T17:26:39+0000",
            "content": "Commit 1584099 from hossman@apache.org in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1584099 ]\n\nSOLR-5795: harden leader check to log cleanly (no NPE) in transient situations when there is no leader due to election in progress (merge r1584097) "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13957965",
            "date": "2014-04-02T18:06:14+0000",
            "content": "Hoss, instead of slice.getLeader(), you should use ZkStateReader.getLeaderRetry method. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13958070",
            "date": "2014-04-02T19:25:27+0000",
            "content": "Hoss, instead of slice.getLeader(), you should use ZkStateReader.getLeaderRetry method.\n\nThat was actually a deliberate choice:\n\nThese deletes are low priority and will re-occur frequently - so it's fine to abort quickly as a No-Op, no need to block waiting for a leader. These leader checks will also happen very often and on every node very often - so we don't want to be hammering Zk with active leader checks/retries in a potential high load / leader election / outage situation.  The cached ClusterState info is \"good enough\" \u2013 even if it's stale, the worst case scenario is that multiple nodes trigger a handfull redundant deletes, or the deletes are skipped for one cycle \u2013 but the next one will take care of it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13958468",
            "date": "2014-04-03T02:41:57+0000",
            "content": "That makes sense, Thanks for explaining. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13982590",
            "date": "2014-04-27T23:25:50+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}