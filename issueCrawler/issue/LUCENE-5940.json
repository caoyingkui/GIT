{
    "id": "LUCENE-5940",
    "title": "change index backwards compatibility policy.",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [],
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": []
    },
    "description": "Currently, our index backwards compatibility is unmanageable. The length of time in which we must support old indexes is simply too long.\n\nThe index back compat works like this: everyone wants it, but there are frequently bugs, and when push comes to shove, its not a very sexy thing to work on/fix, so its hard to get any help.\n\nCurrently our back compat \"promise\" is just a broken promise, because we cannot actually guarantee it for these reasons.\n\nI propose we scale back the length of time for which we must support old indexes.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14130103",
            "author": "Ryan Ernst",
            "content": "Big 1.  Our current policy has us supporting indexes 4 years old, and given how long 4x is lasting, that will just keep stretching. Obviously there needs to be an upgrade path, but I don't think it needs to be so easy for someone that hasn't upgraded in 4 years.\n\nMy concrete proposal is supporting the current major release, plus the last minor release of the previous major release.  That should provide an upgrade path by first updating to the last minor release of the major release you are using, followed by the lastest of the next major release.  Given the 4.x architecture with codecs, this should be much easier than it has been to maintain 3x index formats.  ",
            "date": "2014-09-11T14:48:59+0000"
        },
        {
            "id": "comment-14130104",
            "author": "Adrien Grand",
            "content": "My concrete proposal is supporting the current major release, plus the last minor release of the previous major release.\n\nThat is what I was thinking about as well when reading the issue description. Having to keep bw compat for all 4.x codecs once 5.0 is released would be a nightmare. ",
            "date": "2014-09-11T14:50:53+0000"
        },
        {
            "id": "comment-14130121",
            "author": "Tim Smith",
            "content": "i understand the desire for changing the policy here. i wish i didn't have to care about backwards compat support, but its just the nature of things. people have large indexes that can take a significant amount of time to reindex (due to a slow source, or complex processing)\n\nthe current proposal here would be problematic for any lucene users who do not release versions in lock step with lucene versions. Solr obviously would have limited issues here since a user could just upgrade to solr 4.99 (assuming 4.99 is the final 4.x version) and then solr 5.0 and no problems.\n\nhowever, if product X released with lucene 4.88 and the last minor version in 4.x line was 4.99, then the upgrade process to get to a lucene 5.0 index is now convoluted and will require creation of custom offline tools to provide an upgrade path.  This backwards compatibility requirement is now just shifted from the lucene devs to the lucene users and can no longer be a seamless transition.\n\nthe current policy does not have these issues since all that i would need to do is fire up the next version, do a forceMerge, and everything is up to date on latest codecs. (no offline processes required, search can continue to work during upgrade)\n\n ",
            "date": "2014-09-11T15:08:36+0000"
        },
        {
            "id": "comment-14130127",
            "author": "Robert Muir",
            "content": "Tim but the \"policy\" is really a joke. it just locks things in with releases.\n\nCurrently if you are on lucene 2, then in order to get to lucene 4, you have to move to 3.x first.\n\nIf we released 5.0 right now, we would not have to deal with 3.x indexes anymore. We could release 6.0 e.g. within a year of that, and we'd contain the problem.\n\nI think if i actually proposed 5.0 and took it seriously, no one would really complain. But its bogus to do this and issue releases with not so many features just because it makes everyone feel better, when its really the policy that is broken. That is what we should fix.\n\nThis is from someone who has spent the the last 2 days doing nothing but fight back compat in lucene. ",
            "date": "2014-09-11T15:15:02+0000"
        },
        {
            "id": "comment-14130128",
            "author": "Robert Muir",
            "content": "\nHaving to keep bw compat for all 4.x codecs once 5.0 is released would be a nightmare.\n\nRight, the fallback plan is to release 5.0, then rapidly release 6.0 (maybe just a few days after) so we can drop all the shit. That doesn't require a change to the backwards compatibility policy. But i hope everyone understands how ridiculous that is when we can just be reasonable instead. ",
            "date": "2014-09-11T15:16:53+0000"
        },
        {
            "id": "comment-14130131",
            "author": "Uwe Schindler",
            "content": "however, if product X released with lucene 4.88 and the last minor version in 4.x line was 4.99, then the upgrade process to get to a lucene 5.0 index is now convoluted and will require creation of custom offline tools to provide an upgrade path. the current policy does not have these issues since all that i would need to do is fire up the next version, do a forceMerge, and everything is up to date on latest codecs. (no offline processes required, search can continue to work during upgrade)\n\nWe have a tool that does this without forceMerge. It just upgrades those segments that need upgrade and writes a new commit point. It is called IndexUpgrader and has a main method.\n\nMy idea would be to privtde that tool, including all stuff as a self-executing JAR file, so you just need: java -jar lucene-indexupgrader-4.10.0.jar indexdir (basically it is already like that, but you need to build classpath and command line manually). ",
            "date": "2014-09-11T15:20:20+0000"
        },
        {
            "id": "comment-14130138",
            "author": "Tim Smith",
            "content": "5.0 should not be saddled with supporting 3.x index. 100% agree there\n\nhowever, 5.0 should ideally continue to support 4.0-4.99 indexes (at least from the codec/index reading perspective)\n\nthe best place to handle backwards compat is in the core of lucene.\notherwise, you are just going to have uses all over the place doing their own interpretation of \"backwards compat\", getting it wrong, broken, etc. and will subsequently result in lots of irate user filing tickets.\n\nif you only support the last minor version from the previous release, it makes it difficult for everyone who was not at that exact minor release. \n\n\nalso, to uwe's point the \"indexupgrade\" tool is an offline process. also, in my situation, i would need custom packaging of that tool in order to provide ease of use/proper codec usage, etc. vs just fire up index on 5.0 and forceMerge. the custom packaging would also require including an \"old\" version of lucene in my project that would be packaged separately, and would just be a nightmare to maintain.\n\nalternatively, i would just grab the source for all removed 4.x codecs i need and pull them into my project (this is not ideal since they are no longer maintained by lucene devs and may have dependency issues that would require porting) ",
            "date": "2014-09-11T15:25:18+0000"
        },
        {
            "id": "comment-14130141",
            "author": "Robert Muir",
            "content": "\nhowever, 5.0 should ideally continue to support 4.0-4.99 indexes (at least from the codec/index reading perspective)\n\nWho will do the work? Who will maintain this? \n\nWon't be me. ",
            "date": "2014-09-11T15:29:17+0000"
        },
        {
            "id": "comment-14130144",
            "author": "Adrien Grand",
            "content": "Maybe another option would be to have a policy that is purely time-based? Eg. codecs would be removed, even in minor releases, when they have not been the default codec for more than one year? ",
            "date": "2014-09-11T15:32:11+0000"
        },
        {
            "id": "comment-14130148",
            "author": "Robert Muir",
            "content": "\"firefox\" release policy is the other option. We can just release new major versions every few months and keep things contained.\n\nWe can do this now, without any change to the \"policy\"  ",
            "date": "2014-09-11T15:35:24+0000"
        },
        {
            "id": "comment-14130149",
            "author": "Tim Smith",
            "content": "time based would be much more reasonable\n\nas long as people are on a 4.x release that is less 1-2 years old, they should be able to move directly to 5.0\n\nsupporting indexes 4+ years old is asking a bit much, but assuming an external release cycle of 1 year, a 1-2 year cutoff is manageable ",
            "date": "2014-09-11T15:36:05+0000"
        },
        {
            "id": "comment-14130151",
            "author": "Tim Smith",
            "content": "\"firefox\" does not need to worry about an upgrade path for terabytes worth of data\n\nthey only need to worry about upgrading bookmarks and thats about it ",
            "date": "2014-09-11T15:37:30+0000"
        },
        {
            "id": "comment-14130155",
            "author": "Uwe Schindler",
            "content": "I would also agree with a time-based policy.\n\nAbout the proposed \"indexupgrader\" JAR file: The idea was to make it self-contained: means it contains all classes that are needed for upgrade, ideally jarjared (or Maven Shade plugin) to have a different package name containing the version. You can then also bundle it with a project and then start the upgrade. ",
            "date": "2014-09-11T15:41:03+0000"
        },
        {
            "id": "comment-14130156",
            "author": "Robert Muir",
            "content": "The way i see it, back compat is just like any other feature. If people dont step up to contribute to make it happen, then we drop it. \n\nI'm done wasting days and days on it when i don't care about it. ",
            "date": "2014-09-11T15:41:07+0000"
        },
        {
            "id": "comment-14130169",
            "author": "Tim Smith",
            "content": "i fully understand the pain associated with maintaining back compat\n\ni guess it would be good if you (and others) could enumerate all the issues related here for full perspective (description does not list them)\n\nalso, it should be on the developer who removes write support (or removes a codec) to add the backwards compat support/testing.\n\ncreating a new codec that supplants an old codec should not inherently require removal of write support for old codec. ",
            "date": "2014-09-11T15:51:18+0000"
        },
        {
            "id": "comment-14130180",
            "author": "Ryan Ernst",
            "content": "Maybe another option would be to have a policy that is purely time-based?\n\nI had thought about this before making my suggesting, but I think this has the problem of being very arbitrary, and hard to know what upgrade path is needed.  For example, if the policy is 1 year, and I am at 4.3, and the latest is 5.6, how do I know what I need to upgrade to in order to get to 5.6? Is it 5.3.1 or 5.2.4?  I think maintaining this table as old versions are dropped would be difficult in itself.\n\nMy idea would be to privtde that tool, including all stuff as a self-executing JAR file\n\nThis is a great idea! In fact, I think we can make one better.  We could provide this tool, as well as a \"meta\" tool, which knows how to download those tools for each release.  It could then output something like:\n\nFound index version 4.3.2\nLatest version is 6.7.0\nUpgrading index to 4.99.0...done\nUpgrading index to 5.99.0...done\nUpgrading index to 6.7.0...done\n\n ",
            "date": "2014-09-11T15:59:16+0000"
        },
        {
            "id": "comment-14130218",
            "author": "Tim Smith",
            "content": "the problem with the upgrade tool approach is that it doesn't scale to clusters with large numbers of indexes.\n\nfor instance, a cluster that has 50 indexes spread across a bunch of machines.\nthis is now an involved manual task put in the hands of system administrators who don't really know whats going on under the hood. \n\nthats just asking for trouble\n\nit seems like the whole power of codecs is that you can avoid all this and allow for seamless transitions by having read only codecs for previous index formats.\n\nare there technical issues here i'm unaware of beyond creating and maintaining the backwards compat tests?\nsomething outside of the codec mechanism that causes problems?\n\nif not, just dump the read only codecs for old versions in an contrib module and let people upgrade at their leisure (and let the community find/fix bugs as they are encountered) ",
            "date": "2014-09-11T16:11:12+0000"
        },
        {
            "id": "comment-14130247",
            "author": "Uwe Schindler",
            "content": "if not, just dump the read only codecs for old versions in an contrib module and let people upgrade at their leisure (and let the community find/fix bugs as they are encountered)\n\nAlready done in Lucene trunk: There is a new backwards module. In trunk you can read previous indexes only with this jar is the classpath (loaded via SPI). ",
            "date": "2014-09-11T16:28:33+0000"
        },
        {
            "id": "comment-14130262",
            "author": "Robert Muir",
            "content": "\nare there technical issues here i'm unaware of beyond creating and maintaining the backwards compat tests?\nsomething outside of the codec mechanism that causes problems?\n\nThere are plenty, first of all, maintaining back compat codecs has a real cost to improving lucene in the future, because if e.g. I want to make a change to the codec API, i have to make deal with tons of medieval index formats. Same goes with structural changes like making docvalues updatable (shai had to fight a lot here). Even stuff like simple code refactoring is expensive because its just a ton of code.\n\nAlso the old codecs hang behind on features. They might not support various features like offsets in the postings, payloads in the term vectors, missing bitsets for docvalues, or whole datastructure types (SORTED_SET/SORTED_NUMERIC), or even whole parts of the index (3.x with docvalues at all). They are missing various useful statistics, etc. These are just ones i've worked on myself recently, there are more, and there are more coming (like Mike's range prefix feature). This makes things like testing difficult.\n\nBackwards compat drags around a lot of stuff for a long time (see the packed ints api) that makes it more complex and hard to work with and make changes to. It prevents and discourages real improvements to lucene. \n\nThere are plenty of bugs in the back compat, the last few indexes have been riddled with them, some of them bad. Its undertested, overcomplex, and undermaintained. Again, not sexy stuff to work on, nobody wants to improve it.\n\nFinally, users want to have more options, but until we can minimize this backwards compat, i'm personally going to push back very hard on any \"options\", because we simply cannot take on more back compat. So the codec API goes mostly wasted. Maybe we should rename it \"backcompat\" api, because thats all its currently good for. Backcompat hurts the users here in this case. If we didn't have so many ancient formats, we could instead provide (and actually support) \"breadth\" instead, such as various options for the way to encode data so users really can take advantage of it. ",
            "date": "2014-09-11T16:41:53+0000"
        },
        {
            "id": "comment-14130270",
            "author": "Ryan Ernst",
            "content": "the problem with the upgrade tool approach is that it doesn't scale to clusters with large numbers of indexes.\n\nCan you elaborate more?  Your example of 50 indexes spread across many machines doesn't make me understand how it would be difficult to run this tool. I see the steps as:\n\n\tInstall the newest lucene (you would already have to do this)\n\tRun the meta tool. This will download the necessary indexupgrader self contained jar for previous releases, and follow the upgrade path to get to the current release.\n\n\n\nare there technical issues here i'm unaware of beyond creating and maintaining the backwards compat tests?\n\nI'd just like to reiterate what Robert said.  Have you looked at how much code is involved in maintaining backcompat?  Just for the current 3x and 4x, it is enormous.  And you can't assume the codec API will stay the same. Changing the codec api means updating old codecs in some way that they still work as expected (Robert's example with updateable DV). Minimizing that effort for a developer allows more rapid experimentation and iteration.  \n\nThe advantage to the indexupgrader tool Uwe described is it is completely self contained.  All the old codecs are there, and when that jar was created, it was tested thoroughly with the upgrade paths it supports. But those old codecs and upgrade paths don't have to be in the current codebase, which makes changing the current code easier. ",
            "date": "2014-09-11T16:46:18+0000"
        },
        {
            "id": "comment-14130272",
            "author": "Robert Muir",
            "content": "I agree, thats the worst part of all. \"trunk\" should not be burdened with this stuff, but its already overwhelmed completely with back compat. ",
            "date": "2014-09-11T16:48:24+0000"
        },
        {
            "id": "comment-14130307",
            "author": "Tim Smith",
            "content": "i would not consider old indexes not containing support for new features an issue.\nif you want to use new options/features/structures, you need to reindex, no problem here. \n\nyou don't have to convince me that supporting back compat sucks. i agree, but lucene is used by a lot of people for a lot of disparate use cases. removing support for back compat will drive people away since it removes seamless upgrade paths. \n\nthink what would have happened if microsoft release 64-bit windows with no support for running old 32-bit programs.\npeople still want to run old dos programs on windows (go figure, but they want/need it)\n\nit hurts adoption of new versions if you don't provide the back compat. this just leaves a bunch of people running ancient versions of lucene because they don't have any good upgrade path other than complete reindexing.\n\nif there is a bug in \"feature x\", a possible solution is to just remove \"feature x\", but this is gonna piss off everyone who relies on it, regardless of how much you may personally hate \"feature x\"\n\nthe main thing i see as a challenge that you mention here is that you want (or new features may require) refactoring the codec api.  \n\nthis is an engineering challenge and would just require some thought out design to decide what \"final api refactors\" should be needed to support flexibility, addition of new features, and growth without requiring mucking with old codecs in the future. \n\nright now, the IndexWriter and codecs are pretty muddled together in some cases. cleaning up these interfaces and making the codecs self contained should be a goal for any refactors to allow future innovation/addition of features.\n\nas a lucene user, if back compat is yanked and not provided in 5.0 for all 4.x indexes, i will be extremely resistant to upgrade. I would be more inclined to fork the latest 4.x and ditch 5.0. 5.0 would have to offer something REALLY compelling to get me to adopt it.\n\n\n ",
            "date": "2014-09-11T17:10:18+0000"
        },
        {
            "id": "comment-14130308",
            "author": "Robert Muir",
            "content": "\ni would not consider old indexes not containing support for new features an issue.\nif you want to use new options/features/structures, you need to reindex, no problem here. \n\nBecause you are not even considering the developer pain. The tests man, maintaining the tests. ",
            "date": "2014-09-11T17:11:55+0000"
        },
        {
            "id": "comment-14130314",
            "author": "Tim Smith",
            "content": "Can you elaborate more? Your example of 50 indexes spread across many machines doesn't make me understand how it would be difficult to run this tool. I see the steps as:\n\nhere's the issues i would have with an \"upgrade tool\" approach here.\n\n1. external network connectivity is not guaranteed \n2. i have special metadata written in the segment metadata that is important\n3. i use custom codec configuration that upgrade tool would need to use\n4. replicated indexes need a lot of care\n5. this tool would need to be run once for each directory containing an index, for every node that contains indexes\n\n\tthis is an ops nightmare since i won't personally be running the tool. this leaves lots of room for user error that is avoided completely if the index upgrade is seamless (via read only codecs for old versions)\n6. custom directory implementations may muck up the works\n\n\n\nin general, i don't see any way this \"upgrade tool\" would be useful to me without repackaging and adding a ton of extra code to do all the things i need to ensure a consistent index is emitted ",
            "date": "2014-09-11T17:17:18+0000"
        },
        {
            "id": "comment-14130320",
            "author": "Robert Muir",
            "content": "No offense Tim, but your comments exactly fit my description of this issue.\n\n\nThe index back compat works like this: everyone wants it, but there are frequently bugs, and when push comes to shove, its not a very sexy thing to work on/fix, so its hard to get any help.\n\nI don't care what happens on this issue, personally, I'm done working on back compat completely until the policy changes. That includes the current in-progress 4.10.1 release. I've done more than my fair share of fighting it, and it just causes me endless frustration.\n\nIf people care about back compat, then they can go do things like regenerate indexes from previous lucene versions to ensure they arent buggy like LUCENE-5939 and that its actually working. They can try to refactor out old cruft in some way and work on improving the APIs of \"dead index formats\".\n\nBut thats not for me.  ",
            "date": "2014-09-11T17:17:54+0000"
        },
        {
            "id": "comment-14130324",
            "author": "Tim Smith",
            "content": "Because you are not even considering the developer pain. The tests man, maintaining the tests.\n\nthe pain will continue to exist, you are just shifting who feels it. again, i get how painful it is, but best to have that pain felt at the source (and handled properly and consistently by people who fully understand it) as opposed to pushing it all downstream, polluting the waters ",
            "date": "2014-09-11T17:19:42+0000"
        },
        {
            "id": "comment-14130331",
            "author": "Robert Muir",
            "content": "No thats not correct. what you are saying there is \"fuck you man, you do the work\".\n\nI will not. ",
            "date": "2014-09-11T17:22:58+0000"
        },
        {
            "id": "comment-14130333",
            "author": "Tim Smith",
            "content": "I don't care what happens on this issue, personally, I'm done working on back compat completely until the policy changes. That includes the current in-progress 4.10.1 release. I've done more than my fair share of fighting it, and it just causes me endless frustration.\n\nfully your prerogative, this is a volunteer community.\n\ni'm just putting in my 2 cents here since a change here will really be painful to me personally\n\nof course i'm not a committer, so i have no final say ",
            "date": "2014-09-11T17:23:06+0000"
        },
        {
            "id": "comment-14130493",
            "author": "Michael McCandless",
            "content": "+1 to relax the policy\n\n+1 for the \".99\" approach: I think it's easier to \"grok\" than the time-based approach.\n\nBut if we do relax the policy I think we should also improve IndexUpgrader (or make a new top-level tool, which is what we expose to users, hiding the current IndexUpgrader, i.e. Ryan Ernst's idea) to do this upgrade across any 4.x to any 5.x (or across more than 1 major release). ",
            "date": "2014-09-11T18:52:00+0000"
        },
        {
            "id": "comment-14131905",
            "author": "Ryan Ernst",
            "content": "\nof course i'm not a committer, so i have no final say\n\nTim, please don't think that we are trying to ignore your concerns. While I understand your frustration (more work), I don't think the pain you could feel is really any different than today?  There is no specific measurement that goes into what constitutes enough work for a release, just community sway.  Technically, if someone is willing to do the work (LUCENE-5944), and there are 3 +1's, and more +1's than -1's, a release can happen.  I don't mean this as a threat, I only mean it to demonstrate how arbitrary the process can be, not guaranteeing you any kind of time between major releases.  Because of this, you could be in the same situation you described with the shorter BWC policy.\n\nThe suggested policy would greatly simplify the work needed on the development side, and give us a clean slate for each major release.  And at the same time, I think this could theoretically extend the ability to upgrade old indexes over a longer span .  The meta tool I have proposed could be the link between all major versions.  All it needs to do is be able to read what version an index was written with, so it knows the major version (and this ability can be segregated to that tool, as this should be relatively simple to copy if how to do that changes).  I think this is much more powerful than today's policy, while at the same time allowing the API to be improved in significant ways across major releases, compared to now, where it cannot really change without enormous effort because of the need to continue reading the entire previous major version.\n\nSo from a user perspective, we want to make this work; it is not just for developers.  Your main concerns seem to be about the tool being offline, the writing special segment metadata, and the network connectivity to grab the old upgraders.\n\nFirst, I don't see a way around it being offline; the apis between major versions could differ in significant ways. But it is no different than if you had a 3x index today, and we released 5.0 tomorrow: you would first have to upgrade to a 4x index, why wouldn't you upgrade to 4.99? And that process would have to be offline, so adding an additional step of first going to 3.99 doesn't seem unreasonable.\n\nRegarding special metadata, I think most users are just using the default codec as written. When you use non default setup, it will (most likely always) require additional work.  I understand this pain, but it is pain you have put upon yourself.  But if you already have code for 4x, then upgrading to 4.99 before changing your code to work with 5.0 should not be difficult, since within a major release the APIs should be stable.\n\nAs for network connectivity, it seems like this could just be a packaging issue?  Would it help if each release had the metatool containing the necessary subjars for each previous release, so that it would not have to download (it would just make it a bit bigger)?\n\nAs developers we need this to happen, to maintain any kind of sanity in our ability to guarantee compatibility. As users you want backward compatibility to work as long as possible.  I think this would actually serve both purposes, in a way that is advantageous for both sides. ",
            "date": "2014-09-12T18:42:18+0000"
        },
        {
            "id": "comment-14131951",
            "author": "Tim Smith",
            "content": "i fully understand the reasons for wanting to change the policy here. i absolutely hate maintaining backwards compat myself. its just a nightmare and leaves lots of rotting code laying around waiting to wreak havoc and makes it dicey to add new functionality. i'm fully on board with that sentiment\n\nbut, i have to support it, and do so in a seamless online manner that is not prone to user error.\n\ni also get the feeling a lot of the lucene devs in general don't think \"full reindexing\" is an issue and can just be done at any point with minimal cost (just a vibe i've picked up). my experience is that this can be a many months long process (slow sources). this seems to influence support for backwards compatibility, as well as support for changing configuration/schema options, for existing fields, etc\n\nby all means, create a good upgrade tool people can use. however, it won't be useful for me and i will need to find a different solution (which will likely result in slowing my adoption of 5.0 when it is released)\n\ni am in no way advocating that 5.0 should support reading 3.x indexes.\n\nagain, i'm just adding my perspective here so informed people can make a decision based on all points of view\n\nif the policy changes, i will just have to adapt as necessary ",
            "date": "2014-09-12T19:04:15+0000"
        },
        {
            "id": "comment-14132382",
            "author": "Robert Muir",
            "content": "Actually 5.0 doesn't even need to read 4.x indexes. I had forgotten when I opened this JIRA issue that \nwe already voted on this in 2010. (this vote passed).\n\n\n[VOTE] Take 2: Open up a separate line for unstable Solr/Lucene development\n\nThis is a vote for the proposal discussed on the 'Proposal about\nVersion API \"relaxation\"' thread.  This thread replaces the first\nVOTE thread!\n\nThe vote is to open up a separate parallel line of development, called\nunstable (on trunk), where non-back-compatible changes, slated for the\nnext major release, may be safely developed.\n\nBut it's not a free for all: the back compat break must still be\ncarefully tracked in detail (maybe in CHANGES, maybe in a separate\nmore detailed \"guide\" -- tbd), including migration instructions, so\nthat this becomes the \"migration guide\" on how users can move to the\nnew major release.  If there are changes that break the index, we will\ntry very hard to create an index migration tool.\n\n ",
            "date": "2014-09-13T00:34:22+0000"
        },
        {
            "id": "comment-14132444",
            "author": "Robert Muir",
            "content": "\n The meta tool I have proposed could be the link between all major versions.\n\nI agree. In fact its the current \"policy\", we voted in it 4 years ago, Uwe even wrote the tool, but everyone forgot \n\nA few notes:\n\n\tFirst of all, people dont realize that you cant take your 3.x index to 4.0, issue some commits, and bring it to 5.0 and use it. Its always been this way, you have to actually ensure every single segment is in the supported format. So some \"upgrade process\" is always necessary (a forceMerge(), or use of IndexUpgrader).\n\tThere are bugs in this today, because we dont test the \"partially supported\" situation. For example if someone takes their 3.x index today to 4.0, kisses it with some commits, but it still have some 3x segments, then tries to read it with trunk, AFAIK they wont get IndexFormatTooOldException. Instead they will get a confusing SPI failure for \"Lucene3x\". So really, we should make a TooOldCodec that throws IndexFormatTooOldException and register it in SPI with every single codec that is unsupported.\n\tFinally, IMO we have an upgrade tool. pretend we cut 5.0 today. The instructions are simple, just run java -cp lucene-4.10.0-core.jar org.apache.lucene.index.IndexUpgrader <index> and you are ready.\n\n ",
            "date": "2014-09-13T01:30:23+0000"
        },
        {
            "id": "comment-14140022",
            "author": "Shawn Heisey",
            "content": "i also get the feeling a lot of the lucene devs in general don't think \"full reindexing\" is an issue and can just be done at any point with minimal cost (just a vibe i've picked up).\n\nYou're definitely not wrong here.  When first getting an index into production (often daily or even more frequently), and later when the application needs change, the user must make changes to the code (Lucene) or schema (Solr, elasticsearch, or other product) that are incompatible with the existing index.  When a user obtains help from a mailing list or other support resource, such a change is VERY likely.\n\nReindexing is part and parcel of search.  Users who are unable to efficiently perform a reindex will usually find themselves without the search capabilities that they really need, because they made incorrect early assumptions that can't be fixed without reindexing.  This can be the case even if they go years without upgrading their Lucene libraries.  If the actual source data is difficult to obtain, it's strongly recommended that it be gathered into an intermediate store with excellent retrieval characteristics, such as a database. ",
            "date": "2014-09-19T05:26:03+0000"
        },
        {
            "id": "comment-14140703",
            "author": "Tim Smith",
            "content": "Reindexing is part and parcel of search\n\ni think the general goal should be that this is not the case, especially as search is adopted more and more as replacements for systems that do not have these limitations/requirements (databases). obviously this is an ambitious goal that can likely never be fully realized. \n\nalso, \"reindexing\" comes in 2 distinct flavors:\n\n\tcold reindexing - rm -rf the index dir, re feed\n\t\n\t\trequires 2x hardware or downtime\n\t\n\t\n\tlive reindexing - change config, restart system, re feed all docs, change is \"live\" once all docs have been reindexed\n\t\n\t\tobviously a good idea to snapshot any previous index and config so you can restore later on error\n\t\tminimal downtime (just restart)\n\t\tminimal search interruption (some queries related to the change may not match old documents until reindex is complete)\n\t\told content can be replaced slowly over time to receive full functionality\n\t\n\t\n\n\n\n\nlive reindexing does have lots of pitfalls and may not always be viable. for instance, right now it is not possible to add offsets to an index using this approach. as soon as the a new segment is merged with an old one, the offsets are blown away. i had filed a ticket for this. i'm not looking to reopen old wounds here, just pointing out an issue i had with this and had to work around.\n\nlive reindexing is the goal i strive to achieve when reindexing is required (always comes with a caveat to backup your index first for safety). some smart choices when designing the internal schema can reduce or eliminate many prospective issues here even without any core changes to lucene.\n\nit's strongly recommended that it be gathered into an intermediate store\n\nthese recommendations are always valid to make (and i will make them), however this adds an entire new system to the mix. as well as new hardware, services, maintenance, security, etc. also, given the scale and perhaps complexity of the documents, this may not even be enough and will still require a large amount of processing hardware to process these documents as fast as the index can index them in a reasonable amount of time (days vs months). in general, this is just extra complexity that will be dropped due to the higher price tag and maintenance cost. then, when it finally is time to upgrade the end-user expectation is that \"oh, we already have the data indexed, why can't we just use that with the new software\". this expectation is set due to the fact that many customers/users are used to working with databases. i do not have this expectation myself, however i have people downstream that do have these expectations and i need to do my best to accommodate them whether i like it or not.\n\n\nnote, i'm not trying to force any requirements on lucene devs, or soliciting advice on specific functionality, just pointing out some real world use cases i encounter related to discussion here. ",
            "date": "2014-09-19T15:06:53+0000"
        },
        {
            "id": "comment-14140723",
            "author": "Shawn Heisey",
            "content": "\nalso, \"reindexing\" comes in 2 distinct flavors:\n\n\tcold reindexing - rm -rf the index dir, re feed\n\t\n\t\trequires 2x hardware or downtime\n\t\n\t\n\tlive reindexing - change config, restart system, re feed all docs, change is \"live\" once all docs have been reindexed\n\t\n\t\tobviously a good idea to snapshot any previous index and config so you can restore later on error\n\t\tminimal downtime (just restart)\n\t\tminimal search interruption (some queries related to the change may not match old documents until reindex is complete)\n\t\told content can be replaced slowly over time to receive full functionality\n\t\n\t\n\n\n\nI use Solr.  My reindexing method is actually a combination of the two you've mentioned.  For every shard, I have a live core and a build core.  When a reindex is required, I start importing from my database into the build cores.  In the meantime, the live cores are still being updated once a minute with new data and deletes.  When the full import is done, I apply all relevant changes to the build cores, then swap them with the live cores.  Once that copy of my index is rebuilt, I re-enable it so that the load balancer can use it again. ",
            "date": "2014-09-19T15:18:38+0000"
        }
    ]
}