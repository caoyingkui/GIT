{
    "id": "LUCENE-6365",
    "title": "Optimized iteration of finite strings",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.0",
        "components": [
            "core/other"
        ],
        "labels": "",
        "fix_versions": [
            "5.3",
            "6.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "Replaced Operations.getFiniteStrings() by an optimized FiniteStringIterator.\n\nBenefits:\nAvoid huge hash set of finite strings.\nAvoid massive object/array creation during processing.\n\n\"Downside\":\nIteration order changed, so when iterating with a limit, the result may differ slightly. Old: emit current node, if accept / recurse. New: recurse / emit current node, if accept.\n\nThe old method Operations.getFiniteStrings() still exists, because it eases the tests. It is now implemented by use of the new FiniteStringIterator.",
    "attachments": {
        "LUCENE-6365.patch": "https://issues.apache.org/jira/secure/attachment/12743577/LUCENE-6365.patch",
        "FiniteStrings_reuse.patch": "https://issues.apache.org/jira/secure/attachment/12743394/FiniteStrings_reuse.patch",
        "FiniteStrings_noreuse.patch": "https://issues.apache.org/jira/secure/attachment/12743537/FiniteStrings_noreuse.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14367046",
            "author": "Markus Heiden",
            "date": "2015-03-18T12:16:14+0000",
            "content": "Patch containing the proposed changes. "
        },
        {
            "id": "comment-14369645",
            "author": "Dawid Weiss",
            "date": "2015-03-19T16:33:00+0000",
            "content": "I looked at the patch (briefly, without delving into all the details \u2013 I'm on vacation . \n\nI like the idea overall. The only thing that bugs me is the explicitness of FiniteStringsIterator. I would rather have something like:\n\nIterator<IntsRef> Operations.iterateFiniteStrings(...)\n\n\nso that we can leave FiniteStringsIterator as a package-private implementation and not proliferate it around. If Iterator interface gives you a headache (which it can) then I'd leave FiniteStringsIterator as an explicit return type but leave the factory method inside Operations.\n\nThe assignments inside the conditional part of for loops are very likely to drive somebody crazy sooner or later.\n\nAgain, I only looked at the patch, I don't even have a proper environment with me to check on all the details. "
        },
        {
            "id": "comment-14373083",
            "author": "Markus Heiden",
            "date": "2015-03-21T22:41:48+0000",
            "content": "I prefer a \"standard\" Iterator too. But by implementing it, my implementation loses some of its benefits, because an iterator needs a look-ahead for hasNext().\n\nI chose the current implementation because:\n1) I can reuse the iterator for different iterations (similar to Transition): See AnalyzingSuggester. So e.g. the internal stack does not need to be reallocated. This avoids not just the initial allocation, but the resizes during the iterations too. BTW: May we choose a bigger initial size for the stack?\n2) By avoiding a look-ahead, my implementation can return the internal IntRefs (from the used IntsRefBuilder) in next(), without the need for a deep copy. "
        },
        {
            "id": "comment-14373092",
            "author": "Robert Muir",
            "date": "2015-03-21T23:15:01+0000",
            "content": "\nBTW: May we choose a bigger initial size for the stack?\n\nNo, this is too much as a library, it impacts too many uses and makes the library too difficult to use (special jvm flags must be supplied). "
        },
        {
            "id": "comment-14374820",
            "author": "Markus Heiden",
            "date": "2015-03-22T06:54:29+0000",
            "content": "I talked about the stack like structure inside the iterator not the jvm stack size. This stack holds the PathNodes of the current finite string. It has an initial size of just 4, so it has to be dynamicly resized in almost all use cases. "
        },
        {
            "id": "comment-14482044",
            "author": "Michael McCandless",
            "date": "2015-04-06T21:59:29+0000",
            "content": "I like this change overall ... it means you can iterate over many more finite strings if you want.  I think a dedicated iterator is OK; just mark it @lucene.experimental so we are free to improve it later.\n\nIteration order changed, so when iterating with a limit, the result may differ slightly. \n\nI think that's fine and it's an impl detail (which strings you'll get when you hit the limit) ... can you update the javadocs to say so? "
        },
        {
            "id": "comment-14482753",
            "author": "Dawid Weiss",
            "date": "2015-04-07T07:43:34+0000",
            "content": "Oops, apologies Markus, this slipped my agenda somehow. As for implementing Iterator \u2013 ok, you don't have to stick to Java's one, I understand the lookahead is hairy. But I'd still change those for loops with compound assignment/ comparison to something that is less cryptic  (like a simple while loop with the variable thrown out). The JIT doesn't care about it anyway I bet (variables undergo liveness analysis). "
        },
        {
            "id": "comment-14490888",
            "author": "Markus Heiden",
            "date": "2015-04-11T09:43:08+0000",
            "content": "Are you talking about this?\n\nfor (IntsRef finiteString; (finiteString = iterator.next()) != null;)\n\n\nFor me it is the standard iteration pattern for non-lookahead iterations, like e.g. iterating over an input stream (see e.g. FileCopyUtils of Spring framework).\n\nDoes this one look better for you?\n\nfor (IntsRef finiteString = iterator.next(); finiteString != null; finiteString = iterator.next())\n\n\nI like my version better, because it is shorter and the iterator.next() is not doubled, but I will you use it, if you like it better.\n\nA simple while loop looks even more bloated to me. It unnecessarily widens the scope of finiteString and splits things which belong together, which both is error prone for coding:\n\nIntsRef finiteString = iterator.next();\nwhile (finiteString != null) {\n   // do something\n\n   finiteString = iterator.next();\n}\n\n\n\nSomething different:\nI marked Operations.getFiniteStrings() as deprecated in my patch, because it should be replaced by the new iterator. But I consider to remove the deprecated, because this method is easier to use for single iterations of small finite strings sets and makes some tests cases simpler. What do you think?\n\nAgain something different:\nWhat about the initial stack size in the new iterator (which needs to be at least as big as the max. length of the iterated finite strings)? May I raise it from 4 to e.g. 16? In my opinion this would be needed for roughly 90% of all cases. "
        },
        {
            "id": "comment-14491672",
            "author": "Dawid Weiss",
            "date": "2015-04-12T19:31:04+0000",
            "content": "\nfor (IntsRef finiteString; (finiteString = iterator.next()) != null;)\n\n\n\nMany people have strong feelings about assignments in conditional expressions. In fact I just recently stumbled upon an Eclipse JDT refactoring bug that was evaluating these (and refactoring these) incorrectly. My comment was actually meant to go together with the \"why don't we make it an iterator\" one... If you did that then the problem of what kind of loop it is pretty much disappears.\n\nAnyway, this is really minor and a matter of style rather than correctness. It can go in as-is.\n\nI marked Operations.getFiniteStrings() as deprecated in my patch [...] What do you think?\n\nNo strong opinion. If it's only used from tests then you can mark it as deprecated I guess; no need to support redundant code.\n\nMay I raise it from 4 to e.g. 16?\n\nIt very likely won't matter in practice at all. I think increasing it to 16 won't do anybody any harm (you could try to squeeze it into a single line cache, but I think it's an overkill and premature optimization; it'll vanish in other processing noise). "
        },
        {
            "id": "comment-14491743",
            "author": "Michael McCandless",
            "date": "2015-04-12T22:14:26+0000",
            "content": "Can you add in the javadocs of the new iterator that the order that accepted strings are returned is implementation dependent and free to change across releases?\n\nI marked Operations.getFiniteStrings() as deprecated\n\nCan we simply remove it?  The Operations methods are experimental and dangerous ... it need not live on? "
        },
        {
            "id": "comment-14530470",
            "author": "Markus Heiden",
            "date": "2015-05-06T12:51:51+0000",
            "content": "Sorry for the delay. \n\nI moved Operations.getFiniteStrings() to TestOperations.getFiniteStrings(), because using the iterator for assertions is a pain. Production code using this method has been replaced by direct usage of the new iterator. \n\nI got one problem with that:\nI am not sure if the implementation change of CompletionTokenStream is OK, because I set the position attribute at the end of the iteration instead of at the start of the iteration. The tests run fine, but someone should review that.\n\nI marked the new iterator as @lucene.experimental and added a comment, that the iteration order may change. "
        },
        {
            "id": "comment-14534633",
            "author": "Michael McCandless",
            "date": "2015-05-08T14:47:52+0000",
            "content": "Thanks Markus Heiden, new patch looks great.\n\nCan we remove the limit to FiniteStringsIterator.init?  Seems like this (\"abort iteration after N items\") should be the caller's job?\n\nCan we just pass the automaton to FSI's ctor?  I don't think we need a reuse API here...\n\nI am not sure if the implementation change of CompletionTokenStream is OK, because I set the position attribute at the end of the iteration instead of at the start of the iteration. The tests run fine, but someone should review that.\n\nIt is weird that CompletionTokenStream hijacks PositionIncrementAttribute like that, and I can't see anywhere that reads from that (and indeed tests pass if I comment it out).  Maybe Areek Zillur knows?  I think we should just remove it? "
        },
        {
            "id": "comment-14535302",
            "author": "Areek Zillur",
            "date": "2015-05-08T19:12:38+0000",
            "content": "+1 to removing PositionIncrementAttr from CompletionTokenStream. "
        },
        {
            "id": "comment-14535559",
            "author": "Markus Heiden",
            "date": "2015-05-08T21:19:11+0000",
            "content": "Removed PositionIncrementAttribute from CompletionTokenStream. "
        },
        {
            "id": "comment-14535585",
            "author": "Markus Heiden",
            "date": "2015-05-08T21:28:21+0000",
            "content": "Removing init(): One my design goals was to be able to reuse this iterator, mainly to optimize the performance of the AnalyzingSuggester. This mainly comes from avoiding the step by step allocation of the stack inside the iterator over and over again. So maybe I can provide an additional constructor by combining the default constructor with the init() method?\n\nRemoving limit support: I like the idea, but as I tried to implement the change, it seems that there are some downsides: At least the AnalyzingSuggester and the CompletionTokenStream are using this feature. Especially annoying is to change the implementation of CompletionTokenStream, because it has no for loop. Maybe I can add a second init() method without limit, to not need to pass -1 each time the limitation is not needed? "
        },
        {
            "id": "comment-14536310",
            "author": "Dawid Weiss",
            "date": "2015-05-09T09:14:47+0000",
            "content": "> mainly to optimize the performance of the AnalyzingSuggester. This mainly comes from avoiding the step by step allocation of the stack inside the iterator over and over again. \n\nI wouldn't worry about it, Markus. With a TLAB these allocations are fairly cheap. You could just add a reasonably large initial stack and that's it. If it needs to reallocate, so be it. Unless it becomes a real bottleneck in real life (which it won't, I assure you), there is no need for premature optimizations at the cost of making the code more complex. "
        },
        {
            "id": "comment-14537160",
            "author": "Markus Heiden",
            "date": "2015-05-10T12:10:38+0000",
            "content": "For the normal use case I agree. But I had problems with long build times for lookups for big dictionaries (using the AnalyzingSuggester). I profiled the creation of the lookups and one hotspot was the allocation of the internal stack. One problem is, that the initial size of the internal stack is too small (4 entries), so the internal stack gets resized over and over again. I will increase its initial size to 16. "
        },
        {
            "id": "comment-14543443",
            "author": "Michael McCandless",
            "date": "2015-05-14T09:52:55+0000",
            "content": "I would really prefer not make API compromises (reuse, init method) for such optimizations, nor for the \"limit\" case (this is really the caller's responsibility...).\n\nEspecially annoying is to change the implementation of CompletionTokenStream, because it has no for loop.\n\nIt's fine to just add a member to the class tracking how many strings it has pulled so far from the iterator...\n\nAlso, you can remove that //TODO: make this return a Iterator<IntsRef> instead? since you are doing exactly that, here... "
        },
        {
            "id": "comment-14547895",
            "author": "Markus Heiden",
            "date": "2015-05-18T11:38:34+0000",
            "content": "The Iterator interface is not possible with the current implementation, because the lookahead of hasNext() would destroy the current value provided by the previous next(). Anyway, I removed that comment.\n\nI provided now both interfaces (single use and multiple use) with the newest patch, so for the default case you can use the more intuitive one (single use). I hope you like it.\n\nI kept the limit inside the iterator, but provided the new class LimitedFiniteStringsIterator for it. Because it was too complicated to transfer the limit into the yet complex iteration in AnalyzingSuggester etc. "
        },
        {
            "id": "comment-14552643",
            "author": "Michael McCandless",
            "date": "2015-05-20T16:47:55+0000",
            "content": "The Iterator interface is not possible with the current implementation, because the lookahead of hasNext() would destroy the current value provided by the previous next(). Anyway, I removed that comment.\n\nOK it's fine if we don't implement Java's Iterator; the spirit of that TODO can still be removed \n\nI provided now both interfaces (single use and multiple use) with the newest patch, so for the default case you can use the more intuitive one (single use). I hope you like it.\n\nSorry, I really don't want to pollute such a low level API with reuse ... can you remove the init reuse method?\n\nI kept the limit inside the iterator, but provided the new class LimitedFiniteStringsIterator for it. Because it was too complicated to transfer the limit into the yet complex iteration in AnalyzingSuggester etc.\n\nOK that seems like a good compromise... "
        },
        {
            "id": "comment-14612534",
            "author": "Markus Heiden",
            "date": "2015-07-02T21:15:59+0000",
            "content": "I adapted my patch to the latest changes in trunk. \n\nI think the reuse of the iterator is one core part of this whole patch. I tried to rework the api of the iterator so that the reuse case and the no-reuse case are handled in a similar way. I hope you like it now (at least a bit). Lucene does this kind of reuse already, e.g. see Transition.\n\nFuzzyCompletionQuery has been added lately and relies on the old big set of finite strings. I am not sure how to rework it. Currently it still uses the set, maybe it is better to use the iterator inside of FuzzyCompletionWeight, but this means recomputing the finite strings over and over again. What do you think?\n\nBTW topoSortStates() is implemented by AnalyzingSuggester and CompletionTokenStream identically. Maybe it should be moved to one place, maybe to Operations?  "
        },
        {
            "id": "comment-14613274",
            "author": "Michael McCandless",
            "date": "2015-07-03T16:11:46+0000",
            "content": "I adapted my patch to the latest changes in trunk.\n\nThanks.\n\nI think the reuse of the iterator is one core part of this whole patch. I tried to rework the api of the iterator so that the reuse case and the no-reuse case are handled in a similar way. I hope you like it now (at least a bit).\n\nAlas I still don't think this is an appropriate place for object reuse.\n\nLucene does this kind of reuse already, e.g. see Transition.\n\nThat's true: Lucene does reuse objects in many low-level places, but this is ugly and cancerous and dangerous (can easily cause bugs, e.g. accidentally reusing one iterator across threads) and anti-Java, etc., and it really should be used only sparingly, and should be the exception not the rule.  I don't think this API qualifies, i.e. it's a bad tradeoff to pollute the API to eeek out a bit of GC perf gain that in real usage would be negligible because the cost of building an automaton and the cost of consuming each string that's iterated would normally dwarf the small GC cost of creating a new iterator per automaton.  APIs are hard enough to \"get right\" as it is ...\n\nFuzzyCompletionQuery has been added lately and relies on the old big set of finite strings. I am not sure how to rework it. Currently it still uses the set, maybe it is better to use the iterator inside of FuzzyCompletionWeight, but this means recomputing the finite strings over and over again. What do you think?\n\nIt's fine to leave this as the full Set<String> for now.  It's no worse   Progress not perfection...\n\nBTW topoSortStates() is implemented by AnalyzingSuggester and CompletionTokenStream identically. Maybe it should be moved to one place, maybe to Operations?\n\nWoops, I'll go move that to Operations now, good idea, thank you! "
        },
        {
            "id": "comment-14613295",
            "author": "ASF subversion and git services",
            "date": "2015-07-03T16:33:18+0000",
            "content": "Commit 1689046 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1689046 ]\n\nLUCENE-6365: add Operations.topoSort "
        },
        {
            "id": "comment-14613296",
            "author": "ASF subversion and git services",
            "date": "2015-07-03T16:34:45+0000",
            "content": "Commit 1689047 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1689047 ]\n\nLUCENE-6365: add Operations.topoSort "
        },
        {
            "id": "comment-14613341",
            "author": "Markus Heiden",
            "date": "2015-07-03T17:35:40+0000",
            "content": "I see, I am not able to convince you  So I attached a version of the patch with eliminated reuse api.\n\nI agree that reuse is no good design, but the profiler pointed me to that spot. I already did a patch for Automaton (LUCENE-5959) for the same reasons.\n\nIt would be nice, if Automaton knows the size of the longest word it produces. That would eliminated the resizing of the internal stack array inside the iterator and could convince me that reuse is not needed for the iterator. "
        },
        {
            "id": "comment-14613432",
            "author": "ASF subversion and git services",
            "date": "2015-07-03T20:03:25+0000",
            "content": "Commit 1689079 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1689079 ]\n\nLUCENE-6365: fix buggy Operations.topoSort; add test "
        },
        {
            "id": "comment-14613433",
            "author": "ASF subversion and git services",
            "date": "2015-07-03T20:04:06+0000",
            "content": "Commit 1689081 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1689081 ]\n\nLUCENE-6365: fix buggy Operations.topoSort; add test "
        },
        {
            "id": "comment-14613667",
            "author": "Michael McCandless",
            "date": "2015-07-04T10:30:48+0000",
            "content": "Thanks Markus Heiden, the patch looks good.\n\nI do still think this is a very nice change even without the reuse!\n\nWas it accidental to remove @lucene.experimental from CompletionTokenStream's javadocs?  I put that back.\n\nI also made a couple additional members of FiniteStringsIterator final, and fixed up the javadocs a bit ... new patch attached.\n\nI'll commit soon, thank you! "
        },
        {
            "id": "comment-14613686",
            "author": "Uwe Schindler",
            "date": "2015-07-04T10:54:51+0000",
            "content": "I agree that reuse is no good design, but the profiler pointed me to that spot. I already did a patch for Automaton (LUCENE-5959) for the same reasons.\n\nThe problem with profilers is: They don't run the code with all hotspot optimizations turned on (because they add extra commands into the loop) to take measurement points. In general Object creation is in most cases not bad, except when they tend to live long time (e.g., across several method). Short living objects never affect GC, because they are created on stack automatically (escape analysis) - and those are in most cases never shown correctly by profiler, because escape analysis is mostly broken with profilers.\n\nReuse is only suitable for cases where the object goes through many stages of  a query and needs to be cached between method calls and involves disk I/O,... This is not the case here.\n\nPatch without reuse looks fine, although I dont fully understand it (not my area of work). "
        },
        {
            "id": "comment-14613789",
            "author": "Markus Heiden",
            "date": "2015-07-04T12:29:46+0000",
            "content": "@Michael: The removal of @lucene.experimental was a mistake of mine during merging.Thanks for your rework and your patience.\n\n@Uwe: I measured the cpu runtime in sampling mode, so (almost) no additional overhead should occur. I did the reuse because there is not just one allocation of the array, but many. During runtime the array will be resized over and over again, because the initial size was rather small (4 entries). I changed that to 16 so the resizing occurs less frequent. My test case was the build of dictionary of 100000s of words, so even small things accumulate.\n\nA better solution to that problem would be, if automatons know the length of their longest word. In that case that above mentioned array could initially be sized right. But I don't know, if that length is always known during construction of automatons. "
        },
        {
            "id": "comment-14614029",
            "author": "ASF subversion and git services",
            "date": "2015-07-04T19:47:37+0000",
            "content": "Commit 1689192 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1689192 ]\n\nLUCENE-6365: switch to iterator API to get all finite strings from an Automaton "
        },
        {
            "id": "comment-14614034",
            "author": "ASF subversion and git services",
            "date": "2015-07-04T20:07:17+0000",
            "content": "Commit 1689193 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1689193 ]\n\nLUCENE-6365: switch to iterator API to get all finite strings from an Automaton "
        },
        {
            "id": "comment-14614036",
            "author": "Michael McCandless",
            "date": "2015-07-04T20:09:27+0000",
            "content": "Markus Heiden maybe try the honest profiler: https://github.com/RichardWarburton/honest-profiler/wiki ?\n\nI haven't tried it yet, but it apparently avoids the safepoint sampling bias ... "
        },
        {
            "id": "comment-14614263",
            "author": "Michael McCandless",
            "date": "2015-07-05T14:09:02+0000",
            "content": "Hmm this failure just appeared on Jenkins:\n\n\n   [junit4] Started J0 PID(3197@localhost).\n   [junit4] Suite: org.apache.lucene.util.automaton.FiniteStringsIteratorTest\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=FiniteStringsIteratorTest -Dtests.method=testRandomFiniteStrings1 -Dtests.seed=4A938C5F6E728DCC -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=es_CU -Dtests.timezone=America/Porto_Velho -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 0.26s | FiniteStringsIteratorTest.testRandomFiniteStrings1 <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: expected:<445> but was:<446>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([4A938C5F6E728DCC:17BF51EE97CD2662]:0)\n   [junit4]    > \tat org.apache.lucene.util.automaton.FiniteStringsIteratorTest.assertFiniteStringsRecursive(FiniteStringsIteratorTest.java:204)\n   [junit4]    > \tat org.apache.lucene.util.automaton.FiniteStringsIteratorTest.testRandomFiniteStrings1(FiniteStringsIteratorTest.java:82)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene53): {}, docValues:{}, sim=RandomSimilarityProvider(queryNorm=true,coord=no): {}, locale=es_CU, timezone=America/Porto_Velho\n   [junit4]   2> NOTE: Linux 3.13.0-46-generic amd64/Oracle Corporation 1.8.0_40 (64-bit)/cpus=8,threads=1,free=483134008,total=504889344\n   [junit4]   2> NOTE: All tests run in this JVM: [FiniteStringsIteratorTest]\n   [junit4] Completed [1/1] in 0.41s, 1 test, 1 failure <<< FAILURES!\n\n\n\nIt reproduces ... I haven't dug into it yet. "
        },
        {
            "id": "comment-14614668",
            "author": "Markus Heiden",
            "date": "2015-07-06T07:51:55+0000",
            "content": "I have executed the tests in lucene/core again and they do not fail on the state of the trunk when I created the patch. There have to be changes elsewhere which lead to this failure. "
        },
        {
            "id": "comment-14614706",
            "author": "Dawid Weiss",
            "date": "2015-07-06T08:42:49+0000",
            "content": "I don't think so. The test in question (and the seed in question) fails because FiniteStringsIterator returns the same sequence twice (\"h\"), I just compared the outputs of the recursive and non-recursive outputs.\n\nLooks like the seed in question adds the same string to the input automaton twice (\"h\") and then doesn't minimize/ determinize. getFiniteStringsRecursive makes sure no string is emitted twice, FiniteStringsIterator doesn't have this check.\n\nPerhaps we should require that the input automaton doesn't have duplicates (Mike?). Alternatively, we could add compare-to-previous to the iterator and skip duplicates. "
        },
        {
            "id": "comment-14614770",
            "author": "Markus Heiden",
            "date": "2015-07-06T09:29:44+0000",
            "content": "Sorry, I missed to use the seed.\n\nThe de-duplication has prior been implemented by the big set containing the result, which has been removed. So no de-duplication takes places. Should this be the responsibility of the iterator at all?\n "
        },
        {
            "id": "comment-14614919",
            "author": "Dawid Weiss",
            "date": "2015-07-06T11:42:13+0000",
            "content": "I think it even makes more sense to leave the code as is (without removing duplicates). It should be an assertion that no duplicates occurred in the automaton, but let Michael McCandless confirm this. "
        },
        {
            "id": "comment-14615041",
            "author": "Michael McCandless",
            "date": "2015-07-06T13:52:44+0000",
            "content": "Ahh thanks for digging Dawid Weiss and Markus Heiden.\n\nMarkus Heiden usually all that's necessary to reproduce a test is to copy/past the exact text after \"Reproduce with: ...\", in this case:\n\n\nant test  -Dtestcase=FiniteStringsIteratorTest -Dtests.method=testRandomFiniteStrings1 -Dtests.seed=4A938C5F6E728DCC -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=es_CU -Dtests.timezone=America/Porto_Velho -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\n\n\nThat usually reproduces the failure, though sometimes you'll need the exact JVM version, added JVM flags, etc.\n\nI agree it should not be this iterator's job to deal with duplicates: I think if you pass a non-minimal automaton to it, it's fair game for it to return dups ... so this is a test bug. "
        },
        {
            "id": "comment-14615048",
            "author": "Michael McCandless",
            "date": "2015-07-06T14:01:26+0000",
            "content": "I'll fix the test to de-dup itself, and add a comment in FiniteStringsIterator's javadocs about this ... "
        },
        {
            "id": "comment-14615051",
            "author": "ASF subversion and git services",
            "date": "2015-07-06T14:03:48+0000",
            "content": "Commit 1689404 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1689404 ]\n\nLUCENE-6365: fix test to not add duplicate strings "
        },
        {
            "id": "comment-14615053",
            "author": "ASF subversion and git services",
            "date": "2015-07-06T14:04:15+0000",
            "content": "Commit 1689405 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1689405 ]\n\nLUCENE-6365: fix test to not add duplicate strings "
        },
        {
            "id": "comment-14615453",
            "author": "Dawid Weiss",
            "date": "2015-07-06T18:38:10+0000",
            "content": "Thanks Mike, thanks Markus. "
        },
        {
            "id": "comment-14713343",
            "author": "Shalin Shekhar Mangar",
            "date": "2015-08-26T13:06:29+0000",
            "content": "Bulk close for 5.3.0 release "
        }
    ]
}