{
    "id": "SOLR-11423",
    "title": "Overseer queue needs a hard cap (maximum size) that clients respect",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "fix_versions": [
            "7.2",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "When Solr gets into pathological GC thrashing states, it can fill the overseer queue with literally thousands and thousands of queued state changes.  Many of these end up being duplicated up/down state updates.  Our production cluster has gotten to the 100k queued items level many times, and there's nothing useful you can do at this point except manually purge the queue in ZK.  Recently, it hit 3 million queued items, at which point our entire ZK cluster exploded.\n\nI propose a hard cap.  Any client trying to enqueue a item when a queue is full would throw an exception.  I was thinking maybe 10,000 items would be a reasonable limit.  Thoughts?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-09-29T19:36:08+0000",
            "content": "CC: Joshua Humphries Noble Paul Shalin Shekhar Mangar ",
            "author": "Scott Blum",
            "id": "comment-16186282"
        },
        {
            "date": "2017-09-30T07:01:10+0000",
            "content": "It makes sense to do a hard cap. make it a configurable cluster property with a sensible default. I guess 10K is a pretty small limit  ",
            "author": "Noble Paul",
            "id": "comment-16186952"
        },
        {
            "date": "2017-09-30T15:04:53+0000",
            "content": "What version of Solr? There has been some work done recently to reduce the number of tasks the Overseer needs to process and I'm wondering if your observations include those changes or not. ",
            "author": "Erick Erickson",
            "id": "comment-16187121"
        },
        {
            "date": "2017-10-02T20:50:37+0000",
            "content": "Erick Erickson I have backported changes to reduce overseer task counts.  This isn't an issue with normal operation.  Think of this more as an automatic safety shutoff on a nuclear reactor. ",
            "author": "Scott Blum",
            "id": "comment-16188803"
        },
        {
            "date": "2017-10-02T20:51:22+0000",
            "content": "Commit ef8e0934fb27530f0c9450b58872b2b11028f50a in lucene-solr's branch refs/heads/SOLR-11423 from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ef8e093 ]\n\nSOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect ",
            "author": "ASF subversion and git services",
            "id": "comment-16188807"
        },
        {
            "date": "2017-10-02T21:21:20+0000",
            "content": "GitHub user dragonsinth opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/256\n\n    SOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect\n\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/apache/lucene-solr SOLR-11423\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/256.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #256\n\n\ncommit ef8e0934fb27530f0c9450b58872b2b11028f50a\nAuthor: Scott Blum <dragonsinth@apache.org>\nDate:   2017-10-02T20:50:57Z\n\n    SOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-16188866"
        },
        {
            "date": "2017-10-02T21:22:44+0000",
            "content": "Noble Paul I went with 20k.  We could make it configurable, but again, I intend this to be a general purpose safety valve to protect Zookeeper from exploding, which makes me think we could pick a universal value. ",
            "author": "Scott Blum",
            "id": "comment-16188867"
        },
        {
            "date": "2017-10-02T21:37:24+0000",
            "content": "Patch passes tests for me. ",
            "author": "Scott Blum",
            "id": "comment-16188887"
        },
        {
            "date": "2017-10-02T22:16:09+0000",
            "content": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/256 ",
            "author": "ASF GitHub Bot",
            "id": "comment-16188949"
        },
        {
            "date": "2017-10-02T23:21:57+0000",
            "content": "GitHub user dragonsinth opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/257\n\n    SOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect\n\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/apache/lucene-solr jira/SOLR-11423\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/257.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #257\n\n\ncommit ef8e0934fb27530f0c9450b58872b2b11028f50a\nAuthor: Scott Blum <dragonsinth@apache.org>\nDate:   2017-10-02T20:50:57Z\n\n    SOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-16189037"
        },
        {
            "date": "2017-10-04T20:08:21+0000",
            "content": "Shalin Shekhar Mangar Noble Paul either of you want to take a look at this change and +1 or -1? ",
            "author": "Scott Blum",
            "id": "comment-16191962"
        },
        {
            "date": "2017-10-05T09:22:05+0000",
            "content": "This looks fine. I'm just worried about the extra cost of the extra cost of Stat stat = zookeeper.exists(dir, null, true); for each call.\n\nI wish we ZK had a conditional set operation . \nAnyway +1 ",
            "author": "Noble Paul",
            "id": "comment-16192660"
        },
        {
            "date": "2017-10-05T11:39:48+0000",
            "content": "Another point to consider is , is the rest of Solr code designed to handle this error? I guess, the thread should wait for a few seconds and retry if the no: of items fell below the limit. This will dramatically reduce the no:of other errors in the system. ",
            "author": "Noble Paul",
            "id": "comment-16192769"
        },
        {
            "date": "2017-10-05T18:50:09+0000",
            "content": "Noble Paul both good questions!\n\n> This looks fine. I'm just worried about the extra cost of the extra cost of Stat stat = zookeeper.exists(dir, null, true); for each call.\n\nIndeed!  That's why in my second pass, I added `offerPermits`.  As long as the queue is mostly empty, each client will only bother checking the stats about every 200 queued entries.  Agreed on a \"create sequential node if parent node has fewer then XXX entries\".\n\n> Another point to consider is , is the rest of Solr code designed to handle this error? I guess, the thread should wait for a few seconds and retry if the no: of items fell below the limit. This will dramatically reduce the no: of other errors in the system.\n\nI thought about this point quite a bit, and came down on the side of erroring immediately.  Again, I'm thinking of this mostly like an automatic emergency shutoff in a nuclear reactor: you hope you never need it.  The point being, if you're in a state where you have 20k items in the queue, you're already in a pathologically bad state.  I can't see how adding latency and hoping things get better will improve the situation vs. erroring out immediately.  I've never seen a solr cluster recover on its own once the queue got that high, it always required manual intervention. ",
            "author": "Scott Blum",
            "id": "comment-16193435"
        },
        {
            "date": "2017-10-05T18:52:58+0000",
            "content": "Commit 9419366a8e68a599efa2d8a230a0158d1763d10d in lucene-solr's branch refs/heads/master from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9419366 ]\n\nSOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect ",
            "author": "ASF subversion and git services",
            "id": "comment-16193440"
        },
        {
            "date": "2017-10-05T21:19:16+0000",
            "content": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/257 ",
            "author": "ASF GitHub Bot",
            "id": "comment-16193719"
        },
        {
            "date": "2017-10-05T22:31:37+0000",
            "content": "Waiting can help. If every node errors out immediately after the cut off the items in queue can get cleared immediately and there will be  starvation . If we wait and retry after sometime , we can ensure that the events are generated at a lower rate. It helps in cases where a GC pause caused a pile up.  ",
            "author": "Noble Paul",
            "id": "comment-16193846"
        },
        {
            "date": "2017-10-06T17:22:50+0000",
            "content": "Perhaps you're right.  In our cluster though, Overseer is not able to chew through 20k entries very fast; it takes a long time (many minutes) to get through that number of items.  Our current escape valve is a tiny separate tool that just literally just goes through the queue and deletes everything.  ",
            "author": "Scott Blum",
            "id": "comment-16194900"
        },
        {
            "date": "2017-10-13T08:28:16+0000",
            "content": "Scott Blum Should this ticket be backported to branch_7x? ",
            "author": "Cao Manh Dat",
            "id": "comment-16203192"
        },
        {
            "date": "2017-10-13T08:54:24+0000",
            "content": "there must be a system property to change the value before we can mark this as resolved ",
            "author": "Noble Paul",
            "id": "comment-16203208"
        },
        {
            "date": "2017-10-13T18:36:25+0000",
            "content": "I'm happy to defer on this issue, but I just want to be clear that I actively dislike having a system property.  It feels like a not useful piece of config, and worse, what happens if you don't set the same cap on every node?  Remember this is enforced client side, not server side.  If you accidentally have a mix of nodes where half of them cap at 20k, and half of them cap at 40k, then the moment you get above 20k any badly behaving 40k nodes are going to starve out the 20k nodes.  It becomes unfair contention. ",
            "author": "Scott Blum",
            "id": "comment-16204004"
        },
        {
            "date": "2017-10-14T23:41:47+0000",
            "content": "I understand that it's not fool proof. However, when a user has hit a limit and he has no way of changing it other than downgrading to an older solr is worse. If we are on customer support this is a nightmare ",
            "author": "Noble Paul",
            "id": "comment-16204860"
        },
        {
            "date": "2017-10-15T00:31:31+0000",
            "content": "Is it possible to pick a number related to ZK's inherent limits?  That's really the goal here, prevent Solr from destroying ZK. ",
            "author": "Scott Blum",
            "id": "comment-16204874"
        },
        {
            "date": "2017-10-17T04:12:58+0000",
            "content": "Should we modify the behavior here a little bit, instead of throw an IllegalStateException() (which can lead to many errors cause this is an unchecked exception) should we try first, if the queue is full, retry until timeout.\nScott Blum I really want to hear about your cluster status after SOLR-11443 get applied ( maybe we do not need this hard cap at all if the Overseer can process messages fast enough ) ",
            "author": "Cao Manh Dat",
            "id": "comment-16206976"
        },
        {
            "date": "2017-11-02T19:04:24+0000",
            "content": "Thanks for adding this Scott Blum. One question\n\n// Allow this client to push up to 1% of the remaining queue capacity without rechecking.\nofferPermits.set(maxQueueSize - stat.getNumChildren() / 100);\n\n\nDon't you want offerPermits.set((maxQueueSize - stat.getNumChildren()) / 100);, or maybe offerPermits.set(remainingCapacity / 100); ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16236399"
        },
        {
            "date": "2017-11-02T22:09:23+0000",
            "content": "Oh wow, nice catch.  How'd I mess that one up? ",
            "author": "Scott Blum",
            "id": "comment-16236685"
        },
        {
            "date": "2017-11-02T22:27:06+0000",
            "content": "Commit 0637407ea4bf3a49ed5bdabadcfee650a8e0a200 in lucene-solr's branch refs/heads/master from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0637407 ]\n\nSOLR-11423: fix typo ",
            "author": "ASF subversion and git services",
            "id": "comment-16236706"
        },
        {
            "date": "2017-12-07T18:45:04+0000",
            "content": "Scott Blum Is this issue good to resolve? I see commits have been pushed but it is still open? ",
            "author": "Adrien Grand",
            "id": "comment-16282305"
        },
        {
            "date": "2017-12-07T18:46:56+0000",
            "content": "The situation is quite weird as this change has been pushed to master only but has a CHANGES entry under 7.1. ",
            "author": "Adrien Grand",
            "id": "comment-16282306"
        },
        {
            "date": "2017-12-07T23:29:33+0000",
            "content": "I didn't resolve due to Noble's #comment-16203208 but I have no objection to resolving.  I dropped it on master because I wasn't sure what branches we'd want to backport to. ",
            "author": "Scott Blum",
            "id": "comment-16282711"
        },
        {
            "date": "2017-12-08T17:50:10+0000",
            "content": "In my experience, 20k state updates in the queue is beyond the point of no return too. I think we should backport as it is now, and if needed in the future we can use a cluster property to address Scott's point. Lets get this to 7.2 ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16283908"
        },
        {
            "date": "2017-12-08T17:57:00+0000",
            "content": "+1 to what Tomas suggested ",
            "author": "Varun Thacker",
            "id": "comment-16283926"
        },
        {
            "date": "2017-12-08T18:20:13+0000",
            "content": "Sounds good to me.  So backport to branch_7_2 and branch_7x? ",
            "author": "Scott Blum",
            "id": "comment-16283972"
        },
        {
            "date": "2017-12-08T18:26:13+0000",
            "content": "Sounds good to me. So backport to branch_7_2 and branch_7x?\n+1. And lets fix CHANGES.txt\nIs this OK Adrien Grand? This is not a bugfix, but as you said, it's in an odd state right now, and since it was included in the 7.1 CHANGES I feel we should correct ASAP ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16283981"
        },
        {
            "date": "2017-12-08T18:51:25+0000",
            "content": "I'm fixing solr/CHANGES.txt while cherry-picking into 7x and 7_2.  I'll push a change to master to move it when that's done.\n\nNote: the solr/CHANGES.txt that shipped with 7.1 does not erroneously report this bugfix being included, it's just master/7x/7_2 that has it wrong. ",
            "author": "Scott Blum",
            "id": "comment-16284021"
        },
        {
            "date": "2017-12-08T19:21:09+0000",
            "content": "Commit 311105f1b0dad7f20ff6cd55be1d2eb9cd4246d6 in lucene-solr's branch refs/heads/branch_7x from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=311105f ]\n\nSOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect ",
            "author": "ASF subversion and git services",
            "id": "comment-16284072"
        },
        {
            "date": "2017-12-08T19:45:53+0000",
            "content": "Commit 576b0b5d659527a61ebdd80347c6fa14dc0a086f in lucene-solr's branch refs/heads/branch_7_2 from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=576b0b5 ]\n\nSOLR-11423: Overseer queue needs a hard cap (maximum size) that clients respect ",
            "author": "ASF subversion and git services",
            "id": "comment-16284118"
        },
        {
            "date": "2017-12-08T19:46:46+0000",
            "content": "Commit 3a7f1071644ffe11ee74c96cfd4946204b6544b5 in lucene-solr's branch refs/heads/master from Scott Blum\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a7f107 ]\n\nSOLR-11423: fix solr/CHANGES.txt, fixed in 7.2 not 7.1 ",
            "author": "ASF subversion and git services",
            "id": "comment-16284120"
        },
        {
            "date": "2017-12-09T19:57:49+0000",
            "content": "Sorry for the late response\n+1 ",
            "author": "Noble Paul",
            "id": "comment-16284975"
        }
    ]
}