{
    "id": "LUCENE-2407",
    "title": "make CharTokenizer.MAX_WORD_LEN parametrizable",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "3.0.1",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "as discussed here http://n3.nabble.com/are-long-words-split-into-up-to-256-long-tokens-tp739914p739914.html it would be nice to be able to parametrize that value.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-21T14:38:09+0000",
            "content": "This is also a problem for some asian languaes. If ThaiAnalyzer would use CharTokenizer, very long passages could get lost, as ThatWordFilter would not get the complete string (thai is not tokenized by the tokenizer, but later in the filter)\n\nThis also applies to StandardTokenizer, maybe we should set a good default when analyzing Thai text (ThaiAnalyzer should init StandardTokenizer with a large/infinite value). ",
            "author": "Uwe Schindler",
            "id": "comment-12859373"
        },
        {
            "date": "2013-07-23T18:44:52+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717085"
        },
        {
            "date": "2014-04-16T12:54:43+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970857"
        }
    ]
}