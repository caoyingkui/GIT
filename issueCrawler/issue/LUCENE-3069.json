{
    "id": "LUCENE-3069",
    "title": "Lucene should have an entirely memory resident term dictionary",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index",
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.7"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.",
    "attachments": {
        "LUCENE-3069.patch": "https://issues.apache.org/jira/secure/attachment/12591969/LUCENE-3069.patch",
        "df-ttf-estimate.txt": "https://issues.apache.org/jira/secure/attachment/12592153/df-ttf-estimate.txt",
        "example.png": "https://issues.apache.org/jira/secure/attachment/12591970/example.png"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-06-17T11:53:40+0000",
            "content": "LUCENE-3209 is a new codec that puts everything (terms + postings) in RAM.  For this issue I think we should make this controllable, ie so terms can be in RAM but postings remain in the Directory. ",
            "author": "Michael McCandless",
            "id": "comment-13051003"
        },
        {
            "date": "2013-03-30T04:50:21+0000",
            "content": "I'd love to see this come to pass.  I've been thinking about what goes on a layer beneath TermsEnum (i.e. how it is implemented) as I work on spatial stuff. Geohash prefixes are a natural fit for FSTs; it should compress ridiculously well.  There is an approach to building a heatmap (spatial grid faceting) that I'm thinking of that would do 2500 seek()'s for a 50x50 grid; I'd like those seek's to be as fast as possible.  I have another approach in mind requiring a slightly different encoding, but it would do 2500 next()'s which should be faster.  Nonetheless; it's a lot \u2013 ideally the terms dict would be entirely memory resident. ",
            "author": "David Smiley",
            "id": "comment-13617982"
        },
        {
            "date": "2013-04-07T13:52:39+0000",
            "content": "This project is quite interesting!\n\nSince we already have an entirely memory resident PF, the target of this project seems to be as below:\n1. implement a simplified version of BlockTreeTerms*;\n2. change the API of current PostingsBastFormat, so that some non-block-based term dic will be possible to plug in it.(ideally, MemoryPF should work with this) ",
            "author": "Han Jiang",
            "id": "comment-13624919"
        },
        {
            "date": "2013-04-26T15:55:10+0000",
            "content": "This is my inital proposal for this project: https://google-melange.appspot.com/gsoc/proposal/review/google/gsoc2013/billybob/34001\nI'm looking forward to your feedbacks.  ",
            "author": "Han Jiang",
            "id": "comment-13642971"
        },
        {
            "date": "2013-04-27T10:11:01+0000",
            "content": "Han would like to tackle this for GSoC 2013... ",
            "author": "Michael McCandless",
            "id": "comment-13643620"
        },
        {
            "date": "2013-06-01T17:22:54+0000",
            "content": "the detail ideas/wild thoughts will be put here: https://gist.github.com/sleepsort/5642021 ",
            "author": "Han Jiang",
            "id": "comment-13672167"
        },
        {
            "date": "2013-06-16T10:49:40+0000",
            "content": "[lucene3069 commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1493493\n\nLUCENE-3069: merge trunk changes over ",
            "author": "Commit Tag Bot",
            "id": "comment-13684616"
        },
        {
            "date": "2013-06-16T14:40:02+0000",
            "content": "[lucene3069 commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1493516\n\nLUCENE-3069: add nocommit/TODO ",
            "author": "Commit Tag Bot",
            "id": "comment-13684674"
        },
        {
            "date": "2013-06-16T14:44:32+0000",
            "content": "[lucene3069 commit] han\nhttp://svn.apache.org/viewvc?view=revision&revision=1493517\n\nLUCENE-3069: setField now expose per-field info to term dict ",
            "author": "Commit Tag Bot",
            "id": "comment-13684676"
        },
        {
            "date": "2013-07-04T13:13:56+0000",
            "content": "Commit 1499744 from Han Jiang\n[ https://svn.apache.org/r1499744 ]\n\nLUCENE-3069: writer part ",
            "author": "ASF subversion and git services",
            "id": "comment-13700059"
        },
        {
            "date": "2013-07-08T16:08:33+0000",
            "content": "Commit 1500814 from Han Jiang\n[ https://svn.apache.org/r1500814 ]\n\nLUCENE-3069: reader part, update logic in outputs ",
            "author": "ASF subversion and git services",
            "id": "comment-13702086"
        },
        {
            "date": "2013-07-10T15:56:09+0000",
            "content": "Commit 1501811 from Han Jiang\n[ https://svn.apache.org/r1501811 ]\n\nLUCENE-3069: use more compact outputs i/o ",
            "author": "ASF subversion and git services",
            "id": "comment-13704677"
        },
        {
            "date": "2013-07-11T08:35:36+0000",
            "content": "Commit 1502152 from Han Jiang\n[ https://svn.apache.org/r1502152 ]\n\nLUCENE-3069: steal bit to encode TTF ",
            "author": "ASF subversion and git services",
            "id": "comment-13705604"
        },
        {
            "date": "2013-07-12T06:44:36+0000",
            "content": "Uploaded patch, it is the main part of changes I commited to branch3069.\n\nThe picture shows current impl of outputs (it is fetched from one field in wikimedium5k).\n\n\n\tlong[] (sortable metadata)\n\tbyte[] (unsortable, generic metadata)\n\tdf, ttf (term stats)\n\n\n\nA single byte flag is used to indicate whether/which fields current outputs maintains, \nfor PBF with short byte[], this should be enough. Also, for long-tail terms, the totalTermFreq\nan safely be inlined into docFreq (for body field in wikimedium1m, 85.8% terms have df == ttf).\n\n\nSince TermsEnum is totally based on FSTEnum, the performance of term dict should be similar with \nMemoryPF. However, for PK tasks, we have to pull docsEnum from MMap, so this hurts.\n\n\nFollowing is the performance comparison:\n\n\npure TempFST vs. Lucene41 + Memory(on idField), on wikimediumall\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 Respell       48.13      (4.4%)       15.38      (1.0%)  -68.0% ( -70% -  -65%)\n                  Fuzzy2       51.30      (5.3%)       17.47      (1.3%)  -65.9% ( -68% -  -62%)\n                  Fuzzy1       52.24      (4.0%)       18.50      (1.2%)  -64.6% ( -67% -  -61%)\n                Wildcard        9.31      (1.7%)        6.16      (2.2%)  -33.8% ( -37% -  -30%)\n                 Prefix3       23.25      (1.8%)       19.00      (2.2%)  -18.3% ( -21% -  -14%)\n                PKLookup      244.92      (3.6%)      225.42      (2.3%)   -8.0% ( -13% -   -2%)\n                 LowTerm      295.88      (5.5%)      293.27      (4.8%)   -0.9% ( -10% -    9%)\n              HighPhrase       13.62      (6.5%)       13.54      (7.4%)   -0.6% ( -13% -   14%)\n                 MedTerm       99.51      (7.8%)       99.19      (7.7%)   -0.3% ( -14% -   16%)\n               MedPhrase      154.63      (9.4%)      154.38     (10.1%)   -0.2% ( -17% -   21%)\n                HighTerm       28.25     (10.7%)       28.25     (10.0%)   -0.0% ( -18% -   23%)\n              OrHighHigh       16.83     (13.3%)       16.86     (13.1%)    0.2% ( -23% -   30%)\n        HighSloppyPhrase        9.02      (4.4%)        9.03      (4.5%)    0.2% (  -8% -    9%)\n               LowPhrase        6.26      (3.4%)        6.27      (4.1%)    0.2% (  -7% -    8%)\n               OrHighMed       13.73     (13.2%)       13.77     (12.8%)    0.3% ( -22% -   30%)\n               OrHighLow       25.65     (13.2%)       25.73     (13.0%)    0.3% ( -22% -   30%)\n         MedSloppyPhrase        6.63      (2.7%)        6.66      (2.7%)    0.5% (  -4% -    6%)\n              AndHighMed       42.77      (1.8%)       43.13      (1.5%)    0.8% (  -2% -    4%)\n         LowSloppyPhrase       32.68      (3.0%)       32.96      (2.8%)    0.8% (  -4% -    6%)\n             AndHighHigh       22.90      (1.2%)       23.18      (0.7%)    1.2% (   0% -    3%)\n             LowSpanNear       29.30      (2.0%)       29.83      (2.2%)    1.8% (  -2% -    6%)\n             MedSpanNear        8.39      (2.7%)        8.56      (2.9%)    2.0% (  -3% -    7%)\n                  IntNRQ        3.12      (1.9%)        3.18      (6.7%)    2.1% (  -6% -   10%)\n              AndHighLow      507.01      (2.4%)      522.10      (2.8%)    3.0% (  -2% -    8%)\n            HighSpanNear        5.43      (1.8%)        5.60      (2.6%)    3.1% (  -1% -    7%)\n\n\n\n\n\npure TempFST vs. pure Lucene41, on wikimediumall\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 Respell       49.24      (2.7%)       15.51      (1.0%)  -68.5% ( -70% -  -66%)\n                  Fuzzy2       52.01      (4.8%)       17.61      (1.4%)  -66.1% ( -68% -  -63%)\n                  Fuzzy1       53.00      (4.0%)       18.62      (1.3%)  -64.9% ( -67% -  -62%)\n                Wildcard        9.37      (1.3%)        6.15      (2.1%)  -34.4% ( -37% -  -31%)\n                 Prefix3       23.36      (0.8%)       18.96      (2.1%)  -18.8% ( -21% -  -16%)\n               MedPhrase      155.86      (9.8%)      152.34      (9.7%)   -2.3% ( -19% -   19%)\n               LowPhrase        6.33      (3.7%)        6.23      (4.0%)   -1.6% (  -8% -    6%)\n              HighPhrase       13.68      (7.2%)       13.49      (6.8%)   -1.4% ( -14% -   13%)\n               OrHighMed       13.78     (13.0%)       13.68     (12.7%)   -0.8% ( -23% -   28%)\n        HighSloppyPhrase        9.14      (5.2%)        9.07      (3.7%)   -0.7% (  -9% -    8%)\n              OrHighHigh       16.87     (13.3%)       16.76     (12.9%)   -0.6% ( -23% -   29%)\n               OrHighLow       25.71     (13.1%)       25.58     (12.8%)   -0.5% ( -23% -   29%)\n         MedSloppyPhrase        6.69      (2.7%)        6.67      (2.4%)   -0.3% (  -5% -    4%)\n         LowSloppyPhrase       33.01      (3.2%)       32.99      (2.6%)   -0.1% (  -5% -    5%)\n                 MedTerm       99.64      (8.0%)       99.67     (10.9%)    0.0% ( -17% -   20%)\n                 LowTerm      294.52      (5.5%)      295.72      (7.2%)    0.4% ( -11% -   13%)\n             LowSpanNear       29.61      (2.6%)       29.76      (2.7%)    0.5% (  -4% -    5%)\n                  IntNRQ        3.13      (1.8%)        3.16      (7.8%)    0.8% (  -8% -   10%)\n             MedSpanNear        8.49      (3.0%)        8.57      (3.4%)    0.9% (  -5% -    7%)\n              AndHighMed       42.86      (1.4%)       43.35      (1.4%)    1.1% (  -1% -    3%)\n             AndHighHigh       22.98      (0.6%)       23.26      (0.5%)    1.2% (   0% -    2%)\n            HighSpanNear        5.51      (3.4%)        5.58      (3.4%)    1.3% (  -5% -    8%)\n                HighTerm       28.32     (10.5%)       28.76     (15.0%)    1.6% ( -21% -   30%)\n              AndHighLow      509.60      (2.2%)      526.17      (1.9%)    3.3% (   0% -    7%)\n                PKLookup      156.59      (2.2%)      225.47      (2.8%)   44.0% (  38% -   50%)\n\n\n\nTo revive the performance on automaton queries, intersect methods should be implemented.\n\nAnd index size comparison:\n(actually, after LUCENE-5029, TempBlock has a little larger (5%) index size than Lucene41)\n\n\n\n          wikimedium1m         wikimediumall\nMemory       2,212,352            /\nLucene41       448,164            12,104,520        \nTempFST        525,888            12,770,700\n\n\n\nas for term dict size:\n\n\n\n                     wikimedium1m         wikimediumall\nLucene41(.tim+.tip)  157776               2059744\nTempFST(.tmp)        233636               2779784\n                     48%                  35%\n\n\n\nSome unresolved problems: \n\n-* Currently, TempFST uses the default option to build FST (i.e. doPacked = false), when this option is switched on, the index size on wikimedium1m becomes smaller, but on wikimediumall it becomes larger?- ",
            "author": "Han Jiang",
            "id": "comment-13706703"
        },
        {
            "date": "2013-07-13T01:58:50+0000",
            "content": "\nAlso, for long-tail terms, the totalTermFreq\nan safely be inlined into docFreq (for body field in wikimedium1m, 85.8% terms have df == ttf).\n\nCool idea! I wonder how many of those are df == ttf == 1?\n\nWe would currently waste a byte in this case (because we write a vInt for docFreq of 1, and then a vInt of totalTermFreq - docFreq of 0).\n\nMaybe we could try writing a vInt of 0 for docFreq to indicate that both docFreq and totalTermFreq are 1? ",
            "author": "Robert Muir",
            "id": "comment-13707634"
        },
        {
            "date": "2013-07-13T03:44:02+0000",
            "content": "Cool idea! I wonder how many of those are df == ttf == 1?\n\nI didn't try a very precise estimation, but the percentage will be large:\n\nFor the index of wikimedium1m, the larget segment has a 'body' field with:\n\n\nbitwidth/7  df==ttf   df\n1           1324400 / 1542987\n2           110     / 18951\n3           0       / 175\n4           0       / 0\n5           0       / 0\n\n\n\nThat is where 85.8% comes. 'bitwidth/7' means the 'ceil(bitwidth of df / 7)' since we're using VInt encoding. \nSo, for this field, we can save (1324400+110*2) bytes by stealing one bit.\n\nMaybe we could try writing a vInt of 0 for docFreq to indicate that both docFreq and totalTermFreq are 1?\n\nYes, that may helps! I'll try to test the percentage. But still we should note that, df is a small part in term dict data. ",
            "author": "Han Jiang",
            "id": "comment-13707649"
        },
        {
            "date": "2013-07-13T10:01:06+0000",
            "content": "I did a checkindex on wikimediumall.trunk.Lucene41.nd33.3326M:\n\nHere is the bit width summary for \"body\" field:\n\n\n\n\n\nbit\n#(df==ttf)\n#df\n#ttf\n\n\n 1\n 43532656 \n 48860170\n 43532656\n\n\n 2\n 10328824 \n 13979539\n 16200377\n\n\n 3\n 2682453 \n 5032450\n 6532755\n\n\n 4\n 836109 \n 2471794\n 3134437\n\n\n 5\n 262696 \n 1324704\n 1718862\n\n\n 6\n 86487 \n 755797\n 990563\n\n\n 7\n 29276 \n 442974\n 571996\n\n\n 8\n 11257 \n 263874\n 339382\n\n\n 9\n 4627 \n 161402\n 205662\n\n\n10\n 2060 \n 102198\n 128034\n\n\n11\n 979 \n 63955\n 79531\n\n\n12\n 386 \n 39377\n 48805\n\n\n13\n 170 \n 24321\n 30113\n\n\n14\n 65 \n 14686\n 18437\n\n\n15\n 10 \n 9055\n 10918\n\n\n16\n 2 \n 5229\n 6821\n\n\n17\n 0 \n 2669\n 3595\n\n\n18\n 0 \n 1312\n 1897\n\n\n19\n 0 \n 696\n 914\n\n\n20\n 0 \n 209\n 509\n\n\n21\n 0 \n 44\n 148\n\n\n22\n 0 \n 4\n 38\n\n\n23\n 0 \n 0\n 8\n\n\n24\n 0 \n 0\n 1\n\n\n...\n0\n0\n0\n\n\ntot\n57778057\n73556459\n73556459\n\n\n\n\n\nSo we have 66.4% docFreq with df==1, and 78.5% with df==ttf.\nUsing following estimation, the old size for (df+ttf) here is 148.7MB.\n\nWhen we steal one bit to mark whether df==ttf, it is reduced to 91.38MB.\nWhen we use df==0 to mark df==ttf==1, wow, it is reduced to 70.31MB, thanks Robert!\n\n\nold_size = col[2] * vIntByteSize(rownumber)   + col[3] * vIntByteSize(rownumber)\nnew_size = col[2] * vIntByteSize(rownumber+1) + (col[3] - col[1]) * vIntByteSize(rownumber)\nopt_size = col[2] * vIntByteSize(rownumber) + (rownumber == 1) ? 0 : col[3] * vIntByteSize(rownumber)\n\n\n\n\nBy the way, I am quite lured to omit frq blocks in Luene41PostingsReader.\nWhen we know that df==ttf, we can always make sure the in-doc frq==1. So for example, \nwhen bit width ranges from 2 to 8(inclusive), since df is not large enough to create ForBlocks, \nwe have to VInt encode each in-doc freq. For this 'body' field, I think the index size we can reduce is about 67.5MB \n(here I only consider vInt block, since 1-bit ForBlock is usually small) (ah I forgot we already steals bit for this case in Lucene41PBF.\n\nI'll test this later. ",
            "author": "Han Jiang",
            "id": "comment-13707709"
        },
        {
            "date": "2013-07-13T16:45:19+0000",
            "content": "Uploaded detail data for wikimediumall.\n\nOh, sorry, there is an error when I \ncaculated index size for df==0 trick, \nit should be 105MB instead of 70MB.\n\nBut the real test is still beyond \nestimation (weird...). df==0 tricks\ngains similar compression.\n\nIndex size are below(KB):\n\nv0:                   13195304\nv1 = v0 + flag byte:  12847172\nv2 = v1 + steal bit:  12770700\nv3 = v1 + zero df:    12780884\n\n\n\nAnother thing that surprised me is, with the same code/conf, \nluceneutil creates different sizes of index? I tested \nthat df==0 trick several times on wikimedium1m, the \nindex size varies from 514M~522M... Will multi-threading affects\nmuch here? ",
            "author": "Han Jiang",
            "id": "comment-13707780"
        },
        {
            "date": "2013-07-14T16:31:01+0000",
            "content": "Commit 1502991 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1502991 ]\n\nLUCENE-3069: remove redundant info for fields without payload ",
            "author": "ASF subversion and git services",
            "id": "comment-13708065"
        },
        {
            "date": "2013-07-15T12:54:31+0000",
            "content": "\nAnother thing that surprised me is, with the same code/conf, \nluceneutil creates different sizes of index? I tested \nthat df==0 trick several times on wikimedium1m, the \nindex size varies from 514M~522M... Will multi-threading affects\nmuch here?\n\nUsing threads means the docs are assigned to different segments each time you run ... it's interesting this can cause such variance in the index size though.\n\nIt is known that e.g. sorting docs by web site (if you are indexing content from different sites) can give good compression; maybe that's the effect we're seeing here? ",
            "author": "Michael McCandless",
            "id": "comment-13708428"
        },
        {
            "date": "2013-07-15T13:52:41+0000",
            "content": "The new code on the branch looks great!  I can't wait to see perf results after we\nimplement .intersect()..\n\nSome small stuff in TempFSTTermsReader.java:\n\n\n\tIn next(), when we handle seekPending=true, I think we should assert\n    that the seekCeil returned SeekStatus.FOUND?  Ie, it's not\n    possible to seekExact(TermState) to a term that doesn't exist.\n\n\n\n\n\tuseCache is an ancient option from back when we had a terms dict\n    cache; we long ago removed it ... I think we should remove\n    useCache parameter too?\n\n\n\n\n\tIt's silly that fstEnum.seekCeil doesn't return a status, ie that\n    we must re-compare the term we got to differentiate FOUND vs\n    NOT_FOUND ... so we lose some perf here.  But this is just a\n    future TODO ...\n\n\n\n\n\t\"nocommit: this method doesn't act as 'seekExact' right?\" \u2013 not\n    sure why this is here; seekExact is working as it should I think.\n\n\n\n\n\tMaybe instead of term and meta members, we could just hold the\n    current pair?\n\n\n\nIn TempTermOutputs.java:\n\n\n\tlongsSize, hasPos can be final?  (Same with TempMetaData's fields)\n\n\n\n\n\tTempMetaData.hashCode() doesn't mix in docFreq/tTF?\n\n\n\n\n\tIt doesn't impl equals (must it really impl hashCode?)\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13708465"
        },
        {
            "date": "2013-07-15T14:14:55+0000",
            "content": "I think we should assert that the seekCeil returned SeekStatus.FOUND?\n\nOk! I'll commit that.\n\nuseCache is an ancient option from back when we had a terms dict cache\n\nYes, I suppose is is not 'clear' to have this parameter.\n\nseekExact is working as it should I think.\n\nCurrently, I think those 'seek' methods are supposed to change the enum pointer based on\ninput term string, and fetch related metadata from term dict. \n\nHowever, seekExact(BytesRef, TermsState) simply 'copy' the value of termState to enum, which \ndoesn't actually operate 'seek' on dictionary. \n\nMaybe instead of term and meta members, we could just hold the current pair?\n\nOh, yes, I once thought about this, but not sure: like, can the callee always makes sure that,\nwhen 'term()' is called, it will always return a valid term?\nThe codes in MemoryPF just return 'pair.output' regardless whether pair==null, is it safe?\n\nTempMetaData.hashCode() doesn't mix in docFreq/tTF?\n\nOops! thanks, nice catch!\n\nIt doesn't impl equals (must it really impl hashCode?)\n\nHmm, do we need equals? Also, NodeHash relys on hashCode to judge whether two fst nodes can be 'merged'.\nOops, I forgot it still relys on equals to make sure two instance really matches, ok, I'll add that.\n\nBy the way, for real data, when two outputs are not 'NO_OUTPUT', even they contains the same metadata + stats, \nit seems to be very seldom that their arcs can be identical on FST (increases less than 1MB for wikimedium1m if \nequals always return false for non-singleton argument). Therefore... yes, hashCode() isn't necessary here. ",
            "author": "Han Jiang",
            "id": "comment-13708486"
        },
        {
            "date": "2013-07-15T17:03:59+0000",
            "content": "Patch according to previous comments.\n\nWe still somewhat need the existance of\nhashCode(), because in NodeHash, it will \ncheck whether the frozen node have the same \nhashcode with uncompiled node (NodeHash.java:128).\n\nAlthough later, for nodes with outputs, it'll hardly \nfind a same node from hashtable. ",
            "author": "Han Jiang",
            "id": "comment-13708638"
        },
        {
            "date": "2013-07-16T14:18:35+0000",
            "content": "Patch: revert hashCode() ",
            "author": "Han Jiang",
            "id": "comment-13709799"
        },
        {
            "date": "2013-07-16T15:55:19+0000",
            "content": "However, seekExact(BytesRef, TermsState) simply 'copy' the value of termState to enum, which doesn't actually operate 'seek' on dictionary.\n\nThis is normal / by design.  It's so that the case of seekExact(TermState) followed by .docs or .docsAndPositions is fast.  We only need to re-load the metadata if the caller then tries to do .next()\n\n\nMaybe instead of term and meta members, we could just hold the current pair?\n\nOh, yes, I once thought about this, but not sure: like, can the callee always makes sure that,\nwhen 'term()' is called, it will always return a valid term?\nThe codes in MemoryPF just return 'pair.output' regardless whether pair==null, is it safe?\n\nWe can't guarantee that, but I think we can just check if pair == null and return null from term()?\n\n\nBy the way, for real data, when two outputs are not 'NO_OUTPUT', even they contains the same metadata + stats, \nit seems to be very seldom that their arcs can be identical on FST (increases less than 1MB for wikimedium1m if \nequals always return false for non-singleton argument). Therefore... yes, hashCode() isn't necessary here.\nHmm, but it seems like we should implement it?  Ie we do get a smaller FST when implementing it? ",
            "author": "Michael McCandless",
            "id": "comment-13709867"
        },
        {
            "date": "2013-07-16T16:18:05+0000",
            "content": "Commit 1503781 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1503781 ]\n\nLUCENE-3069: remove some nocommits, update hashCode() & equal() ",
            "author": "ASF subversion and git services",
            "id": "comment-13709886"
        },
        {
            "date": "2013-07-16T17:00:14+0000",
            "content": "Commit 1503797 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1503797 ]\n\nLUCENE-3069: merge trunk changes over ",
            "author": "ASF subversion and git services",
            "id": "comment-13709928"
        },
        {
            "date": "2013-07-23T09:48:58+0000",
            "content": "Upload patch: implemented IntersectEnum.next() & seekCeil()\nlots of nocommits, but passed all tests\n\nThe main idea is to run a DFS on FST, and backtrack as early as\npossible (i.e. when we see this label is rejected by automaton)\n\nFor this version, there is one explicit perf overhead: I use a \nreal stack here, which can be replaced by a Frame[] to reuse objects.\n\nThere're several aspects I didn't dig deep: \n\n\n\tcurrently, CompiledAutomaton provides a commonSuffixRef, but how\n  can we make use of it in FST?\n\tthe DFS is somewhat a 'goto' version, i.e, we can make the code\n  cleaner with a single while-loop similar to BFS search. \n  However, since FST doesn't always tell us how may arcs are leaving \n  current arc, we have problem dealing with this...\n\twhen FST is large enough, the next() operation will takes much time\n  doing the linear arc read, maybe we should make use of \n  CompiledAutomaton.sortedTransition[] when leaving arcs are heavy.\n\n ",
            "author": "Han Jiang",
            "id": "comment-13716269"
        },
        {
            "date": "2013-07-23T16:45:06+0000",
            "content": "Patch looks great!  Wonderful how you were able to share some code in\nBaseTermsEnum...\n\nIt looks like you impl'd seekCeil in general for the IntersectEnum?  Wild \n\nYou should not need to .getPosition / .setPosition on the fstReader:\nthe FST APIs do this under-the-hood.\n\ncurrently, CompiledAutomaton provides a commonSuffixRef, but how can we make use of it in FST?\n\nI think we can't really make use of it, which is fine (it's an\noptional optimization).\n\n\nwhen FST is large enough, the next() operation will takes much time\ndoing the linear arc read, maybe we should make use of \nCompiledAutomaton.sortedTransition[] when leaving arcs are heavy.\n\nInteresting ... you mean e.g. if the Automaton is very restrictive\ncompared to the FST, then we can do a binary search.  But this can\nonly be done if that FST node's arcs are array'd right?\n\nSeparately, supporting ord w/ FST terms dict should in theory be not\nso hard; you'd need to use getByOutput to seek by ord.  Maybe (later,\neventually) we can make this a write-time option.  We should open a\nseparate issue ... ",
            "author": "Michael McCandless",
            "id": "comment-13716577"
        },
        {
            "date": "2013-07-23T18:44:20+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716917"
        },
        {
            "date": "2013-07-24T02:39:22+0000",
            "content": "You should not need to .getPosition / .setPosition on the fstReader:\n\nOh, yes! I'll fix.\n\nI think we can't really make use of it, which is fine (it's an optional optimization).\n\nOK, actually I was quite curious why we don't make use of commonPrefixRef \nin CompiledAutomaton. Maybe we can determinize the input Automaton first, then\nget commonPrefixRef via SpecialOperation? Is it too slow, or the prefix isn't\nalways long enough to take into consideration?\n\nBut this can only be done if that FST node's arcs are array'd right?\n\nYes, array arcs only, and we might need methods like advance(label) to do the search,\nand here gossip search might work better than traditional binary search.\n\n\nSeparately, supporting ord w/ FST terms dict should in theory be not\nso hard; you'd need to use getByOutput to seek by ord. Maybe (later,\neventually) we can make this a write-time option. We should open a\nseparate issue ...\n\nAh, yes, but seems that getByOutput doesn't rewind/reuse previous state?\nWe always have to start from first arc during every seek. However, I'm \nnot sure in what kinds of usecase we need the ord information.\n\n\nI'll commit current version first, so we can iterate. ",
            "author": "Han Jiang",
            "id": "comment-13717911"
        },
        {
            "date": "2013-07-24T02:40:34+0000",
            "content": "Commit 1506385 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1506385 ]\n\nLUCENE-3069: support intersect operations ",
            "author": "ASF subversion and git services",
            "id": "comment-13717912"
        },
        {
            "date": "2013-07-24T03:22:54+0000",
            "content": "Commit 1506389 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1506389 ]\n\nLUCENE-3069: no need to reseek FSTReader, update nocommits ",
            "author": "ASF subversion and git services",
            "id": "comment-13717926"
        },
        {
            "date": "2013-07-24T08:19:07+0000",
            "content": "Commit 1506439 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1506439 ]\n\nLUCENE-3069: stack reuses objects during DFS ",
            "author": "ASF subversion and git services",
            "id": "comment-13718114"
        },
        {
            "date": "2013-07-24T16:15:02+0000",
            "content": "Commit 1506612 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1506612 ]\n\nLUCENE-3069: accumulate metadata lazily ",
            "author": "ASF subversion and git services",
            "id": "comment-13718526"
        },
        {
            "date": "2013-07-30T16:31:32+0000",
            "content": "Previous design put much stress on decoding of Outputs. \nThis becomes disaster for wildcard queries: like for f*nd, \nwe usually have to walk to the last character of FST, then\nfind that it is not 'd' and automaton doesn't accept this.\nIn this case, TempFST is actually iterating all the result\nof f*, which decodes all the metadata for them...\n\nSo I'm trying another approach, the main idea is to load \nmetadata & stats as lazily as possible. \nHere I use FST<Long> as term index, and leave all other stuff \nin a single term block. The term index FST holds the relationship \nbetween <Term, Ord>, and in the term block we can maintain a skip list\nfor find related metadata & stats.\n\nIt is a little similar to BTTR now, and we can someday control how much\ndata to keep memory resident (e.g. keep stats in memory but metadata on \ndisk, however this should be another issue).\nAnother good part is, it naturally supports seek by ord.(ah, \nactually I don't understand where it is used).\n\nTests pass, and intersect is not implemented yet.\nperf based on 1M wiki data, between non-intersect TempFST and TempFSTOrd:\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                PKLookup      373.80      (0.0%)      320.30      (0.0%)  -14.3% ( -14% -  -14%)\n                  Fuzzy1       43.82      (0.0%)       47.10      (0.0%)    7.5% (   7% -    7%)\n                 Prefix3      399.62      (0.0%)      433.95      (0.0%)    8.6% (   8% -    8%)\n                  Fuzzy2       14.26      (0.0%)       15.95      (0.0%)   11.9% (  11% -   11%)\n                 Respell       40.69      (0.0%)       46.29      (0.0%)   13.8% (  13% -   13%)\n                Wildcard       83.44      (0.0%)       96.54      (0.0%)   15.7% (  15% -   15%)\n\n\n\nperf hit on pklookup should be sane, since I haven't optimize the skip list.\n\nI'll update intersect() later, and later we'll cutover to \nPagedBytes & PackedLongBuffer. ",
            "author": "Han Jiang",
            "id": "comment-13724024"
        },
        {
            "date": "2013-07-30T18:50:56+0000",
            "content": "Wow, those are nice perf results, without implementing intersect!\n\nIntersect really is an optional operation, so we could stop here/now and button everything up \n\nI like this approach: you moved all the metadata (docFreq, totalTermFreq, long[] and byte[] from the PostingsFormatBase) into blocks, and then when we really need a term's metadata we go to its block and scan for it (like block tree).\n\nI wonder if we could use MonotonicAppendingLongBuffer instead of long[] for the in-memory skip data?  Right now it's I think 48 bytes per block (block = 128 terms), so I guess that's fairly small (.375 bytes per term).\n\n\nIt is a little similar to BTTR now, and we can someday control how much\ndata to keep memory resident (e.g. keep stats in memory but metadata on \ndisk, however this should be another issue).\nThat's a nice (future) plus; this way the app can keep \"only\" the terms+ords in RAM, and leave all term metadata on disk.  But this is definitely optional for the project and we should separately explore it ...\n\n\nAnother good part is, it naturally supports seek by ord.(ah, \nactually I don't understand where it is used).\n\nThis is also a nice side-effect! ",
            "author": "Michael McCandless",
            "id": "comment-13724253"
        },
        {
            "date": "2013-07-30T21:24:14+0000",
            "content": "Nice work!  The spatial prefix trees will have even more awesome performance with all terms in RAM.  It'd be nice if I could configure the docFreq to be memory resident but, as Mike said, adding options like that can be explored later. ",
            "author": "David Smiley",
            "id": "comment-13724450"
        },
        {
            "date": "2013-07-31T01:42:25+0000",
            "content": "Commit 1508705 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1508705 ]\n\nLUCENE-3069: add TempFSTOrd, with FST index + specialized block ",
            "author": "ASF subversion and git services",
            "id": "comment-13724736"
        },
        {
            "date": "2013-07-31T03:19:58+0000",
            "content": "Patch, revive IntersectTermsEnum in TempFSTOrd.\n\nMike, since we already have an intersect() impl, maybe we can still keep this? By the way, it is easy to migrate from TempFST to TempFSTOrd. ",
            "author": "Han Jiang",
            "id": "comment-13724808"
        },
        {
            "date": "2013-07-31T07:12:26+0000",
            "content": "Performance result after last patch(intersect) is applied.\n\nOn wiki 33M data, between TempFST(with intersect) and TempFSTOrd(with intersect):\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                PKLookup      232.47      (1.0%)      205.28      (2.0%)  -11.7% ( -14% -   -8%)\n                 Prefix3       26.93      (1.2%)       28.40      (1.4%)    5.5% (   2% -    8%)\n                Wildcard        6.75      (2.1%)        7.37      (1.5%)    9.2% (   5% -   13%)\n                  Fuzzy1       29.86      (1.8%)       51.87      (3.7%)   73.7% (  67% -   80%)\n                  Fuzzy2       30.82      (1.6%)       53.82      (2.7%)   74.7% (  69% -   80%)\n                 Respell       27.30      (1.2%)       49.55      (2.6%)   81.5% (  76% -   86%)\n\n\n\nSo the decoding of outputs is really the main hurt.\n\nAnd now we should start to compare it with trunk (base=Lucene41, comp=TempFSTOrd):\nHmm, I must have done something wrong on wildcard query here.\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                Wildcard       19.21      (2.1%)        7.30      (0.3%)  -62.0% ( -63% -  -60%)\n                 Prefix3       33.69      (1.2%)       28.18      (0.9%)  -16.4% ( -18% -  -14%)\n                  Fuzzy1       61.59      (2.1%)       52.36      (0.8%)  -15.0% ( -17% -  -12%)\n                  Fuzzy2       60.94      (1.0%)       54.15      (1.3%)  -11.1% ( -13% -   -8%)\n                 Respell       54.21      (2.8%)       49.54      (1.2%)   -8.6% ( -12% -   -4%)\n                PKLookup      148.40      (1.0%)      208.07      (3.6%)   40.2% (  35% -   45%)\n\n\n\nI'll commit current version so we can iterate on it. ",
            "author": "Han Jiang",
            "id": "comment-13724955"
        },
        {
            "date": "2013-07-31T07:15:02+0000",
            "content": "Commit 1508744 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1508744 ]\n\nLUCENE-3069: introduce intersect() to TempFSTOrd ",
            "author": "ASF subversion and git services",
            "id": "comment-13724957"
        },
        {
            "date": "2013-07-31T13:57:43+0000",
            "content": "Mike, since we already have an intersect() impl, maybe we can still keep this? \n\n+1\n\nIt's odd that WildcardQuery is so angry; I wonder if it's because we can't use the commonSuffix opto.  Maybe try testing on a different wildcard query, e.g. something like a*b* (that does not have a commonSuffix)? ",
            "author": "Michael McCandless",
            "id": "comment-13725258"
        },
        {
            "date": "2013-07-31T14:28:42+0000",
            "content": "Maybe try testing on a different wildcard query, e.g. something like a*b* (that does not have a commonSuffix)?\n\nI replace all the ab*c in tasks file with ab*c*, but the performance hit is still heavy:\n\n33M wikidata, Lucene41 vs. TempFSTOrd\n\nWildcard        7.40      (1.9%)        4.63      (1.2%)  -37.5% ( -39% -  -34%)\n\n  ",
            "author": "Han Jiang",
            "id": "comment-13725288"
        },
        {
            "date": "2013-08-02T15:57:29+0000",
            "content": "Uploaded patch.\n\nIt is optimized for wildcardquery, and I did a quick test on 1M wiki data:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                PKLookup      314.63      (1.5%)      314.64      (1.2%)    0.0% (  -2% -    2%)\n                  Fuzzy1       91.32      (3.7%)       92.50      (1.6%)    1.3% (  -3% -    6%)\n                 Respell      104.54      (3.9%)      106.97      (1.6%)    2.3% (  -2% -    8%)\n                  Fuzzy2       38.22      (4.1%)       39.16      (1.2%)    2.5% (  -2% -    8%)\n                Wildcard      109.56      (3.1%)      273.42      (5.0%)  149.6% ( 137% -  162%)\n\n\n\nand TempFSTOrd vs. Lucene41, on 1M data:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 Respell      134.85      (3.7%)      106.30      (0.6%)  -21.2% ( -24% -  -17%)\n                  Fuzzy2       47.78      (4.1%)       39.03      (0.9%)  -18.3% ( -22% -  -13%)\n                  Fuzzy1      112.02      (3.0%)       91.95      (0.6%)  -17.9% ( -20% -  -14%)\n                Wildcard      326.68      (3.5%)      273.41      (1.9%)  -16.3% ( -20% -  -11%)\n                PKLookup      194.61      (1.8%)      314.24      (0.7%)   61.5% (  57% -   65%)\n\n\n\nBut I'm not happy with it , the hack I did here is to consume another big block to store the last byte of each term. So for wildcard query ab*c, we have external information to tell the ord of nearest term like *c. Knowing the ord, we can use a similar approach like getByOutput to jump to the next target term.\n\nPreviously, we have to walk on fst to the stop node to find out whether the last byte is 'c', so this optimization comes to be a big chunk.\n\nHowever I don't really like this patch , we have to increase index size (521M => 530M), and the code comes to be mess up, since we always have to foresee the next arc on current stack.  ",
            "author": "Han Jiang",
            "id": "comment-13727752"
        },
        {
            "date": "2013-08-13T04:06:38+0000",
            "content": "Commit 1513336 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1513336 ]\n\nLUCENE-3069: merge trunk changes over ",
            "author": "ASF subversion and git services",
            "id": "comment-13737782"
        },
        {
            "date": "2013-08-13T11:33:20+0000",
            "content": "Hi, currently, we have problem when migrating the codes to trunk:\n\nThe API refactoring on PostingsReader/WriterBase now splits term metadata into two parts:\nmonotonic long[] and generical byte[], the former is known by term dictionary for better\nd-gap encoding. \n\nSo we need a 'longsSize' in field summary, to tell reader the fixed length of this monotonic\nlong[]. However, this API change actually breaks backward compability: the old 4.x indices didn't \nsupport this, and for some codec like Lucene40, since their writer part are already deprecated, \ntheir tests won't pass.\n\nIt seems like we can put all the metadata in generic byte[] and let PBF do its own buffering \n(like we do in old API: nextTerm() ), however we'll have to add logics for this, in every PBF then.\n\nSo... can we solve this problem more elegantly? ",
            "author": "Han Jiang",
            "id": "comment-13738105"
        },
        {
            "date": "2013-08-13T14:16:47+0000",
            "content": "Patch with backward compability fix on Lucene41PBF (TempPostingsReader is actually a fork of Lucene41PostingsReader). ",
            "author": "Han Jiang",
            "id": "comment-13738268"
        },
        {
            "date": "2013-08-15T13:13:42+0000",
            "content": "Patch, update BlockTerms dict so that it follows refactored API. ",
            "author": "Han Jiang",
            "id": "comment-13740933"
        },
        {
            "date": "2013-08-15T13:15:47+0000",
            "content": "Commit 1514253 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1514253 ]\n\nLUCENE-3069: API refactoring on BlockTerms dict ",
            "author": "ASF subversion and git services",
            "id": "comment-13740948"
        },
        {
            "date": "2013-08-19T15:26:43+0000",
            "content": "Commit 1515469 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1515469 ]\n\nLUCENE-3069: API refactoring on Pulsing PF ",
            "author": "ASF subversion and git services",
            "id": "comment-13743897"
        },
        {
            "date": "2013-08-22T03:02:14+0000",
            "content": "Commit 1516365 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1516365 ]\n\nLUCENE-3069: API refactoring on Sep/IntBlock PF ",
            "author": "ASF subversion and git services",
            "id": "comment-13747180"
        },
        {
            "date": "2013-08-23T01:42:38+0000",
            "content": "Commit 1516677 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1516677 ]\n\nLUCENE-3069: API refactoring on MockRandom, revert supress codec in compatibility test ",
            "author": "ASF subversion and git services",
            "id": "comment-13748193"
        },
        {
            "date": "2013-08-23T08:29:19+0000",
            "content": "Commit 1516742 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1516742 ]\n\nLUCENE-3069: API refactoring on Lucene40RW ",
            "author": "ASF subversion and git services",
            "id": "comment-13748388"
        },
        {
            "date": "2013-08-23T13:29:33+0000",
            "content": "Patch, it will show how current codecs (Block/BlockTree + Lucene4X/Pulsing/Mock*) are changed according to our API refactoring. TestBackwardsCompatibility still fails, and I'll work on the impersonation later. ",
            "author": "Han Jiang",
            "id": "comment-13748540"
        },
        {
            "date": "2013-08-23T14:02:50+0000",
            "content": "Patch looks great on quick look!  I'll look more when I'm back\nonline...\n\nOne thing: I think e.g. BlockTreeTermsReader needs some back-compat\ncode, so it won't try to read longsSize on old indices? ",
            "author": "Michael McCandless",
            "id": "comment-13748556"
        },
        {
            "date": "2013-08-23T14:34:48+0000",
            "content": "Commit 1516860 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1516860 ]\n\nLUCENE-3069: merge 'temp' codes back ",
            "author": "ASF subversion and git services",
            "id": "comment-13748579"
        },
        {
            "date": "2013-08-23T14:37:22+0000",
            "content": "Patch looks great on quick look! I'll look more when I'm back\nonline...\n\nOK! I commit it so that we can see later changes.\n\nOne thing: I think e.g. BlockTreeTermsReader needs some back-compat\ncode, so it won't try to read longsSize on old indices?\n\nYes, both two Block* term dict will have a new VERSION variable to mark the\nchange, and if codec header shows a previous version, they will not read\nthat longSize VInt. ",
            "author": "Han Jiang",
            "id": "comment-13748582"
        },
        {
            "date": "2013-08-27T11:33:30+0000",
            "content": "Commit 1517792 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1517792 ]\n\nLUCENE-3069: add version check for impersonation ",
            "author": "ASF subversion and git services",
            "id": "comment-13751191"
        },
        {
            "date": "2013-08-28T08:56:43+0000",
            "content": "Patch, to show the impersonation hack for Pulsing format. \n\nWe cannot perfectly impersonate old pulsing format yet: the old format divided metadata block as inlined bytes and wrapped bytes, so when the term dict reader reads the length of metadata block, it is actually the length of 'inlined block'... And the 'wrapped block' won't be loaded for wrapped PF.\n\nHowever, to introduce a new method in PostingsReaderBase doesn't seem to be a good way... ",
            "author": "Han Jiang",
            "id": "comment-13752215"
        },
        {
            "date": "2013-08-28T16:10:37+0000",
            "content": "PostingsReaderBase.pulsed is quite crazy ... really the terms dict\nshould not need this information, ideally.\n\nPulsing has no back-compat guarantees, so it's fine to only support\nwriting the \"new\" format and being able to read it.  Ie, if this\nchange is only for impersonation then we shouldn't need to do it, I\nthink?\n\nAlso, this is spooky:\n\n\nint start = (int)in.getFilePointer();\n\n\n\nIsn't that unsafe in general?  Ie it could overflow int... ",
            "author": "Michael McCandless",
            "id": "comment-13752528"
        },
        {
            "date": "2013-08-30T15:06:45+0000",
            "content": "Commit 1518989 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1518989 ]\n\nLUCENE-3069: merge trunk changes ",
            "author": "ASF subversion and git services",
            "id": "comment-13754768"
        },
        {
            "date": "2013-09-03T02:36:55+0000",
            "content": "Commit 1519542 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1519542 ]\n\nLUCENE-3069: update javadocs, fix impersonator bug ",
            "author": "ASF subversion and git services",
            "id": "comment-13756320"
        },
        {
            "date": "2013-09-03T12:38:05+0000",
            "content": "The uploaded patch should show all the changes against trunk: I added two different implementations of term dict, and refactored the PostingsBaseFormat to plug in non-block based term dicts.\n\nI'm still working on the javadocs, and maybe we should rename that 'temp' package, like 'fstterms'?\n ",
            "author": "Han Jiang",
            "id": "comment-13756569"
        },
        {
            "date": "2013-09-04T03:05:24+0000",
            "content": "Commit 1519909 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1519909 ]\n\nLUCENE-3069: javadocs ",
            "author": "ASF subversion and git services",
            "id": "comment-13757419"
        },
        {
            "date": "2013-09-04T11:24:22+0000",
            "content": "Thanks for uploading the diffs against trunk, Han; I'll review this.\n\nCan you explain the two new terms dict impls?  And maybe write up a brief summary of all the changes (to help others understand the patch)?\n\nMaybe we can put the new \"all in memory\" terms dict impls under oal.codecs.memory?  FSTTerms* seems like a good name?  (Just because in the future maybe we have other impls of \"all in memory\" terms dicts)... ",
            "author": "Michael McCandless",
            "id": "comment-13757668"
        },
        {
            "date": "2013-09-04T11:35:56+0000",
            "content": "OK! These two term dicts are both FST-based:\n\n\n\tFST term dict directly uses FST to map term to its metadata & stats (FST<TermData>)\n\tFSTOrd term dict uses FST to map term to its ordinal number (FST<Long>), and the ordinal is then used to seek metadata from another big chunk.\n\n\n\nI prefer the second impl since it puts much less stress on FST.\n\nI have updated the detailed format explaination in last commit. Hmm, I'll create another patch for this... ",
            "author": "Han Jiang",
            "id": "comment-13757676"
        },
        {
            "date": "2013-09-04T13:24:08+0000",
            "content": "I like FSTOrd as well.  Presumably this one also exposes it via TermsEnum.ord()? ",
            "author": "David Smiley",
            "id": "comment-13757741"
        },
        {
            "date": "2013-09-04T13:49:47+0000",
            "content": "Yes, with slight changes, it can support seek by ord. (With FST.getByOutput).  ",
            "author": "Han Jiang",
            "id": "comment-13757771"
        },
        {
            "date": "2013-09-04T14:29:50+0000",
            "content": "Commit 1520034 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1520034 ]\n\nLUCENE-3069: move TermDict impls to package 'memory', nuke all 'Temp' symbols ",
            "author": "ASF subversion and git services",
            "id": "comment-13757814"
        },
        {
            "date": "2013-09-04T14:40:24+0000",
            "content": "Patch from last commit, and summary:\n\nPreviously our term dictionary were both block-based: \n\n\n\tBlockTerms dict breaks terms list into several blocks, as a linear\n  structure with skip points. \n\n\n\n\n\tBlockTreeTerms dict uses a trie-like structure to decide how terms are\n  assigned to different blocks, and uses an FST index to optimize seeking \n  performance.\n\n\n\nHowever, those two kinds of term dictionary don't hold all the term \ndata in memory. For the worst case there would be at least two seeks:\none from index in memory, another from file on disk. And we already have \nmany complicated optimizations for this...\n\nIf by design a term dictionary can be memory resident, the data structure \nwill be simpler (after all we don't need maintain extra file pointers for \na second-time seek, and we don't have to decide heuristic for how terms \nare clustered). And this is why those two FST-based implementation are \nintroduced.\n\nAnother big change in the code is: since our term dictionaries were both \nblock-based, previous API was also limited. It was the postings writer who \ncollected term metadata, and the term dictionary who told postings writer \nthe range of terms it should flush to block. However, encoding of terms \ndata should be decided by term dictionary part, since postings writer \ndoesn't always know how terms are structured in term dictionary...\nPrevious API had some tricky codes for this, e.g. PulsingPostingsWriter had\nto use terms' ordinal in block to decide how to write metadata, which is \nunnecessary.\n\nTo make the API between term dict and postings list more 'pluggable' and \n'general', I refactored the PostingsReader/WriterBase. For example, the \npostings writer should provide some information to term dictionary, like \nhow many metadata values are strictly monotonic, so that term dictionary \ncan optimize delta-encoding itself. And since the term dictionary now fully\ndecides how metadata are written, it gets the ability to utilize \nintblock-based metadata encoding.\n\nNow the two implementations of term dictionary can easily be plugged with \ncurrent postings formats, like:\n\n\tFST41 =\n    FSTTermdict + Lucene41PostingsBaseFormat,\n\tFSTOrd41 =\n    FSTOrdTermdict + Lucene41PostingsBaseFormat. \n\tFSTOrdPulsing41 =\n    FSTOrdTermsdict + PulsingPostingsWrapper + Lucene41PostingsFormat\n\n\n\nAbout performance, as shown before, those two term dict improve on primary \nkey lookup, but still have overhead on wildcard query (both two term dict \nhave only prefix information, and term dictionary cannot work well with \nthis...). I'll try to hack this later. ",
            "author": "Han Jiang",
            "id": "comment-13757821"
        },
        {
            "date": "2013-09-05T20:58:18+0000",
            "content": "Commit 1520422 from Michael McCandless in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1520422 ]\n\nLUCENE-3069: small javadoc fixes ",
            "author": "ASF subversion and git services",
            "id": "comment-13759448"
        },
        {
            "date": "2013-09-05T20:58:35+0000",
            "content": "Patch looks great.  It's nice how postings writers no longer need\ntheir own redundant PendingTerm instances to track the term's metadata\n/ blocking; just use their existing TermState class instead.  And how\npostings readers don't have to deal w/ blocking either.\n\nIn general, couldn't the writer re-use the reader's TermState?\nE.g. Lucene40PostingsWriter just use Lucene40PostingsReader's\nStandardTermState, rather than make its own?  (And same for\nLucene41PostingsWriter/Reader).\n\nHave you run \"first do no harm\" perf tests?  Ie, compare current trunk\nw/ default Codec to branch w/ default Codec?  Just to make sure there\nare no surprises...\n\nWhy does Lucene41PostingsWriter have \"impersonation\" code?  Was that\njust for debugging during dev?  Can we remove it (it should always\nwrite the current format)?  The reader needs it of course ... but it\nshouldn't be commented as \"impersonation\" but as back-compat?\n\nIn the javadocs for encodeTerm, don't we require that the long[] are\nalways monotonic?  It's not \"optional\"?  Also, \"monotonical\" should be\n\"monotonic\" there.\n\nMaybe we should add a \"reset\" method to each PF's TermState, so\ninstead of doing newTermState() when absolute, we can .reset(), and\nlikewise in the reader.\n\nI forget: why does the postings reader/writer need to handle delta\ncoding again (take an absolute boolean argument)?  Was it because of\npulsing or sep?  It's fine for now (progress not perfection) ... but\nnot clean, since \"delta coding\" is really an encoding detail so in\ntheory the terms dict should \"own\" that ...\n\n\"monotonical\" appears several times but I think it should instead be\n\"monotonic\".\n\nThe new .smy file for Pulsing is sort of strange ... but necessary\nsince it always uses 0 longs, so we have to store this somewhere\n... you could put it into FieldInfo attributes instead?\n\nIt's nice how small the FST terms dicts are!  Much simpler than the\nhairy BlockTree code...\n\nShould we backport this to 4.x?  In theory this should not be so hard\n... 3.x indices already have their own PF impls, and the change is\nback-compatible to current 4.x indices ... ",
            "author": "Michael McCandless",
            "id": "comment-13759449"
        },
        {
            "date": "2013-09-06T12:04:21+0000",
            "content": "Mike, thanks for the review!\n\nIn general, couldn't the writer re-use the reader's TermState?\n\nI'm afraid this somewhat makes codes longer? I'll make a patch to see this.\n\n\nHave you run \"first do no harm\" perf tests? Ie, compare current trunk\nw/ default Codec to branch w/ default Codec? Just to make sure there\nare no surprises...\n\nYes, no surprise yet.\n\nWhy does Lucene41PostingsWriter have \"impersonation\" code? \n\nYeah, these should be removed.\n\n\nI forget: why does the postings reader/writer need to handle delta\ncoding again (take an absolute boolean argument)? Was it because of\npulsing or sep? It's fine for now (progress not perfection) ... but\nnot clean, since \"delta coding\" is really an encoding detail so in\ntheory the terms dict should \"own\" that ...\n\nAh, yes, because of pulsing.\n\nThis is because.. PulsingPostingsBase is more than a PostingsBaseFormat. \nIt somewhat acts like a term dict, e.g. it needs to understand how terms are \nstructured in one block (term No.1 uses absolute value, term No.x use delta value)\nthen judge how to restruct the inlined and wrapped block (No.1 still uses absolute value,\nbut the first-non-pulsed term will need absolute encoding as well). \n\nWithout the argument 'absolute', the real term dictionary will do the delta encoding itself,\nthen PulsingPostingsBase will be confused, and all wrapped PostingsBase have to encode \nmetadata values without delta-format.\n\n\n\n\nThe new .smy file for Pulsing is sort of strange ... but necessary\nsince it always uses 0 longs, so we have to store this somewhere\n... you could put it into FieldInfo attributes instead?\n\nYeah, it is another hairy thing... the reason is, we don't have a 'PostingsTrailer'\nfor PostingsBaseFormat. Pulsing will not know the longs size for each field, until \nall the fields are consumed... and it should not write those longsSize to termsOut in close()\nsince the term dictionary will use the DirTrailer hack here. (maybe every term dictionary\nshould close postingsWriter first, then write field summary and close itself? I'm not sure \nthough). \n\n\nShould we backport this to 4.x? \n\nYeah, OK! ",
            "author": "Han Jiang",
            "id": "comment-13760160"
        },
        {
            "date": "2013-09-06T15:12:25+0000",
            "content": "Commit 1520592 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1520592 ]\n\nLUCENE-3069: remove impersonate codes, fix typo ",
            "author": "ASF subversion and git services",
            "id": "comment-13760259"
        },
        {
            "date": "2013-09-06T16:04:52+0000",
            "content": "Commit 1520618 from Han Jiang in branch 'dev/branches/lucene3069'\n[ https://svn.apache.org/r1520618 ]\n\nLUCENE-3069: reuse customized TermState in PBF ",
            "author": "ASF subversion and git services",
            "id": "comment-13760304"
        },
        {
            "date": "2013-09-06T16:24:44+0000",
            "content": "I think this is ready to commit to trunk now, and I'll wait for a day or two before committing it.  ",
            "author": "Han Jiang",
            "id": "comment-13760325"
        },
        {
            "date": "2013-09-06T16:26:07+0000",
            "content": "Thanks Han.  I think we can just leave the .smy as is for now, and keep passing \"boolean absolute\" down.  We can later improve these ...\n\nI think we should first land this on trunk and let jenkins chew on it for a while ... and if all seems good, then back port. ",
            "author": "Michael McCandless",
            "id": "comment-13760328"
        },
        {
            "date": "2013-09-09T16:07:57+0000",
            "content": "Commit 1521173 from Han Jiang in branch 'dev/trunk'\n[ https://svn.apache.org/r1521173 ]\n\nLUCENE-3069: Lucene should have an entirely memory resident term dictionary ",
            "author": "ASF subversion and git services",
            "id": "comment-13761960"
        },
        {
            "date": "2013-09-30T19:39:12+0000",
            "content": "nice one! I am happy that this one made it in 2.5 years after opening! Great work Han!! ",
            "author": "Simon Willnauer",
            "id": "comment-13782141"
        },
        {
            "date": "2013-10-09T08:15:38+0000",
            "content": "Commit 1530520 from Han Jiang in branch 'dev/trunk'\n[ https://svn.apache.org/r1530520 ]\n\nLUCENE-3069: add CHANGES, move new postingsformats to oal.codecs ",
            "author": "ASF subversion and git services",
            "id": "comment-13790156"
        },
        {
            "date": "2014-01-22T14:32:29+0000",
            "content": "Note that all the commit messages at the end of this issue (generated by Jira's FishEye plugin I think) incorrectly state that \"Han Lee\" committed changes here.\n\nThis is due to an issue in FishEye with username collision ... Han Jiang's (who really committed here) apache username is \"han\", but in Jira that user name belongs to Han Lee, which leads to this mis-labeling.\n\nHere's the INFRA issue: https://issues.apache.org/jira/browse/INFRA-3243 but it's currently WONTFIX unfortunately ... ",
            "author": "Michael McCandless",
            "id": "comment-13878687"
        },
        {
            "date": "2014-01-22T15:38:08+0000",
            "content": "I think it's actually two different things - our commit messages are generated by a script that subscribes to svnpubsub, and it does some look ups to figure out the right user name. The fisheye stuff is what you see if you look at the source tab I think. So it might be easier to fix, since I think it's in our control (INFRA's anyway).\n\nhttps://svn.apache.org/repos/infra/infrastructure/trunk/projects/svngit2jira/ ",
            "author": "Mark Miller",
            "id": "comment-13878765"
        },
        {
            "date": "2014-01-22T18:03:39+0000",
            "content": "Thanks Mark.\n\nYes, you can see FishEye's comments under Source and and also the All tab.\n\n\"Our\" (svnpubsub) commit messages are correct here (they say \"Han Jiang\"), but the FishEye comments are incorrect (they say \"Han Lee\"). ",
            "author": "Michael McCandless",
            "id": "comment-13878917"
        },
        {
            "date": "2014-01-22T18:12:31+0000",
            "content": "Ah, sorry. I'm usually in Comments view and I took \"Note that all the commit messages at the end of this issue \" as referring to the ASF subversion and git services commit tags. Given past experience, I don't trust the fisheye or the like integrations. We might wake up one day and they will just be gone along with their history... ",
            "author": "Mark Miller",
            "id": "comment-13878932"
        },
        {
            "date": "2014-01-23T03:36:19+0000",
            "content": "Thanks for catching this Mike! I wasn't quick to get that username  ",
            "author": "Han Jiang",
            "id": "comment-13879423"
        },
        {
            "date": "2014-01-29T14:40:40+0000",
            "content": "I'd like to commit this to 4.7 as well ... I'll backport & commit soon. ",
            "author": "Michael McCandless",
            "id": "comment-13885387"
        },
        {
            "date": "2014-01-29T16:21:50+0000",
            "content": "Commit 1562497 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1562497 ]\n\nLUCENE-3069: port fully RAM-resident terms FST dictionary implementations to 4.x ",
            "author": "ASF subversion and git services",
            "id": "comment-13885474"
        },
        {
            "date": "2014-01-29T16:23:23+0000",
            "content": "Commit 1562498 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1562498 ]\n\nLUCENE-3069: move CHANGES entries under 4.7 ",
            "author": "ASF subversion and git services",
            "id": "comment-13885476"
        },
        {
            "date": "2014-01-29T16:38:45+0000",
            "content": "Commit 1562506 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1562506 ]\n\nLUCENE-3069: merge the back-compat indices from 4.x ",
            "author": "ASF subversion and git services",
            "id": "comment-13885502"
        },
        {
            "date": "2014-01-30T04:55:23+0000",
            "content": "Thanks Mike! ",
            "author": "Han Jiang",
            "id": "comment-13886256"
        },
        {
            "date": "2014-01-30T10:40:12+0000",
            "content": "Commit 1562771 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1562771 ]\n\nLUCENE-3069: also exclude MockRandom from this test case ",
            "author": "ASF subversion and git services",
            "id": "comment-13886478"
        },
        {
            "date": "2014-03-16T10:13:14+0000",
            "content": "Woops, thanks for fixing the gsoc label Han! ",
            "author": "Michael McCandless",
            "id": "comment-13937065"
        },
        {
            "date": "2014-03-16T11:15:08+0000",
            "content": "Had to reopen it because jira doesn't permit label change  ",
            "author": "Han Jiang",
            "id": "comment-13937095"
        }
    ]
}