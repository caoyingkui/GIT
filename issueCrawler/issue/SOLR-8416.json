{
    "id": "SOLR-8416",
    "title": "The collections create API should return after all replicas are active.",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5.1",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "Currently the collection creation API returns once all cores are created. In large cluster the cores may not be alive for some period of time after cores are created. For any thing requested during that period, Solr appears unstable and can return failure. Therefore it's better  the collection creation API waits for all cores to become alive and returns after that.",
    "attachments": {
        "SOLR-8416.patch": "https://issues.apache.org/jira/secure/attachment/12778050/SOLR-8416.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-12-16T17:52:59+0000",
            "author": "Michael Sun",
            "content": "A patch is uploaded. Here are some thoughts:\n\n1. The patch pulls shard status from zk and returns if they are all active during preset time or throw exception.  An alternative is to wait for zk notifications but I am not sure how much is the gain.\n2. The total wait time should be configurable to fit large cluster. Is it good to be a solr config or collection config? It's more natural to be collection config but it may be easy for user if it's a solr config that can be set in CM.\n ",
            "id": "comment-15060396"
        },
        {
            "date": "2015-12-16T17:59:39+0000",
            "author": "Mark Miller",
            "content": "Thanks for the patch, a couple comments:\n\n\n\tUsually it's better to use SolrException over RuntimeException\n\tWhere you catch the interrupted exception, you should restore the interrupted status.\n\tThe failure exception should probably give detail on which replicas were not found to be live and ACTIVE.\n\tShould not use hard coded 'active' but the relevant constants.\n\tShould probably check if the replicas node is listed under live nodes as well as if it's active?\n\n ",
            "id": "comment-15060419"
        },
        {
            "date": "2015-12-16T18:11:16+0000",
            "author": "Mark Miller",
            "content": "1. The patch pulls shard status from zk and returns if they are all active during preset time or throw exception. An alternative is to wait for zk notifications but I am not sure how much is the gain.\n\nI agree, the complication is probably not worth it.\n\n\nIs it good to be a solr config or collection config?\n\nI would make it a solr config - we need that ease of use at least. Later, if someone wants to override per collection or something, we can look at adding that in a way that you can override. ",
            "id": "comment-15060441"
        },
        {
            "date": "2015-12-16T21:29:49+0000",
            "author": "Michael Sun",
            "content": "Thanks Mark Miller for reviewing.   ",
            "id": "comment-15060912"
        },
        {
            "date": "2015-12-17T01:03:54+0000",
            "author": "Gregory Chanan",
            "content": "The patch claims to be looking at all shards being active but is actually looking at all replicas being active, right?  It's also inconsistent with the other creation commands now, e.g. CREATESHARD/ADDREPLICA will return as soon as the replicas are created while this will wait until all the replicas are active.\n\nFrom a client perspective, what do you actually want?  I don't think it's that all replicas are active at one time; given a large enough cluster that can be unlikely.  Some possibilities:\n1) all the replicas were able to become active (you'd have to track this separately)\n2) the collection is \"usable\" from the client \u2013 each shard has a leader that is live and active? ",
            "id": "comment-15061258"
        },
        {
            "date": "2015-12-17T01:20:28+0000",
            "author": "Michael Sun",
            "content": "the collection is \"usable\" from the client \u2013 each shard has a leader that is live and active?\n\nFrom client perspective, the collection or cluster is usable if the leader is active. However if high load of indexing starts before all replica becomes active, I am not sure if there will be too much sync traffic once replica starts.  ",
            "id": "comment-15061272"
        },
        {
            "date": "2015-12-17T01:32:15+0000",
            "author": "Mark Miller",
            "content": "Yeah, the load is a good point. It's a lot harder to go active once a load is started. And you would hope that right after creating the collection, everything would actually be healthy, but I see Greg's point too. I've always wrestled a bit with these ideas when considering this feature. It's not easy to make everything happy. ",
            "id": "comment-15061287"
        },
        {
            "date": "2015-12-17T02:03:06+0000",
            "author": "Gregory Chanan",
            "content": "I'd see what hbase or other systems do. ",
            "id": "comment-15061319"
        },
        {
            "date": "2015-12-17T02:23:45+0000",
            "author": "Michael Sun",
            "content": "Yeah. It's not easy to make tradeoff, in particular optimized for large cluster and high load like this. In addition to looking at other systems, another choice (which probably always works) is to add an option to allow user to decide if Solr should wait for all shard leaders to be active or all replicas at run time.\n\nThe additional option need to be documented and adds a bit to the learning curve for users. But in general it won't create much burden since users usually are prepared to do some extra optimization for large cluster under high load.  ",
            "id": "comment-15061338"
        },
        {
            "date": "2015-12-17T15:35:39+0000",
            "author": "Michael Sun",
            "content": "Just upload an updated patch for discussion. Here is the changes\n\n1. add a property to set max wait time\n2. add a property to decide if it waits for all shard leaders to be active or all replicas\n3. fix issues in Mark Miller's review except for the following one.\n\nShould probably check if the replicas node is listed under live nodes as well as if it's active?\nMark Miller Can you give me more details about it? Thanks. ",
            "id": "comment-15062197"
        },
        {
            "date": "2015-12-17T17:19:18+0000",
            "author": "Mark Miller",
            "content": "Should probably check if the replicas node is listed under live nodes as well as if it's active?\nCan you give me more details about it?\n\nFor technical reasons, the actual state of a replica is a combination of whether it's ephemeral live node exists in zookeeper and the state listed in the cluster state. We make a best effort on shutdown to publish DOWN for all the states, but it's simply best effort and crashes and other probably common things can mean any state is in the cluster state. You can really only count on it being an accurate state if you also check that the node is live. The ClusterState object has a helper method for this if I remember right.  ",
            "id": "comment-15062370"
        },
        {
            "date": "2015-12-17T19:46:28+0000",
            "author": "Michael Sun",
            "content": "Here is an updated patch which includes checking for live nodes. Thanks Mark Miller for suggestion. ",
            "id": "comment-15062659"
        },
        {
            "date": "2015-12-18T13:13:55+0000",
            "author": "Mark Miller",
            "content": "Thanks Michael,\n\n\n\tLooks like a bunch of imports were moved above the license header?\n\tWe probably want to use real solr.xml config for this. Or make it params for the collection create call with reasonable defaults. We generally only use system properties for kind of internal fail safe options we don't expect to really be used. I'd be fine with reasonable defaults that could be overridden per collection create call, but we could also allow the defaults to be configurable via solr.xml.\n\n+    Integer numRetries = Integer.getInteger(\"createCollectionWaitTimeTillActive\", 10);\n+    Boolean checkLeaderOnly = Boolean.getBoolean(\"createCollectionCheckLeaderActive\");\n\n\n\tWe should handle the checked exceptions this might throw like we do in other spots rather than use a catch-all Exception. There should be plenty of code to reference where we handle keeper and interrupted exception and do the right thing for each.\n\n+      try {\n+        zkStateReader.updateClusterState();\n+        clusterState = zkStateReader.getClusterState();\n+      }  catch (Exception e) {\n+        throw new SolrException(ErrorCode.SERVER_ERROR, \"Can't connect to zk server\", e);\n+      }\n\n\n\tI'd probably combine the following into one IF statement:\n\n+          if (!clusterState.liveNodesContain(replica.getNodeName())) {\n+            replicaNotAlive = replica.getCoreUrl();\n+            nodeNotLive = replica.getNodeName();\n+            break;\n+          }\n+          if (!state.equals(Replica.State.ACTIVE.toString())) {\n+            replicaNotAlive = replica.getCoreUrl();\n+            replicaState = state;\n+            break;\n+          }\n\n\n\tShould probably restore interrupt status and throw a SolrException.\n\n+      try {\n+        Thread.sleep(1000);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+      }\n\n\n\tI'm not sure the return message is quite right. If a nodes state is not ACTIVE, it does not mean it's not Live. It can be DOWN and live or RECOVERING and Live, etc. A replica is either Live or not and then has a Live State if and only if it is Live.\n\tNeeds some tests.\n\n ",
            "id": "comment-15063934"
        },
        {
            "date": "2016-01-13T23:56:57+0000",
            "author": "Michael Sun",
            "content": "Thanks Mark Miller for review. I made a patch with all fixes except for tests. Can you give me some suggestion about how to design test for it? I can add a verification in CollectionsAPIDistributedZkTest.createCollection()  to replace waitForRecoveriesToFinish(). But to test the patch completely, it seems to me that it requires injection of delays or failure in core creation which I didn't find a good way to do.  ",
            "id": "comment-15097315"
        },
        {
            "date": "2016-01-14T14:47:33+0000",
            "author": "Mark Miller",
            "content": "requires injection of delays or failure in core creation which I didn't find a good way to do\n\nThis has been lacking in our test framework for a long time for a variety of reasons. I recently started working on something to inject faults though - we have to have it.\n\nTake a look at https://github.com/apache/lucene-solr/blob/trunk/solr/core/src/java/org/apache/solr/util/TestInjection.java\n\nWe should use it carefully, and in the right places, but I think testing collection creation fail cases fits the bill.\n\nWe use assertions to call the fault injection methods so that they cannot be called by mistake in production (Solr tests require assertions are on, and we don't properly support running with assertions on in production). ",
            "id": "comment-15098194"
        },
        {
            "date": "2016-01-14T20:35:48+0000",
            "author": "Michael Sun",
            "content": "It's great you have been working on injection. I can use it as an example. Thanks Mark Miller for suggestion. ",
            "id": "comment-15098800"
        },
        {
            "date": "2016-01-15T22:24:47+0000",
            "author": "Michael Sun",
            "content": "Here is the patch with delay in core creation injected to simulate slow core creation or recovering situations in real world scenarios. Basically it extends the TestInjection.java to inject delay.\n\nThe patch also clean up the code for configurations, exceptions and error messages.\ncc Mark Miller ",
            "id": "comment-15102583"
        },
        {
            "date": "2016-01-26T17:42:40+0000",
            "author": "Mark Miller",
            "content": "I have to spend a bit of time playing with this manually still, but latest patch looks great! ",
            "id": "comment-15117617"
        },
        {
            "date": "2016-02-09T00:21:15+0000",
            "author": "Mark Miller",
            "content": "I've started making a few changes - all the tests were not yet consistently passing, and I had some thoughts about things we might improve.\n\nI'd still like to look at not waiting for replicas we know failed creating, but it's a little tricky.\n\nHere is my current progress attached. ",
            "id": "comment-15138097"
        },
        {
            "date": "2016-02-09T14:38:36+0000",
            "author": "Mark Miller",
            "content": "Another patch, I think this is almost ready. I punted for now on making it not wait for specific failures (there are some annoying complications around error reporting atm), but I made some other improvements. ",
            "id": "comment-15139001"
        },
        {
            "date": "2016-02-10T19:37:52+0000",
            "author": "Michael Sun",
            "content": "Ah yes, it makes sense to skip waiting for replicas to be alive for async calls or in case there is failure. \n\nOne question, it uses both rsp.getException() and response.getResponse().get(\"exception\"). Are they pointing to the same exception? Thanks.\n\nAlso there seems a typo error that return is not included.\n\n    if (response.getResponse().get(\"failure\") != null) {\n      // TODO: we should not wait for Replicas we know failed\n    }\n\n ",
            "id": "comment-15141508"
        },
        {
            "date": "2016-02-10T20:08:32+0000",
            "author": "Mark Miller",
            "content": "Are they pointing to the same exception? \nrsp.getException happens when the response is not code 200 okay - but the collections API works a little different in that sometimes it will put failures and exceptions in a call that returns 200 okay. We just check both cases.\n\nAlso there seems a typo error that return is not included.\n\nNot waiting for individual replicas that did not create is left as a TODO there, we don't want to do anything yet.\n\nI also moved the waiting code to the CollectionsHandler. I think it's more efficient and 'safer' to pull the waiting out of the Overseer processing.\n\nI think we can commit this as a good start and use further JIRA's to make any improvements. ",
            "id": "comment-15141566"
        },
        {
            "date": "2016-02-10T23:08:47+0000",
            "author": "Michael Sun",
            "content": "I see. Thanks Mark Miller. ",
            "id": "comment-15141911"
        },
        {
            "date": "2016-02-19T21:15:58+0000",
            "author": "Mark Miller",
            "content": "For some reason the commit doesn't seem to have been tagged in JIRA. This is committed though. SHA:31437c9b43cf93128e284e278470a39b2012a6cb ",
            "id": "comment-15154879"
        },
        {
            "date": "2016-02-19T21:34:58+0000",
            "author": "Mark Miller",
            "content": "Thanks Michael! ",
            "id": "comment-15154897"
        },
        {
            "date": "2016-03-10T19:39:38+0000",
            "author": "Erick Erickson",
            "content": "Mark Miller The fix version is just \"master\". I found the entry in CHANGES.txt in 6x and trunk, should 6x be added to the fix versions? ",
            "id": "comment-15189832"
        },
        {
            "date": "2016-04-19T20:36:35+0000",
            "author": "Anshum Gupta",
            "content": "Opening to back port for 5.5.1. ",
            "id": "comment-15248578"
        },
        {
            "date": "2016-04-19T23:06:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit a494ac95fc2c22004d89952f7262ceac8368b6c9 in lucene-solr's branch refs/heads/branch_5x from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a494ac9 ]\n\nSOLR-8416: The collections create API should return after all replicas are active. ",
            "id": "comment-15248865"
        },
        {
            "date": "2016-04-19T23:37:02+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 4c88ea5532c2e23d7b29ba88ce41e19f7c58e691 in lucene-solr's branch refs/heads/branch_5_5 from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4c88ea5 ]\n\nSOLR-8416: The collections create API should return after all replicas are active. ",
            "id": "comment-15248931"
        },
        {
            "date": "2016-04-21T12:43:57+0000",
            "author": "Mark Miller",
            "content": "The fix version is just \"master\". I found the entry in CHANGES.txt in 6x and trunk, should 6x be added to the fix versions?\n\nmaster was supposed to be renamed to 6. ",
            "id": "comment-15251837"
        },
        {
            "date": "2016-04-21T15:10:59+0000",
            "author": "Anshum Gupta",
            "content": "Think you accidentally removed the 5.5.1 fix version . I had back ported this to be released with 5.5.1. I'll add that fix version back. ",
            "id": "comment-15252030"
        },
        {
            "date": "2016-05-09T15:41:45+0000",
            "author": "Stephan Lagraulet",
            "content": "Can you close the issue as it is commited on the 5.5 branch? ",
            "id": "comment-15276523"
        }
    ]
}