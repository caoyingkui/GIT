{
    "id": "LUCENE-628",
    "title": "Intermittent FileNotFoundException for .fnm when using rsync",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.9",
        "resolution": "Incomplete",
        "status": "Resolved"
    },
    "description": "We use Lucene 1.9.1 to create and search indexes for web applications. The application runs in Jboss402 on Redhat ES3. A single Master (Writer) Jboss instance creates and writes the indexes using the compound file format , which is optimised after all updates. These index files are replicated every few hours using rsync, to a number of other application servers (Searchers). The rsync job only runs if there are no lucene lock files present on the Writer. The Searcher servers that receive the replicated files, perform only searches on the index. Up to 60 searches may be performed each minute. \n\nEverything works well most of the time, but we get the following issue on the Searcher servers about 10% of the time. \nFollowing an rsync replication one or all of the Searcher server throws\n\nIOException caught when creating and IndexSearcher\njava.io.FileNotFoundException: /..../_1zm.fnm (No such file or directory)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n        at org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)\n        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)\n        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)\n        at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:56)\n        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:144)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:129)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:110)\n        at org.apache.lucene.index.IndexReader$1.doBody(IndexReader.java:154)\n        at org.apache.lucene.store.Lock$With.run(Lock.java:109)\n        at org.apache.lucene.index.IndexReader.open(IndexReader.java:143)  \n\nAs we use the compound file format I would not expect .fnm files to be present. When replicating, we do not delete the old .cfs index files as these could still be referenced by old Searcher threads. We do overwrite the segments and deletable files on the Searcher servers. \n\nMy thoughts are: Either we are occasionally overwriting a file at the exact time a new searcher is being created, or the lock files are removed from the Writer server before the compaction process is completed, we then replicate a segments file that still references a ghost .fnm file.\n\nI would greatly appreciate any ideas and suggestions to solve this annoying issue.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2006-07-18T17:59:31+0000",
            "content": "\nMy best guess on what's happening here is, on one of your Searcher\nboxes:\n\n\n\trdist has copied over the new segments file but not yet the actual\n    _1zm.cfs file\n\n\n\n\n\tIndexSearcher is re-instantiated at this moment and reads this new\n    segments file\n\n\n\n\n\tIndexSearcher then tries to load the _1zm.cfs (referenced by the\n    new segments file), but because it does not yet exist (rdist\n    hasn't copied it yet), it falls back to non compound file\n    (_1zm.fnm) which also does not exist, and hits that exception\n\n\n\nThe one thing that's odd in your traceback above is line 154 of\nIndexReader.java is only used when there are more than 1 segment in\nyour index.  Are you allowing rdist to make a copy after IndexWriter\nhas added docs (and closed) but before optimize is called?  Otherwise\nI can't explain why the index on your Searcher box has more than one\nsegment.\n\nNote that there are two lock files on the Writer machine: the write\nlock, held for a long time (whenever an IndexWriter is open), and the\ncommit lock, held briefly while a new segments file is written.\n\nI think you need to change your approach to more correctly use\nLucene's locking:\n\n\n\tOn the Writer box, before rdist can run, it must hold (acquire)\n    the write lock, for the full duration of the copy.  Just checking\n    that the write lock file doesn't exist isn't generally sufficient\n    because an IndexWriter may wake up and start changing things while\n    your rdist is running (unless that can't happen in your current\n    design, for example if from a single Java process you close the\n    IndexWriter, run rdist, repeat).\n\n\n\n\n\tOn each Searcher box, before rdist can copy to it, you need to\n    acquire the commit lock and hold it for the full duration of the\n    copy, then release it.  Note that no IndexSearcher (IndexReader)\n    can be instantiated during this time (it will block on commit lock\n    acquire until the rdist copy is done).\n\n\n\nNote that the Solr project:\n\n  http://incubator.apache.org/solr/features.html\n  http://incubator.apache.org/solr/tutorial.html \n\nhas an excellent solution for correctly distributing an index from\nsingle Writer to multiple Searhcers (they call it \"snaphots\").  It\nalso uses rdist to move snapshots around.  You might want to try Solr,\nor perhaps \"borrow\" it's approach, especially the neat \"cp -l -r\"\ntrick for quickly creating a snapshot of the index on the Writer\nmachine.\n\nSee also this recent thread that touched on similar issues:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/37593\n ",
            "author": "Michael McCandless",
            "id": "comment-12421908"
        },
        {
            "date": "2006-07-19T11:48:25+0000",
            "content": "Hi Michael,\n\nMany thanks for this input. Your comments are very sound and I will look into your suggestions and report back.\n\nCheers. ",
            "author": "Simon Lorenz",
            "id": "comment-12422096"
        },
        {
            "date": "2009-01-04T15:05:04+0000",
            "content": "Hey Simon, anything to report back on this issue? I'd like to close it out if you have worked out what happened. ",
            "author": "Mark Miller",
            "id": "comment-12660581"
        }
    ]
}