{
    "id": "SOLR-1931",
    "title": "Schema Browser does not scale with large indexes",
    "details": {
        "affect_versions": "3.6,                                            4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "Admin UI"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The Schema  Browser JSP by default causes the Luke handler to \"scan the world\". In large indexes this make the UI useless.\n\nOn an index with 64m documents & 8gb of disk space, the Schema Browser took 6 minutes to open and hogged all disk I/O, making Solr useless.",
    "attachments": {
        "SOLR-1931-trunk.patch": "https://issues.apache.org/jira/secure/attachment/12508990/SOLR-1931-trunk.patch",
        "SOLR-1931-3x.patch": "https://issues.apache.org/jira/secure/attachment/12508989/SOLR-1931-3x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12873197",
            "date": "2010-05-29T00:35:28+0000",
            "content": "This is correctly marked as \"Improvement\", but I don't see any proposal.\n\nSome off the top of my head:\n\n\tdon't start scanning by default... have a \"START\" button or something\n\thave a progress bar\n\tmake it cancelable\n\n\n\nBut since it looks like the schema browser goes through a request handler (luke), things like cancellation look hard.  Any ideas? "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12873200",
            "date": "2010-05-29T00:43:05+0000",
            "content": "The UI should change to a drill-down UI that starts at a fast summary of the index schema and fields, then drills down to deep scans of different fields.\n\nLukeRequestHandler shall default to \"no fields\" instead of \"all fields\".\nLukeRequestHandler shall have a new call that returns a list of fields and nothing else.\n\nThe Ajax UI would fetch the list of fields and then fetch individual fields as is does now.\nThe Ajax UI would include a button that delves deeper into fields, doing the various scans it does now. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12873207",
            "date": "2010-05-29T01:02:42+0000",
            "content": "LukeRequestHandler shall default to \"no fields\" instead of \"all fields\".\n\nI don't really see a need to change the default on LukeRequestHandler \u2013 if you are hitting it direclty you should really know what you're doing.  improving how the schema browser uses luke seems orthoginal to that. (but i could be convined otherwise)\n\nLukeRequestHandler shall have a new call that returns a list of fields and nothing else.\n\nisn't this essentially what \"/admin/luke?numTerms=0\" does? ... all of the other data returned when numTerms=0 is essentially free.\n\nif you have a crazy ass big number of dynamicFields, then it still might be slow \u2013 but if you are in that situation schema browser isn't going to ever be of much use to you unless we add some more comprehensive UI elements for \"searching\" the list of fields.  (the existing \"/admin/luke?show=schema\" call can be used as the source of this data ... schema browser already uses that to build up the list of fieldtypes and dynamic fields)\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12873222",
            "date": "2010-05-29T02:06:49+0000",
            "content": "This is my test index:\n65m documents\n2 text fields each with 10m and 14m unique terms, 'text0' and 'text1'.\nseveral more string fields with 1 to 10 unique terms: 'protocol' has 4 unique facets\nNo dynamic fields.\n\n\n\tnumTerms=0\n\t\n\t\tReturns immediately with the field list.\n\t\n\t\n\tnumTerms=10\n\t\n\t\t130-160 seconds\n\t\n\t\n\tnumTerms=10&fl=protocol\n\t\n\t\t45 seconds\n\t\n\t\n\tnumTerms=10&fl=text0\n\t\n\t\t60 seconds\n\t\n\t\n\tnumTerms=10&fl=text1\n\t\n\t\t60 seconds\n\t\n\t\n\n\n\n\n\tshow=schema\n\t\n\t\t18 seconds after above warmup queries\n\t\n\t\n\n\n\nThese numbers are consistent, run multiple times against the same index load, in various orders.\n\nGiven the above numbers, the commands should be:\n\n\tto get a list of fixed fields\n\t\n\t\tnumTerms=0\n\t\n\t\n\tto find dynamic fields\n\t\n\t\tshow=schema\n\t\n\t\n\tto find unique terms for a field\n\t\n\t\tallow user to choose between\n\t\t\n\t\t\tnumTerms=X&fl=field\n\t\t\tfacet call\n\t\t\n\t\t\n\t\n\t\n\n\n\nIt needs a new show=schema option that does not scan for dynamic fields. That would be called on page open, then the individual fields can have drill-downs and there can be a 'scan for dynamic fields' button that does the current show=schema scan.\n\nDoes this make sense?\n\nOther possible features: \n\n\tinfo on segments\n\t\n\t\tseparate above results on segments?\n\t\n\t\n\tshortest, longest, mean, standard deviation of text field lengths\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12873224",
            "date": "2010-05-29T02:27:07+0000",
            "content": "Anything with numTerms !=0 is likely to be slow, and the more fields you ask for the slower it will be.\n\nThis one though seems like we should fix it...\n\nshow=schema\n* 18 seconds after above warmup queries\n\n...that's really weird.  i havn't looked at the code, but skimming the show=schema output again i realized that for some reason that mode computes the total number of terms in the index for you (which must be a full walk of hte TermEnum) ... so we should definitley fix that up ... counting the terms is expensive enough it should definitely require it's own param.\n\nIt needs a new show=schema option that does not scan for dynamic fields\n\n...i'm not following you there.  listing hte dynamic field declarations should be dirty cheap (listing the \"real\" fields that result from those dynamic fields could be costly if you have millions of them, but that doesn't seem to be what you're refering to .. like i said though: if you are in that boat, the schema browser is never oging to be useful for you.) "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12873225",
            "date": "2010-05-29T02:43:46+0000",
            "content": "\n...that's really weird. i havn't looked at the code, but skimming the show=schema output again i realized that for some reason that mode computes the total number of terms in the index for you (which must be a full walk of hte TermEnum) ... so we should definitley fix that up ... counting the terms is expensive enough it should definitely require it's own param.\nCool! That's one down.\n..i'm not following you there. listing hte dynamic field declarations should be dirty cheap (listing the \"real\" fields that result from those dynamic fields could be costly if you have millions of them, but that doesn't seem to be what you're refering to .. like i said though: if you are in that boat, the schema browser is never oging to be useful for you.)\nThis was me being confused. Never mind. \n\nThe drill-down interface should be able to ask for # of terms for a field. The above enhancement to show=schema (with a per-field option) supplies everything needed for a fully drill-down index scanner that scales up to zillions of docs. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12873356",
            "date": "2010-05-30T01:39:30+0000",
            "content": "I can't find where the index is scanned. If any of you can point me in the right direction, please do. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12873381",
            "date": "2010-05-30T05:01:33+0000",
            "content": "I can't find where the index is scanned\n\ntake a look at  getIndexInfo and it's \"countTerms\" boolean\n\nby the looks of how it's used, it seems like the assumption was that if the numTerms param != 0, then you're clearly okay with it taking some time, so it might as well compute the total number of terms in the index for you \u2013 but the default value of numTerms is \"20\", so in the \"/admin/luke?show=schema\" case, it computes the total number of terms even though it isn't finding top terms for any field.\n\nif we just add an explicit \"countTerms\" boolean param that should make it possible to speed things up a lot. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13176796",
            "date": "2011-12-28T20:21:25+0000",
            "content": "In the trunk (4.x) version, (from Muir) below. I haven't looked at this yet, but being able to get some approximation back from Luke quickly would be a big help. Maybe we can make this happen on trunk?\n\nThe use-case I'm interested in is the one in which we're really only looking for outrageous numbers of unique terms. Having unique terms per segment would go a long way towards that use-case.\n\n*******\nIs it really necessary to see the 'top level' number of distinct terms\nsummed across all segments?\nMaybe its good enough to list the information on a per-segment basis.\nThen it would always be instant-fast:\n\nyou would just use FieldsEnum api to list all the fields, and for each\nfield call .terms() and then Terms.getUniqueTermCount()\n\nNote: getUniqueTermCount won't work (returns -1) for any 3.x segments\nthat haven't yet been upgraded to the 4.0 format.\nThe old 3.x format only allows you to get the uniqueTermCount across\nall fields in the segment (Fields.getUniqueTermCount), because fields\nare not clearly separated.\n\n\t\n\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13176961",
            "date": "2011-12-29T03:00:51+0000",
            "content": "Is that actually true?  What if one is looking at a completely optimized index? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13177153",
            "date": "2011-12-29T12:50:23+0000",
            "content": "bq: What if one is looking at a completely optimized index?\n\nI wondered about that myself, and I suspect this would work just as you indicate, optimizing the index would get you the exact unique counts for all the fields. Which conveniently leaves it up to the user to decide just how necessary getting exact information is....\n\nHere's a code snippet from Muir (thanks!!!) that we should preserve, 4.x only.\n    new ReaderUtil.Gather(reader) {\n      @Override\n      protected void add(int base, IndexReader r) throws IOException {\n        System.out.println(\"segment: \" + r.toString());\n        FieldsEnum e = r.fields().iterator();\n        String field;\n        while ((field = e.next()) != null) \n{\n          System.out.println(\"\\t\" + field + \": \" +\ne.terms().getUniqueTermCount());\n        }\n      }\n    }.run();\n\nsegment: _34(4.0):C1802000/89498\n        body: 4886489\n        date: 136729\n        datenum: 631685\n        group100: 100\n        group100K: 100000\n        group10K: 10000\n        group1M: 999999\n        groupblock: 180200\n        groupend: 1\n        id: 1802000\n        timesecnum: 73524\n        title: 139038\n        titleTokenized: 73144\nsegment: _67(4.0):C1802000/89561\n        body: 4985143 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13178021",
            "date": "2011-12-31T14:34:58+0000",
            "content": "Well, there are a couple of issues here. I've attached patches for trunk and 3x for consideration.\n\nI fixed a structural flaw that traversed all the terms in all the fields twice, once to get the total number of terms across all the fields and once to get the individual counts.\n\nBut that's not where the bulk of the time gets spent. It turns out that getting the count of documents in which each term appears is the culprit. These two lines are executed for each field\n  Query q = new TermRangeQuery(fieldName, null, null, false, false);\n  TopDocs top = searcher.search(q, 1);\n\nand top.totalHits is reported. I have an index with 99M documents, mostly integer data that takes 360 seconds to return data when the above is executed and 150 without. Both versions traverse all the terms once, so these times would be greater without the patch due to the second traversal.\n\nSo the attached patches default to NOT doing the above and there's a new parameter reportDocCount that can be set to true to collect that information. What do people think? And is there a better way to get the count of documents in which the term appears? And do any alternate methods respect deleted docs like this one does?\n\nI tried spinning through using TermDocs (3.6) but soon realized that the people who wrote TermRangeQuery probably got there first.\n\nSo I guess my real question is whether people object to the change in behavior, that users must explicitly request doc counts. Which also means that the admin/schema browser doesn't report this by default and I haven't made it optional from that interface. I'm not inclined to since that interface is going away, but if people feel strongly I might be persuaded. That info is available by admin/luke?fl=myfield&reportDocCount=true in a less painful fashion for a particular field anyway.\n\nAlong the way I alphabetized the fields without my other kludge of putting comparators in other classes. I'll kill that JIRA if this one goes forward.\n\nNote that this still doesn't scale all that well, on my test index it's still a 5 minute wait. But then I guess that this kind of data gathering will take time by its nature.\n\nIf nobody objects, I'll commit this early next week after I've had a chance to put it down for a while and look at it with fresh eyes and do some more testing. I think there's some inefficiencies in the single pass that I can wring out (about 30 seconds is spent just gathering the data in the single term enumeration loop). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13178034",
            "date": "2011-12-31T16:03:26+0000",
            "content": "And is there a better way to get the count of documents in which the term appears?\n\nIn which any term for the field appears?  In trunk, there is Terms.getDocCount() "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13178598",
            "date": "2012-01-03T02:24:21+0000",
            "content": "Thanks Robert and Yonik for pointing me at the new 4x capabilities, they make a huge difference. But you knew that.\n\nThe killer for 3.x was getting the document counts via a range query, I don't think there's a good way to get the counts and not pay the penalty, so there's a new parameter recordDocCounts.\n\nHere's my latest and close-to-last cut at this, both for 3x and 4x.\n\nThe data set is 89M documents, times in seconds.\n\n3.5 \n637 getting doc counts\n\n\n3x with this patch\n552 getting doc counts\n 53 Stats without doc counts, but\n    histogram etc. No option to do \n    this before.\n\n4x, original\n450 or so as I remember, getting doc\n    counts, histograms, etc..\n\n4x with patch, histograms still work.\n158 Getting the doc counts the old way\n   (span queries). I mean,\n    you guys said ranges were going \n    to be faster.\n 39 Getting the doc counts with\n    terms.getDocCount(). \n    (including histograms)\n\n\nHere's my proposal, I'll probably commit this next weekend at the latest unless there are objections:\n\n1> I'll let these stew for a couple of days, and look them over again. Anyone who wants to look too, please feel free.\n\n2> Live with getting the doc counts in 4x including the deleted docs and remove the reportDocCounts parameter (it'll live in 3.6 and other 3x versions). I think the performance is fine without carrying that kind of kludgy option forward. I could be persuaded otherwise, but an optimized index will take care of the counting of deleted documents problem if anyone really cares. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13178602",
            "date": "2012-01-03T02:40:03+0000",
            "content": "why is it still 39seconds? shouldn't tools like this just use statistics and not enumerate terms or any anything else by default so that they return instantly?\n\nits 4.0, why not just backwards break and make it fast?\n\nInstead of doing enumerations and stuff, you could display all of the Terms-level statistics per segment per field: \n\n\tuniqueTermCount (# of terms)\n\tsumDocFreq (# of postings/term-doc mappings)\n\tsumTotalTermFreq (# of positions/tokens)\n\tdocCount (# of documents with at least one posting for the field)\n\n\n\nThis would all be basically instantaneous and would give a more thorough picture of the performance characteristics of the index (e.g. how many positions).\nYou could also compute derived stats like average field length etc too. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13178608",
            "date": "2012-01-03T03:18:50+0000",
            "content": "bq: why is it still 39 seconds?\n\nHistograms and collecting the top N terms by frequency. Still gotta spin through all the terms to collect either statistic. Take that bit out and the response is less than 0.5 seconds.\n\n39 seconds isn't bad at all for an index this size, and one can still specify particular fields of interest if the index is more complex than this one. I can probably be argued out of their importance although it'll take a little doing. This is really for, from my perspective, troubleshooting at a high level and that information is valuable.\n\nBesides, I told you I had to look it over after a while. I just saw something horribly trivial that cuts it down to 15 seconds. There's a loop where, after the histo stuff is collected, we test to see if the current term frequency is above the threshold of the already-collected items.... and changing it from\n\nif (freq < tiq.minfreq) continue;\nto, essentially, \nif (freq <= tiq.minfreq) continue;\n\nmeans that the pathological case of inserting every last <uniqueKey> in the tracking priority queue doesn't happen. Siiigggh.\n\nOh, and the patch I'll attach in a couple of minutes actually compiles. I half cleaned up the stupid recordDocCount parameter by removing the definition, but not getting it from the parameters. Fella has to go to sleep more often.\n\nAlso, this index is a little peculiar in that many of the fields have only a very few values so YMMV.\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13178609",
            "date": "2012-01-03T03:21:19+0000",
            "content": "Trunk that, you know, actually compiles or something, mea culpa.\n\nAlso reduces the 4x time down to 15 seconds after fixing a stupid oversight. Really gotta let this stew for a while and look at it with less-tired eyes. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13180132",
            "date": "2012-01-05T02:24:07+0000",
            "content": "Final patches attached. All honor unto whoever wrote the tests for the binary writers, I discovered that a TreeMap is unacceptable. In other words, all the tests pass now.\n\nUnless there are objections, I intend to commit these tomorrow or Friday. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13181025",
            "date": "2012-01-06T01:26:19+0000",
            "content": "Fixed in:\ntrunk: 1227924\n3.x:   1227926 "
        }
    ]
}