{
    "id": "LUCENE-8524",
    "title": "Nori (Korean) analyzer tokenization issues",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "status": "Resolved",
        "resolution": "Fixed",
        "fix_versions": [
            "7.6",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Major",
        "type": "Bug"
    },
    "description": "I opened this originally as an\u00a0Elastic bug, but was asked to re-file it here. (Sorry for the poor formatting. \"pre-formatted\" isn't behaving.)\n\nElastic version\n\n{\n \"name\" : \"adOS8gy\",\n \"cluster_name\" : \"elasticsearch\",\n \"cluster_uuid\" : \"GVS7gpVBQDGwtHl3xnJbLw\",\n \"version\" : \n{\n \"number\" : \"6.4.0\",\n \"build_flavor\" : \"default\",\n \"build_type\" : \"deb\",\n \"build_hash\" : \"595516e\",\n \"build_date\" : \"2018-08-17T23:18:47.308994Z\",\n \"build_snapshot\" : false,\n \"lucene_version\" : \"7.4.0\",\n \"minimum_wire_compatibility_version\" : \"5.6.0\",\n \"minimum_index_compatibility_version\" : \"5.0.0\"\n }\n,\n \"tagline\" : \"You Know, for Search\"\n}\n\n\n Plugins installed: [analysis-icu, analysis-nori]\n\nJVM version:\n openjdk version \"1.8.0_181\"\n OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1~deb9u1-b13)\n OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n\nOS version:\n Linux vagrantes6 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\n\nDescription of the problem including expected versus actual behavior:\n\nI've uncovered a number of oddities in tokenization in the Nori analyzer. All examples are from Korean Wikipedia\u00a0or Korean Wiktionary\u00a0(including non-CJK examples). In rough order of importance:\n\nA. Tokens are split on different character POS types (which seem to not quite line up with Unicode character blocks), which leads to weird results for non-CJK tokens:\n\n\t\u03b5\u1f30\u03bc\u03af is tokenized as three tokens: \u03b5/SL(Foreign language) + \u1f30/SY(Other symbol) + \u03bc\u03af/SL(Foreign language)\n\tka\u0320k\u031at\u0361\u0255\u0348a\u0320k\u031a is tokenized as ka/SL(Foreign language) + \u0320/SY(Other symbol) + k/SL(Foreign language) + \u031a/SY(Other symbol) + t/SL(Foreign language) + \u0361\u0255\u0348/SY(Other symbol) + a/SL(Foreign language) + \u0320/SY(Other symbol) + k/SL(Foreign language) + \u031a/SY(Other symbol)\n\t\u0411\u0430\u0300\u043b\u0442\u0438\u0447\u043a\u043e\u0304 is tokenized as \u0431\u0430/SL(Foreign language) + \u0300/SY(Other symbol) + \u043b\u0442\u0438\u0447\u043a\u043e/SL(Foreign language) + \u0304/SY(Other symbol)\n\tdon't is tokenized as don + t; same for don\u2019t (with a curly apostrophe).\n\t\u05d0\u05d5\u05b9\u05d2\u05f3\u05d5\u05bc is tokenized as \u05d0\u05d5\u05b9\u05d2/SY(Other symbol) + \u05d5\u05bc/SY(Other symbol)\n\t\u041coscow (with a Cyrillic \u041c and the rest in Latin) is tokenized as \u043c + oscow\n\n\n\nWhile it is still possible to find these words using Nori, there are many more chances for false positives when the tokens are split up like this. In particular, individual numbers and combining diacritics are indexed separately (e.g., in the Cyrillic example above), which can lead to a performance hit on large corpora like Wiktionary or Wikipedia.\n\nWork around: use a character filter to get rid of combining diacritics before Nori processes the text. This doesn't solve the Greek, Hebrew, or English cases, though.\n\nSuggested fix: Characters in related Unicode blocks\u2014like \"Greek\" and \"Greek Extended\", or \"Latin\" and \"IPA Extensions\"\u2014should not trigger token splits. Combining diacritics should not trigger token splits. Non-CJK text should be tokenized on spaces and punctuation, not by character type shifts. Apostrophe-like characters should not trigger token splits (though I could see someone disagreeing on this one).\n\nB. The character \"arae-a\" (\u318d, U+318D) is sometimes used instead of a middle dot (\u00b7, U+00B7) for lists. When the arae-a is used, everything after the first one ends up in one giant token. \ub3c4\ub85c\u318d\uc9c0\ubc18\u318d\uc218\uc790\uc6d0\u318d\uac74\uc124\ud658\uacbd\u318d\uac74\ucd95\u318d\ud654\uc7ac\uc124\ube44\uc5f0\uad6c is tokenized as \ub3c4\ub85c + \u318d\uc9c0\ubc18\u318d\uc218\uc790\uc6d0\u318d\uac74\uc124\ud658\uacbd\u318d\uac74\ucd95\u318d\ud654\uc7ac\uc124\ube44\uc5f0\uad6c.\n\n\tNote that \"HANGUL LETTER ARAEA\" (\u318d, U+318D) is used this way, while \"HANGUL JUNGSEONG ARAEA\" (\u119e, U+119E) is used to create syllable blocks for which there is no precomposed Unicode character.\n\n\n\nWork around: use a character filter to convert arae-a (U+318D) to a space.\n\nSuggested fix: split tokens on all instances of arae-a (U+318D).\n\nC. Nori splits tokens on soft hyphens (U+00AD) and zero-width non-joiners (U+200C), splitting tokens that should not be split.\n\n\thyphen\u00adation (with a soft hyphen in the middle) is tokenized as hyphen + ation.\n\t\u0628\u0627\u0632\u06cc\u200c\u0647\u0627\u06cc  (with a zero-width non-joiner) is tokenized as \u0628\u0627\u0632\u06cc + \u0647\u0627\u06cc.\n\n\n\nWork around: use a character filter to strip soft hyphens and zero-width non-joiners before Nori.\n\nSuggested fix: Nori should strip soft hyphens and zero-width non-joiners.\n\nD. Analyzing \uadf8\ub808\uc774\ub9e8 generates an extra empty token after it. There may be others, but this is the only one I've found. Work around: at a min length token filter with a minimum length of 1.\n\nE. Analyzing \ud29c\ud1a0\ub9ac\uc5bc generates a token with an extra space at the end of it. There may be others, but this is the only one I've found. No work around needed, I guess, since this is only the internal representation of the token. I'm not sure if it has any negative effects.\n\nSteps to reproduce:\n\n1. Set up Nori analyzer\n\ncurl -X PUT \"localhost:9200/nori?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"settings\" : {\n    \"index\": {\n      \"analysis\": {\n        \"analyzer\": {\n          \"text\": \n{\n            \"type\": \"nori\"\n          }\n        }\n      }\n    }\n  }\n}\n'\n\u00a0\n\n2. Analyze example tokens:\n\nA. POS Types cause token splits\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\u03b5\u1f30\u03bc\u03af\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\u03b5\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 1,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u1f30\",\n          \"start_offset\" : 1,\n          \"end_offset\" : 2,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        {\n          \"token\" : \"\u03bc\u03af\",\n          \"start_offset\" : 2,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 2,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n      ]\n    }\n  }\n\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"ka\u0320k\u031at\u0361\u0255\u0348a\u0320k\u031a\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"ka\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 2,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u0320\",\n          \"start_offset\" : 2,\n          \"end_offset\" : 3,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        \n{\n          \"token\" : \"k\",\n          \"start_offset\" : 3,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 2,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u031a\",\n          \"start_offset\" : 4,\n          \"end_offset\" : 5,\n          \"type\" : \"word\",\n          \"position\" : 3,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        \n{\n          \"token\" : \"t\",\n          \"start_offset\" : 5,\n          \"end_offset\" : 6,\n          \"type\" : \"word\",\n          \"position\" : 4,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u0361\u0255\u0348\",\n          \"start_offset\" : 6,\n          \"end_offset\" : 9,\n          \"type\" : \"word\",\n          \"position\" : 5,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        \n{\n          \"token\" : \"a\",\n          \"start_offset\" : 9,\n          \"end_offset\" : 10,\n          \"type\" : \"word\",\n          \"position\" : 6,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u0320\",\n          \"start_offset\" : 10,\n          \"end_offset\" : 11,\n          \"type\" : \"word\",\n          \"position\" : 7,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        \n{\n          \"token\" : \"k\",\n          \"start_offset\" : 11,\n          \"end_offset\" : 12,\n          \"type\" : \"word\",\n          \"position\" : 8,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        {\n          \"token\" : \"\u031a\",\n          \"start_offset\" : 12,\n          \"end_offset\" : 13,\n          \"type\" : \"word\",\n          \"position\" : 9,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n      ]\n    }\n  }\n}\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\u0411\u0430\u0300\u043b\u0442\u0438\u0447\u043a\u043e\u0304\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\u0431\u0430\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 2,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        \n{\n          \"token\" : \"\u0300\",\n          \"start_offset\" : 2,\n          \"end_offset\" : 3,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        \n{\n          \"token\" : \"\u043b\u0442\u0438\u0447\u043a\u043e\",\n          \"start_offset\" : 3,\n          \"end_offset\" : 9,\n          \"type\" : \"word\",\n          \"position\" : 2,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        {\n          \"token\" : \"\u0304\",\n          \"start_offset\" : 9,\n          \"end_offset\" : 10,\n          \"type\" : \"word\",\n          \"position\" : 3,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n      ]\n    }\n  }\n}\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"don'\"'\"'t\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"don\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 3,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        {\n          \"token\" : \"t\",\n          \"start_offset\" : 4,\n          \"end_offset\" : 5,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n      ]\n    }\n  }\n}\n\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"don\u2019t\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"don\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 3,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        {\n          \"token\" : \"t\",\n          \"start_offset\" : 4,\n          \"end_offset\" : 5,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n      ]\n    }\n  }\n}\n\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\u05d0\u05d5\u05b9\u05d2\u05f3\u05d5\u05bc\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\u05d0\u05d5\u05b9\u05d2\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        {\n          \"token\" : \"\u05d5\u05bc\",\n          \"start_offset\" : 5,\n          \"end_offset\" : 7,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n      ]\n    }\n  }\n}\n\nB. arae-a as middle dot\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\ub3c4\ub85c\u318d\uc9c0\ubc18\u318d\uc218\uc790\uc6d0\u318d\uac74\uc124\ud658\uacbd\u318d\uac74\ucd95\u318d\ud654\uc7ac\uc124\ube44\uc5f0\uad6c\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\ub3c4\ub85c\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 2,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"NNG(General Noun)\"\n        }\n,\n        {\n          \"token\" : \"\u318d\uc9c0\ubc18\u318d\uc218\uc790\uc6d0\u318d\uac74\uc124\ud658\uacbd\u318d\uac74\ucd95\u318d\ud654\uc7ac\uc124\ube44\uc5f0\uad6c\",\n          \"start_offset\" : 2,\n          \"end_offset\" : 24,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"UNKNOWN(Unknown)\"\n        }\n      ]\n    }\n  }\n}\n\nC. soft hyphens and zero-width non-joiners\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"hyphen\u00adation\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"hyphen\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 6,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n,\n        {\n          \"token\" : \"ation\",\n          \"start_offset\" : 7,\n          \"end_offset\" : 12,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SL(Foreign language)\"\n        }\n      ]\n    }\n  }\n}\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\u0628\u0627\u0632\u06cc\u200c\u0647\u0627\u06cc\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\u0628\u0627\u0632\u06cc\",\n          \"start_offset\" : 0,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n,\n        {\n          \"token\" : \"\u0647\u0627\u06cc\",\n          \"start_offset\" : 5,\n          \"end_offset\" : 8,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"SY(Other symbol)\"\n        }\n      ]\n    }\n  }\n}\n\n\n\u00a0\n\nD. \uadf8\ub808\uc774\ub9e8 generates empty token\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\uadf8\ub808\uc774\ub9e8\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        \n{\n          \"token\" : \"\uadf8\ub808\uc774\",\n          \"start_offset\" : 1,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"NNG(General Noun)\"\n        }\n,\n        {\n          \"token\" : \"\",\n          \"start_offset\" : 4,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 1,\n          \"leftPOS\" : \"NNG(General Noun)\"\n        }\n      ]\n    }\n  }\n}\n\n\nE. \ud29c\ud1a0\ub9ac\uc5bc has a space added during tokenization\n\ncurl -sk localhost:9200/nori/_analyze?pretty -H 'Content-Type: application/json' -d '\n{\"analyzer\": \"text\", \"text\" : \"\ud29c\ud1a0\ub9ac\uc5bc\", \"attributes\" : [\"leftPOS\"], \"explain\": true }\n'\n\n{\n  \"detail\" : {\n    \"custom_analyzer\" : false,\n    \"analyzer\" : {\n      \"name\" : \"text\",\n      \"tokens\" : [\n        {\n          \"token\" : \"\ud29c\ud1a0\ub9ac\uc5bc \",\n          \"start_offset\" : 0,\n          \"end_offset\" : 4,\n          \"type\" : \"word\",\n          \"position\" : 0,\n          \"leftPOS\" : \"NNG(General Noun)\"\n        }\n      ]\n    }\n  }\n}",
    "attachments": {
        "LUCENE-8524.patch": "https://issues.apache.org/jira/secure/attachment/12945190/LUCENE-8524.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16639118",
            "author": "Tomoko Uchida",
            "content": "I have not looked closely this yet, so it is my intuition rather than strong opinion...\n\nAbout problem A:\n\nI think this is not a problem of tokenizer itself but the built-in dictionary.\n Nori includes mecab-ko-dic (https://bitbucket.org/eunjeon/mecab-ko-dic) as built-in dectionary, it is a derivative of MeCab IPADIC (https://sourceforge.net/projects/mecab/), a widely used Japanese dictionary.\n JapaneseTokeniezr (a.k.a Kuromoji), this includes mecab ipadic, behaves in the same manner. In fact, original MeCab does not handle such non-CJK tokens correnctly.\n\nI cannot say it is a fault of MeCab IPADIC, it was originally built for handling Japanse texts, before Unicode era.\n But we can apply patches to the dictionary seed (CSVs) when building the dictionary.\n A patch to\u00a0`seed/char.def` file (and also unk.def if needed,) it is used for \"unknown word\" handling, is sufficient, I think. ",
            "date": "2018-10-05T01:09:48+0000"
        },
        {
            "id": "comment-16660378",
            "author": "Jim Ferenczi",
            "content": "Sorry for the late reply, D and E are bugs due to invalid rules in the mecab-ko-dic. I'll provide a patch shortly.\n\nI was not aware of B but it should be easy to add the interpunct as a separator.\n\nA can be discussed but I think it needs a separate issue since this is more a feature than a bug. This is a design choice and I am not sure that splitting is really an issue here. We could add a mode that join multiple alphabet together but it's not a major concern since this mixed terms should appear very rarely.\n\nRegarding C, IMO it's a normalization issue, if you don't want to break on this character you should remove it with a char filter. ",
            "date": "2018-10-23T10:06:33+0000"
        },
        {
            "id": "comment-16660382",
            "author": "Jim Ferenczi",
            "content": "Here is a patch to fix D and E (invalid empty terms and trailing spaces). It also adds the\u00a0Hangul Letter Araea as a separator. ",
            "date": "2018-10-23T10:08:51+0000"
        },
        {
            "id": "comment-16662833",
            "author": "Trey Jones",
            "content": "A can be discussed but I think it needs a separate issue since this is more a feature than a bug. This is a design choice and I am not sure that splitting is really an issue here. We could add a mode that join multiple alphabet together but it's not a major concern since this mixed terms should appear very rarely.\nAll of the examples except for the \u041coscow\u00a0one are taken from Korean Wikipedia or Wiktionary, so they do occur. Out of a sample of 10,000 random Korean Wikipedia articles (with ~2.4M tokens), 100 Cyrillic and 126 Greek tokens were affected. An additional 2758 ID-like tokens (e.g., BH115E) were affected. 96 Phonetic Alphabet tokens were affected. 769 tokens with apostrophes were affected, too; most were possessives with \u2019s, but also included were words like An'gorso, Na\u2019vi, and O'Donnell. Out of 2.4M tokens, these are rare, but there are still a lot of them\u2014especially when you scale up 10K sample 43x to the full 430K articles on Wikipedia.\n\nIt's definitely seems like a bug that a Greek word like \u03b5\u1f30\u03bc\u03af gets split into three tokens, or \u0411\u0430\u0300\u043b\u0442\u0438\u0447\u043a\u043e\u0304 gets split into four. The Greek seems to be the worse case, since \u1f30 is in the \u201cGreek Extended\u201d Unicode block while the rest are \u201cGreek and Coptic\u201d block, which aren\u2019t really different character sets.\n\nThanks for fixing B, D, and E! ",
            "date": "2018-10-24T20:58:19+0000"
        },
        {
            "id": "comment-16664372",
            "author": "Lucene/Solr QA",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 2 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   0m 35s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   0m 29s \n  the patch passed  \n\n\n +1 \n  javac  \n   0m 29s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   0m 29s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   0m 29s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   0m 29s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n -1 \n  unit  \n   0m 24s \n  nori in the patch failed.  \n\n\n  \n   \n   3m 38s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n lucene.analysis.ko.dict.TestTokenInfoDictionary \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n LUCENE-8524 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12945190/LUCENE-8524.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  \n\n\n uname \n Linux lucene1-us-west 4.4.0-137-generic #163~14.04.1-Ubuntu SMP Mon Sep 24 17:14:57 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-LUCENE-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / 8d10939 \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on July 24 2018 \n\n\n Default Java \n 1.8.0_172 \n\n\n unit \n https://builds.apache.org/job/PreCommit-LUCENE-Build/112/artifact/out/patch-unit-lucene_analysis_nori.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-LUCENE-Build/112/testReport/ \n\n\n modules \n C: lucene/analysis/nori U: lucene/analysis/nori \n\n\n Console output \n https://builds.apache.org/job/PreCommit-LUCENE-Build/112/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "date": "2018-10-25T22:41:01+0000"
        },
        {
            "id": "comment-16664882",
            "author": "ASF subversion and git services",
            "content": "Commit 6f291d402b93ca534eccfef620fa392d0cd2b892 in lucene-solr's branch refs/heads/master from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6f291d4 ]\n\nLUCENE-8524: Add the Hangul Letter Araea (interpunct) as a separator in Nori's tokenizer.\nThis change also removes empty terms and trim surface form in Nori's Korean dictionary. ",
            "date": "2018-10-26T08:30:09+0000"
        },
        {
            "id": "comment-16664887",
            "author": "ASF subversion and git services",
            "content": "Commit 403babcfd6d024affc8afad00f8fb78c07053e82 in lucene-solr's branch refs/heads/branch_7x from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=403babc ]\n\nLUCENE-8524: Add the Hangul Letter Araea (interpunct) as a separator in Nori's tokenizer.\nThis change also removes empty terms and trim surface form in Nori's Korean dictionary. ",
            "date": "2018-10-26T08:32:48+0000"
        },
        {
            "id": "comment-16664890",
            "author": "Jim Ferenczi",
            "content": "Thanks Trey Jones\u00a0! I opened https://issues.apache.org/jira/browse/LUCENE-8548\u00a0to continue the discussion around scripts boundary breaks. ",
            "date": "2018-10-26T08:33:38+0000"
        }
    ]
}