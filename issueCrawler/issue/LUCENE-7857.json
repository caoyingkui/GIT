{
    "id": "LUCENE-7857",
    "title": "CharTokenizer-derived tokenizers and KeywordTokenizer emit multiple tokens when the max length is exceeded",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [],
        "fix_versions": [
            "6.7",
            "7.0"
        ]
    },
    "description": "Assigning to myself to not lose track of it.\n\nLUCENE-7705 introduced the ability to define the allowable token length for these tokenizers other than hard-code it to 255. It's always been the case that when the hard-coded limit was exceeded, multiple tokens would be emitted. However, the tests for LUCENE-7705 exposed a problem.\n\nSuppose the max length is 3 and the doc contains \"letter\". Two tokens are emitted and indexed: \"let\" and \"ter\".\n\nNow suppose the search is for \"lett\". If the default operator is AND or phrase queries are constructed the query fails since the tokens emitted are \"let\" and \"t\". Only if the operator is OR is the document found, and even then it won't be correct since searching for \"lett\" would match a document indexed with \"bett\" because it would match on the bare \"t\".\n\nProposal: \n\nThe remainder of the token should be ignored when maxTokenLen is exceeded.\n\nRobert MuirSteve RoweTom\u00e1s Fern\u00e1ndez L\u00f6bbe comments? Again, this behavior was not introduced by LUCENE-7705, it's just that it would be very hard to notice with the default 255 char limit.\n\nI'm not quite sure why master generates a parsed query of:\nfield:let field:t\nand 6x generates\nfield:\"let t\"\nso the tests succeeded on master but not on 6x....",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16030373",
            "date": "2017-05-30T23:40:13+0000",
            "content": "my opinion: behavior should be consistent with StandardTokenizer & co.\n\nI don't think we should do heroic efforts to do great things with too-long tokens. If someone wants maxTokenLen of 3 or something, then i think its better to look at n-grams for that case. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16030459",
            "date": "2017-05-31T01:07:58+0000",
            "content": "I agree with Robert.\n\nSee my answer to a question about why StandardTokenizer effectively splits tokens that are longer than maxTokenLength in this recent java-user mailing list thread: https://lists.apache.org/thread.html/42af955be9522cff0d28b47d7fa723d90846ad011157503fcf687f99@%3Cjava-user.lucene.apache.org%3E.\n\nThe workaround I outlined on that thread would work here too: set maxTokenLength super-high, then use LengthFilter to remove tokens longer than what you want to keep. ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-16031588",
            "date": "2017-05-31T17:51:11+0000",
            "content": "OK, then to paraphrase:\n\nThe behavior is correct, fix the tests  ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16036158",
            "date": "2017-06-04T03:21:36+0000",
            "content": "Fixed by a test fix patch in LUCENE-7705 ",
            "author": "Erick Erickson"
        }
    ]
}