{
    "id": "LUCENE-5112",
    "title": "FilteringTokenFilter is double incrementing the position increment in incrementToken",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [],
        "affect_versions": "4.0",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Not A Problem",
        "status": "Closed"
    },
    "description": "The following code from FilteringTokenFilter#incrementToken() seems wrong.\n\n    if (enablePositionIncrements) {\n      int skippedPositions = 0;\n      while (input.incrementToken()) {\n        if (accept()) {\n          if (skippedPositions != 0) {\n            posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement() + skippedPositions);\n          }\n          return true;\n        }\n        skippedPositions += posIncrAtt.getPositionIncrement();\n      }\n    } else {\n\n \nThe skippedPositions variable should probably be incremented by 1 instead of posIncrAtt.getPositionIncrement(). As it is, it seems to be double incrementing, which is a problem if your data is full of stop words and your position increment integer overflows.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2013-07-15T09:01:03+0000",
            "content": "The workaround seems to be to always use setEnablePositionIncrements(false) on any stop filter being used. ",
            "author": "George Rhoten",
            "id": "comment-13708324"
        },
        {
            "date": "2013-07-15T09:14:00+0000",
            "content": "For reference, this issue causes this exception:\n\njava.lang.IllegalArgumentException: position overflow for field 'labels'\n\tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:135)\n\tat org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:307)\n\tat org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:244)\n\tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:373)\n\tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1124)\n\n ",
            "author": "George Rhoten",
            "id": "comment-13708334"
        },
        {
            "date": "2013-07-15T11:06:11+0000",
            "content": "I think the code is correct: we accumulate posInc of all tokens that were not accepted, plus the final posInc of the token that was accepted.  I don't see how this leads to integer overflows when a StopFilter is used ... can you make a contained test showing that? ",
            "author": "Michael McCandless",
            "id": "comment-13708388"
        },
        {
            "date": "2013-07-15T15:53:42+0000",
            "content": "This can happen if a consumer is not calling reset(), either code pulling the tokens or a filter overrides reset but doesn't invoke the superclass reset to pass it down the chain. ",
            "author": "Robert Muir",
            "id": "comment-13708568"
        },
        {
            "date": "2013-07-15T16:03:16+0000",
            "content": "The code is correct. As Robert Muir says - if you don't call reset() before consuming, the overflow might happen. ",
            "author": "Uwe Schindler",
            "id": "comment-13708580"
        },
        {
            "date": "2013-07-15T18:52:04+0000",
            "content": "Calling clearAttributes() at the start of incrementToken() in our custom Tokenizer seems to resolve this issue too. It would be helpful if the purpose of clearAttributes() in incrementToken() for a typical tokenizer was made clearer. This part of the API contract is not very clear. ",
            "author": "George Rhoten",
            "id": "comment-13708810"
        },
        {
            "date": "2013-07-15T19:02:35+0000",
            "content": "Calling clearAttributes() at the start of incrementToken() in our custom Tokenizer seems to resolve this issue too.\n\nThis is mandatory, yes. If you don't do this ugly things can happen. I would suggest that you use BaseTokenStreamTestCase as base class for your tokenizer/tokenfilter tests. This class is part of the lucene-test framework. It will detect such errors. ",
            "author": "Uwe Schindler",
            "id": "comment-13708829"
        }
    ]
}