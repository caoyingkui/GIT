{
    "id": "SOLR-1306",
    "title": "Support pluggable persistence/loading of solr.xml details",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.1"
        ],
        "components": [
            "multicore"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "Persisting and loading details from one xml is fine if the no:of cores are small and the no:of cores are few/fixed . If there are 10's of thousands of cores in a single box adding a new core (with persistent=true) becomes very expensive because every core creation has to write this huge xml. \n\nMoreover , there is a good chance that the file gets corrupted and all the cores become unusable . In that case I would prefer it to be stored in a centralized DB which is backed up/replicated and all the information is available in a centralized location. \n\nWe may need to refactor CoreContainer to have a pluggable implementation which can load/persist the details . The default implementation should write/read from/to solr.xml . And the class should be pluggable as follows in solr.xml\n\n<solr>\n  <dataProvider class=\"com.foo.FooDataProvider\" attr1=\"val1\" attr2=\"val2\"/>\n</solr>\n\n\nThere will be a new interface (or abstract class ) called SolrDataProvider which this class must implement",
    "attachments": {
        "SOLR-1306.patch": "https://issues.apache.org/jira/secure/attachment/12551374/SOLR-1306.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-12872645",
            "date": "2010-05-27T22:09:50+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12915621",
            "date": "2010-09-28T05:30:59+0000",
            "content": "Another way to solve this is to stop changing solr.xml. Instead, have a directory full of CORENAME.xml which is the solr.xml just for that core. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12915705",
            "date": "2010-09-28T10:47:36+0000",
            "content": "Imagine having 10's of 1000's of  such small files in a directory "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043769",
            "date": "2011-06-03T16:46:58+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106431",
            "date": "2011-09-16T14:51:04+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234750",
            "date": "2012-03-21T18:09:10+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13486978",
            "date": "2012-10-30T16:14:53+0000",
            "content": "MUST be applied after SOLR-1028\n\nOK, here's a preliminary cut at this, no tests yet, but I was looking at logging and it seems to be doing what I want, putting up for inspection by the curious...\n\nA couple of notes:\n1> It turns out that to make this work I needed to incorporate SOLR-4013 and SOLR-3980. I'd appreciate anyone looking at the synchronization I did around the member variable \"loadingCores\". The intent here is to keep two threads from creating the same core at the same time.\n1a> I'm assuming that there is exactly one CoreContainer per JVM. Otherwise I don't understand how any of the synchronization works on the member vars....\n1b> Running a simple test things went all to hell in JMX stuff without the synchronization, apparently the multiple thread problem shows up early and often.\n1c> synchronization is always \"interesting\", so the more eyes the better.\n1d> In particular, any good suggestions about bailing out of the sleep loop? Since cores can take quite a while to warm, I'm having a hard time thinking of a good default. I suppose another attribute where the provider is mentioned. There's no reason a custom provider has to be present, so requiring a timeout from the provider doesn't seem workable.\n\n2> I've implemented a trivial CoreDescriptorProvider for a PoC, it's at the bottom. It pre-supposes you have 4 collections, the accompanying Solr.xml is also below.\n\n3> I'm going to put this away for a couple of hours and come back to it with fresh eyes, this copy is purely for the curious/critical...\n\n*****sample custom descriptor provider\npublic class TestCoreContainerProvider implements CoreDescriptorProvider\n{\n  @Override\n  public CoreDescriptor getDescriptor(CoreContainer container, String name) \n{\n    if (! \"collection2\".equals(name) && ! \"collection3\".equals(name) && ! \"collection4\".equals(name)) return null;\n\n    CoreDescriptor cd = new CoreDescriptor(container, name, name); // True hack because I know the dirs are the same as the collection.\n    return cd;\n  }\n\n  @Override\n  public boolean isPersist(String s) \n{\n    return false;\n  }\n\n  @Override\n  public Collection<String> getCoreNames() \n{\n    return new ArrayList<String>(Arrays.asList(\"collection2\", \"collection3\", \"collection4\"));\n  }\n}\n\n\n\t\n\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tsolr.xml. Note no ZK stuff.\n<solr persistent=\"false\" sharedLib=\"../../../../../blahblah/out/artifacts/provider_jar\">\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n  <cores adminPath=\"/admin/cores\" defaultCoreName=\"collection1\" host=\"${host:}\" hostPort=\"${jetty.port:}\" hostContext=\"${hostContext:}\" zkClientTimeout=\"${zkClientTimeout:15000}\" coreDescriptorProviderClass=\"blah.TestCoreContainerProvider\">\n    <core name=\"collection1\" instanceDir=\"collection1\" />\n  </cores>\n</solr> "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13487025",
            "date": "2012-10-30T17:13:35+0000",
            "content": "I should add that I know that about 10 tests fail, I'll look into them this afternoon. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13487763",
            "date": "2012-10-31T13:26:17+0000",
            "content": "Hmmm. Looking at this a little more, the isPersist method looks like one of those ideas that doesn't work. Originally I thought it would be a way to manipulate the persisted values in solr.xml, but there's a chicken-and-egg problem here.\n\nThere's no provision for getting a CoreDescriptor unless the core is loaded. And persisting a core that has not been loaded seems like a bad idea. So I'm removing the method from the interface. My thinking now is that anything provided by a CoreDescriptorProvider should just NOT be persisted.\n\nWe can revisit this later if there's a demonstrated need, but for the nonce I think it's unwise to conflate persistence with this. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13488171",
            "date": "2012-10-31T19:58:41+0000",
            "content": "Current progress. All the failing tests pass, although I'm getting a failure that also fails without this patch that I'm looking into.\n\nThis removes the silly lots_of_solr_cores.xml I was using, cleans up a number of details. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13490652",
            "date": "2012-11-05T14:21:40+0000",
            "content": "Patch adding multi-thread core creation that insures that multiple calls to create the same core all return the same object. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13491619",
            "date": "2012-11-06T17:10:26+0000",
            "content": "took out some extraneous crud that made it into the last patch.\n\nWhen creating a custom core descriptor, the following changes need to be made to solr.xml:\n1> add a sharedLib directive to the <solr> tag to a directory containing the your custom jar\n2> add coreDescriptorProviderClass to the <cores> tag. Here's an example:\n\n<solr persistent=\"false\" sharedLib=\"../../../../../your/path/here/\">\n\n<cores [all the other opts] coreDescriptorProviderClass=\"your.company.TheCoreDescriptorProvider /> "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13491998",
            "date": "2012-11-07T00:54:43+0000",
            "content": "Fix for problem when no CoreDescriptorProvider was supplied but a bunch of cores were specified as loadOnStartup=\"false\". CoreContainer.getCoreNames was not returning these cores. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13494148",
            "date": "2012-11-09T17:36:42+0000",
            "content": "Last fix broke some tests, this fixes them. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13496145",
            "date": "2012-11-13T12:38:33+0000",
            "content": "I'm thinking of committing this this weekend (to trunk, not 4.x yet) unless people object. I want to write a stress test and bang away at this thing first, and reconcile the CoreDescriptorProvider I came up with with the one already in there for Zookeeper.\n\nLet me know\nErick "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13496241",
            "date": "2012-11-13T15:05:00+0000",
            "content": "At first blush, this seems to go in the wrong direction.\nRather than keep meta-data about a core/directory further away from the actual index for that directory, it seems like we should move it closer (i.e. the meta-data for collection1 should be kept under the collection1 directory or even the collection1/data directory).\n\nWouldn't it be nice to be able to back up a collection/shard by simply copying a single directory?\nThis applies to cloud too - it seems like info about the shard / collection the index belongs to should ride around next to the index.\nOne should be able to bring down two solr servers, move a directory from one server to another, then start back up and have everything just work. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13496370",
            "date": "2012-11-13T18:00:52+0000",
            "content": "Well, the use case here is explicitly that the core information is kept in a completely extra-solr repository (extra ZK too for that matter). Managing 100K cores by moving directories around is non-trivial, especially since there will probably be some system-of-record for where all the information lives anyway.\n\nAs it stands, this patch doesn't really affect the way Solr works OOB. It only comes into play if the people implementing the provider require it (and want to implement the complexity).\n\nBut let me think about this a bit. Are you suggesting that the whole notion of solr.xml be replaced by some kind of crawl/discovery process? Off the top of my head, I can imagine a degenerate solr.xml that just lists one or more directories. Then the load process consists of crawling those directories looking for cores and loading them, possibly with some kind of configuration files at the core level. For the 10s of K cores/machine case we don't want to put the data in solrconfig.xml or anything like that, I'm thinking of something very much simpler, on the order of a java.properties file. I've skipped thinking about how to \"find a core\" or how that plays with using common schemas to see if this is along the lines you're thinking of \"getting meta-data closer to the index\".\n\nIt does make the whole coordination issue a lot easier, though. You no longer have the loose coupling between having core information in solr.xml and then having to be sure the files/dirs corresponding to what's in solr.xml \"just happen\" to map to what's actually on disk.... Moving something from one place to another would consist of\n1> shutting down the servers\n2> moving the core directory from one server to another\n3> starting up the servers again.\n\nI can imagine doing this a bit differently...\n1> copy the core from one server to another\n2> issue an unload for the core on the source server\n3> issue a create for the core on the dest server\n\nThere'd probably have to be some kind of background loading, but we're already talking about parallelizing multicore loads...\n\nFrom an admin perspective, the poor soul trying to maintain this all could pretty easily enumerate where all the cores were just by asking each server for a list of where things are.\n\nAnyway, is the in the vicinity of \"moving the metadata closer to the index\"? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13496496",
            "date": "2012-11-13T20:01:58+0000",
            "content": "I'm with yonik on this one - I think we should drop the top level config (eg solr.xml). Instead, we should auto load folders - no config required, but if you want to override some things, the config lives with the core folder. If you want to be able to place core folders in other locations, we could have a sys prop that added locations. Anything required for settings (like zkHost) would be passed on startup as sys props instead.\n\nYou can still load cores in parallel this way. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13496844",
            "date": "2012-11-14T04:14:45+0000",
            "content": "think we should drop the top level config (eg solr.xml). Instead, we should auto load folders \n+1 \n\nThere are often groups of cores with the same schema- shards in the same solr, for example. How would this dynamic discovery support groups of collections?\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13497010",
            "date": "2012-11-14T10:47:28+0000",
            "content": "Please keep in mind that keeping all cores in the directory is not feasible if you have 10's of K cores. The file system ends up being very slow with that many no:of directories. So, we had to put the cores in 'n' different buckets to overcome the performance issue "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13497055",
            "date": "2012-11-14T12:42:07+0000",
            "content": "But isn't this just another CoreDescriptorProvider? Perhaps the default one? The way the code works, you don't have to define any cores in solr.xml, all you have to do is provide a class for a pluggable descriptor provider. Unless I'm missing something, we could have a default one that enumerated the cores tree and the stock Solr could ship without a solr.xml at all. Maybe back up a step and have a custom CoreContainer instead/too that could be specified when needed as a sysprop when starting Solr?  Or do we just need a few system properties rather than a new class? That would go a long way towards simply nuking the need to ever have a solr.xml at all.\n\nAnd people who had other needs could still do something other/more complex as needed.\n\nBesides Paul's comment about 10s of K cores at the top level (and, Paul, I'm thinking of walking a tree, so not all the top-levels of the cores would be in the same directory), there's still the startup cost in the case I'm working on of enumerating, say, 15K cores on startup. The pluggable core provider allows that cost to be amortized over some period of time and/or fetched from a faster source.\n\nThis is outside the separate issue of lazy core loading, the lazy core cache, etc.\n\nSo I'm not sure I see any incompatibilities here now I've thought about it a bit more. In fact, this patch seems like a step to accomplish that goal. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13497083",
            "date": "2012-11-14T13:21:36+0000",
            "content": "Yes you are right the custom implementation should take care of walking the tree  and identifying the cores. But at the same time we have to keep in mind that deletion of cores will not just happen immediately as as I fire the command. The actual cleanup of the file systems will happen a bit later. So we should have some kind of a marker to say that if that core is actually a live one. \n\nIMHO As much as possible we should avoid completely pluggable solutions , because we clearly know that there are only a couple of scenarios.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13497114",
            "date": "2012-11-14T14:15:20+0000",
            "content": "You must have a solr home for each core anyway - I'm not sure the advantage of having this open ended pluggable point here. Keeping this simple and just doing solrhome auto discovery (with the ability to add multiple root folders to look in) just seems much simpler. I'm not convinced there are use cases that are worth the cost of making all this pluggable?\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13498034",
            "date": "2012-11-15T14:13:15+0000",
            "content": "The use-case has come up on the list for quite a while. Multi-tenant situations that are sparsely used, where the necessary balancing of tenants to machines is carried out by physically moving cores around different machines as usage patterns/core sized change. FWIW, this is not theoretical.\n\nAnd I'm not sure what the complexity issue is. Most of the actual code changes in SOLR-1028 are all about making it possible to lazily load cores and they're more organizational (extracted a couple of methods that kinda makes the diff output look way more scary than it is).\n\nBut I'll tell you what. I'll sweeten the pot with https://issues.apache.org/jira/browse/SOLR-4083 to back up my claim that this work makes nuking solr.xml easier. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13498096",
            "date": "2012-11-15T16:08:33+0000",
            "content": "I still don't understand the motivation. You can do the same thing with the on disk auto discover strategy without adding another plugin point to support and complicating this stuff.\n\nWhat does this plugin gain you over the model of just loading cores we find in folders? You can still physically move cores all you want - in fact, it's probably a lot easier. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13498629",
            "date": "2012-11-16T06:15:21+0000",
            "content": "I agree with Mark. If we move to the model where cores can be discovered from directories it should be just fine "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13498796",
            "date": "2012-11-16T13:10:53+0000",
            "content": "Yeah, I'm reluctantly agreeing with him too <G>....\n\nSee: https://issues.apache.org/jira/browse/SOLR-4083. I did some experiments and on a spinning-disk machine (mac 2009) we can read about 1K configs/second with the cores in 150 directories, 100 cores/dir.\n\nSo I'll be cogitating on this a bit, but this effort may take a sharp left turn soon.\n\nStill doesn't change the need for SOLR-1028 though. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13504938",
            "date": "2012-11-27T20:48:07+0000",
            "content": "See the discussion at SOLR-4083. Rather than a pluggable core descriptor provider, we'll walk the cores directory under with certain rules and discover all the cores present. Simpler that way since the cores need to be physically present anyway in order to be referenced from a pluggable architecture. "
        }
    ]
}