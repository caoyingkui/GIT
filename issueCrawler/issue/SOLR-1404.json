{
    "id": "SOLR-1404",
    "title": "Random failures with highlighting",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "highlighter",
            "Schema and Analysis"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "With a recent Solr nightly, we started getting errors when highlighting.\n\nI have not been able to reduce our real setup to a minimal one that is failing, but the same error seems to pop up with the configuration below. Note that the QUERY will mostly fail, but it will work sometimes. Notably, after running \"java -jar start.jar\", the QUERY will work the first time, but then start failing for a while. Seems that something is not being reset properly.\n\nThe example uses the deprecated HTMLStripWhitespaceTokenizerFactory but the problem apparently also exists with other tokenizers; I was just unable to create a minimal example with other configurations.\n\n\nSCHEMA\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\n<schema name=\"example\" version=\"1.2\">\n\n  <types>\n    <fieldType name=\"string\" class=\"solr.StrField\" />\n\n    <fieldtype name=\"testtype\" class=\"solr.TextField\">\n      <analyzer>\n        <tokenizer class=\"solr.HTMLStripWhitespaceTokenizerFactory\" />\n      </analyzer>\n    </fieldtype>\n </types>\n\n <fields>\n   <field name=\"id\" type=\"string\" indexed=\"true\" stored=\"false\" />\n   <field name=\"test\" type=\"testtype\" indexed=\"false\" stored=\"true\" />\n </fields>\n\n <uniqueKey>id</uniqueKey>\n\n</schema>\n\nINDEX\n\nURL=http://localhost:8983/solr/update\n\ncurl $URL --data-binary '<add><doc><field name=\"id\">1</field><field name=\"test\">test</field></doc></add>' -H 'Content-type:text/xml; charset=utf-8'\ncurl $URL --data-binary '<commit/>' -H 'Content-type:text/xml; charset=utf-8'\n\nQUERY\n\ncurl 'http://localhost:8983/solr/select/?hl.fl=test&hl=true&q=id:1'\n\nERROR\n\norg.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token test exceeds length of provided text sized 4\n\norg.apache.solr.common.SolrException: org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token test exceeds length of provided text sized 4\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:328)\n\tat org.apache.solr.handler.component.HighlightComponent.process(HighlightComponent.java:89)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1299)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n\tat org.mortbay.jetty.Server.handle(Server.java:285)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:821)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:513)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:226)\n\tat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442)\nCaused by: org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token test exceeds length of provided text sized 4\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:254)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:321)\n\t... 23 more",
    "attachments": {
        "SOLR-1404.patch": "https://issues.apache.org/jira/secure/attachment/12418722/SOLR-1404.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Igor Motov",
            "id": "comment-12751797",
            "date": "2009-09-05T22:25:47+0000",
            "content": "First of all, HTMLStripWhitespaceTokenizerFactory is deprecated, so it might be better to just replace it with: HTMLStripCharFilterFactory and WhitespaceTokenizerFactory\n\n\n<analyzer>\n  <charFilter class=\"solr.HTMLStripCharFilterFactory\" />\n  <tokenizer class=\"solr.WhitespaceTokenizerFactory\" />\n</analyzer>\n\n\n\nAnyway, there seems to be a bug in reseting a token stream created by the HTMLStripWhitespaceTokenizerFactory. That's why the test works the first time when the token stream is created and fails the next time when it's reused. The problem might have been introduced in revision 802286 (see SOLR-1343), when HTMLStripReader, which was a Reader, became HTMLStripCharFilter, which is CharStream. As a result, super.reset in the following code changed from reset(CharStream input) to  reset(Reader input)\n\n\npublic class HTMLStripWhitespaceTokenizerFactory extends BaseTokenizerFactory {\n  public Tokenizer create(Reader input) {\n    return new WhitespaceTokenizer(new HTMLStripReader(input)) {\n      @Override\n      public void reset(Reader input) throws IOException {\n        super.reset(new HTMLStripReader(input));\n      }\n    };\n  }\n}\n\n\n\nWhitespaceTokenizer inherits from CharTokenizer. But CharTokenizer implements only reset(Reader input) and doesn't reset the stream on reset(CharStream input) which is now called. The simplest fix is to explicitly call super.reset(Reader input). A better fix, perhaps, would be implementing reset(CharStream input) in CharTokenizer in Lucene.  "
        },
        {
            "author": "Anders Melchiorsen",
            "id": "comment-12752044",
            "date": "2009-09-07T08:32:26+0000",
            "content": "Hi Igor, thanks for the patch.\n\nIt does seem to work for me. I will leave it for others to decide whether it is the best fix. If the issue is not fixed at a lower layer, note that the HTMLStripStandardTokenizerFactory seems to have a similar problem.\n\nI reported that this problem exists with other tokenizers as well, including the HTMLStripCharFilterFactory+WhitespaceTokenizerFactory combo that you recommend. Today, however, I cannot reproduce that behaviour. As I have been reporting several issues, I find it likely that I have been confused by having multiple configurations running at the same time. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12753686",
            "date": "2009-09-10T16:25:52+0000",
            "content": "A better fix, perhaps, would be implementing reset(CharStream input) in CharTokenizer in Lucene. \n\nWill LUCENE-1906 fix it (in an alternate way)? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753779",
            "date": "2009-09-10T19:32:59+0000",
            "content": "It will be good to get this fixed, I have experienced problems in analyzing because of the bug and reverted back to HTMLStripReader.   "
        },
        {
            "author": "Igor Motov",
            "id": "comment-12753794",
            "date": "2009-09-10T20:03:08+0000",
            "content": "\nWill LUCENE-1906 fix it (in an alternate way)?\nI guess it depends on how they will decide to resolve it. But it looks like the same issues, that's for sure. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12754162",
            "date": "2009-09-11T14:52:38+0000",
            "content": "Will LUCENE-1906 fix it (in an alternate way)?\n\nIt should fix it. Lucene Tokenizer now do not have separate methods for CharStream anymore. They are simply handled as Readers. The trap of overwriting the wrong method should be fixed now. The offset correction is now done conditionally if the Reader is a CharStream subclass. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12754750",
            "date": "2009-09-13T20:11:22+0000",
            "content": "This has been fixed with the update of Lucene 2.9 RC4 "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775863",
            "date": "2009-11-10T15:52:14+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}