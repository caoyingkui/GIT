{
    "id": "LUCENE-2644",
    "title": "LowerCaseTokenizer Does Not Behave As One Might Expect (or Desire)--Given Its Name",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Wish",
        "fix_versions": [],
        "affect_versions": "3.0.2",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "While I understand some of the reasons for its design, the original LowerCaseTokenizer should have been named LowerCaseLetterTokenizer.\n\nI feel that LowerCaseTokenizer makes too many assumptions about what too tokenize, and I have therefore patched it.  The default behavior will remain as it always has--to avoid breaking any implementations for which it's being used.\n\nI have changed LowerCaseTokenizer to extend CharTokenizer (rather than LetterTokenizer).  LetterTokenizer's functionality was merged into the default behavior of LowerCaseTokenizer.\n\nGetter/Setter methods have been added to the LowerCaseTokenizer Class, allowing you to turn on / off tokenizing by white space, numbers, and special (Non-Alpha/Numeric) characters.",
    "attachments": {
        "LowerCaseTokenizer.patch": "https://issues.apache.org/jira/secure/attachment/12454591/LowerCaseTokenizer.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-09-14T21:17:49+0000",
            "content": "This patch will retain original functionality, while permitting the user to modify the assumptions on which tokens are built. ",
            "author": "Scott Gonyea",
            "id": "comment-12909460"
        },
        {
            "date": "2010-10-19T00:39:10+0000",
            "content": "I don't understand this issue, to me the LowerCaseTokenizer explains clearly what it does:\n\n\n * LowerCaseTokenizer performs the function of LetterTokenizer\n * and LowerCaseFilter together.  It divides text at non-letters and converts\n * them to lower case.\n\n\n\nI think there is a lot of other options if you want to customize tokenization by numbers, punctuation etc, especially Solr's WordDelimiterFilter. ",
            "author": "Robert Muir",
            "id": "comment-12922383"
        },
        {
            "date": "2012-09-18T17:47:12+0000",
            "content": "Scott, I agree with Robert Muir - LowerCaseTokenizer has been around since at least 2001 (when Lucene came to the ASF) with well-defined behavior.\n\nI think the way forward here is to make a new issue that creates a new tokenizer performing the things you want to see, with a new name.\n\nResolving as Won't Fix. ",
            "author": "Steve Rowe",
            "id": "comment-13457986"
        }
    ]
}