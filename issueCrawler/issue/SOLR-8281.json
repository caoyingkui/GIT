{
    "id": "SOLR-8281",
    "title": "Add RollupMergeStream to Streaming API",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "The RollupMergeStream merges the aggregate results emitted by the RollupStream on worker nodes.\n\nThis is designed to be used in conjunction with the HashJoinStream to perform rollup Aggregations on the joined Tuples. The HashJoinStream will require the tuples to be partitioned on the Join keys. To avoid needing to repartition on the group by fields for the RollupStream, we can perform a merge of the rolled up Tuples coming from the workers.\n\nThe construct would like this:\n\nmergeRollup (...\n                      parallel (...\n                                    rollup (...\n                                                hashJoin (\n                                                                  search(...),\n                                                                  search(...),\n                                                                  on=\"fieldA\" \n                                                )\n                                     )\n                         )\n               )\n\n\n\nThe pseudo code above would push the hashJoin and rollup to the worker nodes. The emitted rolled up tuples would be merged by the mergeRollup.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-11-19T00:27:50+0000",
            "author": "Dennis Gove",
            "content": "I'd see us needing to make a couple of changes.\n\nRollupStream:\nInstead of just adding a raw value to the tuple this should add a tuple itself which contains metadata about the metric. Metadata is required to perform merges on certain metrics (such as mean).\n\nMergeRollupStream:\nThe construction of this will first validate that all tuples in substreams are mergable. It can do this by asking the substreams for the metrics it intends to calculate or return. Note that this requires a new function in the TupleStream interface whose job it is to return all metrics calculated in that stream or substreams. \n\nThe read() implementation of this stream will need to read all tuples from the substream (most likely a ParallelStream but doesn't have to be). Each tuple will be added to a map with map[tupleKey] = tuple. tupleKey is whatever defines a unique tuple (ie, the group by fields). If a \"same\" tuple exists in the map already then the existing tuple and read tuple will be merged by calling existingTuple = metric.merge(existingTuple, readTuple) for each metric and then put back into the map. The end result is that the map contains the merged tuples.\n\nMergeRollupStream::read() will then return the first tuple from the map. Note, we can use a sorted map or some way to return sorted values from a map so that we can enforce some sort on the read tuples. Also, this allows us to effectively resort the stream to something useful for wrapping streams.\n\nI may be leaving something out but I believe this approach (or at least the one I've designed in my head) will give us what we need.\n\nAn open question is do we return from the MergeRollupStream metrics containing this metadata or should we strip the metadata out? I think we should return it but am not wedded to that idea. ",
            "id": "comment-15012457"
        },
        {
            "date": "2015-11-19T02:23:56+0000",
            "author": "Joel Bernstein",
            "content": "Thinking about this some more, possibly this is a job for the ReducerStream. We could add Operations to the reducer stream and have the operations perform the merge.\n\nIf we went this route we would scrap the MergeRollupStream and change this ticket to \"Add operations to the ReducerStream\".\n\nThis would also provide a much more powerful ReducerStream for general use. ",
            "id": "comment-15012646"
        },
        {
            "date": "2015-11-19T02:27:46+0000",
            "author": "Dennis Gove",
            "content": "To be honest I think this logic should live in the ParallelStream. As a user of this stream I would expect it to properly merge all workers together, including metrics calculated in those workers. \n\nThat said, putting it in the ReducerStream is also a good idea. I'm on the fence as to which would be better. Adding too much to the ParallelStream might end up hurting us long-term. ",
            "id": "comment-15012651"
        },
        {
            "date": "2015-11-19T04:11:40+0000",
            "author": "Joel Bernstein",
            "content": "Early versions of the ParallelStream handled the merging of Rollups. But I pulled it out because I felt this needed more thought.\n\nThe nice thing about adding operations to the ReducerStream is that it makes the ReducerStream much more useful. So even we don't use it to merge Rollups it's worth doing.\n\nBut this construct seems nice:\n\n\nreduce (...\n                      parallel (...\n                                    rollup (...\n                                                hashJoin (\n                                                                  search(...),\n                                                                  search(...),\n                                                                  on=\"fieldA\" \n                                                )\n                                     )\n                         )\n               )\n\n\n\nActually this is even nicer\n\n\nreduce  (...\n                      parallel (...\n                                    reduce (...\n                                                hashJoin (\n                                                                  search(...),\n                                                                  search(...),\n                                                                  on=\"fieldA\" \n                                                )\n                                     )\n                         )\n               )\n\n\n\nIn this case the ReducerStream replaces the RollupStream. \n\nTo support this we would need an Operation to rollup the Metrics. ",
            "id": "comment-15012755"
        }
    ]
}