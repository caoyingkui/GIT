{
    "id": "SOLR-2583",
    "title": "Make external scoring more efficient (ExternalFileField, FileFloatSource)",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "search"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "External scoring eats much memory, depending on the number of documents in the index. The ExternalFileField (used for external scoring) uses FileFloatSource, where one FileFloatSource is created per external scoring file. FileFloatSource creates a float array with the size of the number of docs (this is also done if the file to load is not found). If there are much less entries in the scoring file than there are number of docs in total the big float array wastes much memory.\n\nThis could be optimized by using a map of doc -> score, so that the map contains as many entries as there are scoring entries in the external file, but not more.",
    "attachments": {
        "patch.txt": "https://issues.apache.org/jira/secure/attachment/12482010/patch.txt",
        "FileFloatSource.java.patch": "https://issues.apache.org/jira/secure/attachment/12481905/FileFloatSource.java.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Martin Grotzke",
            "id": "comment-13046439",
            "date": "2011-06-09T09:52:46+0000",
            "content": "The attached patch changes FileFloatSource to use a map of score by doc. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046564",
            "date": "2011-06-09T14:17:51+0000",
            "content": "Yeah, this will help for sparse fields, but hurt quite a bit for non-sparse ones.\nSeems like we should make it an option (sparse=true/false on the fieldType definition)? "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13046674",
            "date": "2011-06-09T17:03:11+0000",
            "content": "Yes, you're right regarding non-sparse fields. The question for the user will be when to use true or false for sparse. It might also be the case, that files differ, in that some are big, others are small. So I'm thinking about making it adaptive: when the number of lines reach a certain percentage compared to the number of docs, the float array is used, otherwise the doc->score map is used. Perhaps it would be good to allow the user to override this, s.th. like sparse=yes/no/auto.\n\nWhat do you think? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13046675",
            "date": "2011-06-09T17:08:09+0000",
            "content": "a smallfloat option could help too? (1/4 the ram) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046688",
            "date": "2011-06-09T17:37:21+0000",
            "content": "Perhaps it would be good to allow the user to override this, s.th. like sparse=yes/no/auto.\n\nSounds good!  I wonder what the memory cut-off should be for auto... 10% of maxDoc() or so?\n\na smallfloat option could help too? (1/4 the ram)\n\nYep! "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13046692",
            "date": "2011-06-09T17:41:02+0000",
            "content": "Great, sounds like a further optimization for both sparse and non-sparse files. Though, as we had 4GB taken by FileFloatSource objects a reduction to 1/4 would still be too much for us so for our case I prefer the map based approach - then with Smallfloat. "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13046712",
            "date": "2011-06-09T18:00:09+0000",
            "content": "> Sounds good!  I wonder what the memory cut-off should be for auto... 10% of maxDoc() or so?\n\nI'd compare both strategies to see what's the break-even, this should give an absolute number. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13046785",
            "date": "2011-06-09T20:19:04+0000",
            "content": "Though, as we had 4GB taken by FileFloatSource objects a reduction to 1/4 would still be too much for us so for our case I prefer the map based approach - then with Smallfloat.\n\nIf the problem is sparsity, maybe use a two-stage table, still faster than a hashmap and much better for the worst case. "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13046943",
            "date": "2011-06-10T00:50:59+0000",
            "content": "If the problem is sparsity, maybe use a two-stage table, still faster than a hashmap and much better for the worst case.\n\nWhat do you mean with a two-stage table, can you clarify this please? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13046956",
            "date": "2011-06-10T01:37:54+0000",
            "content": "\nWhat do you mean with a two-stage table, can you clarify this please?\n\nSee: http://www.strchr.com/multi-stage_tables\n\ni attached a patch, of a (not great) implementation i was sorta kinda trying to clean up for other reasons... maybe you can use it.\n\nin the sparse case, blocks that share all the default value are folded into one block (in this patch, blocksize=256 but maybe you should be able to configure it).\n\nfor example in your 4GB case (1billion floats), if you use this with SmallFloat the absolute worst case (no sharing) is 1GB + 16MB or so, and the best case (all default values) is 16MB, but the lookups should be a lot faster than hashtables... all primitive types, etc... and it could definitely be improved more.\n\nreally this is still probably overkill, as the datastructure is intended to share blocks with the same values in general, when in reality its probably enough to just share ones that have only the default value set...\n\ni didnt look at the solr side to see if its possible to build it incrementally (this would be better, rather than building then compact()ing, but i wonder if this is possible due to lucenedocid/solr id, etc) "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13049143",
            "date": "2011-06-14T11:53:40+0000",
            "content": "\nSee: http://www.strchr.com/multi-stage_tables\n\ni attached a patch, of a (not great) implementation i was sorta kinda trying to clean up for other reasons... maybe you can use it.\n\nThanx, interesting approach!\n\nI just tried to create a CompactFloatArray based on the CompactByteArray to be able to compare memory consumptions. There's one change that wasn't just changing byte to float, and I'm not sure what's the right adaption in this case:\n\n\ndiff -w solr/src/java/org/apache/solr/util/CompactByteArray.java solr/src/java/org/apache/solr/util/CompactFloatArray.java\n57c57\n...\n202,203c202,203\n<   private void touchBlock(int i, int value) {\n<     hashes[i] = (hashes[i] + (value << 1)) | 1;\n---\n>   private void touchBlock(int i, float value) {\n>     hashes[i] = (hashes[i] + (Float.floatToIntBits(value) << 1)) | 1;\n\n\n\nThe adapted test is green, so it seems to be correct at least. I'll also attach the full patch for CompactFloatArray.java and TestCompactFloatArray.java "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13049256",
            "date": "2011-06-14T16:24:39+0000",
            "content": "I just compared memory consumption of the 3 different approaches, with different number of puts (number of scores) and sizes (number of docs), the memory is in byte:\n\n\nPuts  1.000, size 1.000.000:\t  CompactFloatArray 898.136,\tfloat[] 4.000.016,\tHashMap  72.192\nPuts  10.000, size 1.000.000:\t  CompactFloatArray 3.724.376,\tfloat[] 4.000.016,\tHashMap  702.784\nPuts  100.000, size 1.000.000:\t  CompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  6.607.808\nPuts  1.000.000, size 1.000.000:  CompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  44.644.032\nPuts  1.000, size 5.000.000:\t  CompactFloatArray 1.128.536,\tfloat[] 20.000.016,\tHashMap  72.256\nPuts  10.000, size 5.000.000:\t  CompactFloatArray 8.168.536,\tfloat[] 20.000.016,\tHashMap  704.832\nPuts  100.000, size 5.000.000:\t  CompactFloatArray 20.013.144,\tfloat[] 20.000.016,\tHashMap  7.385.152\nPuts  1.000.000, size 5.000.000:  CompactFloatArray 20.131.160,\tfloat[] 20.000.016,\tHashMap  66.395.584\nPuts  1.000, size 10.000.000:\t  CompactFloatArray 1.275.992,\tfloat[] 40.000.016,\tHashMap  72.256\nPuts  10.000, size 10.000.000:\t  CompactFloatArray 9.289.816,\tfloat[] 40.000.016,\tHashMap  705.280\nPuts  100.000, size 10.000.000:\t  CompactFloatArray 37.130.328,\tfloat[] 40.000.016,\tHashMap  7.418.112\nPuts  1.000.000, size 10.000.000: CompactFloatArray 40.262.232,\tfloat[] 40.000.016,\tHashMap  69.282.496\n\n\n\nI want to share this intermediately, without further interpretation/conclusion for now (I just need to get the train). "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13049674",
            "date": "2011-06-15T08:13:55+0000",
            "content": "The test that produced this output can be found in my lucene-solr fork on github: https://github.com/magro/lucene-solr/commit/b9af87b1\nThe test method that was executed was testCompareMemoryUsage, for measuring memory usage I used http://code.google.com/p/memory-measurer/ and ran the test/jvm with \"-Xmx1G -javaagent:solr/lib/object-explorer.jar\" (just from eclipse).\n\nI just added another test, that uses a fixed size and an increasing number of puts (testCompareMemoryUsageWithFixSizeAndIncreasingNumPuts, https://github.com/magro/lucene-solr/blob/trunk/solr/src/test/org/apache/solr/search/function/FileFloatSourceMemoryTest.java#L56), with the following results:\n\n\nSize: 1000000\nNumPuts 1.000 (0,1%),\t\tCompactFloatArray 918.616,\tfloat[] 4.000.016,\tHashMap  72.128\nNumPuts 10.000 (1,0%),\t\tCompactFloatArray 3.738.712,\tfloat[] 4.000.016,\tHashMap  701.696\nNumPuts 50.000 (5,0%),\t\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  3.383.104\nNumPuts 55.000 (5,5%),\t\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  3.949.120\nNumPuts 60.000 (6,0%),\t\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  4.254.848\nNumPuts 100.000 (10,0%),\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  6.622.272\nNumPuts 500.000 (50,0%),\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  27.262.976\nNumPuts 1.000.000 (100,0%),\tCompactFloatArray 4.016.472,\tfloat[] 4.000.016,\tHashMap  44.649.664\n\n\n\nIt seems that the HashMap is the most efficient solution up to ~5.5%. Starting from this threshold CompactFloatArray and float[] use less memory, while the CompactFloatArray has no advantages over float[] for puts > 5%.\n\nTherefore I'd suggest that we use an adaptive strategy that uses a HashMap up to 5,5% of number of scores compared to numdocs, and starting from this threshold the original float[] approach is used.\n\nWhat do you say? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13049706",
            "date": "2011-06-15T10:43:22+0000",
            "content": "Are you sure real floats are actually needed?\nWhy not use compactbytearray with smallfloat encoding?\n\nit would also good to measure performance... doesn't a hashmap have to box per-docid into an Integer for lookup?\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13049709",
            "date": "2011-06-15T10:46:44+0000",
            "content": "that uses a fixed size and an increasing number of puts\n\nI'm not certain how realistic that is, remember behind the scenes compactbytearray uses blocks,\nand if you touch every one (by putting every K docid or something) then you are just testing \nthe worst case. "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13050435",
            "date": "2011-06-16T14:13:05+0000",
            "content": "Are you sure real floats are actually needed?\nIn our case score values are e.g. 158870000 (one example just taken from one of the files). With this sample this test fails:\n\nbyte small = SmallFloat.floatToByte315(104626500f);\nassertEquals(104626500f, SmallFloat.byte315ToFloat(small), 0f);\n-> AssertionError: expected:<1.04626496E8> but was:<1.00663296E8>\n\n\n\nThis shows that even we have a case where this will produce wrong results, and even if we could fix this in our case there might be someone else with the same issue.\n\n\nit would also good to measure performance...\nI'd not expect that the boxing makes a real difference here, especially in relation to the rest of the time spent during a search request.\nA time based performance comparison that has a real value would take some time, it would have to put in relation to the rest of a search request (how do you do this?) and finally it would require proper interpretation when everything is together. Right now I don't think it's worth the effort.\n\n\n\nthat uses a fixed size and an increasing number of puts\nI'm not certain how realistic that is, remember behind the scenes compactbytearray uses blocks,\nand if you touch every one (by putting every K docid or something) then you are just testing\nthe worst case.\nDo you want to change the test to s.th. that's more realistic?\n\n\n@Yonik: what do you say regarding the suggestion to use HashMap up to ~5.5% and above that using the float[]? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13055437",
            "date": "2011-06-27T09:53:31+0000",
            "content": "I'd like the feature as I'm using ExternalFileField a lot!\n\nwhat do you say regarding the suggestion to use HashMap up to ~5.5% and above that using the float[]?\n\nLooking at your test, I think it is reasonable. But I'd like to use CompactByteArray. I saw it wins over HashMap and float[] when 5% and above in my test.\n\nHow about introducing compact=yes (default is no and float[] is used) with sparse=yes/no/auto? "
        },
        {
            "author": "Martin Grotzke",
            "id": "comment-13055737",
            "date": "2011-06-27T20:37:44+0000",
            "content": "Looking at your test, I think it is reasonable. But I'd like to use CompactByteArray. I saw it wins over HashMap and float[] when 5% and above in my test.\n\nCan you share your test code or s.th. similar? Perhaps you can just fork https://github.com/magro/lucene-solr/ and add an appropriate test that reflects your data? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13056224",
            "date": "2011-06-28T00:19:15+0000",
            "content": "I didn't save the test snippet because I wrote it out of my office (I used stranger's PC). What I did was just using CompactByteArray instead of CompactFloatArray in your FileFloatSourceMemoryTest.java. "
        }
    ]
}