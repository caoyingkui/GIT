{
    "id": "LUCENE-5422",
    "title": "Postings lists deduplication",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [
            "core/codecs",
            "core/index"
        ],
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": []
    },
    "description": "The context:\nhttp://markmail.org/thread/tywtrjjcfdbzww6f\n\nRobert Muir and I have discussed what Robert eventually named \"postings\nlists deduplication\" at Berlin Buzzwords 2013 conference.\n\nThe idea is to allow multiple terms to point to the same postings list to\nsave space. This can be achieved by new index codec implementation, but this jira is open to other ideas as well.\n\nThe application / impact of this is positive for synonyms, exact / inexact\nterms, leading wildcard support via storing reversed term etc.\n\nFor example, at the moment, when supporting exact (unstemmed) and inexact (stemmed)\nsearches, we store both unstemmed and stemmed variant of a word form and\nthat leads to index bloating. That is why we had to remove the leading\nwildcard support via reversing a token on index and query time because of\nthe same index size considerations.\n\nComment from Mike McCandless:\nNeat idea!\n\nWould this idea allow a single term to point to (the union of) N other\nposting lists?  It seems like that's necessary e.g. to handle the\nexact/inexact case.\n\nAnd then, to produce the Docs/AndPositionsEnum you'd need to do the\nmerge sort across those N posting lists?\n\nSuch a thing might also be do-able as runtime only wrapper around the\npostings API (FieldsProducer), if you could at runtime do the reverse\nexpansion (e.g. stem -> all of its surface forms).\n\n\nComment from Robert Muir:\nI think the exact/inexact is trickier (detecting it would be the hard\npart), and you are right, another solution might work better.\n\nbut for the reverse wildcard and synonyms situation, it seems we could even\ndetect it on write if we created some hash of the previous terms postings.\nif the hash matches for the current term, we know it might be a \"duplicate\"\nand would have to actually do the costly check they are the same.\n\nmaybe there are better ways to do it, but it might be a fun postingformat\nexperiment to try.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13916140",
            "author": "Vishmi Money",
            "content": "Hi,\nI am Vishmi Money and I am a third year undergraduate at Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka.\n\nI am familiar with Lucene as I have read and learnt about it for a project in which I have  tried to implement Global Search for moodle. But then I found out that Lucene was a dead end for that as moodle is a php implementation.\n\nAfter going through the discussion you provided, I am very interested to work on this project for GSoc 2014 because I am very intersted in Data Structures and Algorithms area too.\n\nCan you further explain me about the relationship of LUCENE-2082 to LUCENE-5422?\nso that I can start work on this project.\n ",
            "date": "2014-02-28T18:29:55+0000"
        },
        {
            "id": "comment-13921155",
            "author": "Dmitry Kan",
            "content": "Vishmi Money\n\nLUCENE-2082 deals with segment merging which is process performed on Lucene index every now and then.\n\nThis jira deals with the index structure and suggests that compression of index can be achieved for certain (described) use cases. While these jiras are related, this jira can be considered as standalone project in itself.\n\nperhaps Otis Gospodnetic could add something? ",
            "date": "2014-03-05T18:22:46+0000"
        },
        {
            "id": "comment-13921405",
            "author": "Otis Gospodnetic",
            "content": "I think you put it well, Dmitry.\nLUCENE-2082 is about what happens with post lists when Lucene segments are being merged.  The extreme use case is the one where documents are only added and there are no deletes or updates.  In that case post lists from segments being merged can just be copied into the new segment being created.  This should result in faster segment merges, which means less pressure on the JVM and server and thus faster indexing and searching happening on that server.\n\nLUCENE-5422 (this issue) is about how to store post lists in a more efficient manner. ",
            "date": "2014-03-05T21:05:52+0000"
        },
        {
            "id": "comment-13922311",
            "author": "Vishmi Money",
            "content": "Dmitry Kan, Otis Gospodnetic,\nThank you very much for your explanations and now I got a clear idea about the two issues. As new documents are added segments are merged to the index but, if some documents are deleted, we have to keep track on those using skip entries. Meanwhile we have to preserve or improve the performance of the operation. That is the area which is discussed in LUCENE-2082. \nIn LUCENE-5422, we want to make synonyms, exact/inexact terms point to a same posting list also providing wildcard support. Main objective is to save space. Meanwhile, we also have to avoid index bloating much as possible. LUCENE-5422 relates with LUCENE-2082 because anyway LUCENE-5422 has to deal with segment merging. This is the idea I got and please let me know if I am wrong on something.\n\nCurrently I am following LUCENE- 4.7.0 documentation and also being familiar with the source code and coding conventions. I also follow Michael McCandless's Blog and read few posts related like, Visualizing Lucene's segment merges, Building a new Lucene posting format etc. I also started reading \"LUCENE In Action-second edition\" book but then I noticed that it is for LUCENE- 3.0. As LUCENE- 4.0 has switched to a new pluggable codec architecture, I wonder whether all the content of the book is relavent or not. Shall I proceed with the reading or should I only have to look on documentation for LUCENE- 4.0 or above? ",
            "date": "2014-03-06T11:03:21+0000"
        },
        {
            "id": "comment-13922553",
            "author": "Otis Gospodnetic",
            "content": "Maybe Michael McCandless can comment, but I think think you are right as far as Codecs part of Lucene and LIA are concerned. ",
            "date": "2014-03-06T14:19:58+0000"
        },
        {
            "id": "comment-13922627",
            "author": "Michael McCandless",
            "content": "I think reading blogs, javadocs, CHANGES entries, are all good ways to come up to speed on the recent changes in Lucene.\n\nAnd, yes, LUCENE-2082 is about more efficient merging, by appending raw postings bytes instead of decode + re-encode that's done today.  It's analogous to how Lucene used to fully decode and then re-encode each Document (stored fields) during merging, but today we just do bulk copying of bytes when possible (same for term vectors).\n\nI think this issue needs better scoping / maybe a clearer use case, to understand exactly when the postings list deduping should kick in.  And if this incurs a search-time cost (e.g. a merge sort of N postings lists to make it look like a single posting list) that's an added cost that may be the wrong (smaller index and slower searching) tradeoff in most cases. ",
            "date": "2014-03-06T15:11:50+0000"
        },
        {
            "id": "comment-13924036",
            "author": "Vishmi Money",
            "content": "Otis Gospodnetic, thank you.\n\nMichael McCandless, yes I agree with you. As you said if cost is added for merging of posts lists, despite the server space saved, it will affect for performance. Then we have to think about how we can achieve this desired performance while trying to save server space.\nI will keep this in my mind when I look further in to this. ",
            "date": "2014-03-07T16:38:49+0000"
        },
        {
            "id": "comment-13925873",
            "author": "Vishmi Money",
            "content": "I'm sharing the draft of my proposal with you. I will be grateful if you can review it and give me feedback.\n\nlink : https://docs.google.com/document/d/1CWw_mCD9Qv7VcskFbZg4PpRHG_Trh4GNPuCt9pwPGKg/edit?usp=sharing ",
            "date": "2014-03-10T16:39:49+0000"
        },
        {
            "id": "comment-13926081",
            "author": "Dmitry Kan",
            "content": "I agree with Michael McCandless in that the issue should be better scoped. The case with compressing stemmed / non-stemmed terms posting lists is quite tricky and requires more thought.\n\nOne clear case for this issue is storing reversed term along with its original non-reversed version. Both should point to the same posting list (subject to some after-stemming-hash-check).\n\nWhat do you guys think? ",
            "date": "2014-03-10T19:26:30+0000"
        },
        {
            "id": "comment-13940214",
            "author": "Vishmi Money",
            "content": "Dmitry Kan , \nyes, I agree with you. Better scoping is needed and expert ideas are also needed for that. As Michael McCandless said, a clearer use case may solve the problem. If we can come up with a clear use case deciding when deduplication should really happen, it will help a lot to achieve this objective . So that the ideas are needed.\n\nAs I didn't tell you about the progress of my work, I like to let you know that now I'm analyzing Lucene tokenizing and indexing as it is the main area I have to work with, than searching. But  for the improvement or persistence of performance, I also learn about improving search queries for Lucene, according to our objective here. For that understanding and debugging purposes, I use Luke, a Index Browser tool for Lucene. Please let me know if there are other areas which I should look in.\n\nAlso I again remind you about reviewing my proposal, as it will be a great support for me if I can get feedback from you. ",
            "date": "2014-03-19T06:00:47+0000"
        },
        {
            "id": "comment-13940363",
            "author": "Michael McCandless",
            "content": "Can any Lucene/Solr committers volunteer to mentor this project? ",
            "date": "2014-03-19T10:15:34+0000"
        },
        {
            "id": "comment-13942821",
            "author": "Vishmi Money",
            "content": "Hi Dmitry Kan, Otis Gospodnetic, Michael McCandless,\nI was looking forward for a reply for the comment by Michael McCandless as it will be a great support for me if there will be a mentor for this project. So then I can directly get support from him/her to resolve the questions I get when I am proceeding with this. I am kindly hoping for a positive answer.\n\nThank you. ",
            "date": "2014-03-21T05:43:42+0000"
        }
    ]
}