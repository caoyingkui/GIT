{
    "id": "LUCENE-8092",
    "title": "TestRandomChains failure",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Bug",
        "components": [],
        "fix_versions": []
    },
    "description": "https://builds.apache.org/job/Lucene-Solr-NightlyTests-7.2/1/\n\nant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=C006DAD2E1FC77AF -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/Users/romseygeek/projects/lucene-test-data/enwiki.random.lines.txt -Dtests.locale=tr -Dtests.timezone=Europe/Simferopol -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\nReproduces locally on 7.2",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16292539",
            "date": "2017-12-15T13:29:35+0000",
            "content": "This is due to a combination of ShingleFilter and CJKBigramFilter, leading to offsets going backwards.  Digging further. ",
            "author": "Alan Woodward"
        },
        {
            "id": "comment-16292560",
            "date": "2017-12-15T14:00:08+0000",
            "content": "\nSF:ShingleFilter@2d06f345 term=\u11fb\u1191\u114c,bytes=[e1 87 bb e1 86 91 e1 85 8c],startOffset=0,endOffset=3,positionIncrement=1,positionLength=1,type=<HANGUL>,termFrequency=1\nCJ:CJKBigramFilter@20d01b2b term=\u11fb\u1191,bytes=[e1 87 bb e1 86 91],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=<DOUBLE>,termFrequency=1\nCJ:CJKBigramFilter@20d01b2b term=\u1191\u114c,bytes=[e1 86 91 e1 85 8c],startOffset=1,endOffset=3,positionIncrement=1,positionLength=1,type=<DOUBLE>,termFrequency=1\nSF:ShingleFilter@2d06f345 term=\u11fb\u1191\u114c IacUTe,bytes=[e1 87 bb e1 86 91 e1 85 8c 20 49 61 63 55 54 65],startOffset=0,endOffset=14,positionIncrement=0,positionLength=2,type=shingle,termFrequency=1\n\n\n\nSo this is because both ShingleFilter and CJKBigramFilter emit multiple tokens with adjusted offsets.  The CJKBigramFilter is splitting the first unigram emitted by ShingleFilter, and emitting two tokens, the second of which has an increased offset.  The ShingleFilter then emits a combination of the first two terms, with offset set back to 0.\n\nI'm not sure of the best way to fix this.  Maybe both ShingleFilter and CJKBigramFilter need to cache their inputs and check that the underlying TokenStream has moved on before they emit bigrams?  Uwe Schindler what do you think?\n ",
            "author": "Alan Woodward"
        },
        {
            "id": "comment-16293791",
            "date": "2017-12-16T12:50:00+0000",
            "content": "This is indeed a funny edge-case. Maybe the second filter could alternatively reorder the tokens.\n\nBut I am not really sure if this is really a problem in real-world scenarios. It may affect all tokenfilters (not only those 2) that inject tokens with a diffent offset and length. I think the above combination is useless in real-world, but maybe others make sense?\n\nAn alternative would be to reorder tokens as last step in the analysis chain (using a \"fixing\" token filter). It just reorders all tokens with same position so their offsets increase? ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16385188",
            "date": "2018-03-04T15:22:30+0000",
            "content": "CJKBigramFilter isn't really prepared to handle an arbitrary input graph (or maybe even synonyms): its looking for a flat stream of tokens that may include some CJK. \n\nIt already has a ridiculously complex job, its like a shinglefilter but with crazy custom logic: but it does manage that to support the use-case across different tokenizer variants (StandardTokenizer, UAXURLTokenizer, ICUTokenizer). \n\nMaybe it should throw a clear exception if it encounters posinc=0 or poslen > 1 ? It would at least make it totally clear that it won't work, rather than the user getting a more vague exception from indexwriter. Ideally this would be detected earlier though (in construction of the chain). Unfortunately its not so easy to simply require that its input is a tokenizer, see the CJKAnalyzer use-case where the width-filter comes before, because that impacts bigramming. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16385204",
            "date": "2018-03-04T15:52:45+0000",
            "content": "CJKBigramFilter was already in the offsetsAreBroken list for this test, so it used to not fail in such cases. However, IndexWriter now always checks the offsets, so you'll get an exception from indexwriter regardless. Hence, this list doesn't work (if offsets are broken, the tokenfilter is broken). See LUCENE-8191.\n\nThis doesn't fix the fact the filter is buggy, we know that. it just reflects changes made in LUCENE-7626.  ",
            "author": "Robert Muir"
        }
    ]
}