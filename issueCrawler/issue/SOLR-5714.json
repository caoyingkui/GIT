{
    "id": "SOLR-5714",
    "title": "You should be able to use one pool of memory for multiple collection's HDFS block caches.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Currently, you have to specify how much direct memory to allocate per SolrCore. This can be inefficient, and has some negative consequences - for instance, when replicating, many times two HDFS directories will exist for the same index briefly, which will double the RAM used for that SolrCore.",
    "attachments": {
        "SOLR-5714.patch": "https://issues.apache.org/jira/secure/attachment/12628371/SOLR-5714.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13898517",
            "date": "2014-02-11T23:49:29+0000",
            "content": "This patch adds a new init param to HdfsDirectoryFactory that allows it to use a global block cache rather than a block cache per SolrCore instance. It defaults to false if not present in solrconfig.xml for back compatibility. The latest solrconfig.xml will default it to true.\n\nA future improvement I'd like to make is the ability to configure some of the hdfs stuff at the solr.xml level, but I don't want to tie that into this issue.\n\nFirst cut attached. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13898519",
            "date": "2014-02-11T23:53:32+0000",
            "content": "It defaults to false if not present in solrconfig.xml for back compatibility. The latest solrconfig.xml will default it to true.\n\nWhy not just make the implicit default (if not present in config) contingent on the luceneMatchVersion?  if < 4.7, default=false; else default=true ?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13898590",
            "date": "2014-02-12T01:04:46+0000",
            "content": "Yeah, will do. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13917278",
            "date": "2014-03-02T04:04:15+0000",
            "content": "Why not just make the implicit default (if not present in config) contingent on the luceneMatchVersion? if < 4.7, default=false; else default=true ?\n\nOn further thought, I'm not sure I prefer that. It's kind of a special case - the global cache is created with the settings for the first directoryFactory that is created. It seems more normal for this to be per instance by default, and so I still kind of think it's best to default it to on, but require it to be there explicitly for that. It feels right that if you take it away, the configuration would apply per instance.\n\nAnd then clean it up eventually by allowing it to be configured via solr.xml or something. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13917284",
            "date": "2014-03-02T04:47:55+0000",
            "content": "Here is a patch with an additional HDFS test.\n\nIt creates N collections, all using the block cache with read and write caching and it concurrently adds and deletes documents for all of them and then does a search to check the counts. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-13918707",
            "date": "2014-03-03T23:23:51+0000",
            "content": "It's not clear this would cause any bugs, but the handling of creating the blockCache is unclear.\n\nThe assignment:\n\n        blockCache = createBlockCache(numberOfBlocksPerBank, blockSize, bankCount,directAllocation, slabSize, bufferSize, bufferCount);\n\n\n\nhas no effect because the (static) blockCache is actually set in createBlockCache, independent of whether this is static or not:\n\nblockCache = new BlockCache(metrics, directAllocation, totalMemory, slabSize, blockSize);\n\n\n\nProbably should make the blockCache in createBlockCache a local variable to the function, not sharing a name with the static version.\nSame here, it would be better to rename:\n\nBlockCache blockCache = getBlockDirectoryCache\n\n\n\nEdit: actually I think this could cause a bug.  What if one solrconfig.xml uses the global cache and another doesn't.  The one that doesn't can overwrite the first's cache. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13918727",
            "date": "2014-03-03T23:34:51+0000",
            "content": "Thanks Greg! That static name shadowing was definitely hiding some silliness there.\n\nI've renamed the blockCache field to globalBlockCache and put in local variables where they belong. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-13918803",
            "date": "2014-03-04T00:23:09+0000",
            "content": "New change looks good to me.\n\nA couple of questions:\n\n\tIs there a test that checks that the global block cache is actually used?  I see that its turned on randomly, so we know turning it on doesn't break things, but does it actually do what it says it does?\n\tAre there any downsides to using a global block cache?  In the motivation above, it's to promote sharing on the same index, but we are sharing globally.  Why not share on just the same index?\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13918876",
            "date": "2014-03-04T01:15:29+0000",
            "content": "Adds testing that asserts that all the BlockCache instances for each SolrCore are the same instance when global cache is on and checks for different instances as it jumps through each SolrCore when the global cache is off. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13918881",
            "date": "2014-03-04T01:19:22+0000",
            "content": "Are there any downsides to using a global block cache? In the motivation above, it's to promote sharing on the same index, but we are sharing globally. Why not share on just the same index?\n\nThere are not a lot of downsides. Perhaps some extra contention if you have a ton of collections and perhaps different LRU behavior across collections than you might want for some use cases. I think by and large, the benefits of one global pool of memory will out weigh those smaller issues by a good extent. Trying to figure out how much to give each collection is fairly difficult - and some collections may be starving for RAM at certain times while other collections are using very little of their pre allocated RAM. Unless you have so much RAM that you can just give gobs of it to each SolrCore, I imagine one pool will be much more user friendly and efficient in the standard case. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13918888",
            "date": "2014-03-04T01:25:26+0000",
            "content": "To add a bit: I wanted to do this even before realizing the same index issue - because these caches allocate the memory up front, users have already been running into issues when trying to use multiple collections. The same index issue just made matters even worse because it wouldn't like be an issue until the worse time - during recovery.\n\nBy and large though, I think we will have fewer user issues if they can simply decide up front how much RAM they can dedicate to HDFS caching and let Solr work out the rest across their collections. It's more dynamic and takes a large burden off the user. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-13918906",
            "date": "2014-03-04T01:37:20+0000",
            "content": "lgtm +1. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13918976",
            "date": "2014-03-04T03:18:50+0000",
            "content": "Commit 1573847 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1573847 ]\n\nSOLR-5714: You can now use one pool of memory for for the HDFS block cache that all collections share. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13918999",
            "date": "2014-03-04T04:16:54+0000",
            "content": "Commit 1573849 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1573849 ]\n\nSOLR-5714: You can now use one pool of memory for for the HDFS block cache that all collections share. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13919000",
            "date": "2014-03-04T04:17:40+0000",
            "content": "Thanks for the review Greg! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13919343",
            "date": "2014-03-04T12:51:24+0000",
            "content": "Mark, the SVN log pointed me to this issue, did you forget to add/commit StopableIndexingThread? Many unit tests fail to compile due to StopableIndexingThread.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13919398",
            "date": "2014-03-04T13:41:26+0000",
            "content": "Hey Markus, not sure what you are seeing. Looking at the commit tags above:\n\n\"lucene/dev/trunk/solr/test-framework/src/java/org/apache/solr/cloud/StopableIndexingThread.java\tadded\" "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13919402",
            "date": "2014-03-04T13:46:24+0000",
            "content": "Hi Mark! I get stuff like:\n\ncommon.compile-test:\n    [javac] Compiling 365 source files to /home/markus/src/solr/trunk/solr/build/solr-core/classes/test\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:35: error: cannot find symbol\n    [javac] import org.apache.solr.cloud.StopableIndexingThread;\n    [javac]                             ^\n    [javac]   symbol:   class StopableIndexingThread\n    [javac]   location: package org.apache.solr.cloud\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java:134: error: no suitable constructor found for StopableIndexingThread(SolrServer,CloudSolrServer,String,boolean)\n    [javac]         StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n    [javac]                                              ^\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean,int) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java:273: error: no suitable constructor found for StopableIndexingThread(SolrServer,CloudSolrServer,String,boolean)\n    [javac]       super(controlClient, cloudClient, id, doDeletes);\n    [javac]       ^\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean,int) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java:111: error: no suitable constructor found for StopableIndexingThread(SolrServer,CloudSolrServer,String,boolean)\n    [javac]       StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n    [javac]                                            ^\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean,int) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java:69: error: no suitable constructor found for StopableIndexingThread(SolrServer,CloudSolrServer,String,boolean,int)\n    [javac]     indexThread = new StopableIndexingThread(controlClient, cloudClient, \"1\", true, maxDoc);\n    [javac]                   ^\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean,int) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java:72: error: no suitable constructor found for StopableIndexingThread(SolrServer,CloudSolrServer,String,boolean,int)\n    [javac]     indexThread2 = new StopableIndexingThread(controlClient, cloudClient, \"2\", true, maxDoc);\n    [javac]                    ^\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean,int) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac]     constructor AbstractFullDistribZkTestBase.StopableIndexingThread.AbstractFullDistribZkTestBase.StopableIndexingThread(String,boolean) is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:102: error: AbstractFullDistribZkTestBase.StopableIndexingThread is not public in AbstractFullDistribZkTestBase; cannot be accessed from outside package\n    [javac]     List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    [javac]          ^\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:102: error: AbstractFullDistribZkTestBase.StopableIndexingThread is not public in AbstractFullDistribZkTestBase; cannot be accessed from outside package\n    [javac]     List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();\n    [javac]                                                          ^\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:107: error: AbstractFullDistribZkTestBase.StopableIndexingThread is not public in AbstractFullDistribZkTestBase; cannot be accessed from outside package\n    [javac]       StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n    [javac]       ^\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:107: error: AbstractFullDistribZkTestBase.StopableIndexingThread is not public in AbstractFullDistribZkTestBase; cannot be accessed from outside package\n    [javac]       StopableIndexingThread indexThread = new StopableIndexingThread(null, server, \"1\", true, docCount);\n    [javac]                                                ^\n    [javac] /home/markus/src/solr/trunk/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java:113: error: AbstractFullDistribZkTestBase.StopableIndexingThread is not public in AbstractFullDistribZkTestBase; cannot be accessed from outside package\n    [javac]     for (StopableIndexingThread thread : threads) {\n    [javac]          ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 11 errors\n\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13919434",
            "date": "2014-03-04T14:33:15+0000",
            "content": "Are you doing a clean first? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13919445",
            "date": "2014-03-04T14:38:10+0000",
            "content": "I remove the build directory. I just tried again and it failed. However, i did another svn up and i finally got the correct update. It sometimes happens that multiple svn ups do not download all files of the revision, perhaps because im using the EU svn? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13982558",
            "date": "2014-04-27T23:25:44+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}