{
    "id": "LUCENE-8183",
    "title": "HyphenationCompoundWordTokenFilter creates overlapping tokens with onlyLongestMatch enabled",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "6.6",
        "status": "Open",
        "type": "Bug",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": []
    },
    "description": "The\u00a0HyphenationCompoundWordTokenFilter creates overlapping tokens even if\u00a0onlyLongestMatch is enabled.\u00a0\n\nExample:\n\nDictionary: gesellschaft, schaft\n Hyphenator: de_DR.xml //from Apche Offo\n onlyLongestMatch: true\n\n\u00a0\n\n\n\ntext\ngesellschaft\ngesellschaft\nschaft\n\n\nraw_bytes\n[67 65 73 65 6c 6c 73 63 68 61 66 74]\n[67 65 73 65 6c 6c 73 63 68 61 66 74]\n[73 63 68 61 66 74]\n\n\nstart\n0\n0\n0\n\n\nend\n12\n12\n12\n\n\npositionLength\n1\n1\n1\n\n\ntype\nword\nword\nword\n\n\nposition\n1\n1\n1\n\n\n\n\n\nIMHO this includes 2 unexpected Tokens\n\n\tthe 2nd 'gesellschaft' as it duplicates the original token\n\tthe 'schaft' as it is a sub-token 'gesellschaft' that is present in the dictionary",
    "attachments": {
        "lucene-8183.zip": "https://issues.apache.org/jira/secure/attachment/12911784/lucene-8183.zip",
        "LUCENE-8183_20180227_rwesten.diff": "https://issues.apache.org/jira/secure/attachment/12912280/LUCENE-8183_20180227_rwesten.diff",
        "LUCENE-8183_20180223_rwesten.diff": "https://issues.apache.org/jira/secure/attachment/12911711/LUCENE-8183_20180223_rwesten.diff"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16374237",
            "date": "2018-02-23T11:29:25+0000",
            "content": "Added a patch that shows how the unexpected behaviour could be fixed. ",
            "author": "Rupert Westenthaler"
        },
        {
            "id": "comment-16374753",
            "date": "2018-02-23T18:03:19+0000",
            "content": "Hi,\nI have not noticed this with my dictionaries, I have to dig into that. Did you also check the German dictionary which is provided here: https://github.com/uschindler/german-decompounder ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16374886",
            "date": "2018-02-23T19:35:49+0000",
            "content": "Rupert Westenthaler Quick question regarding your patch: What's the reasoning behind not decomposing terms that are part of the dictionary at all?\n\nThe onlyLongestMatch flag currently affects whether all matches or only the longest match should be returned per start character (in DictionaryCompoundWordTokenFilter) or per\u00a0hyphenation start point (in HyphenationCompoundWordTokenFilter).\n\nExample:\n Dictionary \"Schaft\", \"Wirt\", \"Wirtschaft\", \"Wissen\", \"Wissenschaft\" for input \"Wirtschaftswissenschaft\" will return the original input plus tokens \"Wirtschaft\", \"schaft\", \"wissenschaft\", \"schaft\" but not \"Wirt\" or \"Wissen\". \"schaft\" is still returned (even twice) because it's the longest token starting at the respective position.\n\nI like the idea of restricting this further to only the longest terms that touch a certain hyphenation point. This would exclude \"schaft\" in the example above (as \"Wirtschaft\" and \"wissenschaft\" are two longer terms encompassing the respective hyphenation point). On the other hand, there might be examples where you still want to include the \"overlapping\" tokens. For \"Fu\u00dfballpumpe\" and dictionary \"Ball\", \"Ballpumpe\", \"Pumpe\", \"Fu\u00df\", \"Fu\u00dfball\" you would get the tokens \"Fu\u00dfball\" and \"pumpe\" but not \"Ballpumpe\" as \"Ball\" has already been considered part of Fu\u00dfball. Also, not sure if your\u00a0change also improves the situation for languages other than German.\n\nRegarding point 1: The current algorithm always returns the term itself again if it's part of the dictionary. I guess, this could be changed if we don't check against this.maxSubwordSize but against Math.min(this.maxSubwordSize), termAtt.length()-1)\n\nPerhaps these kind of adjustments should rather be done in a TokenFilter similar to RemoveDuplicatesTokenFilter instead of complicating the decompounding algorithm? ",
            "author": "Matthias Krueger"
        },
        {
            "id": "comment-16374888",
            "date": "2018-02-23T19:36:39+0000",
            "content": "AFAIK I use exactly this dictionary and\u00a0hyphenator config. I will provide a Solr core config that can be used to reproduce the described issue. ",
            "author": "Rupert Westenthaler"
        },
        {
            "id": "comment-16374916",
            "date": "2018-02-23T20:05:03+0000",
            "content": "I was not aware that this is the intended behaviour.\n\nFor \"Fu\u00dfballpumpe\" and dictionary \"Ball\", \"Ballpumpe\", \"Pumpe\", \"Fu\u00df\", \"Fu\u00dfball\" you would get the tokens \"Fu\u00dfball\" and \"pumpe\" but not \"Ballpumpe\" as \"Ball\" has already been considered part of Fu\u00dfball. Also, not sure if your change also improves the situation for languages other than German.\n\nThats a good point. Maybe one should still consider parts that are not enclosed by an token that was already decomposed. So for Fu\u00dfballpumpe: ball would be ignored as {Fu\u00dfball is already present, but ballpumpe would still be added as token. Finally pumpe is ignored as ballpumpe is present.\n\nThis reminds me to ALL, NO_SUB and LONGEST_DOMINANT_RIGHT as supported by the Solr Text Tagger\n\n\nPerhaps these kind of adjustments should rather be done in a TokenFilter similar to RemoveDuplicatesTokenFilter instead of complicating the decompounding algorithm?\nI am aware of this possibility. In fact I do use the\u00a0RemoveDuplicatesTokenFilter to remove those tokens. My point was just why they are added in the first place. ",
            "author": "Rupert Westenthaler"
        },
        {
            "id": "comment-16375144",
            "date": "2018-02-23T23:59:25+0000",
            "content": "Rupert Westenthaler: I was not aware that this was my dictionary file! The names in your example (under \"environment in your report) did not look like the example listed here: https://github.com/uschindler/german-decompounder ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16375147",
            "date": "2018-02-24T00:02:14+0000",
            "content": "I am aware of this possibility. In fact I do use the RemoveDuplicatesTokenFilter to remove those tokens. My point was just why they are added in the first place.\n\nI think it's good to not add them in the first place. The change is quite simple, so it can be done here. And it does not really complicate the algorithm as its done at one separated place. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16375154",
            "date": "2018-02-24T00:08:51+0000",
            "content": "I have to check the algorithm, but to make this patch into lucene, the test cases need to be adapted to check this new behaviour. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16376748",
            "date": "2018-02-26T12:07:12+0000",
            "content": "FYI: I pan to spend some time to implement a version of the DictionaryCompoundWordTokenFilter that adds options for\n\n\n\t`noSub`: no tokens are added the are completely enclosed by an longer (`fu\u00dfballpumpe`: `fu\u00dfball`, `ballpumpe`)\n\t`noOverlap`: no overlapping tokens (`fu\u00dfballpumpe`; `fu\u00dfball`, `pumpe`)\n\n\n\nIMO the simplest way is to first emit all tokens and later filter those based on the active options (`onlyLongestMatch`, `noSub`, `noOverlap`).\n\n\nRegarding the test:\n\nProviding good test examples is hard as the current test cases are based on a\u00a0Danish and I do not speak this language\nProviding examples in German would be easy, but this would require a German\u00a0hyphenator and the file is licensed under the\u00a0LaTeX Project Public License and can therefore not be included in the source.\nGiven suitable examples the implementation of the actual test seams to be rather easy as they can be implemented similar to the existing test cases ",
            "author": "Rupert Westenthaler"
        },
        {
            "id": "comment-16376826",
            "date": "2018-02-26T13:06:24+0000",
            "content": "You might want to have a look at \"mocking\" the HyphenationTree (see my patch for LUCENE-8185) which simplifies writing a decompounding test. ",
            "author": "Matthias Krueger"
        },
        {
            "id": "comment-16376887",
            "date": "2018-02-26T13:50:00+0000",
            "content": "Thats helpful indeed! thx ",
            "author": "Rupert Westenthaler"
        },
        {
            "id": "comment-16378763",
            "date": "2018-02-27T15:24:46+0000",
            "content": " Patch: LUCENE-8183_20180227_rwesten.diff \n\nNew Parameters:\n\n\n\tnoSubMatches: true/false\n\tnoOverlappingMatches: true/false\n\n\n\ntogether with the existing onlyLongestMatch those can be used to define what subwords should be added as tokens. Functionality is as described above.\n\nTypically users will only want to include one of the three attributes as enabling noOverlappingMatches is the most restrictive and noSubMatches is more restrictive as onlyLongestMatch. When enabling a more restrictive option the state of the less restrictive does not have any effect.\n\nBecause of that it would be an option to refactor this to an single attribute with different setting, but this would require to think about backward compatibility for configurations that do use onlyLongestMatch=true at the moment.\n\nAlgorithm\n\nIf processing of subWords is deactivated (any of onlyLongestMatch,  noSubMatches, noOverlappingMatches is active) the algorithm first checks if the token is part of the dictionary. If so it returns immediately. This is to avoid adding tokens for subwords if the token itself is in the dictionary (see #testNoSubAndTokenInDictionary for more info).\n\nI changed the iteration direction of the inner for loop to start with the longest possible subword as this simplified the code. \n\nNOTE: that this also changes the order of the Tokens in the token stream but as all tokens are at the same position that should not make any difference. I had however to modify some existing tests as those where sensitive to the ordering\n\nh3 Tests\n\nI added two test methods in TestCompoundWordTokenFilter\n\n1. #testNoSubAndNoOverlap() tests the expected behaviour of the noSubMatches and noOverlappingMatches options\n2. #testNoSubAndTokenInDictionary() tests that no tokens for subwords are added in the case that the token in part of the dictionary\n\nIn addition  TestHyphenationCompoundWordTokenFilterFactory#testLucene8183() asserts that the new configuration options are parsed.\n\nh3 Environment\n\nThis patch is based on master from git@github.com:apache/lucene-solr.git commit: d512cd7604741a2f55deb0c840188f0342f1ba57 ",
            "author": "Rupert Westenthaler"
        }
    ]
}