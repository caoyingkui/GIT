{
    "id": "LUCENE-7342",
    "title": "WordDelimiterFilter should observe KeywordAttribute to pass these tokens through",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "I have a text analysis requirement in which I want certain tokens to not be processed by WordDelimiterFilter \u2013 i.e. they should pass through that filter.  WDF, like several other TokenFilters, has a configurable word list but this list is static producing a concrete CharArraySet.  Thus, for example, I can't filter by a regexp nor can I filter based on other attributes.\n\nA simple solution that makes sense to me is to have WDF use KeywordAttribute to know if it should skip the token.  KeywordAttribute seems fairly generic as to how it can be used, although granted today it's only used by the stemmers.  That attribute isn't named \"StemmerIgnoreAttribute\" or some-such; it's generic so I think it's fine for WDF to use it in a similar way.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15334528",
            "author": "David Smiley",
            "date": "2016-06-16T19:43:00+0000",
            "content": "I considered wrapping WDF; it was an interesting thought experiment with a possible solution but our TokenStream API makes doing this very complex.  It would involve an additional collaborating TokenStream instance to provide as the input to the delegated TokenFilter, thus intercepting the input. The input intercepting TokenFilter would detect a token should pass through and then captureState() in a loop until it finds a token not matching the predicate.  The wrapping TokenFilter would call delegateTokenFilter.incrementToken() but then would see if there are any cached tokens.  If there are, it would captureState, replay the cached tokens, then replay the just captured state. This is a big mess, awkward to use, and has some overhead. "
        },
        {
            "id": "comment-15334544",
            "author": "David Smiley",
            "date": "2016-06-16T19:50:46+0000",
            "content": "A separate issue might be to refactor the APIs of TokenFilters that take a CharArraySet input to instead take a java.util.function.Predicate<CharSequence>.  Advanced users could even construct a Predicate instance with access to the AttributeSource to look at whatever attributes it wants, provided that the TokenFilters only invoke it when the token stream is positioned to the token in question. "
        }
    ]
}