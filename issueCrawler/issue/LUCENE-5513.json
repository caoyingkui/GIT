{
    "id": "LUCENE-5513",
    "title": "Binary DocValues Updates",
    "details": {
        "type": "Wish",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed",
        "components": [
            "core/index"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.8",
            "6.0"
        ]
    },
    "description": "LUCENE-5189 was a great move toward. I wish to continue. The reason for having this feature is to have \"join-index\" - to write children docnums into parent's binaryDV. I can try to proceed the implementation, but I'm not so experienced in such deep Lucene internals. Shai Erera, any hint to begin with is much appreciated.",
    "attachments": {
        "LUCENE-5513.patch": "https://issues.apache.org/jira/secure/attachment/12634409/LUCENE-5513.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13925695",
            "author": "Shai Erera",
            "content": "Mikhail Khludnev, great that you want to take a crack at that. I will help as best as I can. I would start with the high level IW.updateNumericDV API and follow the breadcrumb trail to add the BDV support. The main classes are IW, DW, BufferedDeletes, BufferedDeleteStream, NumericFieldUpdates and ReaderAndUpdates (and of course all the tests: TestNumericDocValuesUpdates and TestIndexWriterExceptions.testNoLostDeletesOrUpdates).\n\nAt first I think it's best to just follow the NumericDV approach (which copies the entire NDV to the update file, altering the affected documents' values). We can then consider other approaches (as BinaryDV is more expensive than NumericDV to just copy around). But I'm fine if we do it in incremental steps. ",
            "date": "2014-03-10T12:18:55+0000"
        },
        {
            "id": "comment-13932464",
            "author": "Shai Erera",
            "content": "Mikhail Khludnev, I started working on this and made some progress. But I've identified a need to do some refactoring to how the updates are represented internally today in order to keep the code (and more importantly, me!) sane. So let me know if you've started to work on it as well, so we can sync. ",
            "date": "2014-03-12T21:59:10+0000"
        },
        {
            "id": "comment-13932942",
            "author": "Mikhail Khludnev",
            "content": "Shai Erera I started too. However, I can't spend much time, and have no deep understanding of the core.\nFor a while I copied testSimple() from Numeric DV update, check that it's red. Now, I'm coming through layers, mostly coping Numeric DV update into Binary DV logic. So, far not so much. ",
            "date": "2014-03-13T07:23:55+0000"
        },
        {
            "id": "comment-13932947",
            "author": "Shai Erera",
            "content": "I'll upload a patch a bit later \u2013 I've made some good progress last night and already started to cutover tests. The only thing I don't yet handle are updates that are coming in while a merge is in flight, as this requires some refactoring to the code. I will handle that too, but will upload a patch before that to checkpoint progress. ",
            "date": "2014-03-13T07:30:25+0000"
        },
        {
            "id": "comment-13933058",
            "author": "Shai Erera",
            "content": "Patch:\n\n\n\tAdd IW.updateBinaryDocValue\n\tMakes necessary changes to DW, BufferedUpdates(Stream), ReaderAndUpdates\n\tAdd new BinaryUpdate and BinaryFieldUpdates\n\tCopied TestNumericDocValuesUpdates and changed to add BDV fields:\n\t\n\t\tI still add numbers as it makes asserting easy, but I encode them as VLongs, so we get different lengths of byte[]\n\t\tThere are some tests still disabled, see below\n\t\n\t\n\n\n\nPatch still doesn't handle updates that came in while a merge was in flight. The reason is that the code in IW.commitMergedDeletes is hairy and adding BinaryDV updates will make it even worse. So I want to refactor how the updates are represented internally, such that there is a single class DocValuesUpdates which captures all DV updates. Since the DV fields cannot overlap (a DV field cannot be both numeric and binary), I think it will allow us to use a single UpdatesIterator from IW.commitMergedDeletes. I'll take a look at that next and re-enable the tests after this has been resolved.\n\nThere's one thing to note \u2013 binary DV updates are more expensive to apply than NDV updates, depends on the length of the BDV. I.e. when we rewrite the DV file, then for NDV we know we write at most 8 bytes per document (compressed), but for BDV we may write a large number of bytes per document. I prefer to leave that as an optimization for later. One idea I have (which applies to NDV as well) is to do that in a sparse DV, or add \"stacked\" DV files. Either will make the producing code more complex, and therefore I prefer to explore that later. ",
            "date": "2014-03-13T09:53:28+0000"
        },
        {
            "id": "comment-13933060",
            "author": "Michael McCandless",
            "content": "Looks good Shai!\n\nI agree we need a refactoring of commitMergedDeletes, and we shouldn't worry about stacking/optimizing now. ",
            "date": "2014-03-13T10:10:56+0000"
        },
        {
            "id": "comment-13937491",
            "author": "Shai Erera",
            "content": "Patch makes the following refactoring changes (all internal API):\n\n\n\tDocValuesUpdate abstract class w/ common implementation for NumericDocValuesUpdate and BinaryDocValuesUpdate.\n\n\n\n\n\tDocValuesFieldUpdates hold the doc+updates for a single field. It mostly defines the API for the Numeric* and Binary* implementations.\n\n\n\n\n\tDocValuesFieldUpdates.Container holds numeric+binary updates for a set of fields. It is as its name says \u2013 a container of updates used by ReaderAndUpdates.\n\t\n\t\tIt helps not bloat the API w/ more maps being passed as well as simplified BufferedUpdatesStream and IndexWriter.commitMergedDeletes.\n\t\tIt also serves as a factory method based on the updates Type\n\t\n\t\n\n\n\n\n\tFinished TestBinaryDVUpdates\n\n\n\n\n\tAdded TestMixedDVUpdates which ports some of the 'big' tests from both TestNDV/BDVUpdates and mixes some NDV and BDV updates.\n\t\n\t\tI'll beast it some to make sure all edge cases are covered.\n\t\n\t\n\n\n\nI may take a crack at simplifying IW.commitMergedDeletes even more by pulling a lot of duplicate code into a method. This is impossible now because those sections modify more than one state variables, but I'll try to stuff these variables in a container to make this method more sane to read.\n\nOtherwise, I think it's ready. ",
            "date": "2014-03-17T06:16:31+0000"
        },
        {
            "id": "comment-13937696",
            "author": "Shai Erera",
            "content": "Fixed silly bug in BinaryDocValuesFieldUpdates.merge(). ",
            "date": "2014-03-17T11:26:33+0000"
        },
        {
            "id": "comment-13938009",
            "author": "Michael McCandless",
            "content": "Hmm, on beasting (TestSortingMergePolicy.testSortingMP -seed 2B748835E48BB14A -jvms 6 -noc) I hit this failure:\n\n\n.mar 17, 2014 5:41:41 EM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\nVARNING: Uncaught exception in thread: Thread[Lucene Merge Thread #8,6,TGRP-TestSortingMergePolicy]\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException\n\tat __randomizedtesting.SeedInfo.seed([2B748835E48BB14A]:0)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:545)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)\nCaused by: java.lang.NullPointerException\n\tat org.apache.lucene.index.IndexWriter.commitMergedDeletesAndUpdates(IndexWriter.java:3468)\n\tat org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3530)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4222)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3679)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\n\nEENOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.\nNOTE: reproduce with: ant test  -Dtestcase=TestSortingMergePolicy -Dtests.method=testSortingMP -Dtests.seed=2B748835E48BB14A -Dtests.linedocsfile=/lucenedata/hudson.enwiki.random.lines.txt -Dtests.locale=sv_SE -Dtests.timezone=Africa/Malabo -Dtests.file.encoding=UTF-8\nNOTE: test params are: codec=Lucene46: {s=PostingsFormat(name=FSTPulsing41)}, docValues:{ndv=DocValuesFormat(name=SimpleText)}, sim=RandomSimilarityProvider(queryNorm=true,coord=yes): {}, locale=sv_SE, timezone=Africa/Malabo\nNOTE: Linux 3.5.0-47-generic amd64/Oracle Corporation 1.7.0_60-ea (64-bit)/cpus=8,threads=1,free=401611472,total=515375104\nNOTE: All tests run in this JVM: [TestSortingMergePolicy]\n\nTime: 2.051\nThere were 2 failures:\n1) testSortingMP(org.apache.lucene.index.sorter.TestSortingMergePolicy)\njava.lang.AssertionError: ndv(89)=8960834324998763998,ndv(90)=-6235091358187467651\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.assertTrue(Assert.java:43)\n\tat org.apache.lucene.index.sorter.TestSortingMergePolicy.assertSorted(TestSortingMergePolicy.java:162)\n\tat org.apache.lucene.index.sorter.TestSortingMergePolicy.testSortingMP(TestSortingMergePolicy.java:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)\n\tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\n\tat org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\n\tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\n\tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\n\tat java.lang.Thread.run(Thread.java:744)\n2) testSortingMP(org.apache.lucene.index.sorter.TestSortingMergePolicy)\ncom.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=21, name=Lucene Merge Thread #8, state=RUNNABLE, group=TGRP-TestSortingMergePolicy]\nCaused by: org.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException\n\tat __randomizedtesting.SeedInfo.seed([2B748835E48BB14A]:0)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:545)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)\nCaused by: java.lang.NullPointerException\n\tat org.apache.lucene.index.IndexWriter.commitMergedDeletesAndUpdates(IndexWriter.java:3468)\n\tat org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3530)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4222)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3679)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\n\n ",
            "date": "2014-03-17T16:42:46+0000"
        },
        {
            "id": "comment-13938260",
            "author": "Shai Erera",
            "content": "Thanks Mike. In an edge case where there are field updates, but also deletes, such that all of the updated documents were deleted, I created the DVFUpdates instances prematurely, leading to the NPE. Patch fixes this as well as integrated BDV updates in TestIWExceptions.testNoLostDeletesOrUpdates. ",
            "date": "2014-03-17T19:31:53+0000"
        },
        {
            "id": "comment-13938416",
            "author": "Michael McCandless",
            "content": "+1 to commit... patch looks great.  And beasting isn't uncovering any more failures... ",
            "date": "2014-03-17T21:31:10+0000"
        },
        {
            "id": "comment-13938808",
            "author": "Shai Erera",
            "content": "Thanks Mike, will wrap up and commit.\n\nOne thing I wanted to note, and specifically emphasized in javadocs, is that IW.updateBinaryDocValue replaces the existing byte[] value of all affected documents. We could also easily implement an append type of update, where the given bytes are appended to all affected documents. It's only a matter of defining that on the update itself and in ReaderAndUpdates, instead of overriding a document's value, we read its current value from the reader and append the new bytes.\n\nUnlike NDV updates, append for Binary (and SortedSet) has more value, since it lets you add values to documents whose existing values may not be currently identical, where the current implementation ignores all existing values and makes all affected documents identical. Perhaps it's acceptable, depending on the nature of the update (e.g. update by PK), but I think we should explore adding update capabilities to Binary and SortedSet DV. And also the IW.update API to allow updating by more than just Term, e.g. this thread: http://markmail.org/message/2wmpvksuwc5t57pg.\n\nThese are all for separate issues though. ",
            "date": "2014-03-18T04:47:36+0000"
        },
        {
            "id": "comment-13938977",
            "author": "ASF subversion and git services",
            "content": "Commit 1578784 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1578784 ]\n\nLUCENE-5513: add IndexWriter.updateBinaryDocValue ",
            "date": "2014-03-18T08:56:13+0000"
        },
        {
            "id": "comment-13938992",
            "author": "ASF subversion and git services",
            "content": "Commit 1578790 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1578790 ]\n\nLUCENE-5513: add IndexWriter.updateBinaryDocValue ",
            "date": "2014-03-18T09:25:27+0000"
        },
        {
            "id": "comment-13938994",
            "author": "Shai Erera",
            "content": "Committed to trunk and 4x. Thanks Mike for the review! ",
            "date": "2014-03-18T09:26:20+0000"
        },
        {
            "id": "comment-13939017",
            "author": "ASF subversion and git services",
            "content": "Commit 1578803 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1578803 ]\n\nLUCENE-5513: suppress codecs in test ",
            "date": "2014-03-18T09:51:12+0000"
        },
        {
            "id": "comment-13939088",
            "author": "ASF subversion and git services",
            "content": "Commit 1578831 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1578831 ]\n\nLUCENE-5513: fix bad svn merge ",
            "date": "2014-03-18T11:35:18+0000"
        },
        {
            "id": "comment-13982504",
            "author": "Uwe Schindler",
            "content": "Close issue after release of 4.8.0 ",
            "date": "2014-04-27T23:25:32+0000"
        }
    ]
}