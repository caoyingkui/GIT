{
    "id": "LUCENE-4547",
    "title": "DocValues field broken on large indexes",
    "details": {
        "components": [],
        "fix_versions": [
            "4.2",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I tried to write a test to sanity check LUCENE-4536 (first running against svn revision 1406416, before the change).\n\nBut i found docvalues is already broken here for large indexes that have a PackedLongDocValues field:\n\n\nfinal int numDocs = 500000000;\nfor (int i = 0; i < numDocs; ++i) {\n  if (i == 0) {\n    field.setLongValue(0L); // force > 32bit deltas\n  } else {\n    field.setLongValue(1<<33L); \n  }\n  w.addDocument(doc);\n}\nw.forceMerge(1);\nw.close();\ndir.close(); // checkindex\n\n\n\n\n[junit4:junit4]   2> WARNING: Uncaught exception in thread: Thread[Lucene Merge Thread #0,6,TGRP-Test2GBDocValues]\n[junit4:junit4]   2> org.apache.lucene.index.MergePolicy$MergeException: java.lang.ArrayIndexOutOfBoundsException: -65536\n[junit4:junit4]   2> \tat __randomizedtesting.SeedInfo.seed([5DC54DB14FA5979]:0)\n[junit4:junit4]   2> \tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:535)\n[junit4:junit4]   2> \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:508)\n[junit4:junit4]   2> Caused by: java.lang.ArrayIndexOutOfBoundsException: -65536\n[junit4:junit4]   2> \tat org.apache.lucene.util.ByteBlockPool.deref(ByteBlockPool.java:305)\n[junit4:junit4]   2> \tat org.apache.lucene.codecs.lucene40.values.FixedStraightBytesImpl$FixedBytesWriterBase.set(FixedStraightBytesImpl.java:115)\n[junit4:junit4]   2> \tat org.apache.lucene.codecs.lucene40.values.PackedIntValues$PackedIntsWriter.writePackedInts(PackedIntValues.java:109)\n[junit4:junit4]   2> \tat org.apache.lucene.codecs.lucene40.values.PackedIntValues$PackedIntsWriter.finish(PackedIntValues.java:80)\n[junit4:junit4]   2> \tat org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:130)\n[junit4:junit4]   2> \tat org.apache.lucene.codecs.PerDocConsumer.merge(PerDocConsumer.java:65)",
    "attachments": {
        "test.patch": "https://issues.apache.org/jira/secure/attachment/12552474/test.patch",
        "LUCENE-4547.patch": "https://issues.apache.org/jira/secure/attachment/12568149/LUCENE-4547.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-11-07T14:54:03+0000",
            "content": "Here was my initial test, just screwing around.\n\nI ran with 'ant test -Dtestcase=Test2GBDocValues -Dtests.nightly=true -Dtests.heapsize=5G' ",
            "author": "Robert Muir",
            "id": "comment-13492401"
        },
        {
            "date": "2012-11-07T15:00:38+0000",
            "content": "There is even a out-of-coffee bug in the test, its only using like 2 bits per value \nSo this is really even worse. \n\nI'm not sure we should be using ByteBlockPool etc here. I think it shouldnt be used outside of the indexer. ",
            "author": "Robert Muir",
            "id": "comment-13492405"
        },
        {
            "date": "2012-11-07T15:07:29+0000",
            "content": "editing description: I think it affects more than PackedIntValues actually?\n\nI think the bug is in how FixedStraightBytesImpl uses byteblockpool.\n\nSo this means the problem should be way more widespread: e.g. if you have lots of documents in general I think you are fucked (as norms should trip it too). ",
            "author": "Robert Muir",
            "id": "comment-13492411"
        },
        {
            "date": "2012-11-07T15:24:36+0000",
            "content": "Another bug is that I had to pass tests.heapsize at all.\n\nI think its bad that docvalues gobbles up so much ram when merging.\nCant we merge this stuff from disk? ",
            "author": "Robert Muir",
            "id": "comment-13492421"
        },
        {
            "date": "2012-11-15T21:16:55+0000",
            "content": "I'm having a look at the branch, and it looks great! I like the fact that there are less types and that values are buffered into memory so that the doc values format can make decisions depending on the number of distinct values, ... Still, I have some questions on what you plan to do with this branch:\n\n\tdo you plan to use this branch to:\n\tfix other issues such as LUCENE-3862?\n\tmerge the FieldCache / FunctionValues / DocValues.Source APIs?\n\tare you going to remove DocValues.Type.FLOAT_*?\n\tare SimpleDVConsumer and SimpleDocValuesFormat going to replace PerDocConsumer and DocValuesFormat?\n\tare you going to remove hasArray/getArray?\n\twill there still be a direct=true|false option at load-time or will it depend on the format impl (potentially with a PerFieldPerDocProducer similarly to the postings formats)?\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13498345"
        },
        {
            "date": "2012-11-15T21:35:03+0000",
            "content": "These are hard questions. My personal goal here for this prototype (currently SimpleText only!) was to:\n\n1. Making merging use (significantly) less RAM, to fix this bug.\n2. Make it easier to write docvalues codecs, to encourage innovations (e.g. FST impls, etc etc)\n3. Simplify the types to make it easier on the user.\n\nthe consumer api I think is simpler (part of #2), but I would like to (in the future) simplify the producer API too.\nI'm not sure if we should do it here though? anyway we can think about the issues you raised one by one and do them separately on their own issues.\n\n\nfix other issues such as LUCENE-3862?\n\nIts my opinion we should do this sooner than later.\n\n\nmerge the FieldCache / FunctionValues / DocValues.Source APIs?\n\nThis really needs to be addressed, but I think not here. Its horrific that algorithms like grouping, sorting, and maybe faceting have to be duplicated for 2 different things (fieldcache and docvalues).\n\n\nare you going to remove DocValues.Type.FLOAT_*?\n\nI think the 3 types we have here are enough. Someone can do a float or double type \"on top of\" the \"number\" type we have.\nLucene is already doing this today: look at norms. I think lucene should just have a number type that stores bits.\n\n\nare SimpleDVConsumer and SimpleDocValuesFormat going to replace PerDocConsumer and DocValuesFormat?\n\nThis is the idea, once we are happy with the APIs we would implement the 4.0 ones with these apis. \n\n\nare you going to remove hasArray/getArray?\n\nI don't care about this. I am unsure similarity impls should be calling this though, definitely at least\nit would be better for them to fall-back: I just cant bring myself to fix it until LUCENE-3862 is fixed \n\n\nwill there still be a direct=true|false option at load-time or will it depend on the format impl (potentially with a PerFieldPerDocProducer similarly to the postings formats)?\n\nI don't want to change this in the branch. Personally i feel like a codec/segmentreader/etc should generally only manage\ndirect, producer exposing the same \"stats\" (minimum, maximum, fixed, whatever) that the consumer apis get (which will also make merging more efficient!) default source impl can be something nice, read the direct impl into a packed ints,\nand so on. Codec could override to e.g. just slurp in their on-disk packed ints directly. So codec still has control\nof the in-memory RAM representation, i think this is important. But i think codec and segmentreader should somehow not\nbe in control of caching: this should be elsewhere (FieldCache.DOCVALUES.xxx????)... ",
            "author": "Robert Muir",
            "id": "comment-13498359"
        },
        {
            "date": "2012-11-16T12:03:54+0000",
            "content": "About the current SimpleDVConsumer API: NumericDocValuesConsumer addNumericField(FieldInfo field, long minValue, long maxValue) only allows for compression based on the width of the range but there are many other ways to compress values (maybe there are very few unique values, maybe the last 3 bits are always the same, etc...). Now that we buffer values into memory, what would you think of changing the API to pass an iterable of longs instead so that the doc values format can make better decisions?\n ",
            "author": "Adrien Grand",
            "id": "comment-13498754"
        },
        {
            "date": "2012-11-16T12:17:24+0000",
            "content": "I like that idea!  So codec could iterate once, gathering whatever stats it needs, and then iterate again to do the writing.  Should we not include the long min/maxValues then...? ",
            "author": "Michael McCandless",
            "id": "comment-13498758"
        },
        {
            "date": "2012-11-16T13:36:47+0000",
            "content": "So codec could iterate once, gathering whatever stats it needs, and then iterate again to do the writing.\n\nYep.\n\nShould we not include the long min/maxValues then...?\n\nI think so? And we could do something similar with addBinaryField and addSortedField. I think there are many possible optimizations based on how much lengths vary, whether bytes refs share prefixes or not, ... ",
            "author": "Adrien Grand",
            "id": "comment-13498819"
        },
        {
            "date": "2012-11-16T14:47:20+0000",
            "content": "FYI - I just added simple numeric & binary impls for Lucene41 test demo passes for Lucene41 execpt of the sorted test (no impls yet) ",
            "author": "Simon Willnauer",
            "id": "comment-13498837"
        },
        {
            "date": "2012-11-16T14:48:37+0000",
            "content": "What would the flush/merge api look like?\nWould it get simple or more complicated?\nCould we still require certain stats from the Producer, so that we can have a default, efficient in-RAM Source impl?\n\n\nI think there are many possible optimizations based on how much lengths vary, whether bytes refs share prefixes or not, ...\n\nmaybe, but arguably we should do the simplest possible thing that can work given the codecs we have today. When\ndesigning these apis, to me these are the only ones that exist... ",
            "author": "Robert Muir",
            "id": "comment-13498838"
        },
        {
            "date": "2012-11-16T16:01:46+0000",
            "content": "The API could look like\n\nclass DocValuesConsumer {\n\n  // add all values in a single call\n  void addNumericField(FieldInfo field,  Collection<Long> values); // values.size() == numDocs\n  void addBinaryField(FieldInfo field, Collection<BytesRef> values); // values.size() == numDocs\n  void addSortedField(FieldInfo field, Collection<BytesRef> ordToValue, Collection<Long> docToOrd); // docToOrd.size() == numDocs, ordToValue.size() == valueCount\n\n  // same merge API\n\n}\n\n\nI don't see why merging would be more complicated but maybe I'm missing something? The default merge impls would need to use lazy collection impls in order to remain memory-efficient.\n\nCould we still require certain stats from the Producer, so that we can have a default, efficient in-RAM Source impl?\n\nWhy would we need stats to make a Source impl efficient?\n\nmaybe, but arguably we should do the simplest possible thing that can work given the codecs we have today\n\nTo me it looks as simple as the current API. ",
            "author": "Adrien Grand",
            "id": "comment-13498877"
        },
        {
            "date": "2012-11-16T16:24:04+0000",
            "content": "1. We don't need to be passing numDocs today: this should be removed because its duplicated with SegmentWriteState\n\n2. Merging becomes more complex because the api requires this collection view: today it does not.\n\n3. We need stats to make the Source impl efficient so we can e.g. use packed integers for Number type. Bye bye arrays.\n\nI don't think we should make anything more flexible than necessary until things like the producer api are sorted out.\nOtherwise its difficult to see the tradeoffs. ",
            "author": "Robert Muir",
            "id": "comment-13498890"
        },
        {
            "date": "2012-11-16T17:36:12+0000",
            "content": "I don't think we should make anything more flexible than necessary until things like the producer api are sorted out.\n\nI can wait for the producer API. I just wanted to point out that by exposing all the values at once, the DocValuesFormat could make more clever choices than by just exposing the width of the range of values. ",
            "author": "Adrien Grand",
            "id": "comment-13498953"
        },
        {
            "date": "2012-11-16T19:00:29+0000",
            "content": "I committed an initial attempt at a simpler producer API ... it's rough!!  But at least TestDemoDocValue passes w/ SimpleText.\n\nI moved SimpleText's \"loaded into RAM\" wrappers up into SimpleDVProducer; this way a codec only must impl the direct source and can impl in-RAM source if it wants to.\n\nSegmentCoreReaders now does the caching of in-RAM sources.\n\nSimon I temporarily disabled the Lucene41DV producer ... I'll go get it working too ...  ",
            "author": "Michael McCandless",
            "id": "comment-13499020"
        },
        {
            "date": "2012-11-19T11:53:51+0000",
            "content": "hey folks,\n\nI looked at the branch and I would want to suggest we move a little slower here. we are doing too many things at once. Like I really don't like the trend to make FieldCache the single source for caching. FieldCache has many problems in my opininon like is uses this DEFAULT singleton, has a single way of how things are cached per reader some users might want to use different access to DV like in ES we don't use FieldCache at all for many reasons.I think we are going into the right direction here but exposing everything through FC is a no-go IMO. I do see why we should merge the interfaces and expose un-inverted fields via the new DV interface - nice! but hiding it behind FC is no good. \nI also don't like the way how \"in-ram\" DV are exposed. I don't think we should have newRAMInstance() on the interface. Lets keep the interface clean and don't mix in how it is represented. I'd rather vote for dedicated producers or SimpleDocValuesProducer#getNumericDocValues(boolean inMemory). Then we can still do caching on top. The producer should really be simple and shouldn't do caching. We can also separate the default in-memory impls in a simple helper class with methods like static NumericDocValues load(NumericDocValues directDocValues) ",
            "author": "Simon Willnauer",
            "id": "comment-13500177"
        },
        {
            "date": "2012-11-19T12:52:08+0000",
            "content": "If you don't want like FieldCache in ES, then use something else.\n\nIts really broken that things like grouping and faceting are coded to a separate API. This makes DV unsuccessful. If we arent going to fix DocValues and Faceting APIs then perhaps we shoudl consider removing DocValues completely from lucene.\n\nPlease try to give the branch some time. I committed some work last night very late and got tired and went to sleep. Its not \"ready\" and i'm not threatening to commit to trunk.\n\nI created this branch to develop publicly. I don't have to do that: I can develop privately instead and not deal with complaints about my every step before I even say things are ready. I just thought it would make it easier for other people to help. ",
            "author": "Robert Muir",
            "id": "comment-13500205"
        },
        {
            "date": "2012-11-19T13:19:58+0000",
            "content": "Its really broken that things like grouping and faceting are coded to a separate API. This makes DV unsuccessful. \n\ndon't get me wrong I think this adds a lot of value. BUT, if the only way to get a DV instance that is cached is FC then this entire thing is inconsistent. We don't ask people to cache TermEnum which can be heavy if you use MemoryPostings for instance. We neither do for StoredFields nor do we have a caching layer that is not used by default. All I am arguing for is that if somebody wants to use DV it should be simple to do so. The distinction between in-memory and on disk should not be in FC.\n\nIf we arent going to fix DocValues and Faceting APIs then perhaps we shoudl consider removing DocValues completely from lucene.\n\nyou mean remove fieldcache? ",
            "author": "Simon Willnauer",
            "id": "comment-13500213"
        },
        {
            "date": "2012-11-19T17:00:55+0000",
            "content": "Just an idea though... while we are on it should we maybe add a 4th type that allows multiple values. that way we can just pull DV from any field and uninvert if needed? ",
            "author": "Simon Willnauer",
            "id": "comment-13500377"
        },
        {
            "date": "2012-11-19T17:27:30+0000",
            "content": "while we are on it should we maybe add a 4th type that allows multiple values\n\n+1. That might allow using DVs for faceted search. ",
            "author": "Shai Erera",
            "id": "comment-13500413"
        },
        {
            "date": "2012-11-19T17:31:05+0000",
            "content": "I don't think this is necessary. Someone can do this \"on top\" of a binary impl themselves. ",
            "author": "Robert Muir",
            "id": "comment-13500418"
        },
        {
            "date": "2012-11-20T15:48:01+0000",
            "content": "Hey folks,\n\nI thought about the IN-RAM vs. ON-Disk distinction we have in DV at this point and how we distinguish API wise. IMO calling $TypeDocValues#newRAMInstance() with different behavior if the instance is already a ram instance is kind of ugly API wise as well as having a binary distinction here might not be sufficient. From my point of view it would logically make most sense to allow the codec to decide if it is in ram or not or if only parts of the values are in memory like in the sorted case where you might wanna use a FST holding a subset of the values. Now giving the control entirely to the code might not be practical. Think about merging where you really don't want to load into memory you should be able to tell don't pull into memory. We can do this already today if we pass in IOContext. Yet, IOContext is the wrong level since its a reader wide setting and might not be true for all fields in the case we open a reader for handling searches. Yet, the idea of IOContext is basically to pass information about the access pattern where merge means sequential access. We might want to use something similar for docvalues that allows us to leave most of the decisions to the codec but if a user decides he really needs stuff in memory he can still pass in something like AccessPattern.SEQUENTIAL and load the values into an auxiliary datastructure. This would allow the codec to optimize under the hood but not making any promises if it's in ram or on disk if AccessPattern.DEFAULT is passed.  ",
            "author": "Simon Willnauer",
            "id": "comment-13501241"
        },
        {
            "date": "2012-11-20T16:19:44+0000",
            "content": "I think multi-valued case could be compelling, but we should probably do later / outside this branch.  EG FieldCache already supports this (DocTermOrds).  It's true that app could do this on top of Binary DV, but I think it's useful enough that a real impl would be worthwhile (eg for facets). ",
            "author": "Michael McCandless",
            "id": "comment-13501271"
        },
        {
            "date": "2012-11-20T16:23:47+0000",
            "content": "I think letting the codec control in-RAM vs on-disk is a great idea!\n\nWhy not let merging load values into RAM if your DVFormat is a RAM-backed impl?  The codec can always override merging if it wants to ... ",
            "author": "Michael McCandless",
            "id": "comment-13501276"
        },
        {
            "date": "2012-11-20T17:51:54+0000",
            "content": "I think letting the codec control in-RAM vs on-disk is a great idea!\nactually that is not what I was saying and I strongly discourage that we require people to make ram vs. on disk decisions ahead of time. Most of those decisions need to be made dynamically based on ram availability and growth.\n\nwhat I was saying is that the user should provide its intend so the codec can optimize. ",
            "author": "Simon Willnauer",
            "id": "comment-13501334"
        },
        {
            "date": "2012-11-20T18:43:13+0000",
            "content": "\nI think letting the codec control in-RAM vs on-disk is a great idea!\nactually that is not what I was saying and I strongly discourage that we require people to make ram vs. on disk decisions ahead of time.\n\nI think this is actually a clean way to do it, and it matches what we\ndo with other codec parts.  Eg with postings you pick MemoryPF if you\nhave the free RAM and want fast lookups for that field, else you pick\nan on-disk postings format.\n\nMost of those decisions need to be made dynamically based on ram availability and growth.\n\nI think making dynamic decisions based on ram availability and growth\nis a more expert use case; eg in Lucene today we don't give you that:\nDeleted docs, norms, field cache entries, doc values (if you sort by\nthem), terms index are all loaded into RAM.  So the only control users\nhave now is which fields they index/sort on...\n\nIf we give control to the codec over whether the DV format is in RAM\nor on disk or something in between (like the terms index), and we make\na PerFieldDVFormat so you can easily switch impls by field, then users\ncan make the decisions themselves, field by field.\n\nIf a given field will be used for sorting or faceting, they can use\nthe fast RAM-based format, but if they are tight on RAM and have lots\nof scoring factors, maybe they use the disk-based impl for those fields.\n\nIf an expert app really need to pick & choose ram vs disk dynamically,\ndepending on how many other indices are open and how much RAM they are\nusing, etc., they can always make a custom DV format ... ",
            "author": "Michael McCandless",
            "id": "comment-13501370"
        },
        {
            "date": "2012-11-20T20:11:35+0000",
            "content": "If an expert app really need to pick & choose ram vs disk dynamically, depending on how many other indices are open and how much RAM they are using, etc., they can always make a custom DV format ...\n\nwhat I am worried about is the lack of communication between the app and the codec. something like this is going to be a major hassle. all I am asking about is to pass in \"hints\" to the codec what I need at a certain point per field. We can't do this and I think we shouldn't allow this. its an encoding / decoding layer and it should be simple. pushing what you call \"experts\" to write their own codecs is a major trap I think. writing a codec is last resort and causes major trouble for non-lucene devs IMO. This is expertexpert \n\nI really like the idea of perfieldDV and I think we should do it. I am just not a big fan of making up-front decisions for this stuff when it comes to on-disk vs. ram. PostingsFormat is a different story, the on disk (low ram useage) have such a perf characteristics that you very unlikely need something else useing lots of ram. For sorting, grouping or scoring you will certainly need that. ",
            "author": "Simon Willnauer",
            "id": "comment-13501436"
        },
        {
            "date": "2012-11-20T20:33:30+0000",
            "content": "one way of merging the two approaches would be a simple boolean that forces on-disk access. that way we can solve the following problems:\n\n\n\tdefault merge impls. a format doesn't need to override merge if its an in-ram by default impl\n\tpeople can build auxiliary structures on top without worrying about ram\n\tleave decisions to the codec what in-memory structure or on-disk structure it uses by default\n\n\n\nthis way we can keep the api clean and support expert users. We can even make this package private so that \"normal\" users won't see it? ",
            "author": "Simon Willnauer",
            "id": "comment-13501447"
        },
        {
            "date": "2012-11-20T20:45:24+0000",
            "content": "This is sounding too complicated. \nI think it sounds ok to remove the distinction of ram/on-disk and just have codec.\nIf someone wants to do expert stuff in their codec thats fine, but we don't need it in our impls or abstract APIS.\n\nLets start with just getting the basics working here and the integration with the rest of lucene simple.\n\nToday (in trunk) on-disk access is a pipe dream (thus, this issue) because the codec api is responsible for too much.\n\nExpert users and flexibility should be our last priority. ",
            "author": "Robert Muir",
            "id": "comment-13501451"
        },
        {
            "date": "2012-11-20T21:10:32+0000",
            "content": "This is sounding too complicated. \na single boolean is too complicated? all I ask for is a way to prevent loading into ram if not necessary. We had this in 4.0 and I think we should make this work in 4.1 too. remember this is a different use-case than postings. I really don't think I ask for much here.  ",
            "author": "Simon Willnauer",
            "id": "comment-13501466"
        },
        {
            "date": "2012-11-20T23:17:49+0000",
            "content": "\na single boolean is too complicated? \n\nI think it is, I feel like it really confuses the API and makes writing codecs harder.\n\nI think it would be better if the codec impl determined this, just like MemoryPostings and so on.\nSo I'd rather have Per-field dv wrapper that configures this.\n\nFor example someone would use a different implementation for their solr __version field than they\nwould use for a scoring factor, and maybe a different implementation for a sort field than a faceting one.\n\nI don't think there is a use case to be able to access a single field's values both from RAM and on disk,\nand for the codec to have to deal with that. It makes things currently very complicated.\n\n\nWe had this in 4.0 and I think we should make this work in 4.1 too.\n\nI don't think thats necessarily true. In 4.0 the one DV impl we had could do a lot, but the codec API is\nvery difficult. I actually contributed to a lot of the codec apis in Lucene, and as a committer I was unable\nto figure out how to write a working DV impl to this api. I think this says a lot.\n\nI'd rather have a simpler codec API, that enables innovation so that we can see cool shit in the future,\nlike implementations geared at sorting and faceting that use less RAM, and so on.\n\nIf someone really needs more fine-grained control than per-field codec API, then there are other ways to achieve\nthat: FileSwitchDirectory, adding such APIs to their own codec, etc. But I'm not sure its mainstream and should\nbe required by all codecs. ",
            "author": "Robert Muir",
            "id": "comment-13501569"
        },
        {
            "date": "2012-11-28T19:38:40+0000",
            "content": "I just got an error at search time fetching DocValues in 4.0.0:\n\nSEVERE: null:java.lang.ArrayIndexOutOfBoundsException\n        at org.apache.lucene.util.PagedBytes$Reader.fillSlice(PagedBytes.java:97)\n        at org.apache.lucene.codecs.lucene40.values.VarStraightBytesImpl$VarStraightSource.getBytes(VarStraightBytesImpl.java:273)\n\n\nI pass in a scratch BytesRef, and give a docId local to the segment.\n\nCould it be related to this issue?  This bug is freaking me out a bit as I may be forced to abandon DocValues. ",
            "author": "David Smiley",
            "id": "comment-13505830"
        },
        {
            "date": "2012-11-28T20:43:35+0000",
            "content": "David: I'm not sure. the problem I opened this issue for actually relates to things like merging.\n\nCan you do a manual bounds check, as docvalues at read time doesn't have explicit checks (IndexOutOfBounds is expected/best effort if you pass a wrong docid):\n\n    if (docID < 0 || docID >= reader.maxDoc())    \n\n\n\nOtherwise if thats not the problem, do you have a test or something you could upload to an issue? ",
            "author": "Robert Muir",
            "id": "comment-13505878"
        },
        {
            "date": "2012-12-05T13:01:28+0000",
            "content": "Just a quick recap on where things stand on the branch:\n\n\n\tWe have the DV 2.0 API, shadowing DV 1.0 API.\n\n\n\n\n\tWe have one codec (SimpleText) that implements it, passes tests\n\n\n\n\n\tCheckIndex does basic tests of DV 2.0, and we also have\n    TestDemoDocValue, but nothing else is cutover yet.\n\n\n\n\n\tLucene41 codec's impl is I think close but was failing some tests\n    (not sure why yet)\n\n\n\n\n\tWe have a MemoryDV but it's very RAM inefficient now\n\n\n\n\n\tWe have Norms 2.0 API too, shadowing current norms, and only\n    SimpleText implements it (but should be easy to get Lucene41 to\n    impl it too).\n\n\n\n\n\tWe need to cut over all uses/tests of DV 1.0 / norms 1.0 and then\n    remove DV/norms 1.0 shadow code.\n\n\n\n\n\tThere are still tons and tons of nocommits ...\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13510456"
        },
        {
            "date": "2013-02-06T01:30:25+0000",
            "content": "Applyable patch to trunk r1442822.\n\nI think this is close: jenkins-blasted, benchmarked, beefed up tests and added many new ones, several codecs with different tradeoffs, per-field configuration, 4.0 file format compat, more efficient 4.2 format, and so on. ",
            "author": "Robert Muir",
            "id": "comment-13572034"
        },
        {
            "date": "2013-02-06T15:04:31+0000",
            "content": "Big +1!\n\nI especially like the fact that doc values\n\n\tnow use the same interface as FieldCache,\n\tcan be configured on a per-field basis thanks to a SPI,\n\tare first buffered in memory (with RAM accounting) so that the codec has more opportunities to perform optimizations.\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13572471"
        },
        {
            "date": "2013-02-07T21:22:14+0000",
            "content": "[trunk commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1443717\n\nLUCENE-4547: DocValues improvements ",
            "author": "Commit Tag Bot",
            "id": "comment-13573944"
        },
        {
            "date": "2013-02-08T04:20:11+0000",
            "content": "[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1443834\n\nLUCENE-4547: DocValues improvements ",
            "author": "Commit Tag Bot",
            "id": "comment-13574227"
        },
        {
            "date": "2013-02-08T11:02:04+0000",
            "content": "Revision 1443717 breaks custom collector code such as:\n\nint[] ints = FieldCache.DEFAULT.getInts(context.reader(), this.fieldName, false);\n\n\n\nDo you suggest we should keep the returned instance of Ints but where is the concrete Ints class? Can't seem to find it.\n\nThanks ",
            "author": "Markus Jelsma",
            "id": "comment-13574391"
        },
        {
            "date": "2013-02-08T11:07:07+0000",
            "content": "where is the concrete Ints class?\n\nThis class is an inner class of FieldCache: oal.search.FieldCache.Ints. Then you should be able to fix your code by replacing ints[i] with ints.get( i ).\n ",
            "author": "Adrien Grand",
            "id": "comment-13574394"
        },
        {
            "date": "2013-02-08T11:10:56+0000",
            "content": "Oh i see, FieldCacheImpl returns and overrides Ints.get(int docId). ",
            "author": "Markus Jelsma",
            "id": "comment-13574397"
        },
        {
            "date": "2013-05-10T10:34:09+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654137"
        }
    ]
}