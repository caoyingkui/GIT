{
    "id": "LUCENE-6842",
    "title": "No way to limit the fields cached in memory and leads to OOM when there are thousand of fields (thousands)",
    "details": {
        "resolution": "Won't Fix",
        "affect_versions": "4.6.1",
        "components": [
            "core/search"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "I am opening this defect to get some guidance on how to handle a case of server running out of memory and it seems like it's something to do how we index. But want to know if there is anyway to reduce the impact of this on memory usage before we look into the way of reducing the number of fields. \n\nBasically we have many thousands of fields being indexed and it's causing a large amount of memory being used (25GB) and eventually leading to application to hang and force us to restart every few minutes.",
    "attachments": {
        "HistogramOfHeapUsage.png": "https://issues.apache.org/jira/secure/attachment/12767287/HistogramOfHeapUsage.png"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14962941",
            "author": "Dawid Weiss",
            "date": "2015-10-19T07:05:20+0000",
            "content": "Can you switch Java versions and use a more modern JVM?... Not that it'll fix the problem (which you should describe in more details \u2013 what data is indexed in those fields and why you need so many fields in the first place), but with this config there is really no \"fix\" other than \"upgrade first\". "
        },
        {
            "id": "comment-14962951",
            "author": "Uwe Schindler",
            "date": "2015-10-19T07:15:38+0000",
            "content": "Please also send stack trace showing the OOMs. It is impossible to see any problems without more information (code, number of fields, total index size, does it happen during indexing or query,...).\nThe histogram you sent looks fine, although the number of HashMap$Entry instances or TreeMap$Entry instances cannot come from Lucene unless you haven millions of fields.\nPlease note that if the OOMs happen during merging of index segments, you should upgrade Lucene, because before 5.0, merging was using much more heap, especially if you have many fields. "
        },
        {
            "id": "comment-14963098",
            "author": "Michael McCandless",
            "date": "2015-10-19T10:03:41+0000",
            "content": "Given how much heap is used by FST and FST$Arc I think you likely do have many, many unique indexed fields?\n\nEach unique indexed field in Lucene requires a few hundred bytes of heap, and while there's been some improvement recently to lower that, it's only by a bit.\n\nYou'll have to re-work how you use Lucene to not require so many unique fields ... "
        },
        {
            "id": "comment-14963251",
            "author": "Bala Kolla",
            "date": "2015-10-19T12:47:23+0000",
            "content": "Yes we literally have thousands of fields (may be a million), I will come back to you with the exact number of fields. BTW, I am not really seeing OOM as our application stops accepting the requests once the heap is exhausted. \nDawid Weiss, I don't think the core issue is the Java version. I think its to do with the many number of unique fields as suggested by Michael McCandless.\nAlso, this memory exhaustion happens by simply opening the index and starting a new search. I am guessing that its trying to load all the fields into memory and wanted to know if there is a way to either limit the number of fields loaded. I would take a hit on performance by limiting the fields loaded into memory than stopping the entire application.\nIf there is no way to limit the fields loaded into memory, then I will go back to my team and question the indexing rules that we have.\n\nThanks and appreciate your help. "
        },
        {
            "id": "comment-14963257",
            "author": "Dawid Weiss",
            "date": "2015-10-19T12:52:27+0000",
            "content": "> I don't think the core issue is the Java version. I think its to do with the many number of unique fields as suggested by Michael McCandless.\n\nYes, but I doubt you'll find much interest in fixing an old Lucene version like the one you're using, on a JVM that is no longer supported... \n\nThis also leaves the question of why you need a million fields... this seems like a very niche application (and would be interesting if you could share the scenario of what you're doing with those fields). "
        },
        {
            "id": "comment-14963265",
            "author": "Bala Kolla",
            "date": "2015-10-19T13:00:51+0000",
            "content": "I am willing to upgrade to latest version of Lucene and the JDK if that's going to fix the issue. Let me get back to you on the data, I have to talk to my team in IBM before I get any specific data. But I can share the generic information about how many fields and how we got into that place.\n\nWe basically let the customers define the indexing rules and would dynamically create the fields while indexing the documents.\nIf you can point me to the code, I am also willing to update the code to limit this cache and will be happy to contribute. I am sure someone else will see this issue sooner or later. "
        },
        {
            "id": "comment-14963276",
            "author": "Dawid Weiss",
            "date": "2015-10-19T13:05:23+0000",
            "content": "> But I can share the generic information about how many fields and how we got into that place.\n\nOk. I don't think many people have a million fields to index... A few dozen, perhaps a few hundred... but millions sounds odd. "
        },
        {
            "id": "comment-14963282",
            "author": "Bala Kolla",
            "date": "2015-10-19T13:15:21+0000",
            "content": "I understand. As we are already in that position we have to think about how we can fix it. If we have to change the way we index then it's going to impact our customers if they are using those for searching. \nThat's why I want to explore if there is a way to limit the loading the fields or at least load only when they are absolutely needed. The way I understood, they are being loaded as soon as we open a reader.  "
        },
        {
            "id": "comment-14963287",
            "author": "Uwe Schindler",
            "date": "2015-10-19T13:19:55+0000",
            "content": "The fields are not cached or loaded completely. But we have to build the corresponding lookup FSTs at opening time. Those are needed to quickly check for term existence or to find the posting lists. There is no way to work around that. "
        },
        {
            "id": "comment-14963326",
            "author": "Ryan Josal",
            "date": "2015-10-19T13:49:13+0000",
            "content": "FWIW, I have seen an OOME due to a large number of fields before.  I had an index with millions of documents and I wanted to experiment with ways to reduce full indexing time, and there was a way to reorganize the data that would reduce the number of docs by a factor of 2000 by adding 2000 fields (using a solr dynamic multivalued String with ~2-3 values on average).  The idea was quickly canned after seeing an OOME during full indexing.  This piece of data we may eventually move out of Lucene and into a datastore more suited like Cassandra. "
        },
        {
            "id": "comment-14963385",
            "author": "Bala Kolla",
            "date": "2015-10-19T14:29:55+0000",
            "content": "Uwe Schindler? Is there a way to minimize the impact.. For example is there a way to lazy load them when they are needed and evict them (when they reach a threshold) once we are done with that data. \n\nI will also investigate if there is a way to reduce the unique number of fields. I think we are looking potentially few million fields per shard and we could load many shards at a given point of time. "
        },
        {
            "id": "comment-14963403",
            "author": "Uwe Schindler",
            "date": "2015-10-19T14:44:56+0000",
            "content": "The problem we have here is that you don't give us enough information about how your index looks like. To me your explanations make me think that you have a large, single index and many customers are placing documents in it, each with a different set of fields. If this is the case, why not create an index for every customer with only the fields the customer needs? You can load and unload them as needed. If you need to search in many indexes (not all, of course), just use MultiReader.\n\nIf you really have so many fields in all documents - is it really needed to have so many fields? We are talking about full text search, so in an ideal world one would have only one single field to search on (google like)  Of course this is generally not so simple, because people want to search in parts of the document, but millions? This is normally something going up to 100 different fields accross your whole corpus.\n\nTo answer your question: No you cannot lazy load term indexes. "
        },
        {
            "id": "comment-14963420",
            "author": "Jack Krupansky",
            "date": "2015-10-19T14:57:34+0000",
            "content": "Generally, Lucene has few hard limits, but the general guidance is that ultimately you will be limited by available system resources such as RAM and CPU. There may not be any hard limit to the number of fields, but that doesn't mean that you can safely assume that a large number of fields will always work for a limited amount of RAM and CPU. Exactly how much RAM and CPU you need will depend on your specific application, that you yourself will have to test for - known as a proof of concept.\n\nGenerally, people have resource problems based on the number of documents rather than the number of fields for each document. You haven't detailed how many documents you are indexing and how many of these fields are actually present in an average document. Who knows, maybe the number of fields is not the problem per se and it is the number of documents that is the cause of the resource issue, or a combination of the two.\n\nThat said, I will defer to the more senior Lucene committers here, but personally I would suggest that \"hundreds\" or \"low thousands\" is a more practical recommended best practice upper limit to total number of fields in a Lucene index. Generally, \"dozens\" or at most \"low hundreds\" would be most recommended and the safest assumption. Sure, maybe 10,000 fields might actually work, but then number of documents and operations and query complexity will also come into play.\n\nAll of that said, I'm sure we are all intently curious why exactly you feel that you need so many fields. "
        },
        {
            "id": "comment-14963473",
            "author": "Bala Kolla",
            "date": "2015-10-19T15:30:13+0000",
            "content": "I agree. We need to fix our indexing. It's definitely not a normal scenarios. I was looking for a quick a way to limit the damage this is doing to the heap but I agree with your assessment that we are not using the way the Lucene is intended for.\nWas hoping a way to limit the impact of this many fields and as per the feedback its not possible. I think even if we somehow fix this issue of loading field info's we might get into other issues and if possible fix the core issue of why we have many fields.\nI am working on it with my team and will get back to you on what I find. Thanks for your help and feedback.  "
        },
        {
            "id": "comment-14969567",
            "author": "David Smiley",
            "date": "2015-10-22T18:12:18+0000",
            "content": "FWIW one of my clients has ~94 thousand fields and it wasn't a problem.  It's some mid to late Solr 4.x version.  Solr's schema browser became unusable though  "
        }
    ]
}