{
    "id": "SOLR-7174",
    "title": "DIH should reset TikaEntityProcessor so that it is capable of re-use.",
    "details": {
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.0",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "Downloaded Solr 5.0.0, on a Windows 7 PC.   I ran \"solr start\" and then \"solr create -c hn2\" to create a new core.\n\nI want to index a load of epub files that I've got in a directory. So I created a data-import.xml (in solr\\hn2\\conf):\n\n<dataConfig>\n    <dataSource type=\"BinFileDataSource\" name=\"bin\" />\n    <document>\n        <entity name=\"files\" dataSource=\"null\" rootEntity=\"false\"\n            processor=\"FileListEntityProcessor\"\n            baseDir=\"c:/Users/gt/Documents/epub\" fileName=\".*epub\"\n            onError=\"skip\"\n            recursive=\"true\">\n            <field column=\"fileAbsolutePath\" name=\"id\" />\n            <field column=\"fileSize\" name=\"size\" />\n            <field column=\"fileLastModified\" name=\"lastModified\" />\n\n            <entity name=\"documentImport\" processor=\"TikaEntityProcessor\"\n                url=\"${files.fileAbsolutePath}\" format=\"text\" dataSource=\"bin\" onError=\"skip\">\n                <field column=\"file\" name=\"fileName\"/>\n                <field column=\"Author\" name=\"author\" meta=\"true\"/>\n                <field column=\"title\" name=\"title\" meta=\"true\"/>\n                <field column=\"text\" name=\"content\"/>\n            </entity>\n        </entity>\n    </document>\n</dataConfig>\n\nIn my solrconfig.xml, I added a requestHandler entry to reference my data-import.xml:\n\n  <requestHandler name=\"/dataimport\" class=\"org.apache.solr.handler.dataimport.DataImportHandler\">\n      <lst name=\"defaults\">\n          <str name=\"config\">data-import.xml</str>\n      </lst>\n  </requestHandler>\n\nI renamed managed-schema to schema.xml, and ensured the following doc fields were setup:\n\n      <field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" />\n      <field name=\"fileName\" type=\"string\" indexed=\"true\" stored=\"true\" />\n      <field name=\"author\" type=\"string\" indexed=\"true\" stored=\"true\" />\n      <field name=\"title\" type=\"string\" indexed=\"true\" stored=\"true\" />\n\n      <field name=\"size\" type=\"long\" indexed=\"true\" stored=\"true\" />\n      <field name=\"lastModified\" type=\"date\" indexed=\"true\" stored=\"true\" />\n\n      <field name=\"content\" type=\"text_en\" indexed=\"false\" stored=\"true\" multiValued=\"false\"/>\n      <field name=\"text\" type=\"text_en\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n\n    <copyField source=\"content\" dest=\"text\"/>\n\nI copied all the jars from dist and contrib* into server\\solr\\lib.\n\nStopping and restarting solr then creates a new managed-schema file and renames schema.xml to schema.xml.back\n\nAll good so far.\n\nNow I go to the web admin for dataimport (http://localhost:8983/solr/#/hn2/dataimport//dataimport) and try and execute a full import.\n\nBut, the results show \"Requests: 0, Fetched: 58, Skipped: 0, Processed:1\" - ie. it only adds one document (the very first one) even though it's iterated over 58!\n\nNo errors are reported in the logs. \n\nI can repeat this on Ubuntu 14.04 using the same steps, so it's not Windows specific.\n\n-----------------\n\nIf I change the data-import.xml to use FileDataSource and PlainTextEntityProcessor and parse txt files, eg: \n\n<dataConfig>  \n\t<dataSource type=\"FileDataSource\" name=\"bin\" />\n\t<document>\n\t\t<entity name=\"files\" dataSource=\"null\" rootEntity=\"false\"\n\t\t\tprocessor=\"FileListEntityProcessor\"\n\t\t\tbaseDir=\"c:/Users/gt/Documents/epub\" fileName=\".*txt\">\n\t\t\t<field column=\"fileAbsolutePath\" name=\"id\" />\n\t\t\t<field column=\"fileSize\" name=\"size\" />\n\t\t\t<field column=\"fileLastModified\" name=\"lastModified\" />\n\n\t\t\t<entity name=\"documentImport\" processor=\"PlainTextEntityProcessor\"\n\t\t\t\turl=\"${files.fileAbsolutePath}\" format=\"text\" dataSource=\"bin\">\n\t\t\t\t<field column=\"plainText\" name=\"content\"/>\n\t\t\t</entity>\n\t\t</entity>\n\t</document> \n</dataConfig> \n\nThis works.  So it's a combo of BinFileDataSource and TikaEntityProcessor that is failing.\n\n\nOn Windows, I ran Process Monitor, and spotted that only the very first epub file is actually being read (repeatedly).\n\n\nWith verbose and debug on when running the DIH, I get the following response:\n\n....\n  \"verbose-output\": [\n    \"entity:files\",\n    [\n      null,\n      \"----------- row #1-------------\",\n      \"fileSize\",\n      2609004,\n      \"fileLastModified\",\n      \"2015-02-25T11:37:25.217Z\",\n      \"fileAbsolutePath\",\n      \"c:\\\\Users\\\\gt\\\\Documents\\\\epubissue018.epub\",\n      \"fileDir\",\n      \"c:\\\\Users\\\\gt\\\\Documentsepub\",\n      \"file\",\n      \"issue018.epub\",\n      null,\n      \"---------------------------------------------\",\n      \"entity:documentImport\",\n      [\n        \"document#1\",\n        [\n          \"query\",\n          \"c:\\\\Users\\\\gt\\\\Documents\\\\epubissue018.epub\",\n          \"time-taken\",\n          \"0:0:0.0\",\n          null,\n          \"----------- row #1-------------\",\n          \"text\",\n          \"< ... parsed epub text - snip ... >\"\n          \"title\",\n          \"Issue 18 title\",\n          \"Author\",\n          \"Author text\",\n          null,\n          \"---------------------------------------------\"\n        ],\n        \"document#2\",\n        []\n      ],\n      null,\n      \"----------- row #2-------------\",\n      \"fileSize\",\n      4428804,\n      \"fileLastModified\",\n      \"2015-02-25T11:37:36.399Z\",\n      \"fileAbsolutePath\",\n      \"c:\\\\Users\\\\gt\\\\Documents\\\\epubissue019.epub\",\n      \"fileDir\",\n      \"c:\\\\Users\\\\gt\\\\Documentsepub\",\n      \"file\",\n      \"issue019.epub\",\n      null,\n      \"---------------------------------------------\",\n      \"entity:documentImport\",\n      [\n        \"document#2\",\n        []\n      ],\n      null,\n      \"----------- row #3-------------\",\n      \"fileSize\",\n      2580266,\n      \"fileLastModified\",\n      \"2015-02-25T11:37:41.188Z\",\n      \"fileAbsolutePath\",\n      \"c:\\\\Users\\\\gt\\\\Documents\\\\epubissue020.epub\",\n      \"fileDir\",\n      \"c:\\\\Users\\\\gt\\\\Documentsepub\",\n      \"file\",\n      \"issue020.epub\",\n      null,\n      \"---------------------------------------------\",\n      \"entity:documentImport\",\n      [\n        \"document#2\",\n        []\n      ],\n....\n....",
    "attachments": {
        "SOLR-7174.patch": "https://issues.apache.org/jira/secure/attachment/12701542/SOLR-7174.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-28T04:05:08+0000",
            "author": "Alexandre Rafalovitch",
            "content": "It looks like the TikaEntityProcessor is not capable of re-entry. This is only triggered when it is an inner entity. The title of the JIRA should probably be renamed.\n\nThe cause is a flag *done* which is set to false in firstInit, is set to true at the end of the first run and is not reset before a second (reused) run.\n\nOne solution is to override init method (and not just firstInit) and move resetting the flag there. ",
            "id": "comment-14341313"
        },
        {
            "date": "2015-02-28T04:06:51+0000",
            "author": "Alexandre Rafalovitch",
            "content": "Proposed patch that allows TikaEntityProcessor to reset currently on reuse. ",
            "id": "comment-14341315"
        },
        {
            "date": "2015-03-02T11:48:59+0000",
            "author": "Gary Taylor",
            "content": "Patched tested OK.  I can now use the above DIH and schema config to index multiple epub docs via TikaEntityProcessor.  Thanks! ",
            "id": "comment-14343076"
        },
        {
            "date": "2015-03-03T19:01:13+0000",
            "author": "Noble Paul",
            "content": "The patch looks fine to me , I shall commit this soon ",
            "id": "comment-14345516"
        },
        {
            "date": "2015-03-04T05:12:07+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1663857 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1663857 ]\n\nSOLR-7174: DIH should reset TikaEntityProcessor so that it is capable of re-use ",
            "id": "comment-14346411"
        },
        {
            "date": "2015-03-04T05:14:50+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1663858 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1663858 ]\n\nSOLR-7174: DIH should reset TikaEntityProcessor so that it is capable of re-use ",
            "id": "comment-14346412"
        },
        {
            "date": "2015-03-04T05:17:31+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "This issue has been added in the \"Other Changes\" section of CHANGES.txt. This should be put under the \"Bug Fixes\" section. ",
            "id": "comment-14346413"
        },
        {
            "date": "2015-03-04T06:42:40+0000",
            "author": "Noble Paul",
            "content": "It's not a bug\nOn Mar 4, 2015 10:48 AM, \"Shalin Shekhar Mangar (JIRA)\" <jira@apache.org>\n ",
            "id": "comment-14346488"
        },
        {
            "date": "2015-03-04T10:46:34+0000",
            "author": "Gary Taylor",
            "content": "Patch verified OK. ",
            "id": "comment-14346726"
        },
        {
            "date": "2015-03-11T12:04:59+0000",
            "author": "Alexandre Rafalovitch",
            "content": "This may actually be a regression, see SOLR-7222 . Which means we need to change the CHANGES.txt, but also that something else maybe affected. \n\nSo, it is either Tika upgrade that did it or something in DIH. Possibly related to RecursiveParserWrapper mentioned in SOLR-7189. ",
            "id": "comment-14356789"
        },
        {
            "date": "2015-03-11T17:26:35+0000",
            "author": "Tim Allison",
            "content": "Could be Tika, but it isn't RecursiveParserWrapper.  That has to be called in the invoking code (e.g. it isn't under the hood of AutoDetectParser), and it would wrap AutoDetectParser or the user configured parser.   ",
            "id": "comment-14357242"
        }
    ]
}