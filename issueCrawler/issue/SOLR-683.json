{
    "id": "SOLR-683",
    "title": "Distributed Search / Shards Deadlock",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.3"
        ],
        "components": [
            "search"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Per this discussion:\nhttp://www.nabble.com/Distributed-Search-Strategy---Shards-td18882112.html\n\nSolr seems to lock up when running distributed search on three servers, with all three using shards of each other.  Thread dump attached.",
    "attachments": {
        "SOLR-683.patch": "https://issues.apache.org/jira/secure/attachment/12388097/SOLR-683.patch",
        "locked.log": "https://issues.apache.org/jira/secure/attachment/12387835/locked.log"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12621584",
            "date": "2008-08-11T20:41:06+0000",
            "content": "Here's the problem: deadlock is possible when the max number of concurrent HTTP requests is less than the number of possible HTTP requests (from both top-level clients, and by other shards).\n\nConsider the simplest case of two shards, each with just a single thread dedicated to handling incoming HTTP requests.  A top-level request comes into each shard, and each shard queries the other.  The second request to each shard blocks because the first thread has not yet completed.  Deadlock. "
        },
        {
            "author": "Cameron",
            "id": "comment-12621594",
            "date": "2008-08-11T20:54:27+0000",
            "content": "So this seems to be a container level issue, not a Solr issue? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12621612",
            "date": "2008-08-11T21:34:19+0000",
            "content": "I duplicated a deadlock with two shards with 1000 client threads making requests.\nWhen I changed the maxThreads parameter from 250 to 10000 (in jetty.xml), the deadlocks went away... I was able to run through 1M requests. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12621616",
            "date": "2008-08-11T21:41:17+0000",
            "content": "So this seems to be a container level issue, not a Solr issue?\n\nYes and no... it's not a low-level solr bug, and it can be solved by upping the number of concurrent threads or http requests in the container.\n\nBut if we could set a read timeout on shard requests, we could also prevent a hard deadlock and return an error instead.  In any case, we should increase the number of threads in the example jetty config and document this issue. "
        },
        {
            "author": "Cameron",
            "id": "comment-12621642",
            "date": "2008-08-11T23:00:25+0000",
            "content": "We could up the number of threads in our container, but this does not completely resolve the issue, as any sort of denial of service attack would potentially cause this to happen with no possible way of recovery.  I would agree that some sort of timeout would be needed to actually solve the issue. "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12621666",
            "date": "2008-08-12T01:17:10+0000",
            "content": "Another way to handle this would be to configure the servlet container to reject incoming connections when all available threads are in use [1][2]. This will cause failed requests which could have been served after a short wait, but eliminates the deadlock problem.\n\n[1] http://docs.codehaus.org/display/JETTY/Configuring+Connectors\n[2] http://tomcat.apache.org/tomcat-6.0-doc/config/http.html "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12621836",
            "date": "2008-08-12T14:46:01+0000",
            "content": "The problem with a read timeout is it would cause otherwise perfectly acceptable requests to fail, even if the system is not under load (since we can't put an upper bound on how long a request can take).\n\nI'm resolving this for now since I upped the max threads in the example jetty.xml to 10K and documented the issue on the distributed search wiki page.\n\nIf the servlet container can be configured to reject requests rather than blocking, that would probably be the ideal scenario.  If anyone knows if Jetty can be configured to do that, we can add it to the solr example. "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12622058",
            "date": "2008-08-13T01:08:08+0000",
            "content": "Attaching patch which adds the configuration parameter to have an accept queue size of 0 to jetty.xml, along with a reference to this issue and a boilerplate warning. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12622700",
            "date": "2008-08-14T21:06:28+0000",
            "content": "Hmmm I see this was just committed, but are we sure it works?\nIsn't acceptQueueSize just the network level connection queue size for the socket (as normally set by the listen sys call)?\nWhen jetty runs out of handler threads, does it not accept new connections, or does it accept the connection and wait for a thread to become free to handle it?\nIf the former, then this patch should work.  If the latter, it won't. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12622705",
            "date": "2008-08-14T21:18:08+0000",
            "content": "\n>> When jetty runs out of handler threads, does it not accept new connections, or does it accept the connection and wait for a thread to become free to handle it?\n\nNot sure if this is still the case, but I believe Jetty did just use the standard socket backlog queue and set it by default to the number of service threads - so you can have that many threadless requests queued up. Dunno if they changed that recently or not.\n\n\n\tMark\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12622706",
            "date": "2008-08-14T21:19:15+0000",
            "content": "Hm, hard to tell from sparse Jetty javadocs.\nhttp://docs.codehaus.org/display/JETTY/Configuring+Connectors states:\n\nacceptQueueSize \t Number of connection requests that can be queued up before the operating system starts to send rejections.\n\nSounds more like the latter than the former.  That is, it sounds like Jetty itself might accept connections until the OS starts complaining.  Hm, either way this doesn't help if one has an actual deadlock, like the one you described in the 1-thread-example, does it? "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12622764",
            "date": "2008-08-15T01:16:00+0000",
            "content": "Not sure if this is still the case, but I believe Jetty did just use the standard socket backlog queue\n\nA quick look at the code suggests that this is still the case (at least for version 6.1.3 bundled with Solr).\n\nWhen jetty runs out of handler threads, does it not accept new connections, or does it accept the connection and wait for a thread to become free to handle it?\n\nWhen it runs out of handler threads it can't accept the connection because there's no thread to handle it. The code where this is implemented looks like this.\nAbstractConnector.java\nThread current = Thread.currentThread();\nsynchronized(AbstractConnector.this)\n            {\n                if (_acceptorThread==null)\n                    return;\n                \n                _acceptorThread[_acceptor]=current;\n            }\n...\nwhile (isRunning())\n                {\n                    try\n                    {\n                        accept(_acceptor); \n                    }\n                }\n\n\n\nThe connection is only accepted if there's a thread to handle it.\n\nIt's clearer in the Tomcat documentation for equivalent parameter (acceptCount in http://tomcat.apache.org/tomcat-6.0-doc/config/http.html). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12622906",
            "date": "2008-08-15T16:13:12+0000",
            "content": "The connection is only accepted if there's a thread to handle it.\n\nYes, but not from the normal pool... it looks like there are acceptor threads that do nothing but accept socket connections.\n\nI just confirmed that setting the acceptQueueSize does not work to reject connections.\nI put in a configurable sleep in the search handler and made requests until they started blocking.  Requests were still accepted and just hung... netstat showed them to be \"ESTABLISHED\".\n\nFurther, setting a really low acceptQueueSize runs the risk of having connections rejected even in a low-load situation because jetty doesn't accept them fast enough. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12622910",
            "date": "2008-08-15T16:18:26+0000",
            "content": "I just rolled back the second commit... I think just upping the thread count should be fine for now. "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12623247",
            "date": "2008-08-18T02:39:02+0000",
            "content": "Yonik, you're right \u2013 there're separate acceptor threads, setting acceptQueueSize just affects how connections are handled when they come in too quickly to be accepted by the available acceptor threads. There's no option to influence handling connections when no executor threads are available. I've verified that Tomcat behaves in the same way.\n\nSo the only thing we can do is up the thread count. Even setting timeouts won't help as this only affects the actual network transfers, not the execution time of the executor threads. "
        }
    ]
}