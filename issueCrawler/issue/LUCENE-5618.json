{
    "id": "LUCENE-5618",
    "title": "DocValues updates send wrong fieldinfos to codec producers",
    "details": {
        "type": "Bug",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.9"
        ]
    },
    "description": "Spinoff from LUCENE-5616.\n\nSee the example there, docvalues readers get a fieldinfos, but it doesn't contain the correct ones, so they have invalid field numbers at read time.\n\nThis should really be fixed. Maybe a simple solution is to not write \"batches\" of fields in updates but just have only one field per gen? \n\nThis removes many-many relationships and would make things easy to understand.",
    "attachments": {
        "LUCENE-5618.patch": "https://issues.apache.org/jira/secure/attachment/12644326/LUCENE-5618.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13975153",
            "author": "Michael McCandless",
            "content": "The problem here is we only store the latest gen for a given field, not all gens that field participated in, so we don't have enough information at read-time to know when loading gen N all fields that were there. ",
            "date": "2014-04-20T14:41:43+0000"
        },
        {
            "id": "comment-13975174",
            "author": "Robert Muir",
            "content": "I think its a design flaw in how this stuff is written. It only \"worked\" because dv reader didn't validate this stuff in init. I'm still unsure if this should block 4.8 or if its ok to defer to 4.9, but this is really bogus. The simple solution is for dv update gens to always only have one field. ",
            "date": "2014-04-20T16:16:06+0000"
        },
        {
            "id": "comment-13977133",
            "author": "Shai Erera",
            "content": "I think its a design flaw in how this stuff is written\n\nToday we pass only the FIs the Codec should \"care\" about, rather than pass all the FIs it \"knows\" about. This allows the Codec to optimize. E.g. today we don't take advantage of that, and a DVP reads all the metadata of a field, even if the field isn't passed in the FIS (and therefore will never be asked for).\n\nIf we write each field in its own gen, then since we don't allow adding new fields through dvUpdates, for gen=-1 we just pass all known dvFieldInfos, and for gen > 0 we will pass a single FI only, therefore the Codec always receives the FIs it knows about, even though for gen=-1 it is given some FIs it shouldn't care about. Our Codecs only read metadata into memory, the actual data is loaded lazily, so perhaps optimizing them is less important at the moment.\n\nI wish we could be more flexible though in our code. It feels odd to me that each field is written in its own gen, just because we cannot add a FIS.exists() check in the Codec. Like, if we always pass all DV FIS to every DVP, each will be able to do the exists() check, but a DVP will see fields it doesn't know about. Is that bad? It still covers the corruption case of a bad field number being encoded in the first place... ",
            "date": "2014-04-22T18:04:45+0000"
        },
        {
            "id": "comment-13977136",
            "author": "Shai Erera",
            "content": "I'm still unsure if this should block 4.8 or if its ok to defer to 4.9\n\nI don't think it should block 4.8... ",
            "date": "2014-04-22T18:05:31+0000"
        },
        {
            "id": "comment-13977145",
            "author": "Robert Muir",
            "content": "Its not \"allows codec to optimize\". The codec should NOT be aware of dv updates. ",
            "date": "2014-04-22T18:08:24+0000"
        },
        {
            "id": "comment-13977152",
            "author": "Shai Erera",
            "content": "The codec should NOT be aware of dv updates.\n\nWhy? Are they so horrible? \n\nSo what if we always pass all FIS to the Codec? ",
            "date": "2014-04-22T18:13:49+0000"
        },
        {
            "id": "comment-13977169",
            "author": "Robert Muir",
            "content": "\nWhy? Are they so horrible? \n\nBecause thats not the codec's responsibility.\n\nPlease, pass all the FieldInfos. thats an easy solution. my check that \"fieldinfo is valid\" is perfectly legitimate. This is totally broken today! ",
            "date": "2014-04-22T18:28:34+0000"
        },
        {
            "id": "comment-13977216",
            "author": "Shai Erera",
            "content": "OK I'll take a look at passing all FIs. I agree the current check is important. If we pass only the set of FIs the Codec sees though, I think we should also check that the Codec read all the fields the FIS say it should read (currently it only checks that all the fields it reads are in the FI, but not if there are \"missing\" fields). But if we pass all FIS, that's not a problem. ",
            "date": "2014-04-22T19:04:34+0000"
        },
        {
            "id": "comment-13982853",
            "author": "Shai Erera",
            "content": "I modified the code to pass all the FIs to the codec, no matter the gen, and tests fail with FileNotFoundException. The reason is that PerFieldDVF tries to open DVPs e.g. of gen=1 of all fields, whether they were written in that gen or not, which leads to the FNFE. I am not sure that we can pass all FIs to the Codec that way ... so our options are:\n\n\n\tPass all the fields that were written in a gen (whether we need them or not) \u2013 this does not make sense to me, as we'll need to track it somewhere, and it seems a waste\n\tAdd leniency in the form of \"here are the fields you should care about\" \u2013 this makes the codec partially updates aware, but I don't think it's a bad idea\n\tWrite each updated field in its own gen \u2013 if you update many fields, many times, this will create many files in the index directory. Technically it's not \"wrong\", it just looks weird\n\tRemain w/ the current code's corruption detection if the read fieldNumber < 0\n\n\n\nAnyway, I think the issue's title is wrong \u2013 DocValues updates do pass the correct fieldInfos to the producers. They pass only the infos that the producer should care about, and we see that passing too many is wrong (PerFieldDVF).\n\nI will think about it more. If you see other alternatives, feel free to propose them. ",
            "date": "2014-04-28T09:01:09+0000"
        },
        {
            "id": "comment-13982888",
            "author": "Robert Muir",
            "content": "\nWrite each updated field in its own gen \u2013 if you update many fields, many times, this will create many files in the index directory. Technically it's not \"wrong\", it just looks weird\n\nWhy? This is how separate norms worked. Its the obvious solution. The current behavior is broken: lets fix the bug. This optimization is what is to blame. The optimization is invalid.\n\n\nAnyway, I think the issue's title is wrong \u2013 DocValues updates do pass the correct fieldInfos to the producers. They pass only the infos that the producer should care about, and we see that passing too many is wrong (PerFieldDVF).\n\nAbsolutely not! You get a different fieldinfos at read time than you get at write. This is broken! ",
            "date": "2014-04-28T10:13:51+0000"
        },
        {
            "id": "comment-13982931",
            "author": "Shai Erera",
            "content": "If we separate each DV update into its own file, I think we will need to track another gen in SegmentCommitInfo: deletes, fieldInfos and dvUpdates. Though each FI writes its dvGen in the FIS file, we need to know from where to increment the gen for the next update. This isn't a big deal, just adds complexity to SCI (4 methods and index format change).\n\nBut why do you think that it's wrong to write 2 fields and then at read time ask to provide only 1 field? I.e. what if the Codecs API was more \"lazy\", or a Codec wants to implement lazy loading of even just the metadata?\n\nPassing all the fields a Codec wrote, e.g. in the gen=-1 case, even though none of them is not going to be used because they were all updated in later gens, seems awkward to me as well.\n\nWhat sort of index corruption does this check detect? As I see it, the Codec gets a subset of the fields that it already wrote. It's worse if it gets a superset of those fields, because you don't know e.g. if there are perhaps missing fields that disappeared from the file system. ",
            "date": "2014-04-28T11:30:58+0000"
        },
        {
            "id": "comment-13984205",
            "author": "Robert Muir",
            "content": "\nWhat sort of index corruption does this check detect? As I see it, the Codec gets a subset of the fields that it already wrote.\n\nThat the field numbers it is using are valid!\n\nPlease, stop pushing back on this. I will make this a blocker issue for 4.9. Maybe we should disable the dv update tests and enable this check in the meantime? This is the most fair.\n\nI dont like this pushing back against completely valid checks. ",
            "date": "2014-04-29T11:15:28+0000"
        },
        {
            "id": "comment-13984218",
            "author": "Shai Erera",
            "content": "I dont like this pushing back against completely valid checks.\n\nI don't push back, I'm trying to have a discussion. Why do you assume that questions indicate push back???\n\nDo you also think that it's OK for a Codec to receive fields it never handled? If not, we should check that too. That to me indicates a bigger problem than sending a subset of fields.\n\nI will look into adding another gen to SCI. But if all that we want to achieve is \"That the field numbers it is using are valid!\", there's another way to do that \u2013 we can pass to a Codec a FieldsValidator or something for this purpose. That way we don't need to pass all FIs to a Codec and don't run into the PerFieldDVF issue I mentioned above, and don't complicate SCI with another gen. Just mentioning there are other ways to achieve consistency checks... ",
            "date": "2014-04-29T11:58:42+0000"
        },
        {
            "id": "comment-13994473",
            "author": "Shai Erera",
            "content": "Patch addresses the following:\n\n\n\tModifies Lucene45/42DocValuesProducer to assert that all encoded fields exist in the FieldInfos.\n\n\n\n\n\tSimplifies ReaderAndUpdates.writeFieldUpdates readability by breaking out the updates to separate methods.\n\n\n\n\n\tEach DocValues field's updates are written to separate files.\n\n\n\n\n\tAdds SegmentCommitInfo.docValuesGen, separate from fieldInfosGen.\n\n\n\n\n\tFixes LUCENE-5636 by tracking per-field updates files, as well as fieldInfos files.\n\t\n\t\tper-generation update files are kept as deprecated, needed for 4.6-4.8 indexes back-compat. These become empty after the segment is merged.\n\t\n\t\n\n\n\n\n\tImproved testDeleteUnusedUpdatesFiles to cover two fields' updates (this exposes the bug on LUCENE-5636).\n\n\n\nIn terms of backwards compatibility, indexes between 4.6-4.8 will continue to reference unneeded files until the segment is merged. This is impossible to fix without breaking back-compat or introduce weird hacks which assume the default codec. This is not terrible though, since the number of unneeded-but-referenced files is limited by the number of DV fields the app has updated.\n\nI'd appreciate a review on this. Before I commit it though, I want to take care of LUCENE-5619, so we're sure the back-compat logic in this patch indeed works. ",
            "date": "2014-05-11T08:39:09+0000"
        },
        {
            "id": "comment-13997598",
            "author": "Shai Erera",
            "content": "After committing LUCENE-5619, with the previous patch TestBackwardsCompatibility failed since it didn't handle pre-4.9 indexes well \u2013 it didn't handle the case where one generation references multiple fields... to resolve that I added in this patch:\n\n\n\tSegmentReader acts accordingly only pre-4.9 indexes: beyond sending all the FieldInfos to a certain DocValuesProducer's gen, it ensures each such DVP is initialized once per generation.\n\n\n\n\n\tLucene45DocValuesProducer does a lenient fields check if the segment's version is pre-4.9.\n\n\n\nNote that I didn't add this leniency to Lucene42DocValuesProducer since that one doesn't support DocValues updates anyway, and so doesn't experience this issue at all. ",
            "date": "2014-05-14T14:40:27+0000"
        },
        {
            "id": "comment-14000666",
            "author": "Robert Muir",
            "content": "This looks good to me. My only concern (which can be a followup issue), is to try to simplify a lot of stuff in SegmentReader.initDocValuesProducers\n\nIn general I think SegmentReader needs a cleanup to ensure things are fast.\n\nThis logic is now more complex than before as there is back compat etc going on, and involves multiple full passes over fieldinfos/dv fields. In general we should really try to avoid this.\n\nIts now quite a bit difficult to see what is happening in the common case (no updates for a segment) via initDocValuesProducers/SegmentDocValues/getNumericXXX codepaths.\n\nOn that issue we should cleanup other inefficiencies while we are there: e.g. we also want to try to reduce the overhead going on in e.g. SR.getNumericDocValues. For example today this is doing two hash lookups, when this method could just try 'dvFields' first and optimize the common case.\n\nBut lets fix the bugs first, this approach looks good to me. Long-term we should also investigate refactoring the livedocs format maybe to use this \"files\" approach recorded in the commit. Because currently the LiveDocs codec api is really horrible, and really its just an updatable 1-bit numeric docvalues. ",
            "date": "2014-05-17T04:51:59+0000"
        },
        {
            "id": "comment-14001008",
            "author": "Shai Erera",
            "content": "Thanks Rob!\n\nMy only concern (which can be a followup issue), is to try to simplify a lot of stuff in SegmentReader.initDocValuesProducers\n\nI tried, but I didn't see it simplifies a lot. However, I now took a second look and I think what prevents the simplification in the no-updates case is the API on SegDocValues, which insists on getting a List<FieldInfo> rather than FieldInfos. I will try to change the API (it's internal) and hopefully it will allow to have a simple if (!si.hasFieldUpdates()) code block.\n\nFor example today this is doing two hash lookups, when this method could just try 'dvFields' first and optimize the common case.\n\nWhere do you see the two hash lookups? Only in the first getNumeric() there are two hash lookups, but after that it's always one. Or did I miss something? ",
            "date": "2014-05-18T06:02:00+0000"
        },
        {
            "id": "comment-14001040",
            "author": "Shai Erera",
            "content": "I modified SegDocValues API to take FieldInfos, and SegReader.initDocValues now distinguishes between no-updates, with updates and back-compat logic. I wish we didn't have the back-compat case, but that's what we have.\n\nI'm still unsure about the getNumeric inefficiencies that you mentioned, so if you can clarify I will try to address them here too. If it's a general SR refactoring and simplification, I agree that we should pursue that in a separate issue. ",
            "date": "2014-05-18T07:23:12+0000"
        },
        {
            "id": "comment-14001103",
            "author": "Michael McCandless",
            "content": "In SegmentReader.initDocValuesProducers, when there are no DV updates,\ncan't you just init dvp right off (not lazily)?  Because up above we\nonly call it if FIS.hasDocValues.\n\nI think what Rob meant by the double-lookup is we should just call\ndvFields.get(field) first, and only if that's null do we do the logic\nto initialize it.  Ie, the common case here is retrieving a DV field\nthat's already loaded.\n\nin this code from ReadersAndUpdates.writeFieldUpdates:\n\n\n    // update the doc-values updates files\n    assert !newDVFiles.isEmpty();\n    for (Entry<Integer,Set<String>> e : info.getDocValuesUpdatesFiles().entrySet()) {\n     if (!newDVFiles.containsKey(e.getKey())) {\n       newDVFiles.put(e.getKey(), e.getValue());\n     }\n    }\n\n\n\nWhy would the newDVFiles contain e.getKey()?  Aren't we only writing\nthe new generation update here?  Also the indent is off a bit. ",
            "date": "2014-05-18T16:01:28+0000"
        },
        {
            "id": "comment-14001186",
            "author": "Shai Erera",
            "content": "\nIn SegmentReader.initDocValuesProducers, when there are no DV updates,\ncan't you just init dvp right off (not lazily)? Because up above we\nonly call it if FIS.hasDocValues.\n\nOoops, you're right. Will fix!\n\n\nI think what Rob meant by the double-lookup is we should just call\ndvFields.get(field) first, and only if that's null do we do the logic\nto initialize it. Ie, the common case here is retrieving a DV field\nthat's already loaded.\n\nThis is the code of getNumeric():\n\n\nNumericDocValues dvs = (NumericDocValues) dvFields.get(field);\nif (dvs == null) {\n  DocValuesProducer dvProducer = dvProducersByField.get(field);\n  assert dvProducer != null;\n  dvs = dvProducer.getNumeric(fi);\n  dvFields.put(field, dvs);\n}\n\n\n\nIt seems already optimized to do one lookup in the common case?\n\nWhy would the newDVFiles contain e.getKey()? Aren't we only writing the new generation update here?\n\nNotice that the key is the fieldNumber (Integer) and not the generation (Long). I modified SegmentCommitInfo to track the files per fieldNumber instead of generation, to avoid future issues, and also I think it lets us be more flexible, i.e. easier back-compat support if we will want to change things again. Therefore it could be that the existing files mapping contain a fieldNumber which we just rewrote (updated), and hence the if.\n\nAlso the indent is off a bit.\n\nThanks, I'll fix. ",
            "date": "2014-05-18T20:14:26+0000"
        },
        {
            "id": "comment-14001187",
            "author": "Shai Erera",
            "content": "Patch folds in latest feedback. ",
            "date": "2014-05-18T20:15:41+0000"
        },
        {
            "id": "comment-14001199",
            "author": "Robert Muir",
            "content": "\nThis is the code of getNumeric():\n...\nIt seems already optimized to do one lookup in the common case?\n\nThats not the code. you are omitting the first lookup! ",
            "date": "2014-05-18T20:40:24+0000"
        },
        {
            "id": "comment-14001291",
            "author": "Shai Erera",
            "content": "Thats not the code. you are omitting the first lookup!\n\nOh, you mean the FieldInfos lookup? That was the original code from before DV updates, so I completely ignored it.. I'll check if it can be moved inside the if dvs == null check and upload a patch later. ",
            "date": "2014-05-19T00:36:15+0000"
        },
        {
            "id": "comment-14001491",
            "author": "Shai Erera",
            "content": "I looked and I can easily pull all the initialization inside the if (dvs == null). But since this involves the DV API as well as getDocsWithField, I'll open a separate issue just to keep this one focused. ",
            "date": "2014-05-19T08:32:11+0000"
        },
        {
            "id": "comment-14001566",
            "author": "Michael McCandless",
            "content": "Therefore it could be that the existing files mapping contain a fieldNumber which we just rewrote (updated), and hence the if.\n\nAhh right.  OK that makes sense.  Maybe just add a comment explaining that? ",
            "date": "2014-05-19T10:24:06+0000"
        },
        {
            "id": "comment-14002748",
            "author": "Shai Erera",
            "content": "Good idea Mike, patch clarifies this code block. ",
            "date": "2014-05-20T03:41:59+0000"
        },
        {
            "id": "comment-14002963",
            "author": "Michael McCandless",
            "content": "+1, thanks Shai. ",
            "date": "2014-05-20T08:40:08+0000"
        },
        {
            "id": "comment-14004643",
            "author": "ASF subversion and git services",
            "content": "Commit 1596570 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1596570 ]\n\nLUCENE-5618, LUCENE-5636: write each DocValues update in a separate file; stop referencing old fieldInfos files ",
            "date": "2014-05-21T13:18:19+0000"
        },
        {
            "id": "comment-14004714",
            "author": "ASF subversion and git services",
            "content": "Commit 1596582 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1596582 ]\n\nLUCENE-5618, LUCENE-5636: write each DocValues update in a separate file; stop referencing old fieldInfos files ",
            "date": "2014-05-21T14:36:41+0000"
        },
        {
            "id": "comment-14004719",
            "author": "Shai Erera",
            "content": "Committed to trunk and 4x. ",
            "date": "2014-05-21T14:39:17+0000"
        }
    ]
}