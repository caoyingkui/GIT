{
    "id": "SOLR-10745",
    "title": "Reliably create nodeAdded / nodeLost events",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "fix_versions": [
            "7.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "When Overseer node goes down then depending on the current phase of trigger execution a nodeLost event may not have been generated. Similarly, when a new node is added and Overseer goes down before the trigger saves a checkpoint (and before it produces nodeAdded event) this event may never be generated.\n\nThe proposed solution would be to modify how nodeLost / nodeAdded information is recorded in the cluster:\n\n\tnew nodes should do a ZK multi-write to both /live_nodes and additionally to a predefined location eg. /autoscaling/nodeAdded/<nodeName>. On the first execution of Trigger.run in the new Overseer leader it would check this location for new znodes, which would indicate that node has been added, and then generate a new event and remove the znode that corresponds to the event.\n\tnode lost events should also be recorded to a predefined location eg. /autoscaling/nodeLost/<nodeName>. Writing to this znode would be attempted simultaneously by a few randomly selected nodes to make sure at least one of them succeeds. On the first run of the new trigger instance (in new Overseer leader) event generation would follow the sequence described above.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-05-30T19:50:57+0000",
            "content": "Thanks Andrzej. I made a pass through the code at jira/SOLR-10745 branch.\n\nA few comments:\n\n\tShould we write events to nodeLost, nodeAdded even when there are no corresponding (active) triggers? \u2013 it seems wasteful and worse the data will keep growing with no one to delete it\n\tI agree with your choice of using persistent znodes for nodeLost events. Same for using ephemeral for nodeAdded because if the node goes away, the znode does too and we obviously never want to fire a nodeAdded trigger if the node itself is no more. I can't think of any cons to using ephemeral here except it is inconsistent with how we handle nodeLost events.\n\tWhile processing these events, i.e. before adding them to the tracking map, we must check actual state of the node at the time e.g. if a node came back, we don't want to add it to the NodeLostTrigger's tracking map\n\tPerhaps add some error handling code which ensures that we mark the node as live even if the multi op fails? I don't think it can fail but I just want to ensure that we fail to start Solr if cannot create the live node.\n\tTriggerIntegrationTest can use SolrZkClient.clean() which does the same thing as deleteChildrenRecursively\n\tnodeNameVsTimeAdded is now ConcurrentHashMap but it is never accessed concurrently?\n\tI'd prefer that retreiving marker paths should be done once during startup in ScheduledTrigger.run(). Doing that each time the trigger is run is redundant.\n\tminor nit - in testNodesEventRegistration, the code comment says \"we want both triggers to fire\" but the latch is initialized with 3.\n\n ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16030023"
        },
        {
            "date": "2017-06-07T14:37:23+0000",
            "content": "Committed to feature/autoscaling. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16040980"
        }
    ]
}