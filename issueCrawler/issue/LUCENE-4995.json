{
    "id": "LUCENE-4995",
    "title": "Remove the strong reference of CompressingStoredFieldsReader on the decompression buffer",
    "details": {
        "components": [],
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "CompressingStoredFieldsReader has a strong reference on the buffer it uses for decompression. Although it makes the reader able to reuse this buffer, this can trigger high memory usage in case some documents are very large. Creating this buffer on demand would help give memory back to the JVM.",
    "attachments": {
        "LUCENE-4995.patch": "https://issues.apache.org/jira/secure/attachment/12582591/LUCENE-4995.patch",
        "cpuGraph_4995Patch.jpg": "https://issues.apache.org/jira/secure/attachment/12586652/cpuGraph_4995Patch.jpg",
        "heapGraph_4995Patch.jpg": "https://issues.apache.org/jira/secure/attachment/12586651/heapGraph_4995Patch.jpg"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-05-10T07:25:03+0000",
            "content": "Patch. CompressingTermVectorsReader doesn't need the fix since it already creates the decompression buffer on demand. ",
            "author": "Adrien Grand",
            "id": "comment-13653611"
        },
        {
            "date": "2013-05-10T12:21:46+0000",
            "content": "Are we sure this is the right thing to do? People have misconfigured threadpools (especially tomcat users) and complain all the time about e.g. the analyzer buffers and so on.\n\nI just want to make sure these misconfigured users arent causing correctly-configured users massive amounts of garbage etc. ",
            "author": "Robert Muir",
            "id": "comment-13654414"
        },
        {
            "date": "2013-05-11T20:23:53+0000",
            "content": "Are we sure this is the right thing to do?\n\nI have no idea at all. On the one hand, it looks reasonable to me to have a reusable per-thread buffer to handle decompression, but on the other hand, it makes me unhappy that its size is unbounded: if an index has a few 1M documents on S segments and T threads, the JVM will have to reserve S*T*1M of heap just to be able to handle decompression. ",
            "author": "Adrien Grand",
            "id": "comment-13655364"
        },
        {
            "date": "2013-05-14T20:54:46+0000",
            "content": "I think the biggest issue from my perspective here is that some docs might be very large and once they are loaded the thread-local will hold on to potentially way too large buffers. I mean maybe we can adjust this based on the average doc size or something here that we reuse in the common case and re-allocate in the exceptional case? just and idea though... ",
            "author": "Simon Willnauer",
            "id": "comment-13657501"
        },
        {
            "date": "2013-06-06T17:26:44+0000",
            "content": "We are currently facing this issue when we upgraded Solr from 3.5 to 4.x. Our documents have large multiValued fields and we do see huge Heap which is not claimed back after GC. 3.5 with same set of documents works very well and the heap usage is around 50 to 65% (on long run with no replication)\n\nSome specs we used about test. (no replication happening anytime during our test)\n#CPU 6 Cores\nRAM 16Gb\nHeap 8GB (tried with 10 and 12 GB too)\nIndex size 4.6GB\nSolr version 4.2.1 (on Stand Alone M/c. Our Production implementation is Master/Slave)\nGC Policy used is CMS (tried G1GC too)\n\n\nOn analyzing the heap we see that the largest object in the heap is the byteRef tracing it back it points to CompressedStoredFieldReader. We are looking into the patch above and test if that helps\n\nif you need more information. I can provide. \n\nthanks\nAditya   ",
            "author": "Aditya",
            "id": "comment-13677265"
        },
        {
            "date": "2013-06-06T17:29:49+0000",
            "content": "Thanks Aditya. Your experiments with the patch would be very useful! Additionally, do you know how large (in bytes) are your largest documents? ",
            "author": "Adrien Grand",
            "id": "comment-13677270"
        },
        {
            "date": "2013-06-06T17:53:24+0000",
            "content": "Sure will update. Our \"Killer Doc\" (that's what we call it these days) is of size 25.3MB when saved from solr response for all fields with javabin writer. \nThis is our largest document in the index. ",
            "author": "Aditya",
            "id": "comment-13677307"
        },
        {
            "date": "2013-06-07T02:29:17+0000",
            "content": "Quick update on our test. We did a test for around 1 hour and it seams that this patch is working. GC does re-claim all the memory back to acceptable margin. (Test duration 17:20 to 18:35)\nI have attached CPU and heap Graph for our 1 hour test (8GB Heap 200qps only document cache with max size 4096 see hit ratio 89%)\nWe are performing some more configuration changes at JVM and Solr Cache size and run full night test to see if the server keeps up.\n\nTwo concerns that i see is \n1. Frequent GCs\n2. Heap is used up very quick.\n\nAny advice to improve, please let me know. ",
            "author": "Aditya",
            "id": "comment-13677767"
        },
        {
            "date": "2013-06-07T06:08:31+0000",
            "content": "Unfortunately, a buffer is needed to decompress data so I think I am going to stick to Simon's proposition and keep the per-CompressingStoredFieldsReader buffer around but only use it for reasonably small documents (I am currently thinking of a threshold of ~ 32kb, twice the block size). I will attach a new patch soon. ",
            "author": "Adrien Grand",
            "id": "comment-13677839"
        },
        {
            "date": "2013-06-09T09:44:52+0000",
            "content": "Here is a patch which only reuses the buffer when there is less than 32kb of data to decompress. ",
            "author": "Adrien Grand",
            "id": "comment-13678986"
        },
        {
            "date": "2013-06-10T08:39:50+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1491374\n\nLUCENE-4995: Don't hold references on large decompression buffers in CompressingStoredFieldsReader.\n\nCompressingStoredFieldsReader now only reuses an internal buffer when there is\nno more than 32kb to decompress. For large blocks, a dedicated decompression\nbuffer will be created on demand. ",
            "author": "Commit Tag Bot",
            "id": "comment-13679385"
        },
        {
            "date": "2013-06-10T08:45:02+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1491376\n\nLUCENE-4995: Added CHANGES entry. ",
            "author": "Commit Tag Bot",
            "id": "comment-13679388"
        },
        {
            "date": "2013-06-10T08:47:46+0000",
            "content": "[branch_4x commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1491378\n\nLUCENE-4995: Don't hold references on large decompression buffers in CompressingStoredFieldsReader (merged from r1491374).\n\nCompressingStoredFieldsReader now only reuses an internal buffer when there is\nno more than 32kb to decompress. For large blocks, a dedicated decompression\nbuffer will be created on demand. ",
            "author": "Commit Tag Bot",
            "id": "comment-13679389"
        },
        {
            "date": "2013-07-23T18:37:05+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716742"
        }
    ]
}