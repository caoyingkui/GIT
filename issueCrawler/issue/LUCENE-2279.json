{
    "id": "LUCENE-2279",
    "title": "eliminate pathological performance on StopFilter when using a Set<String> instead of CharArraySet",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "passing a Set<Srtring> to a StopFilter instead of a CharArraySet results in a very slow filter.\nthis is because for each document, Analyzer.tokenStream() is called, which ends up calling the StopFilter (if used). And if a regular Set<String> is used in the StopFilter all the elements of the set are copied to a CharArraySet, as we can see in it's ctor:\n\npublic StopFilter(boolean enablePositionIncrements, TokenStream input, Set stopWords, boolean ignoreCase)\n  {\n    super(input);\n    if (stopWords instanceof CharArraySet) \n{\n      this.stopWords = (CharArraySet)stopWords;\n    }\n else \n{\n      this.stopWords = new CharArraySet(stopWords.size(), ignoreCase);\n      this.stopWords.addAll(stopWords);\n    }\n    this.enablePositionIncrements = enablePositionIncrements;\n    init();\n  }\n\ni feel we should make the StopFilter signature specific, as in specifying CharArraySet vs Set, and there should be a JavaDoc warning on using the other variants of the StopFilter as they all result in a copy for each invocation of Analyzer.tokenStream().",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-02-23T00:38:56+0000",
            "content": "a yourkit profile (before/after):\n\nhttp://thushw.blogspot.com/2010/02/interesting-performance-characteristic.html ",
            "author": "thushara wijeratna",
            "id": "comment-12837008"
        },
        {
            "date": "2010-02-23T00:48:58+0000",
            "content": "this is because for each document, Analyzer.tokenStream() is called\n\nhave you considered implementing reusableTokenStream? ",
            "author": "Robert Muir",
            "id": "comment-12837016"
        },
        {
            "date": "2010-02-23T18:40:40+0000",
            "content": "isn't the resusableTokenStream created again for a new Document, while there is no need to copy the list of stopwords for a new document? or did i miss something? ",
            "author": "thushara wijeratna",
            "id": "comment-12837373"
        },
        {
            "date": "2010-02-23T19:40:39+0000",
            "content": "reusableTokenStream() is called again for each document. if you don't implement it, the default is to defer to tokenStream(), which must create new instances of StopFilter, LowerCaseFilter, whatever else you have going on in your analyzer.\n\ninstead, if you implement reusableTokenStream(), you can keep a reference to these things, and just reset() your tokenfilters, and pass the reader to your tokenizer's reset(Reader) method.\n\nof course, for this to work, you must implement reset() correctly in any custom filters you have: if they keep some state such as accumulated offsets or something, then these should be reset back to what they are just as if you created a new one.\n\nFor an example, see StandardAnalyzer ",
            "author": "Robert Muir",
            "id": "comment-12837410"
        },
        {
            "date": "2010-02-23T21:56:43+0000",
            "content": "I don't consider this as an issue at all. Each analyzer creating StopFilter instances uses CharArraySet internally and if you write your own you should do so too. The JavaDoc of StopFilter clearly describes what is going on if you use a set in favor of CharArraySet.\nYou should really consider reusabelTokenStream AND use a CharArraySet instance. You should have a look at the current trunk how all the analyzers handle stopwords. Once 3.1 is out you will also be able to subclass ReusableAnalyzerBase which enables reusableTokenStream on the the fly in 99% of the cases.\n\nI tend to close this issue though, Robert?\n ",
            "author": "Simon Willnauer",
            "id": "comment-12837465"
        },
        {
            "date": "2010-02-23T22:00:28+0000",
            "content": "in my opinion the issue states one of my biggest gripes with analysis, this whole tokenstream/reusabletokenstream thing.\n\nwe go to all this trouble to have a reusable attributes-based api, only for this analyzer problem to trip up users.\nmaybe its best to give 3.1's ReusableAnalyzerBase a chance, and see if it clears up the confusion for users.\nbut if it doesnt, in my opinion we should do a hard backwards break and make tokenstream reusable by default. ",
            "author": "Robert Muir",
            "id": "comment-12837467"
        },
        {
            "date": "2010-02-24T10:33:39+0000",
            "content": "Should we deprecate (eventually, remove) Analyzer.tokenStream?\n\nMaybe we should absorb ReusableAnalyzerBase back into Analyzer?\n\nOr.... maybe now is an opportune time to create a separate standalone\nanalyzers package (subproject under the Lucene tlp)?  We've broached\nthis idea in the past, and I think it's compelling.... I think\nLucene/Solr/Nutch need to eventually get to this point (where they\nshare analyzers from a single source), so maybe now is the time.\n\nIt'd be a single place where we would pull in all of Lucene's\ncore/contrib, plus Solr's analyzers, plus new analyzers Robert keeps\nmaking  Robert's efforts to upgrade Solr's analyzers to 3.0\n(currently a big patch waiting on SOLR-1657), plus his various other\npending analyzer bug fixes, could be done in this new analyzers\npackage.  And we could immediately fix \"problems\" we have with the\ncurrent analyzers API (like this reusable/tokenStream amibiguity). ",
            "author": "Michael McCandless",
            "id": "comment-12837736"
        },
        {
            "date": "2010-02-24T11:53:06+0000",
            "content": "Should we deprecate (eventually, remove) Analyzer.tokenStream? \nI would totally agree with that but  I guess we can not remove this method until lucene 4.0 which will be hmm in 2020  - just joking\n\nMaybe we should absorb ReusableAnalyzerBase back into Analyzer?\nThat would be the logical consequence but the problem with ReusableAnalyzerBase is that it will break bw comapt if moved to Analyzer. It assumes both #reusabelTokenStream and #tokenStream to be final and introduces a new factory method. Yet, as an analyzer developer you really want to use the new ReusableAnalyzerBase in favor of Analyzer in 99% of the cases and it will require you writing half of the code plus gives you reusability of the tokenStream\n\nbp. I think Lucene/Solr/Nutch need to eventually get to this point\nHuge +1 from my side. This could also unify the factory pattern solr uses to build tokenstreams. I would stop right here and ask to discuss it on the dev list, thoughts mike?!\n ",
            "author": "Simon Willnauer",
            "id": "comment-12837752"
        },
        {
            "date": "2010-02-24T12:34:42+0000",
            "content": "Yet, as an analyzer developer you really want to use the new ReusableAnalyzerBase in favor of Analyzer in 99% of the cases and it will require you writing half of the code plus gives you reusability of the tokenStream\n\nand the 1% extremely advanced cases that can't reuse, can just use TokenStreams directly when indexing, e.g. the Analyzer class could be reusable by definition. we shouldnt let these obscure cases slow down everyone else.\n\nIt assumes both #reusabelTokenStream and #tokenStream to be final\n\nin my opinion all the core analyzers (you already fixed contrib) should be final. this is another trap, if you subclass one of these analyzers and implement 'tokenStream', its immediately slow due to the backwards code.\n\nI think Lucene/Solr/Nutch need to eventually get to this point\n\nif this is what we should do to remove the code duplication, then i am all for it. i still don't quite understand how it gives us more freedom to break/change the APIs, i mean however we label this stuff, a break is a break to the user at the end of the day. ",
            "author": "Robert Muir",
            "id": "comment-12837759"
        },
        {
            "date": "2010-02-24T14:05:35+0000",
            "content": "I would stop right here and ask to discuss it on the dev list, thoughts mike?!\n\nAgreed... I'll start a thread.\n\n\nMaybe we should absorb ReusableAnalyzerBase back into Analyzer?\n\nThat would be the logical consequence but the problem with ReusableAnalyzerBase is that it will break bw comapt if moved to Analyzer.\n\nRight, this is why I was thinking if we make a new analyzers package, it's a chance to break/improve things.  We'd have a single abstract base class that only exposes reuse API.\n\nin my opinion all the core analyzers (you already fixed contrib) should be final. \n\nI agree, and we should consistently take this approach w/ the new analyzers package...\n\ni still don't quite understand how it gives us more freedom to break/change the APIs, i mean however we label this stuff, a break is a break to the user at the end of the day.\n\nBecause it'd be an entirely new package, so we can create a new base Analyzer class (in that package) that breaks/fixes things when compared to Lucene's Analyzer class.\n\nWe'd eventually deprecate the analyzers/tokenizers/token filters in Lucene/Solr/Nutch in favor of this new package, and users can switch over on their own schedule. ",
            "author": "Michael McCandless",
            "id": "comment-12837792"
        },
        {
            "date": "2010-02-24T16:33:13+0000",
            "content": "\nI would stop right here and ask to discuss it on the dev list, thoughts mike?!\n\nAgreed... I'll start a thread.\n\nOK I just started a thread on general@ ",
            "author": "Michael McCandless",
            "id": "comment-12837859"
        },
        {
            "date": "2011-09-25T04:52:04+0000",
            "content": "since we have merged lucene and solr, and chris has fixed analyzer to have a performant api, not by experts but by default, I think we can mark this issue resolved? ",
            "author": "Robert Muir",
            "id": "comment-13114140"
        },
        {
            "date": "2011-09-25T17:45:11+0000",
            "content": "I have not used this in a while. I just looked at the StopFilter source, and it seems identical to what I noticed before. I think the argument you make is that users should not use that form of the StopFilter constructor. I don't have a strong opinion on this. In any case, it was not a major issue to start with.\n\nThx for the response. ",
            "author": "thushara wijeratna",
            "id": "comment-13114298"
        },
        {
            "date": "2011-09-25T17:50:09+0000",
            "content": "You misunderstood the response: StopFilter indeed did not change. The change is now that in Lucene 4.0 all Analyzers are required to reuse TokenStream instances, so the StopFilter is only produced only once in your application (when the Analyzer is created). ",
            "author": "Uwe Schindler",
            "id": "comment-13114300"
        }
    ]
}