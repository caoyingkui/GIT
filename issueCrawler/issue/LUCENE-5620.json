{
    "id": "LUCENE-5620",
    "title": "LowerCaseFilter.preserveOriginal",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [],
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": []
    },
    "description": "Following closely the model of LUCENE-5437 (which worked on ASCIIFoldingFilter), this patch adds the ability to preserve the original token to LowerCaseFilter.  This is useful if you want an all-lowercase search term to match without regard to case, while search terms with uppercase letters match in a case-sensitive manner.",
    "attachments": {
        "LUCENE-5620.patch": "https://issues.apache.org/jira/secure/attachment/12640953/LUCENE-5620.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13974910",
            "author": "Robert Muir",
            "content": "It appears a can of worms was opened. I think in such cases a separate field should be used. \n\nIts too complicated for all of our analyzers to have to have preserveOriginal options.  ",
            "date": "2014-04-19T17:39:28+0000"
        },
        {
            "id": "comment-13974911",
            "author": "Mike Sokolov",
            "content": "That's the approach we have been using, but it leads to a larger index and a proliferation of fields. If you want query-time control over whether to search case-insensitive, diacritic-insensitive, you have to create four fields, one for each combination (now it's only two because of LUCENE-5437).  I also don't think this leads to very many other places - what would they be? We have sometimes wanted to turn stemming on and off as well, but other than that I don't know which other filters would be likely targets for this kind of enhancement. ",
            "date": "2014-04-19T17:47:18+0000"
        },
        {
            "id": "comment-13974912",
            "author": "Robert Muir",
            "content": "I don't understand the \"larger index\" stuff. Its an inverted index. I think this is a common source of confusion and unfortunately results in these things polluting the codebase.\n\nI think the option added in LUCENE-5437 should be removed. \n\nIf we REALLY must have a preserveOriginal option, then we should add a PreserveOriginalSnapshot/Restore filters that take care of this \"around\" any arbitrary filters. Then all existing preserveOriginal options can be eradicated.\n\nThis way such stuff does not infect basic things like lowercasefilter, which should only be doing lowercasing. ",
            "date": "2014-04-19T18:02:36+0000"
        },
        {
            "id": "comment-13974914",
            "author": "Mike Sokolov",
            "content": "Maybe I am operating based on a misunderstanding, but isn't it the case that each field/term/position/document combination adds a posting to the index?  I do like the idea of a generic \"preserve original\" filter that can work with any filter/combination of filters, since I agree it's silly to duplicate this logic for every filter.  How would you see that working? ",
            "date": "2014-04-19T18:13:29+0000"
        },
        {
            "id": "comment-13974916",
            "author": "Ahmet Arslan",
            "content": "but other than that I don't know which other filters would be likely targets for this kind of enhancement.\nPlease see : https://issues.apache.org/jira/browse/SOLR-5332\n\nIts too complicated for all of our analyzers to have to have preserveOriginal options.\nI agree with this. Any token filter that modifies its input could be a candidate. e.g. TurkishLowerCaseFilter.\n\nHow about making these candidate filters (may be all) respect to KeywordAttribute. And simulating preserve original behavior with KeywordRepeatFilter?\n\n\n<filter class=\"solr.KeywordRepeatFilterFactory\"/>\n<filter class=\"solr.LowerCaseFilterFactory\"/>\n<filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/>\n\n ",
            "date": "2014-04-19T18:16:56+0000"
        },
        {
            "id": "comment-13974919",
            "author": "Robert Muir",
            "content": "I dont like the keyword + repeat stuff, because this was really geared at blocking stemming, suddenly it applies to a new filter and breaks existing analysis chains.\n\nIt also still means modifying each and every filter for this stuff. i dont like that, it makes no sense when they can be completely unaware of it. \n\ni seriously can't imagine a situation where lowercasefilter is doing anything more than lowercasing, sorry.\n\njust add a preservesnapshot/restore. you put the snapshot somewhere in your chain (e.g. before lowercase), it saves term text and maybe posinc into an attribute, then the restore (e.g. after the lowercase) checks if posinc is the same (e.g. nothing was deleted in between) and termtext now differs. if it differs, add the text as a synonym.  ",
            "date": "2014-04-19T18:25:25+0000"
        },
        {
            "id": "comment-13974921",
            "author": "Robert Muir",
            "content": "\nMaybe I am operating based on a misunderstanding, but isn't it the case that each field/term/position/document combination adds a posting to the index?\n\nThats exactly the case. So whether you have field1:UPPER->0 and field1:upper->0, or field1:UPPER->0 and field2:upper->0 makes no difference. ",
            "date": "2014-04-19T18:27:17+0000"
        },
        {
            "id": "comment-13974924",
            "author": "Mike Sokolov",
            "content": "bq  whether you have field1:UPPER->0 and field1:upper->0, or field1:UPPER->0 and field2:upper->0 makes no difference.\n\nYes, I see that.  But if you have field1:lower->0 and field2:lower->0 then you have doubled the number of postings required, and most terms in English are going to be lower-case only. ",
            "date": "2014-04-19T18:31:45+0000"
        },
        {
            "id": "comment-13974926",
            "author": "Mike Sokolov",
            "content": "I'll explore the preserve/restore option.  It makes it slightly more expensive to determine whether there have been modifications, since some filters can optimize that a little, but you only have to do it once for a chain of them, so it is an attractive option I think. ",
            "date": "2014-04-19T18:33:00+0000"
        },
        {
            "id": "comment-13974927",
            "author": "Robert Muir",
            "content": "but doing this selectively (only adding additional terms in some cases) is pretty complicated if you dont want to screw over length normalization. So it probably hurts more than it helps really. As for english language: its irrelevant here. ",
            "date": "2014-04-19T18:34:19+0000"
        },
        {
            "id": "comment-13974929",
            "author": "Robert Muir",
            "content": "As far as more expensive, we shouldnt think that way here. If we wanted it to be as fast as possible, there would be no chain at all! Here its more important to keep it clean. ",
            "date": "2014-04-19T18:39:02+0000"
        },
        {
            "id": "comment-13974932",
            "author": "Mike Sokolov",
            "content": "doing this selectively (only adding additional terms in some cases) is pretty complicated if you dont want to screw over length normalization\n\nInteresting point, although it's debatable how strong the effect is - I guess it depends on how many tokens are affected by the filter chain, and whether this varies in any significant way from document to document: I tend to think that the number of capitalized words, say, will be similar from document to document, but of course there will be exceptions in different data sets. \n\nIt makes me wonder whether length normalization shouldn't use max position instead of term count when it is available. ",
            "date": "2014-04-19T18:50:47+0000"
        },
        {
            "id": "comment-13974977",
            "author": "Manuel Lenormand",
            "content": "My answer regards a Solr usecase but as it uses the Lucene filters I think it can contribute to the discussion.\n\nOn one of our morphology projects we discussed the field splitting issue. We wanted to enable a stemmed an non stemmed search for these different languages, mainly for advanced users who wish to control their search terms.\n\nThe drawbacks of the field splitting were \n1) QParser flexibility- (not being forced to use a dismax defType in order to query multiple fields in a single query.\n 2) \"readability\" - the developer / user could see in a single place all the terms a query could match in an indexed document via the admin UI without asking him to understand a parsedQuery string or the qf param.\n3) term position - enabling a phrase query that would match \"originalTerm stemmedTerm\". Enabling it in a splitted field would mean saving the original term (dictionary and posting) twice,\n3) perf (more of an anecdote) - as the terms were generally suffix stemmed we had good chances of loading the same term block and posting list to memory as they should be sequential.\n\nI do agree a PreserveOriginalSnapshot could be a good resolution ",
            "date": "2014-04-19T21:28:48+0000"
        },
        {
            "id": "comment-13974982",
            "author": "Robert Muir",
            "content": "\nIt makes me wonder whether length normalization shouldn't use max position instead of term count when it is available.\n\nThis is your choice, its whatever your similarity uses. currently most of the similarities shipped with lucene have an option you can choose. The problem is if you are synonyms-heavy, its bad. But I opened and issue and changed the default to exactly this way in lucene 3.1 because so many users were injecting 'fake' terms without thinking about the consequences.\n\n\nThe drawbacks of the field splitting were\n1) QParser flexibility- (not being forced to use a dismax defType in order to query multiple fields in a single query.\n2) \"readability\" - the developer / user could see in a single place all the terms a query could match in an indexed document via the admin UI without asking him to understand a parsedQuery string or the qf param.\n3) term position - enabling a phrase query that would match \"originalTerm stemmedTerm\". Enabling it in a splitted field would mean saving the original term (dictionary and posting) twice,\n3) perf (more of an anecdote) - as the terms were generally suffix stemmed we had good chances of loading the same term block and posting list to memory as they should be sequential.\n\nThose all sound like solr problems: not relevant to any decisions to be made here. In lucene (even the queryparser) you can override phrase queries, to use unstemmed field for example. And if you want to do it that way, just enable documents and frequencies on the stemmed field (no proximity necessary there, just ordinary scoring). ",
            "date": "2014-04-19T21:50:35+0000"
        },
        {
            "id": "comment-13974995",
            "author": "Robert Muir",
            "content": "Here is a prototype patch. The cleaner way is the faster way too:\n\n\tDoesn't slowdown users who dont care about such things (the LowerCaseFilter.preserveOriginal patch would slowdown all lucene users with its call to isUppercase)\n\tDoesn't need to actually capture/restore state: we only need to save term text and position. Its just to inject a synonym, stuff like offsets dont need to be saved, no filter should be changing them anyway \n\n\n\nI didnt yet remove the existing preserveOriginal cancer about the codebase, but thats easy. and the factories that have it can still work with the option (just wrap the filter with these guys). ",
            "date": "2014-04-19T23:20:35+0000"
        },
        {
            "id": "comment-13975118",
            "author": "Mike Sokolov",
            "content": "Yes, that looks good to me.  I like that this can be applied to arbitrary filters.  Thanks for pushing ahead with this, Robert. I think it'll be a nice little addition.   ",
            "date": "2014-04-20T12:05:53+0000"
        },
        {
            "id": "comment-13975292",
            "author": "Mike Sokolov",
            "content": "I tested with some analysis chains, and I realized the new CaptureAttribute will need a stack of term/position states in order to handle multiple preserve/restore pairs either in sequence or nested.\n\nAlso, we should consider what to do with unbalanced preserve/restore pairs.  Maybe just don't restore anything, but it might be helpful to throw an exception? ",
            "date": "2014-04-20T21:47:56+0000"
        },
        {
            "id": "comment-13975298",
            "author": "Robert Muir",
            "content": "wait: in sequence its fine, its always cleared.\n\nI'm not sure we need to support nesting for any reason? What would that accomplish? ",
            "date": "2014-04-20T22:02:51+0000"
        },
        {
            "id": "comment-13975299",
            "author": "Robert Muir",
            "content": "I would simply add a check to the capturingfilter in incrementToken:\n\nif (!captureAtt.isEmpty()) {\n  throw new IllegalStateException(\"No.\");\n}\n\n ",
            "date": "2014-04-20T22:04:49+0000"
        },
        {
            "id": "comment-13975300",
            "author": "Mike Sokolov",
            "content": "Well, nested is probably not a real use case, but it could certainly happen.  In my test I tried:\n\npreserve, lower-case, preserve, ascii-fold, restore, restore\n\nwhich gives \"test\", \"t\u00e9st\", \"T\u00e9st\u201c  for input of \"T\u00e9st\"\n\nwhy you would want to do this I have no idea\n\nPossibly a use case is preserve, lower-case, preserve, kstem, restore, restore ? Since kstem can only handle lower-case terms, although it would work fine the other way. ",
            "date": "2014-04-20T22:07:10+0000"
        },
        {
            "id": "comment-13975304",
            "author": "Robert Muir",
            "content": "i dont think its a use case really, you could just have two balanced pairs. we can detect the unbalancing by adding the if i mentioned above.  ",
            "date": "2014-04-20T22:18:33+0000"
        },
        {
            "id": "comment-13975310",
            "author": "Mike Sokolov",
            "content": "Yeah, OK.  I had a different implementation that was broken somehow when there was a sequence of preserve/restore, but this one is OK.  I agree that nested pairs are not worth supporting unless somebody comes up with a reason for it. ",
            "date": "2014-04-20T22:43:43+0000"
        },
        {
            "id": "comment-13975532",
            "author": "Robert Muir",
            "content": "Even if someone comes up with a reason: that's not enough.\n\nWe don't have to support every esoteric use case on the planet. Its more important that the code is simple, understandable, maintainable, has debuggable tests, and good documentation.\n\nPersonally, i dont care if the patch on this issue ever gets committed, the use case to me does not exist. I only wrote it to have enough ammo to stop any future 'preserveOriginal' additions dead in their tracks. ",
            "date": "2014-04-21T11:46:51+0000"
        },
        {
            "id": "comment-13980426",
            "author": "Ahmet Arslan",
            "content": "Robert Muir , here is another ammo:  ASCIIFoldingFilter's preserveOriginal=true breaks wildcard queries, since it is a MultiTermAwareComponent.\n\n\n  <fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>            \n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.ASCIIFoldingFilterFactory\" preserveOriginal=\"true\"/>\n      </analyzer>      \n    </fieldType>\n\n\n\nWith the above type, q=manu:Belk\u0131* yields: \n\n\n \"error\": {\n    \"msg\": \"analyzer returned too many terms for multiTerm term: Belk\u0131\",\n    \"code\": 400\n  } \n\n\n\nI think preserveOriginal is dangerous for token filters that implement MultiTermAwareComponent. \nWhat is the preferred action here? Document this limitation/behavior? Or consider this as a bug and open a jira? ",
            "date": "2014-04-24T22:49:29+0000"
        },
        {
            "id": "comment-13982404",
            "author": "Mike Sokolov",
            "content": "I would just note that the primary use case for this kind of functionality (whether embedded in the filter, or using a wrapping approach) is on the index side, not on the query side. If you are preserving the original case (or diacritics, or whatever) in the index, why do you need to collapse the terms in the query?  If you leave them alone you get more precise matching, which for me at least, is the whole point.  If you are going to collapse the query terms when searching, there's no reason to preserve the originals in the index.  \n\nSo that last comment may be a bit of a red herring?  Still it is a trap for the unwary, and in general the wrapping preserve/restore filter seems like a better approach to me. ",
            "date": "2014-04-27T17:45:12+0000"
        },
        {
            "id": "comment-13982450",
            "author": "Ahmet Arslan",
            "content": "After re-reading the description, it looks like you have a different use case. If I understand it correctly,  if we have an example document General Motors query GENERAL MOTORS won't return that document. And you are OK with that. In other words following field type represents your use case.\n\n\n    <fieldType name=\"text_preserve\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>            \n        <filter class=\"solr.LowerCaseFilterFactory\" preserveOriginal=\"true\"/>\n      </analyzer>   \n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>       \n      </analyzer>       \n    </fieldType>\n\n\n\nHowever, I thought that preserveOriginal is intended to boost documents that contains original/raw/as-is search terms.   ",
            "date": "2014-04-27T20:09:48+0000"
        },
        {
            "id": "comment-13982467",
            "author": "Mike Sokolov",
            "content": "ah yes, I hadn't considered merely boosting the original term.  I guess my feeling is if a user went to the trouble to type a search term in all caps, that's probably what they meant!  Probably a bias from the audience we serve, who tend to want greater precision. ",
            "date": "2014-04-27T20:56:50+0000"
        }
    ]
}