{
    "id": "SOLR-7798",
    "title": "Improve robustness of ExpandComponent",
    "details": {
        "components": [
            "SearchComponents - other"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "The ExpandComponent causes a NullPointerException if accidentally used without prior collapsing of results.\n\nIf there are multiple documents in the result which have the same term value in the expand field, the size of the ordBytes/groupSet differs from the count value, and the getGroupQuery method creates an incompletely filled bytesRef array, which later causes a NullPointerException when trying to sort the terms.\n\nThe attached patch extends the test to demonstrate the error, and modifies the getGroupQuery methods to create the array based on the size of the input maps.",
    "attachments": {
        "expand-component.patch": "https://issues.apache.org/jira/secure/attachment/12745634/expand-component.patch",
        "expand-npe.patch": "https://issues.apache.org/jira/secure/attachment/12911396/expand-npe.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-02-21T16:12:15+0000",
            "author": "Michael Gibney",
            "content": "Although J\u00f6rg Rathlev's initial description mentions an NPE in ExpandComponent \"if\u00a0accidentally used without prior collapsing of results\" (italics mine), there are applications of ExpandComponent that\u00a0intentionally do not involve prior collapsing of results on the expand field. For example, I'm using cached Join queries to implement tiered deduplication of the search domain across multiple document sources, but do not wish to deduplicate documents against other documents from the same source (and specifically wish to deduplicate the search domain, as opposed to the set of results). The approach is described in a bit more detail here (bullet points 3, 4, and 7 are particularly relevant).\n\nexpand-component.patch looks good to me, as I can't see a reason why\u00a0count is being tracked separately, rather than relying on\u00a0ordBytes.size(). The only potential issue I see with it is that where\u00a0count is used to determine whether\u00a0groupQuery is initialized,\u00a0count now represents a different concept than\u00a0ordBytes.size(). I'm not sure what the desired behavior would be (or for that matter, what the explanation is for the magic \"200\" ceiling on\u00a0count).\n\nI've uploaded an alternative, expand-npe.patch , which differs only in that it leaves the separate tracking of count in place (though I don't think it should have to), and also in that it checks for duplication on addition of ord to groupBits/groupSet, thereby avoiding unnecessary BytesRef.deepCopyOf() in the (normally rare) case where duplicate terms are encountered. ",
            "id": "comment-16371599"
        },
        {
            "date": "2018-02-22T00:29:05+0000",
            "author": "Joel Bernstein",
            "content": "Thanks for the patch. If you can setup a pull request I should have time to review and commit when its ready. ",
            "id": "comment-16372251"
        },
        {
            "date": "2018-02-22T15:40:55+0000",
            "author": "Michael Gibney",
            "content": "Thanks, Joel Bernstein. I'm happy to prep a PR, but would you mind first confirming that count (and its associated ceiling of 200) is intended to represent the number of matching collapse values, as opposed to the number of result documents associated with those values? Assuming that's the case, is there any reason to continue tracking count externally (as opposed to simply relying on ordBytes.size(), as in J\u00f6rg Rathlev's expand-component.patch patch)? ",
            "id": "comment-16372962"
        },
        {
            "date": "2018-02-22T16:46:20+0000",
            "author": "Joel Bernstein",
            "content": "Just getting reacquainted with the code. I believe the 200 magic number is an inflection point for when it makes sense to build a disjunction query for retrieving the group records. I did quite a bit of performance tuning in the original release and I believe 200 was the tipping point for when the disjunction slowed things down, rather then sped things up. I'll need to do a little more digging to fully understand how the patch effects things. ",
            "id": "comment-16373055"
        },
        {
            "date": "2018-02-22T17:02:32+0000",
            "author": "Joel Bernstein",
            "content": "I believe the patch will indeed make things more robust. It looks like to me if there are duplicate values in the ordBytes map then we'll just have duplicate values in the disjunction query, which will make things slower, but still work fine.\n\nSo, I see no problem with the patch.\n\n\u00a0 ",
            "id": "comment-16373072"
        },
        {
            "date": "2018-02-22T17:14:33+0000",
            "author": "Joel Bernstein",
            "content": "Actually there can't be duplicate values in the ordBytes Map, so that's not even an issue.\n\nThis patch looks pretty safe to me. ",
            "id": "comment-16373096"
        },
        {
            "date": "2018-02-22T18:06:09+0000",
            "author": "Michael Gibney",
            "content": "Right, sounds good! Thanks for the explanation of the 200 ceiling. Just submitted PR 325. ",
            "id": "comment-16373197"
        },
        {
            "date": "2018-02-22T18:52:50+0000",
            "author": "Joel Bernstein",
            "content": "I've got the branch locally. I'll do a little more reviewing, run tests etc... and then most likely commit it without change.\n\nThanks! ",
            "id": "comment-16373246"
        },
        {
            "date": "2018-02-22T20:15:55+0000",
            "author": "Joel Bernstein",
            "content": "The test case is failing for me. What version are you working with? It looks like the test might have been developed on a branch that is incompatible with the current master branch.\n\nThe commit workflow is to commit to the master branch and back port to release branch, which is branch_7x. ",
            "id": "comment-16373370"
        },
        {
            "date": "2018-02-22T21:32:30+0000",
            "author": "Michael Gibney",
            "content": "I was indeed developing on master. Which test case is failing for you? I'm getting a couple of failed tests on master (364b680afaf9) that seem to be unrelated to the ExpandComponent changes:\n\n [junit4] Tests with failures [seed: 2723B1A9FC179033]:\n [junit4]   - org.apache.solr.handler.component.DummyCustomParamSpellChecker.initializationError\n [junit4]   - org.apache.solr.handler.component.ResourceSharingTestComponent.initializationError\n\n\n\nbut when I try to selectively run the ExpandComponent test I'm getting:\n\n\nant -Dtests.class=\"org.apache.solr.handler.component.TestExpandComponent\" test\n...\n [junit4] Tests summary: 0 suites, 0 tests\n\n\n\nNo errors ... but I guess something is still amiss. ",
            "id": "comment-16373489"
        },
        {
            "date": "2018-02-22T22:48:12+0000",
            "author": "Joel Bernstein",
            "content": "try:\n\nant test -Dtestcase=TestExpandComponent ",
            "id": "comment-16373618"
        },
        {
            "date": "2018-02-23T04:01:49+0000",
            "author": "Michael Gibney",
            "content": "I was able to reproduce an NPE with the above command; but the test only threw this NPE intermittently, and I was able to reproduce it on master (364b680afaf9) as well. I've included the stack trace to make sure we're talking about the same issue.\n\n[junit4] 2> 2778 ERROR (searcherExecutor-7-thread-1) [ ] o.a.s.s.LRUCache Error during auto-warming of key:org.apache.solr.search.QueryResultKey@7ce6ad2e:java.lang.RuntimeException: java.lang.NullPointerException\n[junit4] 2> at org.apache.solr.search.CollapsingQParserPlugin$CollapsingPostFilter.getFilterCollector(CollapsingQParserPlugin.java:378)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher.getProcessedFilter(SolrIndexSearcher.java:1084)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher.getDocListNC(SolrIndexSearcher.java:1540)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:1416)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher.access$000(SolrIndexSearcher.java:90)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher$3.regenerateItem(SolrIndexSearcher.java:575)\n[junit4] 2> at org.apache.solr.search.LRUCache.warm(LRUCache.java:297)\n[junit4] 2> at org.apache.solr.search.SolrIndexSearcher.warm(SolrIndexSearcher.java:2146)\n[junit4] 2> at org.apache.solr.core.SolrCore.lambda$getSearcher$16(SolrCore.java:2258)\n[junit4] 2> at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n[junit4] 2> at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:188)\n[junit4] 2> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n[junit4] 2> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n[junit4] 2> at java.lang.Thread.run(Thread.java:745)\n[junit4] 2> Caused by: java.lang.NullPointerException\n[junit4] 2> at org.apache.solr.search.CollapsingQParserPlugin$OrdScoreCollector.<init>(CollapsingQParserPlugin.java:514)\n[junit4] 2> at org.apache.solr.search.CollapsingQParserPlugin$CollectorFactory.getCollector(CollapsingQParserPlugin.java:1331)\n[junit4] 2> at org.apache.solr.search.CollapsingQParserPlugin$CollapsingPostFilter.getFilterCollector(CollapsingQParserPlugin.java:367)\n[junit4] 2> ... 13 more\n\n ",
            "id": "comment-16373889"
        },
        {
            "date": "2018-02-23T05:13:49+0000",
            "author": "Michael Gibney",
            "content": "It looks like the randomness comes from Line 58 or TestExpandComponent, when \"hint=top_fc\" is randomly specified; the problem arises when it's specified for a field with no SortedDocValues (the null comes from here). ",
            "id": "comment-16373938"
        },
        {
            "date": "2018-02-23T15:40:20+0000",
            "author": "Joel Bernstein",
            "content": "There were a couple issues I ran into. First, I worked with the pull request, but I found the commit message wasn't formatted quite right and the test wasn't include.\u00a0\n\nNext, I applied the patch, which didn't apply cleanly on master.\u00a0\n\nSo, I decided to hand integrate the changes and the test from the patch. The test case in the patch looked like it might have been written for an older version and was failing every time.\n\nI wouldn't worry about it though. This is a small enough change that I can just fix things up on my own and commit it. It will just take a little longer to get committed because I need to carve out a little more time to work with it. But I will get this committed unless I run into a blocker. ",
            "id": "comment-16374513"
        },
        {
            "date": "2018-02-23T19:10:36+0000",
            "author": "Michael Gibney",
            "content": "Sorry, yes! I see. The test case was from J\u00f6rg Rathlev's patch (July 2015). I incorporated an updated version of the test case (along with a new commit message), and pushed a new commit to PR 325. Feel free to use this as you see fit \u2013 I'm happy to squash-rebase against master if you like. Thanks! ",
            "id": "comment-16374861"
        },
        {
            "date": "2018-08-09T01:01:37+0000",
            "author": "Gavin",
            "content": "Move issue from deprecated 'In  Progress' back to 'Open' ",
            "id": "comment-16574141"
        }
    ]
}