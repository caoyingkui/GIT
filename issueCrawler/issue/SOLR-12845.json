{
    "id": "SOLR-12845",
    "title": "Add a default cluster policy",
    "details": {
        "type": "Improvement",
        "status": "Open",
        "labels": "",
        "fix_versions": [],
        "components": [
            "AutoScaling"
        ],
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None"
    },
    "description": "Varun Thacker commented on SOLR-12739:\n\nWe should also ship with some default policies - \"Don't allow more than one replica of a shard on the same JVM\" , \"Distribute cores across the cluster evenly\" , \"Distribute replicas per collection across the nodes\"\n\nThis issue is about adding these defaults. I propose the following as default cluster policy:\n\n# Each shard cannot have more than one replica on the same node if possible\n{\"replica\": \"<2\", \"shard\": \"#EACH\", \"node\": \"#ANY\", \"strict\":false}\n# Each collections replicas should be equally distributed amongst nodes\n{\"replica\": \"#EQUAL\", \"node\": \"#ANY\", \"strict\":false} \n# All cores should be equally distributed amongst nodes\n{\"cores\": \"#EQUAL\", \"node\": \"#ANY\", \"strict\":false}",
    "attachments": {
        "SOLR-12845.patch": "https://issues.apache.org/jira/secure/attachment/12943188/SOLR-12845.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16642954",
            "content": "The test TestDeleteCollectionOnDownNodes fails sometimes after autoscaling policy was made the default assignment policy. This test makes assumptions that 4 shards, 3 replicas each hosted on 4 nodes will be distributed in such a way that there will always be a leader if you stop two random nodes. This requires that more than 1 replica of the same shard are never on the same node. I'll mark the test as AwaitsFix until we fix this issue which will add such a rule to the default policy. ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-10-09T08:23:47+0000"
        },
        {
            "id": "comment-16643068",
            "content": "One problem with the first two strict rules is that they interfere with autoAddReplicas. Say a node goes down and it hosted 2 replicas and it was also the least loaded node then the engine will refuse to suggest an operation because doing so will increase the load on nodes that are already more loaded. There is a forceOperation flag on suggesters which is today used by IndexSizeTrigger which allows suggestions even if they will increase load on other nodes. However, autoAddReplicas implementation does not use this flag but maybe it should? Either that or all the default rules should be made strict=false. ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-10-09T09:58:29+0000"
        },
        {
            "id": "comment-16643693",
            "content": "Thanks Shalin! This will be a great improvement in terms of how people use the system - Users don't have to think of making rules to distribute load unless they are doing something very specific.\n\n\u00a0\nHowever, autoAddReplicas implementation does not use this flag but maybe it should? Either that or all the default rules should be made strict=fal\n\n...\n\n\tEach shard cannot have more than one replica on the same node if possible\n\n\nMaybe this policy shoud not be strict=false but the other two policies could be ? ",
            "author": "Varun Thacker",
            "date": "2018-10-09T16:20:17+0000"
        },
        {
            "id": "comment-16644537",
            "content": "Maybe this policy shoud not be strict=false but the other two policies could be ?\n\nActually, this has to be non-strict because it effectively sets maxShardsPerNode=1 for everyone. I've opened SOLR-12847 to cut over maxShardsPerNode to a policy rule but until then this has to be strict=false.\n\nMy original proposal (in the description) had strict=false only for the first rule but I have changed the defaults to use strict=false for all rules otherwise it prevents Solr from performing operations that were possible earlier due to violations.\n\nAs I expected, there are tons of test failures with the new defaults due to the assumptions on how replicas will be placed. I'm working through those test failures. ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-10-10T06:59:09+0000"
        },
        {
            "id": "comment-16644547",
            "content": "Linking to SOLR-12739 ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-10-10T07:15:38+0000"
        },
        {
            "id": "comment-16661325",
            "content": "I can think of two more defaults ( both should be strict:false ) that we could add to the autoscaling.json file\u00a0 :\n\n\tDon't have more than two replicas on the same physical host :\n\n{\"replica\": \"<2\", \"host\": \u201c#ANY\u201d}\n\n\n\n\n\n\tDefine a well known system property called \"rack\" in Solr ( SOLR-12907 )\u00a0 -\n\n{\"replica\": \"#EQUAL\", \"shard\": \"#EACH\", \"rack\": \u201c#EACH\u201d}\n\n\n ",
            "author": "Varun Thacker",
            "date": "2018-10-23T21:35:17+0000"
        },
        {
            "id": "comment-16672964",
            "content": "Updated patch that applies to latest master and fixes the failure in TestUtilizeNode ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-11-02T11:24:37+0000"
        },
        {
            "id": "comment-16680527",
            "content": "Blocked by SOLR-12978 ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-11-08T22:43:46+0000"
        }
    ]
}