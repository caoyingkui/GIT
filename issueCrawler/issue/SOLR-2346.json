{
    "id": "SOLR-2346",
    "title": "Non UTF-8 Text files having other than english texts(Japanese/Hebrew) are no getting indexed correctly.",
    "details": {
        "affect_versions": "1.4.1,                                            3.1,                                            4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "contrib - Solr Cell (Tika extraction)"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I am able to successfully index/search non-Engilsh files (like Hebrew, Japanese) which was encoded in UTF-8. However, When I tried to index data which was encoded in local encoding like Big5 for Japanese I could not see the desired results. The contents after indexing looked garbled for Big5 encoded document when I searched for all indexed documents. When I index attached non utf-8 file it indexes in following way\n\n\n\t<result name=\"response\" numFound=\"1\" start=\"0\">\n\t<doc>\n\t<arr name=\"attr_content\">\n  <str>\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd</str>\n  </arr>\n\t<arr name=\"attr_content_encoding\">\n  <str>Big5</str>\n  </arr>\n\t<arr name=\"attr_content_language\">\n  <str>zh</str>\n  </arr>\n\t<arr name=\"attr_language\">\n  <str>zh</str>\n  </arr>\n\t<arr name=\"attr_stream_size\">\n  <str>17</str>\n  </arr>\n\t<arr name=\"content_type\">\n  <str>text/plain</str>\n  </arr>\n  <str name=\"id\">doc2</str>\n  </doc>\n  </result>\n  </response>\n\n\n\nHere you said it index file in UTF8 however it seems that non UTF8 file gets indexed in Big5 encoding.\nHere I tried fetching indexed data stream in Big5 and converted in UTF8.\n\nString id = (String) resulDocument.getFirstValue(\"attr_content\");\n            byte[] bytearray = id.getBytes(\"Big5\");\n            String utf8String = new String(bytearray, \"UTF-8\");\nIt does not gives expected results.\n\nWhen I index UTF-8 file it indexes like following\n\n\n\t<doc>\n\t<arr name=\"attr_content\">\n  <str>\u30de\u30a4 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af</str>\n  </arr>\n\t<arr name=\"attr_content_encoding\">\n  <str>UTF-8</str>\n  </arr>\n\t<arr name=\"attr_stream_content_type\">\n  <str>text/plain</str>\n  </arr>\n\t<arr name=\"attr_stream_name\">\n  <str>sample_jap_unicode.txt</str>\n  </arr>\n\t<arr name=\"attr_stream_size\">\n  <str>28</str>\n  </arr>\n\t<arr name=\"attr_stream_source_info\">\n  <str>myfile</str>\n  </arr>\n\t<arr name=\"content_type\">\n  <str>text/plain</str>\n  </arr>\n  <str name=\"id\">doc2</str>\n  </doc>\n\n\n\nSo, I can index and search UTF-8 data.\n\n\nFor more reference below is the discussion with Yonik.\n    Please find attached TXT file which I was using to index and search.\n\n    curl \"http://localhost:8983/solr/update/extract?literal.id=doc1&uprefix=attr_&fmap.content=attr_content&fmap.div=foo_t&boost.foo_t=3&commit=true&charset=utf-8\" -F \"myfile=@sample_jap_non_UTF-8\"\n\n\nOne problem is that you are giving big5 encoded text to Solr and saying that it's UTF8.\nHere's one way to actually tell solr what the encoding of the text you are sending is:\n\ncurl \"http://localhost:8983/solr/update/extract?literal.id=doc1&uprefix=attr_&fmap.content=attr_content&fmap.div=foo_t&boost.foo_t=3&commit=true\" --data-binary @sample_jap_non_UTF-8.txt -H 'Content-type:text/plain; charset=big5'\n\nNow the problem appears that for some reason, this doesn't work...\nCould you open a JIRA issue and attach your two test files?\n\n-Yonik\nhttp://lucidimagination.com",
    "attachments": {
        "NormalSave.msg": "https://issues.apache.org/jira/secure/attachment/12470231/NormalSave.msg",
        "UnicodeSave.msg": "https://issues.apache.org/jira/secure/attachment/12470232/UnicodeSave.msg",
        "sample_jap_UTF-8.txt": "https://issues.apache.org/jira/secure/attachment/12470106/sample_jap_UTF-8.txt",
        "SOLR-2346.patch": "https://issues.apache.org/jira/secure/attachment/12473094/SOLR-2346.patch",
        "sample_jap_non_UTF-8.txt": "https://issues.apache.org/jira/secure/attachment/12470107/sample_jap_non_UTF-8.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Prasad Deshpande",
            "id": "comment-12989975",
            "date": "2011-02-03T05:51:31+0000",
            "content": "I have verified use case using attached files. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12990060",
            "date": "2011-02-03T11:32:38+0000",
            "content": "\nString id = (String) resulDocument.getFirstValue(\"attr_content\");\nbyte[] bytearray = id.getBytes(\"Big5\");\nString utf8String = new String(bytearray, \"UTF-8\");\nIt does not gives expected results.\n\n\n\nYou cannot convert character sets this way in java (asking for the bytes in big5 but \nthen making a string as utf-8)... this is wrong. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12990172",
            "date": "2011-02-03T16:50:17+0000",
            "content": "From the email thread:\n\nOne problem is that you are giving big5 encoded text to Solr and saying that it's UTF8.\nHere's one way to actually tell solr what the encoding of the text you are sending is:\n\ncurl \"http://localhost:8983/solr/update/extract?literal.id=doc1&uprefix=attr_&fmap.content=attr_content&fmap.div=foo_t&boost.foo_t=3&commit=true\" --data-binary @sample_jap_non_UTF-8.txt -H 'Content-type:text/plain; charset=big5'\n\nNow the problem appears that for some reason, this doesn't work...\nCould you open a JIRA issue and attach your two test files? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12990185",
            "date": "2011-02-03T17:11:42+0000",
            "content": "Right, I agree solr should work with a non-UTF8 encoded doc in this way,\nbut the code is still wrong: its not a correct way to convert characters in java. "
        },
        {
            "author": "Prasad Deshpande",
            "id": "comment-12990444",
            "date": "2011-02-04T03:56:37+0000",
            "content": "I agree, I was just trying to decode the garbled characters so that it can be readable to the user. Still the problem is in indexing, while indexing the all the characters are getting garbled. "
        },
        {
            "author": "Prasad Deshpande",
            "id": "comment-12990547",
            "date": "2011-02-04T12:52:08+0000",
            "content": "Hope following issue could be same.\n\nAbove are the Hebrew *.msg files which I have tried to index using following command.\ncurl \"http://localhost:8983/solr/update/extract?literal.id=doc1&uprefix=attr_&fmap.content=attr_content&fmap.div=foo_t&boost.foo_t=3&commit=true\" -F \"myfile=@NormalSave.msg\" \n\nFile UnicodeSave.msg was saved as \"Outlook Message Format - Unicode\" and normalSave.msg was saved as \"Outlook Message Format\".\nWhen I search with : in solr it gives junk characters in case NormalSave.msg and in case of UnicodeSave.msg it gives empty \"attr_content\". "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13004339",
            "date": "2011-03-09T02:25:28+0000",
            "content": "I've faced the same problem. I'm trying to index a Shift_JIS encoded text file through the following request:\n\nhttp://localhost:8983/solr/update/extract?literal.id=docA0000001&stream.file=/foo/bar/sjis.txt&commit=true&stream.contentType=text%2Fplain%3B+charset%3DShift_JIS\n\nBut Tika's AutoDetectParser doesn't regard Solr's charset (or Solr doesn't set the content type to Tika Parser; I should dig in).\n\nI looked into ExtractingDocumentLoader.java and it seemed that I could select an appropriate parser if I use stream.type parameter:\n\nExtractingDocumentLoader.java\npublic void load(SolrQueryRequest req, SolrQueryResponse rsp, ContentStream stream) throws IOException {\n  errHeader = \"ExtractingDocumentLoader: \" + stream.getSourceInfo();\n  Parser parser = null;\n  String streamType = req.getParams().get(ExtractingParams.STREAM_TYPE, null);\n  if (streamType != null) {\n    //Cache?  Parsers are lightweight to construct and thread-safe, so I'm told\n    MediaType mt = MediaType.parse(streamType.trim().toLowerCase());\n    parser = config.getParser(mt);\n  } else {\n    parser = autoDetectParser;\n  }\n  :\n}\n\n\n\nThe request was:\n\nhttp://localhost:8983/solr/update/extract?literal.id=docA0000001&stream.file=/foo/bar/sjis.txt&commit=true&stream.contentType=text%2Fplain%3B+charset%3DShift_JIS&stream.type=text%2Fplain\n\nI could select TXTParser rather than AutoDetectParser, but the problem wasn't solved.\n\nAnd I looked at Tika Javadoc for TXTParser and it said \"The text encoding of the document stream is automatically detected based on the byte patterns found at the beginning of the stream. The input metadata key HttpHeaders.CONTENT_ENCODING is used as an encoding hint if the automatic encoding detection fails.\":\n\nhttp://tika.apache.org/0.8/api/org/apache/tika/parser/txt/TXTParser.html\n\nSo I tried to insert the following hard coded fix:\n\nExtractingDocumentLoader.java\nMetadata metadata = new Metadata();\nmetadata.add(ExtractingMetadataConstants.STREAM_NAME, stream.getName());\nmetadata.add(ExtractingMetadataConstants.STREAM_SOURCE_INFO, stream.getSourceInfo());\nmetadata.add(ExtractingMetadataConstants.STREAM_SIZE, String.valueOf(stream.getSize()));\nmetadata.add(ExtractingMetadataConstants.STREAM_CONTENT_TYPE, stream.getContentType());\nmetadata.add(HttpHeaders.CONTENT_ENCODING, \"Shift_JIS\");   // <= temporary fix\n\n\n\nand the problem was gone (anymore garbled characters indexed). "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13004346",
            "date": "2011-03-09T03:02:52+0000",
            "content": "By looking at Tika, HtmlParser and TXTParser see HttpHeaders.CONTENT_ENCODING value in metadata.\nI think Solr sould set it in metadata if the charset value is presented by user. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13004357",
            "date": "2011-03-09T03:44:21+0000",
            "content": "The patch that solves my problem,\n\nPrasad, can you try the patch with your Big5 text and see the result? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043640",
            "date": "2011-06-03T16:46:15+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106381",
            "date": "2011-09-16T14:50:50+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Shinichiro Abe",
            "id": "comment-13174637",
            "date": "2011-12-22T06:04:39+0000",
            "content": "I've faced the same problem. Tika parsed my Shift_JIS file as windows-1252, I could not see the desired results. I can index the file correctly by applying Koji's patch. But this patch is effective for remote streaming, not for POST. So, I changed a part of code below.\n\n      //String charset = ContentStreamBase.getCharsetFromContentType(stream.getContentType());\n      String contentType = req.getParams().get(CommonParams.STREAM_CONTENTTYPE, null);\n      String charset = ContentStreamBase.getCharsetFromContentType(contentType);\n\n "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13176446",
            "date": "2011-12-28T05:04:03+0000",
            "content": "I can index the file correctly by applying Koji's patch. But this patch is effective for remote streaming, not for POST.\n\nI don't understand what you said and your fix (I don't understand why you use CommonParams.STREAM_CONTENTTYPE to fix for your POST case).\n\nIf you meant curl command by POST, you can set content type via -H parameter. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13176449",
            "date": "2011-12-28T05:12:16+0000",
            "content": "New patch attached. I updated for current trunk and getCharsetFromContentType() method to remove unnecessary strings after the charset value.\n\nI think this is ready to go. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13176464",
            "date": "2011-12-28T05:51:53+0000",
            "content": "getCharsetFromContentType() method to remove unnecessary strings after the charset value.\n\nMy fault. This is not necessary. I should add --data-binary option to curl. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13176501",
            "date": "2011-12-28T07:37:04+0000",
            "content": "committed trunk and 3x. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13176548",
            "date": "2011-12-28T09:53:29+0000",
            "content": "Nice fix, is in-line with the other charset handling e.g. for XML imports using standard request handler. I fixed the incorrect XML handling in solr a year ago and did the same thing to pass the charset to the XML parser as \"hint\". "
        }
    ]
}