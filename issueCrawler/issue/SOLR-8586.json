{
    "id": "SOLR-8586",
    "title": "Implement hash over all documents to check for shard synchronization",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "An order-independent hash across all of the versions in the index should suffice.  The hash itself is pretty easy, but we need to figure out when/where to do this check (for example, I think PeerSync is currently used in multiple contexts and this check would perhaps not be appropriate for all PeerSync calls?)",
    "attachments": {
        "SOLR-8586.patch": "https://issues.apache.org/jira/secure/attachment/12784449/SOLR-8586.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-01-22T17:11:52+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "A bloom filter with all versions, maybe? ",
            "id": "comment-15112702"
        },
        {
            "date": "2016-01-22T17:20:18+0000",
            "author": "Yonik Seeley",
            "content": "A bloom filter would allow one to estimate (with a known error) if a specific version is contained within the index.  But it's not clear how we would use that info.  All we need here is to know if two indexes are in sync or not.\n\nI was thinking of something as simple as\n\nh = 0\nfor version in versions:\n  h += hash(version)\n\n ",
            "id": "comment-15112725"
        },
        {
            "date": "2016-01-22T17:27:11+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "I see.. My initial thought was that a bloom filter from one replica could be compared against a bloom filter from another replica (bitwise), to arrive at the same checking. And also, it could be re-used later for other purposes, if needed (maybe to find out a missing update, i.e. by running a loop over all updates one replica has and comparing against the bloom filter of the replica that has missing update; but haven't thought about this usecase carefully enough). However, your logic seems to do the needful and comparing two longs is surely faster than two bit arrays (or two arrays of longs), as in case of a bloom filter. ",
            "id": "comment-15112734"
        },
        {
            "date": "2016-01-22T17:37:43+0000",
            "author": "Yonik Seeley",
            "content": "My initial thought was that a bloom filter from one replica could be compared against a bloom filter from another replica (bitwise), to arrive at the same checking. \n\nWe'd need to figure out how big of a bloom filter would be needed to avoid a false match (no idea, off the top of my head).\n\nFor adding up good hashes, 64 bits feels like it should be plenty.  We could always easily extend that by accumulating in multiple buckets (the bucket being chosen by either a few bits of the hash, or a completely different hash). ",
            "id": "comment-15112751"
        },
        {
            "date": "2016-01-26T17:55:42+0000",
            "author": "Yonik Seeley",
            "content": "Here's a draft patch that implements the hash.\nStill needs cleanups, code that calls it in the right place, tests, etc. ",
            "id": "comment-15117642"
        },
        {
            "date": "2016-01-26T20:51:15+0000",
            "author": "David Smiley",
            "content": "Could you please clarify what this issue is all about? I don't get it. ",
            "id": "comment-15117982"
        },
        {
            "date": "2016-01-27T00:20:17+0000",
            "author": "Yonik Seeley",
            "content": "Could you please clarify what this issue is all about? I don't get it.\n\nAre you familiar with PeerSync?  I just linked  SOLR-8129 as well.\n\nPeerSync currently checks for replicas being in-sync by looking at the last 100 updates, and if there are only a few updates missing (judged by a sufficient overlap of those updates) it will grab the missing updates from the peer and then assume that it is in sync.  For whatever reason, updates can sometimes get wildly reordered, and looking at the last N updates is not sufficient.  Hopefully \"Implement hash over all documents to check for shard synchronization\" should now make sense? ",
            "id": "comment-15118332"
        },
        {
            "date": "2016-01-27T03:33:44+0000",
            "author": "David Smiley",
            "content": "Ah, ok.  I wasn't familiar with PeerSync; thanks for educating me.\n\nI wonder if adding hashes of the version might be prone to problems if the version of any given document tends to be identical to many other documents if they were added at once, and assuming a timestamp based version.  Just throwing that out there; maybe it wouldn't be a problem and/or too unlikely to worry about, all things considered. ",
            "id": "comment-15118579"
        },
        {
            "date": "2016-01-27T03:59:33+0000",
            "author": "Yonik Seeley",
            "content": "I wonder if adding hashes of the version might be prone to problems if the version of any given document tends to be identical to many other documents\n\nNope, versions are unique to a shard (the leader assigns a unique version to every update). ",
            "id": "comment-15118596"
        },
        {
            "date": "2016-01-27T11:13:45+0000",
            "author": "Stephan Lagraulet",
            "content": "Would it be possible to increase this \"100 updates\" window as it seems quite low for heavy indexing use cases ? ",
            "id": "comment-15119062"
        },
        {
            "date": "2016-01-27T11:41:03+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "numRecordsToKeep can be configured.\nhttps://cwiki.apache.org/confluence/display/solr/UpdateHandlers+in+SolrConfig#UpdateHandlersinSolrConfig-TransactionLog ",
            "id": "comment-15119093"
        },
        {
            "date": "2016-01-27T11:43:45+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Just want to understand that those updates (at a replica) which are rejected due to reordering, or older versions which have since been updated, would also be counted towards this hash, isn't it?\nOr, instead, would the fingerprint be the sum of hashes of only the latest versions of all docs? ",
            "id": "comment-15119099"
        },
        {
            "date": "2016-01-27T11:48:58+0000",
            "author": "Stephan Lagraulet",
            "content": "Thanks I missed this Solr5 enhancement... ",
            "id": "comment-15119107"
        },
        {
            "date": "2016-01-27T12:47:50+0000",
            "author": "Yonik Seeley",
            "content": "The latter.  We're looking at that is in the index, and that will only have the last version of every non-deleted document. ",
            "id": "comment-15119180"
        },
        {
            "date": "2016-01-28T18:15:46+0000",
            "author": "Yonik Seeley",
            "content": "Here's an updated patch that implements fingerprinting on the request side.\n\nExample:\n\n$curl \"http://localhost:8983/solr/techproducts/get?getVersions=5&fingerprint=true\"\n{\n  \"versions\":[1524634308552163328,\n    1524634308550066176,\n    1524634308544823296,\n    1524634308538531840,\n    1524634308533288960],\n  \"fingerprint\":{\n    \"maxVersionSpecified\":9223372036854775807,\n    \"maxVersionEncountered\":1524634308552163328,\n    \"maxInHash\":1524634308552163328,\n    \"versionsHash\":1830505675324363667,\n    \"numVersions\":32,\n    \"numDocs\":32,\n    \"maxDoc\":32}}\n\n ",
            "id": "comment-15122021"
        },
        {
            "date": "2016-01-28T18:24:45+0000",
            "author": "Yonik Seeley",
            "content": "So the basic idea is that when a replica coming back up syncs to a leader, it can request a fingerprint in addition to the last leader versions.  It can then grab and apply any missing versions, calculate it's own fingerprint, and compare for equality. ",
            "id": "comment-15122039"
        },
        {
            "date": "2016-01-28T20:19:12+0000",
            "author": "Yonik Seeley",
            "content": "PeerSync always returned \"true\" if the core doing the sync was judged to be either equal to or ahead of the remote core.\nSo one outstanding question is: under what circumstances do we change this to only return true on an exact match? ",
            "id": "comment-15122263"
        },
        {
            "date": "2016-01-28T21:08:40+0000",
            "author": "David Smith",
            "content": "Trying to understand this without knowing the internals of the sync process \u2013 apologies in advance if these are dumb questions:\n\nIt isn't stated, but I assume the replica does a full sync if its fingerprint, after sync, does not match the leader's?\n\nAre there any scale concerns around calculating the fingerprint?  Say, if there are 100,000,000 (non-deleted) docs in the index? \n\nIn a high volume situation (1000's updates / sec), will the leader's fingerprint calculation be in perfect sync with the last versions it is communicating to the replica?  Thinking about a searcher being refreshed in the middle of this request, or something like that. ",
            "id": "comment-15122311"
        },
        {
            "date": "2016-01-28T22:13:44+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "The approach looks good to me.\n\n\n\n+    // TODO: this could be parallelized, or even cached per-segment if performance becomes an issue\n\n\n\nI am thinking if per-segment caching would conflict with any potential for in-place docValues updates support (SOLR-5944)? I'm saying this based on my assumption that docValues updates re-writes the docValues file for a previously written segment. Given that, in such a case, version field would be a DV field, would per-segment caching of the fingerprint need to be aware of in-place updates within a segment (whenever that support is built)? ",
            "id": "comment-15122410"
        },
        {
            "date": "2016-01-28T23:03:43+0000",
            "author": "Yonik Seeley",
            "content": "I am thinking if per-segment caching would conflict with any potential for in-place docValues updates\n\nHmmm, excellent thought.\nPreviously, if caching by the \"core\" segment key,  one only needed to take into account deletions.  In this case we could have just subtracted the hash for each deletion to do per-segment caching.  But I don't know how this works with updateable doc values.  They may invalidate previous techniques for per-segment caching (for those fields only of course). ",
            "id": "comment-15122496"
        },
        {
            "date": "2016-01-28T23:30:10+0000",
            "author": "Yonik Seeley",
            "content": "\nIt isn't stated, but I assume the replica does a full sync if its fingerprint, after sync, does not match the leader's?\n\nright.\n\nAre there any scale concerns around calculating the fingerprint? Say, if there are 100,000,000 (non-deleted) docs in the index?\n\nYes, this needs to be tested.  We can do some caching if it's an issue.\n\nIn a high volume situation (1000's updates / sec), will the leader's fingerprint calculation be in perfect sync with the last versions it is communicating to the replica?\n\nNo, but in a high volume situation, we won't be able to sync up by requesting a few missed docs from the leader anyway, so it probably doesn't matter.  This is more for both low update scenarios, and for bringing the whole cluster back up. ",
            "id": "comment-15122527"
        },
        {
            "date": "2016-01-29T03:17:54+0000",
            "author": "Joel Bernstein",
            "content": "This is exactly what we need for implementing alerts (SOLR-8577). ",
            "id": "comment-15122872"
        },
        {
            "date": "2016-02-01T10:41:24+0000",
            "author": "Stephan Lagraulet",
            "content": "I'm trying to gather all issues related to SolrCloud that affects Solr 5.4. Can you affect SolrCloud component to this issue ? ",
            "id": "comment-15126086"
        },
        {
            "date": "2016-02-01T15:05:27+0000",
            "author": "Yonik Seeley",
            "content": "\nPeerSync always returned \"true\" if the core doing the sync was judged to be either equal to or ahead of the remote core.\nSo one outstanding question is: under what circumstances do we change this to only return true on an exact match?\n\nSo I think the answer to this is that we're OK, as long as both peers don't end up returning true. ",
            "id": "comment-15126352"
        },
        {
            "date": "2016-02-01T16:21:50+0000",
            "author": "Yonik Seeley",
            "content": "OK, code is pretty much done I think...  just needs tests now.\nI didn't change the strategy of any of the code that uses peersync.  fingerprinting is on by default, except in SyncStrategy.syncWithReplicas where it is false (this is the leader syncing with it's replicas, and nothing is done with failures in any case). ",
            "id": "comment-15126483"
        },
        {
            "date": "2016-02-02T17:20:25+0000",
            "author": "Yonik Seeley",
            "content": "OK, here's hopefully the complete patch + additional PeerSync tests. ",
            "id": "comment-15128601"
        },
        {
            "date": "2016-02-04T18:08:30+0000",
            "author": "Mark Miller",
            "content": "Any chaos monkey test results yet? ",
            "id": "comment-15132688"
        },
        {
            "date": "2016-02-04T18:27:53+0000",
            "author": "Yonik Seeley",
            "content": "Yep, I've been looping a custom version of the HDFS-nothing-safe test that among other things, only does adds, no deletes.  It's the same test I've been using all along in SOLR-8129 .  I've gotten 66 fails (most due to mismatch with control), but no fails due to shards being out of sync!\n\nI plan on committing this soon. ",
            "id": "comment-15132725"
        },
        {
            "date": "2016-02-04T19:55:42+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 629767be0686d39995f2afc1f1f267f9d1a68cef in lucene-solr's branch refs/heads/master from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=629767b ]\n\nSOLR-8586: add index fingerprinting and use it in peersync ",
            "id": "comment-15132892"
        },
        {
            "date": "2016-02-04T22:51:59+0000",
            "author": "Erick Erickson",
            "content": "OK, does this mean I can commit SOLR-8500 (after this is committed to 5x)? ",
            "id": "comment-15133217"
        },
        {
            "date": "2016-02-04T23:08:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit f6400e9cbb1158178af0b6cb7901a784368ab589 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6400e9 ]\n\nSOLR-8586: Fix forbidden APIS; cleanup of imports ",
            "id": "comment-15133278"
        },
        {
            "date": "2016-02-04T23:12:54+0000",
            "author": "Mark Miller",
            "content": "I think that we should warn that it can result in more often needing to do full index replication for recovery, but I have nothing against it. ",
            "id": "comment-15133286"
        },
        {
            "date": "2016-02-05T01:14:35+0000",
            "author": "Erick Erickson",
            "content": "Yeah, this is kind of a \"use at your own risk in very specialized situations\" kind of thing so I'll be sure and include that warning. ",
            "id": "comment-15133458"
        },
        {
            "date": "2016-02-05T16:14:35+0000",
            "author": "ASF subversion and git services",
            "content": "Commit ff83a400156beb6a8dd2d0845c7f878c28431739 in lucene-solr's branch refs/heads/branch_5x from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ff83a40 ]\n\nSOLR-8586: add index fingerprinting and use it in peersync\n(cherry picked from commit 629767b) ",
            "id": "comment-15134387"
        },
        {
            "date": "2016-02-05T17:02:23+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 629767be0686d39995f2afc1f1f267f9d1a68cef in lucene-solr's branch refs/heads/lucene-6997 from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=629767b ]\n\nSOLR-8586: add index fingerprinting and use it in peersync ",
            "id": "comment-15134476"
        },
        {
            "date": "2016-02-05T17:35:22+0000",
            "author": "ASF subversion and git services",
            "content": "Commit d75abb2539fb62514c506776c1db6182803745bc in lucene-solr's branch refs/heads/branch_5x from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d75abb2 ]\n\nSOLR-8586: Fix forbidden APIS; cleanup of imports ",
            "id": "comment-15134529"
        },
        {
            "date": "2016-02-05T17:39:57+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 629767be0686d39995f2afc1f1f267f9d1a68cef in lucene-solr's branch refs/heads/lucene-6835 from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=629767b ]\n\nSOLR-8586: add index fingerprinting and use it in peersync ",
            "id": "comment-15134542"
        },
        {
            "date": "2016-02-05T17:40:02+0000",
            "author": "ASF subversion and git services",
            "content": "Commit f6400e9cbb1158178af0b6cb7901a784368ab589 in lucene-solr's branch refs/heads/lucene-6835 from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6400e9 ]\n\nSOLR-8586: Fix forbidden APIS; cleanup of imports ",
            "id": "comment-15134545"
        },
        {
            "date": "2016-02-08T13:12:39+0000",
            "author": "Joel Bernstein",
            "content": "Now that this is in place it may make sense to combine this with Streaming. The first thing I see is to compare hashes between the shards and if there is a difference use the ComplementStream to determine which id's are missing. The missing id's could then be automatically fetched from the source and re-indexed. There could be a DaemonStream that lives inside the collection that performs this check periodically. This could also sort out a situation where non of the shards have the complete truth.  ",
            "id": "comment-15136924"
        },
        {
            "date": "2016-02-08T16:55:23+0000",
            "author": "Yonik Seeley",
            "content": "OK, I did some basic performance testing...\nOn an index w/ 5M docs, the first-time fingerprint took 1100ms (most of that time was un-inversion of the version field, which did not use docValues).\nAfter the first time, subsequent fingerprints took ~55ms ",
            "id": "comment-15137227"
        },
        {
            "date": "2016-02-08T17:02:38+0000",
            "author": "Yonik Seeley",
            "content": "The first thing I see is to compare hashes between the shards and if there is a difference use the ComplementStream to determine which id's are missing. \n\nImplementing eventual consistency with this is problematic in a general sense:\nIf one shard has an ID and another doesn't, you don't know what the correct state is.\nThe other general issue is the inability to actually retrieve an arbitrary document from the index (i.e. all source fields must be stored).\n\nIt may still be useful for add-only systems that do store all source fields...  but in that case, we could make things much more efficient by adding in the ability to use hash trees to drastically narrow the ids that need to be communicated. ",
            "id": "comment-15137248"
        },
        {
            "date": "2016-02-08T17:19:31+0000",
            "author": "Joel Bernstein",
            "content": "I think there would need to be a system of truth involved, which there often is. The steps would be:\n\n1) Check the hashes.\n2) If hashes differ find the difference in id's. \n3) Refetch Id's from the system of truth. Streaming data from the system of truth is easily done with streams like the JdbcStream which streams data from a relational database. \n\n\n\n\n\n\n ",
            "id": "comment-15137272"
        },
        {
            "date": "2016-02-11T04:02:39+0000",
            "author": "Yonik Seeley",
            "content": "Yep, I've been looping a custom version of the HDFS-nothing-safe test that among other things, only does adds, no deletes.\n\nUpdate: when I reverted my custom changes to the chaos test (so that it also did deletes), I got a high amount of shard-out-of-sync errors... seemingly even more than before, so I've been trying to track those down.  What I saw were issues that did not look related to PeerSync... I saw missing documents from a shard that replicated from the leader while buffering documents, and I saw the missing documents come in and get buffered, pointing to transaction log buffering or replay issues.\n\nThen I realized that I had tested \"adds only\" before committing, and tested the normal test after committing and doing a \"git pull\".  In-between those times was SOLR-8575, which was a fix to the HDFS tlog!  I've been looping the test for a number of hours with those changes reverted, and I haven't seen a shards-out-of-sync fail so far.  I've also done a quick review of SOLR-8575, but didn't see anything obviously incorrect.  The changes in that issue may just be uncovering another bug (due to timing) rather than causing one... too early to tell.\n\nI've also been running the non-hdfs version of the test for over a day, and also had no inconsistent shard failures. ",
            "id": "comment-15142215"
        },
        {
            "date": "2016-08-11T15:40:20+0000",
            "author": "Yago Riveiro",
            "content": "Is this operation memory bound?\n\nI'm trying to update my SolrCloud from 5.4 to 5.5.2 and I can only update one node, if I start another node with 5.5.2 the first dies with an OOM.\n\nThe second node never pass the phase where is checking if replicas are sync.\n\nThe SolrCloud deploy (2 nodes) has no activity at all, is a cold repository for archived data (around 5 Billion documents).\n ",
            "id": "comment-15417460"
        },
        {
            "date": "2016-08-12T13:16:34+0000",
            "author": "Yonik Seeley",
            "content": "If the version field doesn't have docValues, then it will be un-inverted (i.e. FieldCache entries will be built to support version lookups, and that does require memory).\nSince version lookups are needed in the course of indexing anyway (to detect update reorders on replicas), this should really just change when these FieldCache entries are created... hence the maximum required amount of memory shouldn't be changed. ",
            "id": "comment-15418799"
        },
        {
            "date": "2016-08-12T16:21:44+0000",
            "author": "Yago Riveiro",
            "content": "My index has 12T of data indexed with 4.0, the version field only supports docValues since 4.7.\n\nTo Upgrade to 5.x I ran the lucene-core-5.x over all my data,but with this new feature I need to re-index all my data because I don't have docValues for _version_ field and this feature use instead the un-inverted method that creates a memory struct that doesn't fit the memory of my servers ...\n\nTo be honest, this never should be done in a minor release ... this mandatory feature is based in a optional configuration :/\n\nI will die in 5.4 or spend several months re-indexing data and figure out how to update production without downtime.  Not an easy task.\n ",
            "id": "comment-15419078"
        },
        {
            "date": "2016-08-12T17:15:11+0000",
            "author": "Yonik Seeley",
            "content": "You can set the environment variable solr.disableFingerprint to \"false\" to disable the fingerprint check.\n\nIf your indexes ever have updates to existing documents, then you're still risking OOMs anyway (the first time a replica detects that an update may be reordered will cause the FieldCache to be populated for version for that segment).  The fingerprint makes that happen up-front (what I meant to say in my previous message was \"the maximum required amount of memory shouldn't be changed\"). ",
            "id": "comment-15419145"
        },
        {
            "date": "2016-08-12T17:38:34+0000",
            "author": "Yago Riveiro",
            "content": "Then I do not understand, how this is possible:\n\nhttps://www.dropbox.com/s/a6e2wrmedop7xjv/Screenshot%202016-08-12%2018.19.22.png?dl=0\n\nOnly with 5.5.x and 6.x the heap grows to the infinite. Rolling back to 5.4 the amount of memory needed to become up is constant ...\n\nWith only one node running 5.5.x I have no problems, when I start a second node with 5.5.x they never pass the phase where they are checking replica synchronization. ",
            "id": "comment-15419184"
        }
    ]
}