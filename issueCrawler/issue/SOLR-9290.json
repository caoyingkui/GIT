{
    "id": "SOLR-9290",
    "title": "TCP-connections in CLOSE_WAIT spike during heavy indexing and do not decrease",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5.3",
            "6.2"
        ],
        "affect_versions": "5.3.2,                                            5.4.1,                                            5.5.1,                                            5.5.2,                                            6.0,                                            6.0.1,                                            6.1",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Critical"
    },
    "description": "Heavy indexing on Solr with SSL leads to a lot of connections in CLOSE_WAIT state. \n\nAt my workplace, we have seen this issue only with 5.5.1 and could not reproduce it with 5.4.1 but from my conversation with Shalin, he knows of users with 5.3.1 running into this issue too. \n\nHere's an excerpt from the email Shai Erera sent to the mailing list  (about what we see:\n\n\n1) It consistently reproduces on 5.5.1, but does not reproduce on 5.4.1\n2) It does not reproduce when SSL is disabled\n3) Restarting the Solr process (sometimes both need to be restarted), the\ncount drops to 0, but if indexing continues, they climb up again\n\nWhen it does happen, Solr seems stuck. The leader cannot talk to the\nreplica, or vice versa, the replica is usually put in DOWN state and\nthere's no way to fix it besides restarting the JVM.\n\nHere's the mail thread: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201607.mbox/%3C46cc66220a8143dc903fa34e792059c4@vp-exc01.dips.local%3E\n\nCreating this issue so we could track this and have more people comment on what they see.",
    "attachments": {
        "setup-solr.sh": "https://issues.apache.org/jira/secure/attachment/12817487/setup-solr.sh",
        "SOLR-9290.patch": "https://issues.apache.org/jira/secure/attachment/12818022/SOLR-9290.patch",
        "SOLR-9290-debug.patch": "https://issues.apache.org/jira/secure/attachment/12817488/SOLR-9290-debug.patch",
        "index.sh": "https://issues.apache.org/jira/secure/attachment/12817885/index.sh"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-07-08T06:54:52+0000",
            "author": "Johannes Meyer",
            "content": "We have the same issue on Solr 6.1.0 ",
            "id": "comment-15367313"
        },
        {
            "date": "2016-07-11T22:16:19+0000",
            "author": "Hoss Man",
            "content": "questions specifically for Shai Erera followng up on comments made in the mailing list thread mentioned in the isue summary...\n\n\nWhen it does happen, the number of CLOSE_WAITS climb very high, to the order of 30K+ entries in 'netstat'.\n...\nWhen I say it does not reproduce on 5.4.1 I really mean the numbers\ndon't go as high as they do in 5.5.1. Meaning, when running without\nSSL, the number of CLOSE_WAITs is smallish, usually less than a 10 (I\nwould separately like to understand why we have any in that state at\nall). When running with SSL and 5.4.1, they stay low at the order of\nhundreds the most.\n\n\n\tDoes this only reproduce in your application, with your customized configs of Solr, or can you reproduce it using something trivial like \"modify bin/solr.in.sh to point at an SSL cert, then run; bin/solr -noprompt -cloud.\" ?\n\tDoes the problem only manifest solely with indexing, or with queries as well? ie...\n\t\n\t\tassuming a pre-built collection, and then all nodes restarted, does hammering the cluster with read only queries manifest the problem?\n\t\tassuming a virgin cluster with no docs, does hammering the cluster w/updates but never any queries, manifest the problem?\n\t\n\t\n\tAssuming you start by bringing up a virgin cluster and then begin hammering it with whatever sequences of requests are needed to manifest the problem, how long do you have to wait before the number of CLOSE_WAITS spikes high enough that you are reasonably confident the problem has occured?\n\n\n\nThe last question being a pre-req to wondering if we can just git bisect to identify where/when the problem originated.  \n\nEven if writing a (reliable) bash automation script (to start the cluster, and triggering requests, and monitoring the CLOSE_WAITS to see if they go over a specified threshold in under a specified timelimit, and shut everything down cleanly) is too cumbersome to have faith in running an automated git bisect run test.sh, we could still consider doing some manually driven git bisection to try and track this down, as long as each iteration doesn't take very long.\n\nSpecifically: git merge-base says ffadf9715c4a511178183fc1411b18c1701b9f1d is the common ancestor for 5.4.1 and 5.5.1, and git log says there are 487 commits between that point and the 5.5.1 tag.  Statistically speaking it should only take \n~10 iterations to do a binary search of those commits to find the first problematic one.\n\nAssuming there is a manual process someone can run on a clean git checkout of 5.4.1 that takes under 10 minutes to get from \"ant clean server\" to an obvious splke in CLOSE_WAITS, someone with some CPU cycles to spare who doesn't mind a lot of context switching while they do their day job could be...\n\n\trunning a command to spin up the cluster & client hammering code\n\tsetting a 10 minute timer\n\twhen the timer goes off, check the results of another command to count the CLOSE_WAITS\n\tgit bisect good/bad\n\trepeat\n...and within ~2-3 hours should almost certainly have tracked down when/where the problem started.\n\n\n ",
            "id": "comment-15371758"
        },
        {
            "date": "2016-07-12T18:51:38+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "It is reproducible very easily on stock solr with SSL enabled. My test setup creates two SSL-enabled Solr instances with a 5 shard x 2 replica collection and runs a short indexing program (just 9 update requests with 1 document each and a commit at the end). Keep on running the indexing program repeatedly and the number of connections in the CLOSE_WAIT state gradually increase.\n\nInterestingly, the number of connections stuck in CLOSE_WAIT decrease during indexing and increase again about 10 or so seconds after the indexing is stopped.\n\nI can reproduce the problem on 6.1, 6.0, 5.5.1, 5.3.2. I am not able to reproduce this on master although I don't see anything relevant that has changed since 6.1 \u2013 I tried this only once so it may have just been bad timing?\n\nWhen the connections show in CLOSE_WAIT state, the recv-q buffer always has exactly 70 bytes.\n\nnetstat -tonp | grep CLOSE_WAIT | grep java\ntcp       70      0 127.0.0.1:56538         127.0.1.1:8983          CLOSE_WAIT  21654/java       off (0.00/0/0)\ntcp       70      0 127.0.0.1:47995         127.0.1.1:8984          CLOSE_WAIT  21654/java       off (0.00/0/0)\ntcp       70      0 127.0.0.1:47477         127.0.1.1:8984          CLOSE_WAIT  21654/java       off (0.00/0/0)\ntcp       70      0 127.0.0.1:47996         127.0.1.1:8984          CLOSE_WAIT  21654/java       off (0.00/0/0)\ntcp       70      0 127.0.0.1:56644         127.0.1.1:8983          CLOSE_WAIT  21654/java       off (0.00/0/0)\ntcp       70      0 127.0.0.1:56533         127.0.1.1:8983          CLOSE_WAIT  21654/java       off (0.00/0/0)\n...\n\n\n\nIf I run the same steps with SSL disabled then the connections in CLOSE_WAIT state have just 1 byte in recv-q. I don't see the number of such connections increasing with indexing over time but I know for a fact (from a client) that eventually more and more connections pile up in this state even without SSL.\n\ntcp       1      0 127.0.0.1:41723         127.0.1.1:8983          CLOSE_WAIT  2522/java        off (0.00/0/0)\ntcp       1      0 127.0.0.1:41780         127.0.1.1:8983          CLOSE_WAIT  2640/java        off (0.00/0/0)\n...\n\n\n\nI enabled debug logging for PoolingHttpClientConnectionManager (used in 6.x) and PoolingClientConnectionManager (used in 5.x.x) and after running the indexing program and verifying that some connections are in CLOSE_WAIT, I grepped the logs for connections leased vs released and I always find the number to be the same which means that the connections are always given back to the pool.\n\nNow some connections hanging around in CLOSE_WAIT are to be expected because of the following (quoted from the httpclient documentation):\n\nOne of the major shortcomings of the classic blocking I/O model is that the network socket can react to I/O events only when blocked in an I/O operation. When a connection is released back to the manager, it can be kept alive however it is unable to monitor the status of the socket and react to any I/O events. If the connection gets closed on the server side, the client side connection is unable to detect the change in the connection state (and react appropriately by closing the socket on its end).\nHttpClient tries to mitigate the problem by testing whether the connection is 'stale', that is no longer valid because it was closed on the server side, prior to using the connection for executing an HTTP request. The stale connection check is not 100% reliable. The only feasible solution that does not involve a one thread per socket model for idle connections is a dedicated monitor thread used to evict connections that are considered expired due to a long period of inactivity. The monitor thread can periodically call ClientConnectionManager#closeExpiredConnections() method to close all expired connections and evict closed connections from the pool. It can also optionally call ClientConnectionManager#closeIdleConnections() method to close all connections that have been idle over a given period of time.\n\nI'm going to try adding such a monitor thread and see if this is still a problem. ",
            "id": "comment-15373452"
        },
        {
            "date": "2016-07-12T18:58:55+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "The setup-solr.sh is small script that I use to setup different versions with SSL enabled and debug logging enabled for httpclient.\n\nThe debug logging looks like the following:\n\n2016-07-12 13:25:05.692 DEBUG (httpShardExecutor-4-thread-7-processing-x:xyz_shard3_replica1 r:core_node5 https:////127.0.1.1:8983//solr//xyz_shard3_replica2// n:127.0.1.1:8984_solr s:shard3 c:xyz [https:////127.0.1.1:8983//solr//xyz_shard3_replica2//]) [c:xyz s:shard3 r:core_node5 x:xyz_shard3_replica1] o.a.h.i.c.PoolingClientConnectionManager Connection leased: [id: 17][route: {s}->https://127.0.1.1:8983][total kept alive: 0; route allocated: 5 of 100000; total allocated: 5 of 100000]\n...\n2016-07-12 13:25:05.791 DEBUG (recoveryExecutor-3-thread-4-processing-n:127.0.1.1:8984_solr x:xyz_shard1_replica1 s:shard1 c:xyz r:core_node2) [c:xyz s:shard1 r:core_node2 x:xyz_shard1_replica1] o.a.h.i.c.PoolingClientConnectionManager Connection released: [id: 17][route: {s}->https://127.0.1.1:8983][total kept alive: 8; route allocated: 8 of 100000; total allocated: 8 of 100000]\n\n\n\nThe attached SOLR-9290-debug.patch applies to 5.3.x and changes HttpSolrClient to log the connection details including the client port number for each request. ",
            "id": "comment-15373469"
        },
        {
            "date": "2016-07-13T07:01:19+0000",
            "author": "Mads Tomasg\u00e5rd Bj\u00f8rgan",
            "content": "I performed a bisect, yielding some commit of 5.4.1 as good, and a commit from 5.5.3 as bad. This gave the following commit: ad9b87a7285e444cd61fffb83c0aee06c8f7cef0, as the first bad commit. However, this commit only increases the limits on how many update connections that can be open. I built Solr on top of the last commit from branch_5_4 (7d52c2523c7a4ff70612742b76b934a12b493331), and implemented the commit that was supposed to be bad, and ended up with the same CLOSE_WAIT-leak. Thus - this problem affects version 5.4.1 aswell - but is harder to see as Solr isn't allowed to use that many connections when updating. ",
            "id": "comment-15374478"
        },
        {
            "date": "2016-07-13T10:15:47+0000",
            "author": "Shai Erera",
            "content": "Interestingly, the number of connections stuck in CLOSE_WAIT decrease during indexing and increase again about 10 or so seconds after the indexing is stopped.\n\nI've observed that too and it's not that they decrease, but rather that the connections change their state from CLOSE_WAIT to ESTABLISHED, then when indexing is done to TIME_WAIT and then finally to CLOSE_WAIT again. I believe this aligns with what the HC documentation says \u2013 the connections are not necessarily released, but kept in the pool. When you re-index again, they are reused and go back to the pool.\n\nHowever, this commit only increases the limits on how many update connections that can be open\n\nThat's interesting and might be a temporary workaround for the problem, which I intend to test shortly. In 5.4.1 they were both modified to 100,000:\n\n\n-  public static final int DEFAULT_MAXUPDATECONNECTIONS = 10000;\n-  public static final int DEFAULT_MAXUPDATECONNECTIONSPERHOST = 100;\n+  public static final int DEFAULT_MAXUPDATECONNECTIONS = 100000;\n+  public static final int DEFAULT_MAXUPDATECONNECTIONSPERHOST = 100000;\n\n\n\nThis can explain why we run into trouble with 5.5.1 but not with 5.4.1. Though even in 5.4.1 there are few hundreds of CLOSE_WAIT connections, with 5.5.1 they reach (in our case) the orders of 35-40K, at which point Solr became useless, not being able to talk to the replica or pretty much anything else.\n\nI see these can be defined in solr.xml, though it's not documented how, so I'm going to give it a try and will report back here. ",
            "id": "comment-15374779"
        },
        {
            "date": "2016-07-13T14:19:29+0000",
            "author": "Shai Erera",
            "content": "An update \u2013 I've modified our solr.xml (which is basically the vanilla solr.xml) with these added props (under the solrcloud element) and I do not see the connections spike anymore:\n\n\n    <int name=\"maxUpdateConnections\">10000</int>\n    <int name=\"maxUpdateConnectionsPerHost\">100</int>\n\n\n\nThose changes were part of SOLR-8533. Mark Miller on that issue you didn't explain why the defaults need to be set that high. Was there perhaps an email thread you can link to which includes more details? I ask because one thing I've noticed is that if I query solr/admin/info/system, the system.openFileDescriptorCount is very high when there are many CLOSE_WAITs. Such a change in Solr default probably need to be accompanied by an OS-level setting too, no?\n\nI am still running tests with those props set in solr.xml, on top of 5.5.1. Mads Tomasg\u00e5rd Bj\u00f8rgan would you mind testing in your environment too?\n\nChris Hostetter (Unused), sorry I completely missed your questions. Our solr.xml is the vanilla one, we didn't modify anything in it. We did uncomment the SSL props in solr.in.sh as the ref guide says, but aside from the key name and password, we didn't change any settings. ",
            "id": "comment-15375082"
        },
        {
            "date": "2016-07-13T14:35:44+0000",
            "author": "Mark Miller",
            "content": "The defaults need to be very high to avoid distributed deadlock. ",
            "id": "comment-15375117"
        },
        {
            "date": "2016-07-13T14:53:36+0000",
            "author": "Shai Erera",
            "content": "Thanks Mark Miller. In that case, what's your take on the issue at hand? ",
            "id": "comment-15375151"
        },
        {
            "date": "2016-07-13T15:08:40+0000",
            "author": "Shai Erera",
            "content": "Also Mark Miller, for education purposes, if you have a link to a discussion about why it may lead to a distributed deadlock, I'd be happy to read it. ",
            "id": "comment-15375184"
        },
        {
            "date": "2016-07-13T15:53:21+0000",
            "author": "Yonik Seeley",
            "content": "if you have a link to a discussion about why it may lead to a distributed deadlock, I'd be happy to read it.\nSOLR-683\n\nSame logic applies to any internal general purpose thread pools or connection pools / connection limits.  Think of acquiring a thread like acquiring a lock.  If there are going to be a limited number of resources, then one needs to be very careful under what circumstances those resources can be acquired. ",
            "id": "comment-15375243"
        },
        {
            "date": "2016-07-13T18:31:59+0000",
            "author": "Shai Erera",
            "content": "Thanks Yonik Seeley, I'll read the issue.\n\nI agree with what you write in general, but we do hit an issue with these settings. That that it reproduces easily with SSL enabled suggests that the issue may not be in Solr code at all, but I wonder if we shouldn't perhaps pick smaller default values if SSL is enabled? (Our guess at the moment is that HC keeps more connections in the pool when SSL is enabled because they are more expensive to initiate, but it's just a guess).\n\nAnd maybe the proper solution would be what Shalin Shekhar Mangar wrote above \u2013 have a bg monitor which closes idle/expired connections. I actually wonder why it can't be a property of ClientConnectionManager that you can set to auto close idle/expired connections after a period of time. We can potentially have that monitor act only if SSL is enabled (or at least until non-SSL exhibits the same problems too). ",
            "id": "comment-15375500"
        },
        {
            "date": "2016-07-13T18:55:14+0000",
            "author": "Anshum Gupta",
            "content": "Shalin Shekhar Mangar mentioned that he's able to reproduce this in 5.3.2 as well, which was without SOLR-8533 so we certainly need to look at this more.\n\nShalin, can you confirm if you were running your tests in stock Solr ? ",
            "id": "comment-15375534"
        },
        {
            "date": "2016-07-13T18:58:05+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "This patch applies on 5.3.2. This patch adds a monitor thread for the pool created in UpdateShardHandler and with this applied, I cannot reproduce this problem anymore. There are still a few connections in CLOSE_WAIT at steady state but I verified that they belong to a different HttpClient instance in HttpShardHandlerFactory and other places.\n\nMy hypothesis is that: We have a large limit for maxConnections and maxConnectionsPerHost. As long as the limit isn't met and the servers are decently busy, new connections will continue to be created from the pool. In 5.x and 6.x, we do not have a policy of closing idle connections so httpclient will keep these connections in CLOSE_WAIT for reuse. So we must periodically close such connections once they're idle to avoid the number of such connections increasing to absurd limits.\n\nAlso, I think the reason this wasn't reproducible on master is because SOLR-4509 enabled eviction of idle connections by calling HttpClientBuilder#evictIdleConnections with a 50 second limit. ",
            "id": "comment-15375537"
        },
        {
            "date": "2016-07-13T19:02:02+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\nShalin Shekhar Mangar mentioned that he's able to reproduce this in 5.3.2 as well, which was without SOLR-8533 so we certainly need to look at this more.\n\nShalin, can you confirm if you were running your tests in stock Solr ?\n\nActually it is 5.3.2 with some kerberos patches but the client which originally reported the issue was using stock 5.3.2. I don't think the changes are relevant.\n\nI believe this was a problem all along. It just got amplified with SOLR-8533 in 5.5.x because now the limit is higher. ",
            "id": "comment-15375541"
        },
        {
            "date": "2016-07-13T19:06:18+0000",
            "author": "Shai Erera",
            "content": "Thanks Shalin Shekhar Mangar. Few questions:\n\nAlso, I think the reason this wasn't reproducible on master is because SOLR-4509 enabled eviction of idle threads by calling HttpClientBuilder#evictIdleConnections with a 50 second limit.\n\nIs this something we can apply to 5x/6x too?\n\nThis patch adds a monitor thread for the pool created in UpdateShardHandler and with this applied\n\nI didn't see the monitor in the latest patch, only the log printouts. Did you forget to add it?\n\nThere are still a few connections in CLOSE_WAIT at steady state but I verified that they belong to a different HttpClient instance in HttpShardHandlerFactory and other places.\n\n(1) Can/Should we have a similar monitor for HttpShardHandlerFactory?\n(2) Any reason why the two don't share the same HttpClient instance?\n\nThis patch applies on 5.3.2\nWe have a large limit for maxConnections and maxConnectionsPerHost\n\nI thought that hypothesis holds only after SOLR-8533. Are you saying you also saw it on 5.3.2? If so, what are the values that are set for these properties there? We definitely do not see the problem with 5.4.1, but we didn't test prior versions. ",
            "id": "comment-15375548"
        },
        {
            "date": "2016-07-13T19:08:25+0000",
            "author": "Shai Erera",
            "content": "I thought that hypothesis holds only after SOLR-8533. Are you saying you also saw it on 5.3.2? If so, what are the values that are set for these properties there? We definitely do not see the problem with 5.4.1, but we didn't test prior versions.\n\nWe posted at the same time, I read your answer above. I wonder why we don't see the problem with 5.4.1. I mean, we do see CLOSE_WAITs piling, but stop at ~100 (200 for the leader). ",
            "id": "comment-15375552"
        },
        {
            "date": "2016-07-13T19:15:05+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I didn't see the monitor in the latest patch, only the log printouts. Did you forget to add it?\n\nSorry Shai Erera, I noticed that after uploaded. I have uploaded the right patch now. Please review.\n\n(1) Can/Should we have a similar monitor for HttpShardHandlerFactory?\n\nI think so. This patch was only for my tests.\n\nAny reason why the two don't share the same HttpClient instance?\n\nHmm. I think originally the idea was to keep the pools for indexing and querying separate but now that the limit (for updates) is so high, I wonder if it still makes sense. I mean, yes you can deadlock a distributed search because of high indexing and vice-versa if you share the pool but if you ever reach the high limit of 100,000 connections, you have more serious problems in the cluster anyway.\n\nI wonder why we don't see the problem with 5.4.1. I mean, we do see CLOSE_WAITs piling, but stop at ~100 (200 for the leader)\n\nDo you have only two replicas? Perhaps the maxConnectionsPerHost limit of 100 is kicking in? ",
            "id": "comment-15375560"
        },
        {
            "date": "2016-07-13T19:22:47+0000",
            "author": "Shai Erera",
            "content": "Do you have only two replicas? Perhaps the maxConnectionsPerHost limit of 100 is kicking in?\n\nYes, we do have only 2 replicas and I get why the CLOSE_WAITs stop at 100. I was asking about 5.3.2 \u2013 how could CLOSE_WAITs get high in 5.3.2 when maxConnectionsPerHost was the same as in 5.4.1? ",
            "id": "comment-15375571"
        },
        {
            "date": "2016-07-13T19:28:09+0000",
            "author": "Shai Erera",
            "content": "Regarding the patch, the monitor looks good. Few comments:\n\n\n\tI prefer that we name it IdleConnectionsMonitor (w/ 's', plural connections). It goes for the class, field and thread name.\n\tDo you intend to keep all the log statements around?\n\tDo you think we should make the polling interval (10s) and idle-connections-time (50s) configurable? Perhaps through system properties?\n\n ",
            "id": "comment-15375582"
        },
        {
            "date": "2016-07-13T19:31:54+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I was asking about 5.3.2 \u2013 how could CLOSE_WAITs get high in 5.3.2 when maxConnectionsPerHost was the same as in 5.4.1?\n\n5.3.2 has maxConnectionsPerHost=100 for updates and maxConnectionsPerHost=20 for queries. So on a leader you may have 100*replicationFactor+20*numShards*replicationFactor connections. For a large cluster with many shards and replicas, the overall number of such connections can be quite high. ",
            "id": "comment-15375589"
        },
        {
            "date": "2016-07-13T19:35:51+0000",
            "author": "Shai Erera",
            "content": "Oh I see. So we didn't experience the problem because we run w/ 2 replicas (and one shard currently) and with 5.4.1's settings the math for us results in a low number of connections. But someone running a larger Solr deployment could already hit that problem prior to 5.5. Thanks for the clarification! ",
            "id": "comment-15375601"
        },
        {
            "date": "2016-07-13T20:21:15+0000",
            "author": "David A. Bradley",
            "content": "I don't understand why the preferred approach here is to just have a thread that is trying to close connections. Is the problem that these connections would never otherwise be closed? If that is the case, why can't we solve the problem of them not being closed as a part of their normal usage? It sounds like master doesn't have this problem because of different client settings? :\n\"Also, I think the reason this wasn't reproducible on master is because SOLR-4509 enabled eviction of idle connections by calling HttpClientBuilder#evictIdleConnections with a 50 second limit.\"\n\nWhy not backport that and avoid the problem entirely? Is it a different client version in master or something that makes it not that easy?\n\n\"So we must periodically close such connections once they're idle to avoid the number of such connections increasing to absurd limits.\" It seems from the discussion here that the problem is hitting a high number of connections, which is only allowed to be so high because we asked for it. What if this thread lags behind enough that the connections get too high? It sounds like the purpose of this thread is to race to prevent Solr from doing what we asked it to do.\n\nThe idea of having a thread to deal with any connections that end up in a bad state unexpectedly makes sense, but is the cause of all these CLOSE_WAIT connections really from unexpected behavior?\n\nI feel like I must be missing something. ",
            "id": "comment-15375666"
        },
        {
            "date": "2016-07-13T20:46:26+0000",
            "author": "Scott Lindner",
            "content": "I would like to add something, too.  The problem must stem from some sort of OS-level setting.  In our environment I've noticed that when a given IP+PORT combo reaches ~28k connections in a CLOSE_WAIT state that the OS, itself, cannot allow any more connections to that IP+PORT combo (i.e. even curl fails to that combo - but to other combos, including other ports on that same host - it works just fine).  I mention this because the problem seems related here to whatever settings we configure solr to use and you really must change these things in combination for it to ultimately make sense or you risk hitting this problem at some point - though admittedly with the bg thread it wouldn't be permanent like it is for us today. ",
            "id": "comment-15375727"
        },
        {
            "date": "2016-07-13T21:47:07+0000",
            "author": "Hoss Man",
            "content": "I'm no expert but...\n\nI don't understand why the preferred approach here is to just have a thread that is trying to close connections. Is the problem that these connections would never otherwise be closed? ...\n\n...my understanding is yes: In a situation where indexing load spikes up, you can get a lot of connections which are never completely closed. (even if they are never needed anymore)\n\n...If that is the case, why can't we solve the problem of them not being closed as a part of their normal usage? ...\n\nagain, IIUC: because they are pooled connections maintained by the HTTP layer.  Per the docs shalin quoted, clients are required to call ClientConnectionManager#closeExpiredConnections() if they (ie: \"we\") want to ensure those connections get closed properly.\n\nIt sounds like master doesn't have this problem because of different client settings? ... Why not backport that and avoid the problem entirely? Is it a different client version in master or something that makes it not that easy?\n\nmaster & branch_6x (and earlier) use completely diff http client APIs (see SOLR-4509) ... the HttpClientBuilder.evictIdleConnections method shalin refered to being used on master is on a class (HttpClientBuilder) that is not used at all in branch_6x.\n\nThe docs of that method describe it doing virtually the same exact same thing on the (private connection pool for the) HttpClient as what Shalin's patch does (on the pool in the shared ClientConnectionManager) ...\n\n\nMakes this instance of HttpClient proactively evict idle connections from the connection pool using a background thread. \n\n\n\nWhich makes me wonder...\n\nShalin Shekhar Mangar: why not just re-use the IdleConnectionEvictor class provided by httpcomponents (getting the exact same underlying impl as what master gets from  HttpClientBuilder.evictIdleConnections) ?\n\nhttps://hc.apache.org/httpcomponents-client-4.4.x/httpclient/apidocs/org/apache/http/impl/client/IdleConnectionEvictor.html ",
            "id": "comment-15375821"
        },
        {
            "date": "2016-07-13T22:16:53+0000",
            "author": "Hoss Man",
            "content": "Somebody sanity check my understanding / summary description of the root issue...\n\n\n\tSolr's use of HttpClient for intra-node communication has historically always had the potential to result in connections sitting \"idle\" (ie: in a CLOSE_WAIT state) for possible re-use later \u2013 but these connections are kept open indefinitely.\n\t\n\t\tFor reasons I don't understand, 'idle' connections are more likely to (exist? | be kept around indefinitely?) when  the intra-node communication is over SSL.\n\t\n\t\n\tmaxUpdateConnections and maxUpdateConnectionsPerHost have always set hard upper limits on the number of connections that could ever be created \u2013 let alone in sitting idle in a CLOSE_WAIT state.\n\tPrior to SOLR-8533, the default values for these limits was relatively low, making it unlikely that users could ever observe an extreme # of idle / CLOSE_WAIT threads \u2013 you were more likely to have your Solr cluster crash from deadlocks then notice any serious OS level problem with too many idle connections\n\tAfter SOLR-8533, the increased default values of these limits made the problem much more noticeable\n\tSOLR-4509's changes included use of a new option which results in a background thread checking for an existing idle connections on the master branch\n\tThis issue address the problem for branch_6x (and older) branches via a similar background thread\n\n ",
            "id": "comment-15375885"
        },
        {
            "date": "2016-07-14T06:05:54+0000",
            "author": "Hoss Man",
            "content": "FWIW, I'm attaching a beefed up setup-solr.sh and an index.sh i've been testing with...\n\n\n\tsetup-solr.sh\n\t\n\t\tyou must edit 2 variables: the path to your lucene checkout & an absolute path to the SSL keystore for jetty to use (with a password \"secret)\n\t\tspins up a 3 node cluster, then creates a collection with has 5 shards an rep factor of 3\n\t\n\t\n\tindex.sh\n\t\n\t\tyou must edit one variable to point at the SSL pem file for curl to use\n\t\tloops forever doing a bunch of curl connections indexing the same 9 docs over and over, periodically commiting & sleeping, reporting the # of CLOSE_WAIT java connections at each step\n\t\n\t\n\n\n\nOn master, index.sh never reports any CLOSE_WAIT connections for me.\n\nOn branch_6x, I'll see the CLOSE_WAITS spike up to 40 - even with this (essentially) single threaded indexing, and stay at stead state even after killing the index.sh process\n\nOn branch_6x, with shalin's patch, CLOSE_WAITS start at 15 (which is suspiciously 5x3) as soon as the collection is created \u2013 even w/o indexing \u2013 and stay steady state at 15 forever.\n\nWhich begs the question: why are there 15 CLOSE_WAIT connections that last forever on branch_6x even with this patch? ",
            "id": "comment-15376374"
        },
        {
            "date": "2016-07-14T07:20:14+0000",
            "author": "Shai Erera",
            "content": "Which begs the question: why are there 15 CLOSE_WAIT connections that last forever on branch_6x even with this patch?\n\nI think Shalin's patch only adds this monitor thread to UpdateShardHandler, but not to HttpShardHandlerFactory so these 15 could be from it? ",
            "id": "comment-15376474"
        },
        {
            "date": "2016-07-14T13:22:07+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Hoss has covered most of the things but just a few comments (note that I'm responding to multiple people and comments here):\n\nWhy not backport that and avoid the problem entirely? Is it a different client version in master or something that makes it not that easy?\n\nWe could backport SOLR-4509 to 6.x and deal with the incompatible changes but I'd certainly not backport it to 5x because it is just a huge change and I am not comfortable releasing that in a minor bug-fix release. I am sure many people running 5.x releases would also like a fix to this issue. Adding an idle eviction thread is trivial and unlikely to cause any regressions.\n\n\nShalin Shekhar Mangar: why not just re-use the IdleConnectionEvictor class provided by httpcomponents (getting the exact same underlying impl as what master gets from HttpClientBuilder.evictIdleConnections) ?\nhttps://hc.apache.org/httpcomponents-client-4.4.x/httpclient/apidocs/org/apache/http/impl/client/IdleConnectionEvictor.html\n\nI wasn't aware of this class. But looking deeper, I see that it requires a HttpClientConnectionManager instance but the 6.x and 5.x code uses the deprecated PoolingClientConnectionManager which extends ClientConnectionManager. But now that we know it exists, I can just borrow it from the httpclient project instead of writing my own evictor. It is ASLv2 anyway.\n\nSomebody sanity check my understanding / summary description of the root issue...\n\nThat sounds about right to me Hoss. Thanks for the summary!\n\nFor reasons I don't understand, 'idle' connections are more likely to (exist? | be kept around indefinitely?) when the intra-node communication is over SSL.\n\nPerhaps the SSL setup/teardown overhead adds some latency such that concurrent requests end up opening more connections overall? I am just guessing here.\n\nWhich begs the question: why are there 15 CLOSE_WAIT connections that last forever on branch_6x even with this patch?\n\nAs Shai said, this is likely the HttpShardHandler's pool. The overseer collection processor invokes a core admin create for each replica in parallel so you get 15 connections for 15 replicas that were created by the collection API.\n\nI'm working on a new patch which applies on branch_6x that incorporates Shai's comments as well. We can then backport it to 5x. ",
            "id": "comment-15376893"
        },
        {
            "date": "2016-07-14T14:08:01+0000",
            "author": "Yonik Seeley",
            "content": "I haven't been following this issue, but this caught my eye:\n\nSolr's use of HttpClient for intra-node communication has historically always had the potential to result in connections sitting \"idle\" (ie: in a CLOSE_WAIT state) for possible re-use later\n\nIt's been a long time since I messed around with making sure Solr worked with persistent connections (we're talking CNET days... 2004,2005 \nBut CLOSE_WAIT is when one side has closed the connection... there's no going back to ESTABLISHED from that state (i.e. no reusing that connection). ",
            "id": "comment-15376974"
        },
        {
            "date": "2016-07-14T17:11:06+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Hmm, you're right Yonik. But we've always had an idle timeout for the http connector in jetty set to 50 seconds (I traced this back to SOLR-128). So after 50 seconds of inactivity, Jetty closes that connection from its end and the client's socket goes to CLOSE_WAIT state. As you said, this connection cannot be re-used anymore. When httpclient tries to use the connection, it does the stale check, sees the CLOSE_WAIT state and terminates the connection and gives a new one to Solr.\n\nSo all the connections that suddenly do not show up in CLOSE_WAIT and we assumed that they went to ESTABLISHED state were actually terminated.\n\nSo in summary, our assumption that connections in CLOSE_WAIT are kept around because of re-use is wrong but it still doesn't change the solution that I've proposed. We could also think of increasing the value of Jetty's idle timeout as a separate change but the idle eviction thread would still be necessary. ",
            "id": "comment-15377292"
        },
        {
            "date": "2016-07-14T17:39:47+0000",
            "author": "Mark Miller",
            "content": "why not just re-use the IdleConnectionEvictor class provided by httpcomponents\n\nI've gone down this road. It's not a great solution. This is why we ended up changing to the new API's instead in SOLR-4509. Just having an evictor thread is not enough - you also then want the ability to check connections before use if they have been sitting in the pool too long and that requires HttpClient changes they made in the new API's.\n ",
            "id": "comment-15377345"
        },
        {
            "date": "2016-07-14T17:44:13+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Mark Miller \u2013 But you were trying to remove the stale check and disable Nagle's algorithm as well which exposed you to the NoHttpResponseExceptions. We aren't trying to do that here. We just want to close the idle connections so that they don't keep accumulating in the CLOSE_WAIT state. ",
            "id": "comment-15377355"
        },
        {
            "date": "2016-07-14T18:20:11+0000",
            "author": "Mark Miller",
            "content": "Okay, I didn't catch you were not removing the stale check.\n\nFor reasons I don't understand, 'idle' connections are more likely to (exist? | be kept around indefinitely?) when the intra-node communication is over SSL.\n\nI think I remember reading the SSL handles connection lifecycle differently, based on the SSL spec. ",
            "id": "comment-15377991"
        },
        {
            "date": "2016-07-14T20:37:49+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Patch which starts the idle connection evictor for both UpdateShardHandler and HttpShardHandlerFactory. I changed the HttpShardHandlerFactory to use an external connection pool ala UpdateShardHandler. The defaults are set to sleep for 5 seconds and expire connections and close idle connections older than 40 seconds. I chose 40 seconds because it is slightly lower than the jetty timeout of 50 seconds. Both of these values are configurable for both updates and queries.\n\nThis patch applies to branch_6x. ",
            "id": "comment-15378327"
        },
        {
            "date": "2016-07-15T02:22:41+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Patch updated to fix two test failures: TestCoreContainer and OverseerTest. \n\n\n\tThe TestCoreContainer.testCustomHandlers was calling CoreContainer.load twice leading to leaked threads.\n\tThe OverseerTest doesn't call init on HttpShardHandler causing a NPE on close.\n\tHttpShardHandlerFactory closed the pool before closing the http client itself. It is fixed to be the other way round.\n\n\n\nI think this is ready. ",
            "id": "comment-15378761"
        },
        {
            "date": "2016-07-15T04:47:10+0000",
            "author": "Mark Miller",
            "content": "Patch looks okay to me.\n\n\n+        clientConnectionManager.shutdown();\n+        IOUtils.closeQuietly(defaultClient);\n\n\n\nNot that it likely matters, but I'd reverse this and shut down the pool after the client. ",
            "id": "comment-15378846"
        },
        {
            "date": "2016-07-15T04:51:05+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks for reviewing Mark but I already fixed that in the last patch.\n\nI found a test failure in ZkControllerTest because of a thread leak so I may post another patch soon. ",
            "id": "comment-15378850"
        },
        {
            "date": "2016-07-15T06:02:00+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Patch which fixes the ZkControllerTest failure. Thanks to Varun Thacker for spotting the fix.\n\nI'll run precommit + tests again and then commit this patch to 6x and backport to 5x. ",
            "id": "comment-15378893"
        },
        {
            "date": "2016-07-15T06:54:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit bb7742ebc7f33f5c9f41cc3ad28b30c20a19a380 in lucene-solr's branch refs/heads/branch_6x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bb7742e ]\n\nSOLR-9290: TCP-connections in CLOSE_WAIT spike during heavy indexing and do not decrease ",
            "id": "comment-15378924"
        },
        {
            "date": "2016-07-15T09:29:06+0000",
            "author": "ASF subversion and git services",
            "content": "Commit d00c44de2eab6d01fb1df39a17b17fb769a0f541 in lucene-solr's branch refs/heads/branch_5x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d00c44d ]\n\nSOLR-9290: TCP-connections in CLOSE_WAIT spike during heavy indexing and do not decrease\n(cherry picked from commit bb7742e) ",
            "id": "comment-15379102"
        },
        {
            "date": "2016-07-15T09:41:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 00ad5efac95f38cb1df9ef33672f17a7167a656f in lucene-solr's branch refs/heads/branch_5x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=00ad5ef ]\n\nSOLR-9290: Adding 5.5.3 section and this issue to CHANGES.txt ",
            "id": "comment-15379122"
        },
        {
            "date": "2016-07-15T10:33:21+0000",
            "author": "ASF subversion and git services",
            "content": "Commit e16fb5aa3073021993595acc061cc62bd575adc2 in lucene-solr's branch refs/heads/branch_5_5 from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e16fb5a ]\n\nSOLR-9290: TCP-connections in CLOSE_WAIT spike during heavy indexing and do not decrease\n(cherry picked from commit bb7742e)\n\n(cherry picked from commit d00c44de2eab6d01fb1df39a17b17fb769a0f541) ",
            "id": "comment-15379204"
        },
        {
            "date": "2016-07-15T10:47:28+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks everyone for the help! ",
            "id": "comment-15379220"
        },
        {
            "date": "2016-07-15T19:09:46+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Hoss pointed out to me privately that the test fixes for ZkController, TestCoreContainer and OverseerTest should be applied to master as well. ",
            "id": "comment-15379940"
        },
        {
            "date": "2016-07-15T19:29:48+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 833c8ee152fc28b7ec767d0e8f8ecd346229d443 in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=833c8ee ]\n\nSOLR-9290: MockCoreContainer should call super.shutdown() ",
            "id": "comment-15379964"
        },
        {
            "date": "2016-07-15T19:30:58+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "It looks like all the test fixes that we made here were already fixed by SOLR-4509 on master. I only had to add a super.shutdown() call in MockCoreContainer just for the sake of correctness. ",
            "id": "comment-15379965"
        },
        {
            "date": "2016-07-17T16:32:44+0000",
            "author": "Shawn Heisey",
            "content": "I recently joined the jetty-users mailing list for other Solr-related issues.\n\nTen days ago (July 7th in my timezone) somebody sent a message to that list about encountering a large number of CLOSE_WAIT connections when using SSL.\n\n\nWe have an ensemble of three jetty-servers running jetty 9.3.8 on CentOS. There is a fairly high rate of communication between the servers. When we run the ensemble without SSL, everything works perfectly, but once SSL is activated, exactly one of the servers start to get a massive amount of connections in CLOSE_WAIT (more than 50 000). This, again, causes the Old Gen-part of the heap memory in the JVM to fill up, and the server becomes unable to communicate with exactly one of the other two, with Couldn\u2019t resolve address. However, the other machine can still communicate with the one that breaks down, and the 3^rd machine can communicate with both.\nIt seems that other people using Jetty (not just Solr) are experiencing a buildup of CLOSE_WAIT connections when using SSL.\n\nI'm going to mention this issue on the mailing list thread, and try to find out whether they are having the problem with software other than Solr. ",
            "id": "comment-15381414"
        },
        {
            "date": "2016-08-26T14:00:17+0000",
            "author": "Michael McCandless",
            "content": "Bulk close resolved issues after 6.2.0 release. ",
            "id": "comment-15439031"
        }
    ]
}