{
    "id": "LUCENE-211",
    "title": "[Patch] replace DocumentWriter with InvertedDocument for performance",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "I've found a way to improve Lucene's indexing performance by about 45% for my dataset.\n\nHere's how it works:  currently the indexing process goes like this:\n\n\n\tuse DocumentWriter to create an inverted index and serialize a one-document segment to a\nRAMDirectory\n\twhen enough documents have been read, deserialize the one-document segments in the\nRAMDirectory and merge them, writing the merged segment to disk.\n\n\n\nWhat I've done instead is create a new class, InvertedDocument, that keeps the inverted index in a Map, \nand can also be used directly as input for a merge.  This avoids the serialization/deserialization step, \nand the RAMDirectory is no longer used when indexing.\n\nThe patch applies to the contents of CVS as of today (April 3).  (It's a big patch and includes some \nminor style tweaks that aren't directly related.)\n\nI did the performance testing using a simple application that creates an index from a file containing \nmessages extracted from a bulletin board.  It could index about 100 kilobytes/second with Lucene 1.3, \nand 145 kilobytes/second with the patch.  This is on an 700Mhz eMac, which is pretty slow at Java, and \nthe documents being indexed are, on average, less than a screenful.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--inverted-doc.patch": "https://issues.apache.org/jira/secure/attachment/12312339/ASF.LICENSE.NOT.GRANTED--inverted-doc.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-04-04T06:38:01+0000",
            "content": "Created an attachment (id=11120)\ninverted document patch ",
            "author": "Brian Slesinsky",
            "id": "comment-12321680"
        },
        {
            "date": "2004-04-07T00:04:29+0000",
            "content": "It would be interesting to know how much more RAM this uses, since serialization\nalso performs compression.  If, for example, it uses 10 times more memory, then\nthe appropriate comparision would be to set minMergeDocs to 10 when\nInvertedDocument is used and to 100 when DocumentWriter is used.\n\nAlso, please refrain from including style tweaks in patches!  It makes them much\nharder to read. ",
            "author": "cutting@apache.org",
            "id": "comment-12321681"
        },
        {
            "date": "2004-04-07T10:35:58+0000",
            "content": "Yes, the patch is not the easiest to read.  Sorry about that!  I thought I'd send it in to get the \nconversation started.  Unfortunately I needed to do a bit of refactoring before starting (to make it \npossible to subclass SegmentInfo), so it wouldn't be a very clean patch anyway.\n\nAs far as memory usage goes, in my particular case I don't think it matters.   I'm running java with \n256M of memory on a machine with  768M, and java isn't making much use of the memory it has.  \n(Reducing Java's memory to 128M results in only a 10% slowdown due to increased garbage collection.)  \nRaising minMergeDocuments beyond about 500 seems to result in no performance improvement, either \nwith or without the patch.  The task is CPU-bound with extra memory and I/O bandwidth available, so it \nlooks like trading memory for performance makes sense even if the memory usage is higher.\n\nHowever, perhaps this isn't true for other machines and/or other datasets?  Do you have an example of \na workload where memory is the bottleneck?  Or perhaps I'm missing something about how to tune \nLucene? ",
            "author": "Brian Slesinsky",
            "id": "comment-12321682"
        },
        {
            "date": "2004-04-09T00:03:55+0000",
            "content": "When the index becomes larger than memory then i/o can become a bottleneck. \nStill, your optimization may be useful.  I'm interested to see the cleaned up\nversion.  Thanks. ",
            "author": "cutting@apache.org",
            "id": "comment-12321683"
        },
        {
            "date": "2007-12-30T18:56:38+0000",
            "content": "This is a very similar idea to LUCENE-843, which is already committed. ",
            "author": "Michael Busch",
            "id": "comment-12555015"
        }
    ]
}