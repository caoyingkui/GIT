{
    "id": "LUCENE-6119",
    "title": "Add auto-io-throttle to ConcurrentMergeScheduler",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "This method returns number of \"incoming\" bytes IW has written since it\nwas opened, excluding merging.\n\nIt tracks flushed segments, new commits (segments_N), incoming\nfiles/segments by addIndexes, newly written live docs / doc values\nupdates files.\n\nIt's an easy statistic for IW to track and should be useful to help\napplications more intelligently set defaults for IO throttling\n(RateLimiter).\n\nFor example, an application that does hardly any indexing but finally\ntriggered a large merge can afford to heavily throttle that large\nmerge so it won't interfere with ongoing searches.\n\nBut an application that's causing IW to write new bytes at 50 MB/sec\nmust set a correspondingly higher IO throttling otherwise merges will\nclearly fall behind.",
    "attachments": {
        "LUCENE-6119.patch": "https://issues.apache.org/jira/secure/attachment/12687713/LUCENE-6119.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14249713",
            "author": "Michael McCandless",
            "date": "2014-12-17T11:06:43+0000",
            "content": "Simple patch + test. "
        },
        {
            "id": "comment-14249837",
            "author": "Michael McCandless",
            "date": "2014-12-17T13:34:00+0000",
            "content": "Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.\n\nE.g. when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe. "
        },
        {
            "id": "comment-14251573",
            "author": "Michael McCandless",
            "date": "2014-12-18T12:04:56+0000",
            "content": "OK new patch with a completely different approach, moving the\ntracking under Directory.\n\nI added a new AdaptiveRateLimitedDirectoryWrapper: it watches the\naverage bytes/sec written by non-merges, and then (based on a\nmultiplier) sets the merge throttling accordingly.  It uses a rolling\ntimestamps window of the last 1 GB of writes with 1 MB resolution, and\nlets you set min/max on the merge throttle.\n\nAlso, I removed RateLimitedDirectoryWrapper: I think it's dangerous\nhow it encourages you to throttle anything except merges, and\nencourages you to just set a fixed rate (one size does NOT fit\nall...).  If you want \"similar\" behavior you can use\nAdaptiveRateLimitedDirectoryWrapper but set min and max to the same\nvalue. "
        },
        {
            "id": "comment-14253197",
            "author": "Michael McCandless",
            "date": "2014-12-19T10:00:44+0000",
            "content": "I ran some tests with this approach and I think it's no good.\n\nThis creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.\n\nI think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ... "
        },
        {
            "id": "comment-14255635",
            "author": "Michael McCandless",
            "date": "2014-12-22T10:47:55+0000",
            "content": "OK here's yet another approach that is promising, but this is still a\nrough work in progress...\n\nFirst, it removes all \"cross cutting\" merge abort checking and instead\ndoes it \"down low\" by wrapping all IndexOutputs created for merging:\nnice cleanup!\n\nSecond, it puts all throttling (hard stall of incoming index threads,\npause/unpause merges, IO rate limiting) responsibility in CMS, which\nmakes sense because it's the merge scheduler that \"knows\" whether\nmerges are falling behind, that sees how many merges need running,\netc.  (Plus CMS is already doing throttling...).\n\nEach merge is given its own MergeRateLimiter by IndexWriter, which\nhandles 1) checking for abort, 2) pausing/unpausing merges, 3)\noptionally \"io nice\" (write MB/sec rate limit) each merge thread.\n\nCMS has a simplistic estimator of requires bytes/sec (records last 1K\nmerges and computes required bytes/sec) ... I think this is too\nsimplistic (e.g. doesn't handle a slow index that suddenly picks\nup)... still thinking about how it can better tune itself.\n\nI also removed CMS tweaking thread priorities: this seems ineffective\nin practice, and I think the \"io nice\" approach is better. "
        },
        {
            "id": "comment-14255713",
            "author": "Robert Muir",
            "date": "2014-12-22T13:18:31+0000",
            "content": "great to see progress nuking checkabort! "
        },
        {
            "id": "comment-14262536",
            "author": "Michael McCandless",
            "date": "2015-01-01T11:49:26+0000",
            "content": "New patch... I think it's close.\n\nThis adds \"enable/disableAutoIOThrottle\" methods to CMS, to have CMS\npick a reasonable IO throttle over time so merges don't fall behind\nbut also don't suck up all available IO.  It's a live setting, and\ndefault is on.  CMS.getIORateLimitMBPerSec returns the current\nauto-IO-rate.\n\nAll \"merge abort checks\" are gone and instead handled by the per-merge\nrate limiter that IW sets up for each merge.  This gives merge\nschedulers \"io nice\"-like control over each merge thread.\n\nSetting the right IO throttle is a fun control problem (see\nhttp://en.wikipedia.org/wiki/Control_theory), much like the fan in\nyour laptop that changes its speed depending on internal temperature,\nor a factory that must add more workers depending on incoming jobs.\n\nI first tried \"open loop\" control, trying to set the rate based on\nindexing rate or incoming merges rate, but that doesn't work very\nwell since there are many variables (e.g. CFS on or off) that affect\nrequired MB/sec writing.\n\nSo then I switched to a simplistic feedback control: when a merge\narrives, if another merge that's \"close\" to that same size is still\nrunning, we are falling behind and we aggressively (+20%) increase the\nIO throttle.  Else, if there is a prior backlog still, leave the rate\nunchanged.  Else, we decrease it.  In my various tests of \"tiny\nflushed segs\" vs \"big flushed segs\", NRT reopens vs no, CFS or not, 1\n2 or 3 merge threads, this approach seems to work well.\n\nI haven't yet tested on spinning disks though ... will have to wait\nuntil I'm back home ... somehow my beast box died while I'm on\nvacation!  I think fsck must be waiting for me on the console \n\nForced merges have their own separate throttle (defaults to\nunlimited).\n\nI think it's important CMS not have min/max MB/sec throttle control:\nI think this just invites disaster when apps set them to inappropriate\nvalues (but I added a protected CMS method \"escape hatch\" so a\nsubclass can override the control logic).\n\nI also removed RateLimitedDirectoryWrapper: it's too simplistic and\ntoo dangerous.  Finally I cleaned a few things up and improved verbose\ninfoStream logging so we can see more stats for each merge. "
        },
        {
            "id": "comment-14262606",
            "author": "Robert Muir",
            "date": "2015-01-01T17:24:27+0000",
            "content": "\n    // Defensive: sleep for at most 250 msec; the loop above will call us again if we should keep sleeping:\n    if (curPauseNS > 250L*1000000000) {\n      curPauseNS = 250L*1000000000;\n    }\n\n\n\nDid you mean 250 milliseconds or 250 seconds? "
        },
        {
            "id": "comment-14262614",
            "author": "Robert Muir",
            "date": "2015-01-01T17:54:46+0000",
            "content": "+1, I really like this approach. "
        },
        {
            "id": "comment-14262616",
            "author": "Michael McCandless",
            "date": "2015-01-01T17:57:24+0000",
            "content": "Did you mean 250 milliseconds or 250 seconds?\n\nWoops!  I'll fix, thanks. "
        },
        {
            "id": "comment-14264115",
            "author": "Michael McCandless",
            "date": "2015-01-05T00:38:41+0000",
            "content": "New patch, fixing one nocommit, adding some more infoStream logging\naround applying deletes, fixing \"ant precommit\".  I also fixed CFS\nbuilding to also throttle.\n\nI tested on spinning disks ... it seems to behave well: under intense\nindexing, the throttle moves to unlimited since the spinning disk\ncan't keep up.  Under light indexing, it stays low.\n\nI upped the starting rate to 20 MB/sec (from 5 before): this helps it\nmove to less throttling more quickly before merges fall behind in the\nbeginning during heavy indexing.\n\nTests pass ... I think it's ready. "
        },
        {
            "id": "comment-14264402",
            "author": "Adrien Grand",
            "date": "2015-01-05T09:34:39+0000",
            "content": "I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  \n\n\n+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */\n+  private boolean doAutoIOThrottle = true;\n\n\n\nI think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)\n\n\n+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */\n+    public final MergeRateLimiter rateLimiter;\n\n\n\nI think the comment is a bit confusing since this property is not actually set by the index writer?\n\n\n/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */\n  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException\n\n\n\nMaybe having constants or an enum would make the code easier to read? "
        },
        {
            "id": "comment-14264424",
            "author": "Michael McCandless",
            "date": "2015-01-05T09:48:34+0000",
            "content": "Thanks Adrien Grand, here's a new patch with those fixes. "
        },
        {
            "id": "comment-14264619",
            "author": "ASF subversion and git services",
            "date": "2015-01-05T14:28:29+0000",
            "content": "Commit 1649532 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1649532 ]\n\nLUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate "
        },
        {
            "id": "comment-14264647",
            "author": "ASF subversion and git services",
            "date": "2015-01-05T15:00:33+0000",
            "content": "Commit 1649539 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1649539 ]\n\nLUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate "
        },
        {
            "id": "comment-14267490",
            "author": "ASF subversion and git services",
            "date": "2015-01-07T10:24:17+0000",
            "content": "Commit 1650025 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1650025 ]\n\nLUCENE-6119: fix just arrived merge to throttle correctly "
        },
        {
            "id": "comment-14267491",
            "author": "ASF subversion and git services",
            "date": "2015-01-07T10:27:25+0000",
            "content": "Commit 1650026 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650026 ]\n\nLUCENE-6119: fix just arrived merge to throttle correctly "
        },
        {
            "id": "comment-14267493",
            "author": "ASF subversion and git services",
            "date": "2015-01-07T10:27:40+0000",
            "content": "Commit 1650027 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650027 ]\n\nLUCENE-6119: fix just arrived merge to throttle correctly "
        },
        {
            "id": "comment-14270806",
            "author": "ASF subversion and git services",
            "date": "2015-01-09T09:30:44+0000",
            "content": "Commit 1650463 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1650463 ]\n\nLUCENE-6119: set initial rate for forced merge correctly "
        },
        {
            "id": "comment-14270808",
            "author": "ASF subversion and git services",
            "date": "2015-01-09T09:33:06+0000",
            "content": "Commit 1650464 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650464 ]\n\nLUCENE-6119: set initial rate for forced merge correctly "
        },
        {
            "id": "comment-14271509",
            "author": "ASF subversion and git services",
            "date": "2015-01-09T16:44:45+0000",
            "content": "Commit 1650594 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650594 ]\n\nLUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping "
        },
        {
            "id": "comment-14271516",
            "author": "ASF subversion and git services",
            "date": "2015-01-09T16:48:11+0000",
            "content": "Commit 1650595 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1650595 ]\n\nLUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping "
        },
        {
            "id": "comment-14274925",
            "author": "ASF subversion and git services",
            "date": "2015-01-13T08:54:16+0000",
            "content": "Commit 1651305 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1651305 ]\n\nLUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter "
        },
        {
            "id": "comment-14274938",
            "author": "ASF subversion and git services",
            "date": "2015-01-13T09:02:21+0000",
            "content": "Commit 1651307 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1651307 ]\n\nLUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter "
        },
        {
            "id": "comment-14327139",
            "author": "Dawid Weiss",
            "date": "2015-02-19T09:02:14+0000",
            "content": "I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.\n\nWhen you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:\n\nf = merge_throughput + indexing_throughput\n\n\nperhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:\n\n\n\thow many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),\n\twhen to pause/ resume existing merge threads,\n\twhen to pause/ resume indexing threads.\n\n\n\nWhat's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):\n\n\n\treact to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),\n\n\n\n\n\tfind out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f \u2013 the optimal number of merge threads would emerge by itself from looking at the data).\n\n\n\nNow the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. \n\nI have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done. "
        },
        {
            "id": "comment-14327950",
            "author": "Michael McCandless",
            "date": "2015-02-19T18:57:28+0000",
            "content": "I think auto-tuning merge thread count would be a great addition! "
        },
        {
            "id": "comment-14328094",
            "author": "Dawid Weiss",
            "date": "2015-02-19T20:45:58+0000",
            "content": "I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see. "
        },
        {
            "id": "comment-14332895",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:02:29+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}