{
    "id": "SOLR-7869",
    "title": "Overseer does not handle BadVersionException correctly",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "5.2.1",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "If the /clusterstate.json is modified externally then the Overseer can go into an infinite loop upon a BadVersionException alternately trying to execute main queue and then the work queue:\n\n\nERROR - 2015-08-04 18:49:56.224; [   ] org.apache.solr.cloud.Overseer$ClusterStateUpdater; Exception in Overseer work queue loop\norg.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /clusterstate.json\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)\n        at org.apache.solr.common.cloud.SolrZkClient$8.execute(SolrZkClient.java:362)\n        at org.apache.solr.common.cloud.SolrZkClient$8.execute(SolrZkClient.java:359)\n        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:61)\n        at org.apache.solr.common.cloud.SolrZkClient.setData(SolrZkClient.java:359)\n        at org.apache.solr.cloud.overseer.ZkStateWriter.writePendingUpdates(ZkStateWriter.java:180)\n        at org.apache.solr.cloud.overseer.ZkStateWriter.enqueueUpdate(ZkStateWriter.java:67)\n        at org.apache.solr.cloud.Overseer$ClusterStateUpdater.processQueueItem(Overseer.java:286)\n        at org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:168)\n        at java.lang.Thread.run(Thread.java:745)\nINFO  - 2015-08-04 18:49:56.224; [   ] org.apache.solr.cloud.Overseer$ClusterStateUpdater; processMessage: queueSize: 1, message = {\n  \"operation\":\"state\",\n  \"state\":\"down\",\n  \"base_url\":\"http://127.0.1.1:7574/solr\",\n  \"core\":\"test_shard1_replica1\",\n  \"roles\":null,\n  \"node_name\":\"127.0.1.1:7574_solr\",\n  \"shard\":null,\n  \"collection\":\"test\",\n  \"core_node_name\":\"core_node1\"} current state version: 9\nINFO  - 2015-08-04 18:49:56.224; [   ] org.apache.solr.cloud.overseer.ReplicaMutator; Update state numShards=null message={\n  \"operation\":\"state\",\n  \"state\":\"down\",\n  \"base_url\":\"http://127.0.1.1:7574/solr\",\n  \"core\":\"test_shard1_replica1\",\n  \"roles\":null,\n  \"node_name\":\"127.0.1.1:7574_solr\",\n  \"shard\":null,\n  \"collection\":\"test\",\n  \"core_node_name\":\"core_node1\"}\nINFO  - 2015-08-04 18:49:56.224; [   ] org.apache.solr.cloud.overseer.ReplicaMutator; shard=shard1 is already registered\nERROR - 2015-08-04 18:49:56.225; [   ] org.apache.solr.cloud.Overseer$ClusterStateUpdater; Exception in Overseer main queue loop\norg.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /clusterstate.json\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)\n        at org.apache.solr.common.cloud.SolrZkClient$8.execute(SolrZkClient.java:362)\n        at org.apache.solr.common.cloud.SolrZkClient$8.execute(SolrZkClient.java:359)\n        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:61)\n        at org.apache.solr.common.cloud.SolrZkClient.setData(SolrZkClient.java:359)\n        at org.apache.solr.cloud.overseer.ZkStateWriter.writePendingUpdates(ZkStateWriter.java:180)\n        at org.apache.solr.cloud.overseer.ZkStateWriter.enqueueUpdate(ZkStateWriter.java:67)\n        at org.apache.solr.cloud.Overseer$ClusterStateUpdater.processQueueItem(Overseer.java:286)\n        at org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:213)\n        at java.lang.Thread.run(Thread.java:745)\nINFO  - 2015-08-04 18:49:56.225; [   ] org.apache.solr.common.cloud.ZkStateReader; Updating data for gettingstarted to ver 8",
    "attachments": {
        "SOLR-7869.patch": "https://issues.apache.org/jira/secure/attachment/12748743/SOLR-7869.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-04T22:29:06+0000",
            "author": "Scott Blum",
            "content": "But what's the right fix?  Having looked through the code a bit now, OverSeer.ClusterUpdater has a very baked-in assumption that no one else is updating cluster state.  Copies of ClusterState float around and get updated over and over during processing, with the assumption that the local node is performing an atomic sequence of operations to get to a desired end state.  How can external changes be merged in?  My impulse was to catch BadVersionException, refresh ClusterState from ZK, then re-apply all the queued updates against the refreshed state.  However, I'm afraid that approach violates all of ClusterUpdater's assumptions.  I think the only thing to do is just clobber whatever is in ZK with what Overseer wants to write, even though that seems less than ideal. ",
            "id": "comment-14654472"
        },
        {
            "date": "2015-08-04T22:31:00+0000",
            "author": "Scott Blum",
            "content": "Attached a TEST ONLY that repros the failure.  This is not a fix. ",
            "id": "comment-14654476"
        },
        {
            "date": "2015-08-05T09:20:03+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Scott!\n\nMy impulse was to catch BadVersionException, refresh ClusterState from ZK, then re-apply all the queued updates against the refreshed state.\n\nThat is the right fix. That is how I intended it to work but I obviously didn't write enough tests.\n\nHowever, I'm afraid that approach violates all of ClusterUpdater's assumptions. \n\nOriginally the overseer would force update the cluster state at the beginning of the loop, apply the updates and write to ZK. This was wasteful because most of the time, the Overseer is the only guy writing to ZK state. This is why I introduced a local cluster state which is written to ZK using CAS removing the need for refreshing the cluster state. If that CAS fails then that means that someone has changed state externally or due to a bug multiple overseers have started processing. At this point, we go back to the beginning of the loop, check if we are still leader, force refresh the cluster state, process work queue and the continue on to the main queue. ",
            "id": "comment-14655057"
        },
        {
            "date": "2015-08-05T11:05:54+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "We can fix ZkStateWriter easily with the following:\n\nif (clusterState != null && !prevState.getZkClusterStateVersion().equals(clusterState.getZkClusterStateVersion()))  {\n      // our local cluster state is out of sync with what is in ZK, re-initialize\n      // this is safe because either 1) any pending updates are in #updates and will be applied again, or\n      // 2) they have already been written to ZK\n      // i.e. it is guaranteed that overwriting cluster state with prevState will not discard any updates\n      // that Overseer had performed unless such an act was done externally by the user\n      clusterState = prevState;\n      isClusterStateModified = true;\n    }\n\n\n\nHowever, the test isn't quite right. We can't handle BadVersionException inside ZkStateWriter itself. It has to be propagated to Overseer which should do the logic I outlined in the previous comment. So any test should test at the Overseer level and not just the ZkStateWriter. ",
            "id": "comment-14655191"
        },
        {
            "date": "2015-08-08T05:21:08+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Test + fix. I tried to reproduce this in the OverseerTest but it was proving to be too difficult. The randomized test I had would maybe reproduce once in 5 times so I went back to the test Scott had written and augmented it.\n\n\n\tI wonder if it is better to assert that given cluster state version is greater than ZkStateWriter's internal cluster state instead of blindly using given cluster state when version is not equal.\n\tI also wonder if a better fix is to re-create ZkStateWriter object entirely if refreshClusterState is true in the Overseer? The reason is what if a user modifies a collection's state.json directly but doesn't modify the /clusterstate.json. In that case, our current fix won't work.\n\n ",
            "id": "comment-14662813"
        },
        {
            "date": "2015-08-09T20:16:16+0000",
            "author": "Scott Blum",
            "content": "1) So this doesn't actually fix anything yet, because there are no changes to Overseer itself?  Presumably you'd need to catch BVE in overseer and force-refresh reader clusterState?\n\n2) Just noting that this seems the opposite of what we discussed earlier.  I interpreted your earlier comments to mean that we should blow away the ZK data in favor of the overseer data, since overseer is authoritative.  This patch seems do the opposite, preferring external user changes.  To wit \"it is guaranteed that overwriting cluster state with prevState will not discard any updates that Overseer had performed unless such an act was done externally by the user\".\n\n3) In ZkStateWriterTest, I note that ZkStateWriter isn't super amenable to testing, it's kind of subtle that enqueuing an update sometimes causes a flush, and sometimes does.  Dunno if it's better or worse to have test-visible methods for doing a queue-without-flush and then explicit flush.\n\n4) In ZkStateWriterTest.testExternalModificationToSharedClusterState(), first try block, you're missing a fail() after the enqueueUpdate to test that the exception really did occur.  In the first catch block, I'm not sure it's good to log the expected exception, I always find it confusing when tests log exceptions that don't actually cause the test to fail.  I would remove the second catch block; if you get any other exception than the one you expect, best to just let it escape and let the test framework get it.\n\n5) In a similar fashion, I would remove the second try/catch block entirely, just keeping the body of the try.  You expect that none of it will throw an exception, so just leave it unadorned and the test framework will handle if there is. ",
            "id": "comment-14679319"
        },
        {
            "date": "2015-08-10T06:52:23+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "1) So this doesn't actually fix anything yet, because there are no changes to Overseer itself? Presumably you'd need to catch BVE in overseer and force-refresh reader clusterState?\n\nActually Overseer already has code that catches all exceptions and refreshes the cluster state. The bug is in ZkStateWriter which does not use the refreshed cluster state if maybeRefreshBefore returns true and hence tries to compare-and-set using the outdated cluster state version.\n\n2) Just noting that this seems the opposite of what we discussed earlier. I interpreted your earlier comments to mean that we should blow away the ZK data in favor of the overseer data, since overseer is authoritative. This patch seems do the opposite, preferring external user changes. To wit \"it is guaranteed that overwriting cluster state with prevState will not discard any updates that Overseer had performed unless such an act was done externally by the user\".\n\nMaybe I wasn't clear enough. But I did mean the opposite of what you understood. The user has made some changes either accidentally in which case they totally deserve what's coming  or presumably to fix something that went wrong in the cluster state (which could be because of a genuine bug). In both cases, overwriting stuff that a user has himself done seems wrong to me. We should just roll with it. Therefore the overseer refreshes the cluster state and starts using it as the base for future operations.\n\n3) In ZkStateWriterTest, I note that ZkStateWriter isn't super amenable to testing, it's kind of subtle that enqueuing an update sometimes causes a flush, and sometimes does. Dunno if it's better or worse to have test-visible methods for doing a queue-without-flush and then explicit flush.\n\nYou are right. The testZkStateWriterBatching is a horrible test and I should have written a better one. In particular, maybeFlushAfter also updates the local state (lastStateFormat, lastCollectionName) before the write happens. We should change that. But I am not sure what you mean by \"Dunno if it's better or worse to have test-visible methods for doing a queue-without-flush and then explicit flush.\"?\n\nRe: #4 and #5 \u2013 good point. I'll fix that. ",
            "id": "comment-14679667"
        },
        {
            "date": "2015-08-10T18:57:40+0000",
            "author": "Scott Blum",
            "content": "1) bq \"Actually Overseer already has code that catches all exceptions and refreshes the cluster state.\n\nIn that case, I would suggest an explicit catch block there for BadVersionException, and log something milder than error?  The current KeeperException catch block always logs an error.\n\n2) Okay, I totally buy that argument!  But see note below.\n\n3) I'm merely saying that the refactoring that would be necessary to make ZkStateWriter more easily testable might negatively impact the flow of the existing code.  I'm not sure, you'd have to try it.\n\n\nHaving stewed on this a bit, I think there's somewhat of a fundamental disconnect in how ZkStateWriter's batching interacts with the Overseer work queue.\n\nWork items get removed from the queue before the corresponding change is actually written out to the cluster state.  You can see in the original design that there's this neat peek->poll dance in the Overseer loop that attempts to enforce guaranteed state updates by not discarding the work item until after the state is written out.  But the batching implementation gets rid of this guarantee, and that's why I perceive we're now in a state where the overseer updates and external updates can even be in conflict with each other.\n\nImagine (as a thought experiment) we got rid of batching.  If we did that, no external change could \"lose\" a work item, because we'd be committing one item at a time, so the retry operation on a bad version exception would always re-grab the most recent, unapplied work item.  Now obviously, we don't want to get rid of batching, because efficiency.  But I really do think we're batching in the wrong place.  I think batching actually needs to happen in Overseer, because it has to be tied to discarding work items.  Ideally, we'd peek N work items from the head of the queue, setup all the pending updates / cluster state mods in ZkStateWriter, then try to commit everything.  If it succeeds, great, remove all the processed items from the queue.  If it fails, then reread cluster state and retry all the items again.\n\nMake any sense?\n\n(Now, all that said, that work may be more effort than it's worth, and maybe we should just focus on not having to use the queue to make cluster state updates in the format v2 world.) ",
            "id": "comment-14680558"
        },
        {
            "date": "2015-08-10T20:38:52+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "In that case, I would suggest an explicit catch block there for BadVersionException, and log something milder than error? The current KeeperException catch block always logs an error.\n\n+1\n\nWork items get removed from the queue before the corresponding change is actually written out to the cluster state. You can see in the original design that there's this neat peek->poll dance in the Overseer loop that attempts to enforce guaranteed state updates by not discarding the work item until after the state is written out. But the batching implementation gets rid of this guarantee, and that's why I perceive we're now in a state where the overseer updates and external updates can even be in conflict with each other.\n\nNo, items are never removed until they're flushed to ZK. This invariant is maintained everywhere. If you find that I have missed a case then please do let me know. When operating on the work queue, batching is completely disabled because we have no fallback in case we fail. When operating on the main queue \u2013 we add operation to work queue when onEnqueue event is called and we remove item from work queue once onWrite event is called. This ensures that we never lose an operation.\n\nBut I really do think we're batching in the wrong place. I think batching actually needs to happen in Overseer, because it has to be tied to discarding work items.\n\nThat used to be the case. I refactored Overseer extensively in 5.0 in SOLR-6554\n\n(Now, all that said, that work may be more effort than it's worth, and maybe we should just focus on not having to use the queue to make cluster state updates in the format v2 world.)\n\nYeah, I think a more worthwhile improvement would be SOLR-6760\n ",
            "id": "comment-14680727"
        },
        {
            "date": "2015-08-13T11:01:21+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Here's a better fix which discards ZkStateWriter on a BadVersionException and starts afresh. The previous approach didn't work when an external change was made on state.json with no changes to /clusterstate.json. Although such changes can be detected and resolved inside ZkStateWriter but that would make this class unnecessarily complex.\n\nZkStateWriter will put itself into an invalid state upon a BadVersionException and will disallow all future operations. Callers are expected to discard such an instance and create a fresh ZkStateWriter instance for future use.\n\nI added two tests in ZkStateWriterTest which simulate an external change to /clusterstate.json and a state.json and asserts that an IllegalStateException is thrown on any future invocation of enqueueUpdate or writePendingUpdates.\n\nI also added a test in Overseer which asserts that the overseer can keep processing events on a BadVersionException (indirectly testing that a fresh ZkStateWriter is created upon said exception).\n\nI also added copious amounts of javadocs to the ZkStateWriter class for future reference. ",
            "id": "comment-14695059"
        },
        {
            "date": "2015-08-13T16:27:19+0000",
            "author": "Scott Blum",
            "content": "Great change!  Couple of minor nits:\n\n1) There's a log(SHALIN) to remove in OverseerTest\n\n2) testExternalModificationToSharedClusterState() has 2 try/catch(expected) with no fail() statement at the end of the try block. testExternalModificationToStateFormat2() has 3 instances of this problem. ",
            "id": "comment-14695501"
        },
        {
            "date": "2015-08-13T17:32:58+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks for the review Scott!\n\nBoth of your comments are now incorporated into this patch. I'll run precommit and tests and commit once they succeed. ",
            "id": "comment-14695621"
        },
        {
            "date": "2015-08-13T18:31:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1695746 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1695746 ]\n\nSOLR-7869: Overseer does not handle BadVersionException correctly and, in some cases, can go into an infinite loop if cluster state in ZooKeeper is modified externally ",
            "id": "comment-14695738"
        },
        {
            "date": "2015-08-13T19:36:55+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1695763 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1695763 ]\n\nSOLR-7869: Overseer does not handle BadVersionException correctly and, in some cases, can go into an infinite loop if cluster state in ZooKeeper is modified externally ",
            "id": "comment-14695815"
        },
        {
            "date": "2015-08-13T19:37:38+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Scott! ",
            "id": "comment-14695816"
        }
    ]
}