{
    "id": "LUCENE-7854",
    "title": "Indexing custom term frequencies",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [],
        "fix_versions": [
            "7.0"
        ]
    },
    "description": "When you index a field with IndexOptions.DOCS_AND_FREQS, Lucene will store just the docID and term frequency (how many times that term occurred in that document) for all documents that have a given term.\n\nWe compute that term frequency by counting how many times a given token appeared in the field during analysis.\n\nBut it can be useful, in expert use cases, to customize what Lucene stores as the term frequency, e.g. to hold custom scoring signals that are a function of term and document (this is my use case).  Users have also asked for this before, e.g. see https://stackoverflow.com/questions/26605090/lucene-overwrite-term-frequency-at-index-time.\n\nOne way to do this today is to stuff your custom data into a byte[] payload.  But that's quite inefficient, forcing you to index positions, and pay the overhead of retrieving payloads at search time.\n\nAnother approach is \"token stuffing\": just enumerate the same token N times where N is the custom number you want to store, but that's also inefficient when N gets high.\n\nI think we can make this simple to do in Lucene.  I have a working version, using my own custom indexing chain, but the required changes are quite simple so I think we can add it to Lucene's default indexing chain?\n\nI created a new token attribute, TermDocFrequencyAttribute, and tweaked the indexing chain to use that attribute's value as the term frequency if it's present, and if the index options are DOCS_AND_FREQS for that field.",
    "attachments": {
        "LUCENE-7854.patch": "https://issues.apache.org/jira/secure/attachment/12870070/LUCENE-7854.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16026273",
            "date": "2017-05-26T13:18:01+0000",
            "content": "Initial patch; I think it's close. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16026283",
            "date": "2017-05-26T13:39:30+0000",
            "content": "quick glance: i like the idea overall. stacking terms is not intuitive.\n\nvery glad to see test for any case where you attempt prox \n\nhowever, i would like to see a test about what happens to the norm in this case. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16026287",
            "date": "2017-05-26T13:41:45+0000",
            "content": "and even better if that can be tested via FIS stats (so then we know any sim is correct without worrying about norm details).\n\nalso what happens in docs-only case? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16026293",
            "date": "2017-05-26T13:44:24+0000",
            "content": "+1 Nice!\n\nIt's a shame you had to touch term vector indexing internals... to me that suggests an internal complexity wart.\n\nI've wanted to index custom term frequencies too but for entirely different use-case \u2013 geospatial heatmap but abusing the frequency to instead store some different per-document value for each term.  As I conceived the need I didn't think it would make sense to change Lucene's API for this since my use-case seemed too specialized but your use-case makes a lot of sense to be in core Lucene. ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16026630",
            "date": "2017-05-26T18:21:07+0000",
            "content": "I had wanted this in the past for indexing collaborative filtering similarity scores as a sparse matrix. In that case, say you want to index document-document similarity, basically some function SIM(doc1,doc2). The full matrix is too enormous to store, so you only record the top N most similar docs. One way to store this is to index the LHS in one field, and all the related document ids as terms in another field. Then you can use Lucene's queries to perform weighted averages if you several documents and so on, as well as mixing with other term constraints, but you really want to manipulate the frequencies in order to represent the SIM function. ",
            "author": "Mike Sokolov"
        },
        {
            "id": "comment-16026639",
            "date": "2017-05-26T18:28:21+0000",
            "content": "Finally we get this. Great work. Just this morning I was thinking about this. I have a current customer who needs this. The only way how they can do it is term repetition.\n\nI will look into the patch later, I am looking forward to see this fixed. We just need some additional Timely that can be used in Solr to give the numbers in the terms like we do for the payload case. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16027424",
            "date": "2017-05-27T13:30:17+0000",
            "content": "I had wanted this in the past for indexing collaborative filtering similarity scores as a sparse matrix.\n\nThat's a nice use case Mike Sokolov. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16027425",
            "date": "2017-05-27T13:32:21+0000",
            "content": "OK I folded in the feedback (thank you!):\n\n\n\tAlso catch attempts to index DOCS while using custom term freq\n    and throw an exc similar to if you try to index positions\n\n\n\n\n\tFor norms, I still increment FIS.length just once for each\n    custom-term-freq term, and I added a test case that checks the\n    FieldInvertState.\n\n\n\n\n\tI also added a separate test case for FieldInvertState ... Rob\n    noticed, and I agree, we don't seem to do a good job directly\n    testing this important indexing class.\n\n\n\n\n\tTest totalTermFreq (postings and term vectors) too\n\n\n\nI think the patch is ready. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16027435",
            "date": "2017-05-27T13:55:46+0000",
            "content": "Patch looks good, although I am not sure if we really should check for existence of the attribute. What happens if somebody has configured an analyzer with a TermFreqAttribute and uses it for all fields? If heshe disables freqs for one field this will break indexing. We also don't throw ex, if one has a tokenstream with offsets and positions and we don't index them!!! \n\nIMHO: I'd always add the termfreq attribute when creating the inverter. As the default is \"1\" anyways, if there is no tokenfilter that modifies the attribute all works as it worked before. If we have a filter that changes the attribute it is used. Quite simple and less if/then/else logic. Of course, if we add it by default, we should add the attribute impl to the composite default attribute, too. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16027437",
            "date": "2017-05-27T14:01:20+0000",
            "content": "Mike: I was afraid of that. Can we fix \"length\" to be correct please? (same as if i stacked).\n\nI agree with Uwe, i hate how payloads do this. But maybe it can be a followup (payloads should be fixed, too).\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16027449",
            "date": "2017-05-27T14:51:09+0000",
            "content": "Mike: I was afraid of that. Can we fix \"length\" to be correct please? (same as if i stacked).\n\nWoops, sure I'll fix it.  I like your characterization that this all is just \"sugar\" for stacking.\n\nI am not sure if we really should check for existence of the attribute\n\nOK I'll use .addAttribute, and add to the default packed token impl too.  It will clean up the if/elses... ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16027600",
            "date": "2017-05-27T20:44:18+0000",
            "content": "Another iteration:\n\n\n\tFIS.length is now computed properly\n\n\n\n\n\tThe indexing chain uses addAttribute to get the term freq add, adding it if it's missing.  The value defaults to 1, and I also implement it in PackedTokenAttribute.\n\n ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16027635",
            "date": "2017-05-27T22:31:18+0000",
            "content": "Hi,\nthe packed token impl is not implementing clear(), copyTo, equals, hashcode... and so on. Just grep for positionIncrement in the file to find other occurences that also need to be changed. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16027656",
            "date": "2017-05-28T00:32:32+0000",
            "content": "Woops, another iteration   Thanks Uwe Schindler. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16027782",
            "date": "2017-05-28T10:52:21+0000",
            "content": "Looks fine to me. Only the reflectWith output is different between the packed attribute and the default one. I think it should be normalized. For the other stuff, Robert Muir is the better reviewer. \n\nNow we only need a TokenFilter that allows to parse \"special tokens\" like the payload one does (e.g. \"token^3\"). But that's a separate issue. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16027873",
            "date": "2017-05-28T17:36:04+0000",
            "content": "Good catch Uwe Schindler, I normalized the output of reflectWith between the two. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16029191",
            "date": "2017-05-30T09:02:24+0000",
            "content": "I like the feature too. Two minor problems I noticed in the patch is that the if (invertState.length < 0) test in DefaultIndexingChain is now dead code since you use Math.addExact to accumulate term freqs, and TermVectorsConsumerPerField should also assert that offsets are not indexed when the frequency is not 1 since term vectors allow you to store offsets but not positions. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16029242",
            "date": "2017-05-30T10:21:03+0000",
            "content": "Ooh those are great catches Adrien Grand; I'll fix. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16031284",
            "date": "2017-05-31T15:03:46+0000",
            "content": "New patch, folding in Adrien Grand's last feedback (thank you!).  I think it's ready. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16031770",
            "date": "2017-05-31T19:15:20+0000",
            "content": "Hi,\nLooks fine. Just one question: Why is the attribute called TermDocFrequencyAttribute and the inner value termFrequency? IMHO, the attribute should be called TermFrequencyAttribute.\nIf there is a good reason for this, I am +1 to commit! Great progress! ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16032771",
            "date": "2017-06-01T10:47:30+0000",
            "content": "OK I'll rename to TermFrequencyAttribute / termFrequency. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16033067",
            "date": "2017-06-01T14:30:27+0000",
            "content": "Another iteration, doing the rename Uwe Schindler suggested, and also cleaning up PackedTokenAttributeImpl#end a bit. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16034239",
            "date": "2017-06-02T06:54:11+0000",
            "content": "+1 ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16034278",
            "date": "2017-06-02T07:32:20+0000",
            "content": "+1 ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16039312",
            "date": "2017-06-06T17:37:48+0000",
            "content": "Commit d276acfbbcb8ea6124fe486e35cea9eb928e4702 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d276acf ]\n\nLUCENE-7854: enable indexing custom term frequencies ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16039313",
            "date": "2017-06-06T17:38:02+0000",
            "content": "Thanks everyone. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16039325",
            "date": "2017-06-06T17:42:48+0000",
            "content": "Thanks Mike. Now lets open an issue for a TokenFilter that behaves like they payload one. That's what I need \nWill be back later and make a proposal! We should have this in 7.0, otherwise this feature is \"useless\" for most people with Solr/Elasticsearch. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039606",
            "date": "2017-06-06T20:50:57+0000",
            "content": "Thanks Uwe Schindler, can you open that follow-on issue? ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16039719",
            "date": "2017-06-06T21:52:13+0000",
            "content": "Done: LUCENE-7866 ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16041177",
            "date": "2017-06-07T16:34:17+0000",
            "content": "My Jenkins found a reproducing master seed for a TestIndexWriterExceptions.testTooManyTokens() failure, which git bisect blames on commit d276acfb on this issue:\n\n\nChecking out Revision 1921b61ba8f3c7579bc04975b7ce90167a74e51e (refs/remotes/origin/master)\n[...]\n   [junit4] Suite: org.apache.lucene.index.TestIndexWriterExceptions\n   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIndexWriterExceptions -Dtests.method=testTooManyTokens -Dtests.seed=244E0F9076AF909A -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=sl -Dtests.timezone=Pacific/Rarotonga -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE  344s J0  | TestIndexWriterExceptions.testTooManyTokens <<<\n   [junit4]    > Throwable #1: junit.framework.AssertionFailedError: Unexpected exception type, expected IllegalArgumentException but got java.lang.ArithmeticException: integer overflow\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([244E0F9076AF909A:C9CA5B1B9805BDB9]:0)\n   [junit4]    > \tat org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2679)\n   [junit4]    > \tat org.apache.lucene.index.TestIndexWriterExceptions.testTooManyTokens(TestIndexWriterExceptions.java:2047)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]    > Caused by: java.lang.ArithmeticException: integer overflow\n   [junit4]    > \tat java.lang.Math.addExact(Math.java:790)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:773)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:431)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:393)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:236)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:478)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1570)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1315)\n   [junit4]    > \tat org.apache.lucene.index.TestIndexWriterExceptions.lambda$testTooManyTokens$22(TestIndexWriterExceptions.java:2048)\n   [junit4]    > \tat org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2674)\n[...]\n   [junit4]   2> NOTE: test params are: codec=CheapBastard, sim=RandomSimilarity(queryNorm=false): {content6=LM Jelinek-Mercer(0.700000), field=IB LL-D2, content4=DFR I(n)2, contents=DFR I(F)1, content2=DFR I(ne)B3(800.0), content1=LM Jelinek-Mercer(0.100000), id=DFR I(F)L2, content=DFR I(ne)Z(0.3)}, locale=sl, timezone=Pacific/Rarotonga\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=77388832,total=333447168\n\n ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-16041417",
            "date": "2017-06-07T18:55:32+0000",
            "content": "Hmm thanks Steve, I'll dig. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16041496",
            "date": "2017-06-07T19:44:17+0000",
            "content": "Commit 6c3ece2b9f97926769849b4008ed7f10276f0b14 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6c3ece2 ]\n\nLUCENE-7854: restore the IllegalArgumentException if you index too many tokens in one field ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16045084",
            "date": "2017-06-09T21:52:40+0000",
            "content": "Commit 5844ed4ac95373cbdb512e84b8ad08f78c2baf57 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5844ed4 ]\n\nLUCENE-7854: Add a new DelimitedTermFrequencyTokenFilter that allows to mark tokens with a custom term frequency ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16045093",
            "date": "2017-06-09T21:57:33+0000",
            "content": "Sorry I used wrong issue number for that commit. I added a new fake commit with correct number, ignore GitService's message. ",
            "author": "Uwe Schindler"
        }
    ]
}