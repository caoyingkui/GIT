{
    "id": "LUCENE-2454",
    "title": "Nested Document query support",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "3.0.2",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "A facility for querying nested documents in a Lucene index as outlined in http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene",
    "attachments": {
        "LUCENE-2454.patch": "https://issues.apache.org/jira/secure/attachment/12480123/LUCENE-2454.patch",
        "LuceneNestedDocumentSupport.zip": "https://issues.apache.org/jira/secure/attachment/12472247/LuceneNestedDocumentSupport.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-05-10T15:32:49+0000",
            "content": "Initial attachment is code plus illustrative data/tests. \nFuller unit tests/build scripts etc to follow ",
            "author": "Mark Harwood",
            "id": "comment-12865805"
        },
        {
            "date": "2010-05-11T11:07:30+0000",
            "content": "Robust use of this feature is dependent on careful management of segments i.e. that all compound documents are held in the same segment.\n\nMichael Busch suggested the introduction of a new \"FlushPolicy\" on IndexWriter to offer the required control. (see http://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3C4BE5A14C.6040108@gmail.com%3E )\nSounds sensible to me given that IndexWriter currently manages to muddle 2 alternative policies in the one implementation and it looks like we now need a third.\n\nIs this the place to start the debate on \"FlushPolicy\" ?\nMy guess is this change would involve :\n\n\tDeprecating/removing IndexWriter's setMaxBufferedDocs and setRAMBufferSizeMB.\n\tProviding a new \"FlushPolicy\" abstract class that is called with a \"BufferContext \" class to hold number buffered docs + ram usage. FlushPolicy is asked if flushing of various structures should be triggered given the context\n\tProvide default implementations of FlushPolicy that are number-of-documents-based and RAM-based.\n\tProvide a special \"NestedDocumentFlushPolicy\" that can wrap any other policy (ram/num docs) but only triggers flushes when application code has primed it to say a batch of related documents is completed.\n\n\n\nLet me know where it's best to continue the thinking on these IndexWriter changes. ",
            "author": "Mark Harwood",
            "id": "comment-12866128"
        },
        {
            "date": "2010-05-11T11:53:57+0000",
            "content": "An alternate approach - there was a discussion on narrowing indexing API to something stream-like, whereas Document becomes its default implementation. We can add some flush-boundary signalling methods, or a notion of composite documents to that new API.\n\nI like this approach more, as control does not spread out across different APIs/instances. You don't have to hold reference to your policy in the indexing code, you don't have to raise/lower flags in some remote code to signal things that are internal to yours. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12866133"
        },
        {
            "date": "2010-05-11T11:55:32+0000",
            "content": "Both things can be combined for sure. New stream-like indexing API stuffs docs into IW and controls when flushes /can/ happen, while FlushPolicy decides if they actually /do/ happen, when they /can/. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12866134"
        },
        {
            "date": "2010-05-11T13:56:11+0000",
            "content": "- there was a discussion on narrowing indexing API to something stream-like\n\nAny idea where there that discussion was taking place? Happy to move flush-control discussions elsewhere if that is more appropriate. ",
            "author": "Mark Harwood",
            "id": "comment-12866148"
        },
        {
            "date": "2010-05-17T16:16:06+0000",
            "content": "I thiiiiink, here - LUCENE-2309 ",
            "author": "Earwin Burrfoot",
            "id": "comment-12868268"
        },
        {
            "date": "2010-06-12T22:01:15+0000",
            "content": "Wow, this is absolutely awesome!  This is one of the best enhancement requests to Lucene/Solr that I've seen as it brings a real enhancement this is difficult / impossible to do without.\n\nThe leading concern I have with this implementation is the size of the number of documents in the index as it affects the size of filters and perhaps other areas involving creating BitSet's.  I have a scenario in which the sub-documents number on average over 100 to each primary document.  These sub-documents are at least very small, and they don't share any fields with the parent.  For a large scale search situation, an index containing 3M lucene documents now needs to store over 300M, and thus require 100x the amount of RAM for filter caches as I require now.  Thoughts? ",
            "author": "David Smiley",
            "id": "comment-12878317"
        },
        {
            "date": "2010-06-13T20:48:15+0000",
            "content": "Wow, this is absolutely awesome! \n\nThanks. I've found that this certainly solves problems I previously couldn't address at all in standard Lucene.\n\nThe leading concern I have with this implementation is the size of the number of documents in the index as it affects the size of filters\n\nThese filters can obviously be cached but you'll need one filter per level you \"roll up\" to. Assuming a 300m doc index and only rolling up matches to the root that should only cost 300m /8 bits per byte = 37.5 meg of RAM. Index reloads should avoid the cost of completely rebuilding this filter nowadays because filters are cached at segment level and unchanged segments will retain their cached filters.\nPerhaps a bigger concern is any norms arrays which are allocated one BYTE (as opposed to one bit) per document in the index.\n\nand they don't share any fields with the parent. \n\nFor parents with only 1 child document instance of a given type, these could be safely \"rolled up\" into the parent and stored in the same document.\n ",
            "author": "Mark Harwood",
            "id": "comment-12878434"
        },
        {
            "date": "2010-06-14T13:59:33+0000",
            "content": "35.7MB of RAM for every filter is a LOT compared to the 357KB I need now (100x).  Presumably the facet intersections now take 100x as long too.  I cache nearly a thousand of these per index (lots of faceting!) which is by the way just one Solr shard of many.  No can do.  \n\nI wonder if its plausible to consider a different implementation strategy employing a parallel index with the child documents storing the document IDs to the parent index.  I might even assume I need no more than 1000 child documents and thus index blank documents as filler so that if I am looking at a child document with id 32005 then it is the 6th sub-entity belonging to parent document id 32.  I know that document IDs are a bit transient so I know that some care would be needed to maintain this strategy.  Thoughts? ",
            "author": "David Smiley",
            "id": "comment-12878581"
        },
        {
            "date": "2010-06-14T15:37:05+0000",
            "content": "Yep, I can see an app with a thousand cached filters would have a problem with this impl as it stands. \n\nMaintaining parallel indexes always feels a little flaky to me, not least because of the loss of  transactional integrity you can get from using a single index.\n\nIs another approach to make your cached filters document-type-specific?   I.e. they only hold numbers in the range of zero to number-of-docs-of-this-type.\nTo use a cached doc ID in such a filter you would need to make use of mapping arrays to project the type-specific doc id numbers into global doc-id references and back.\nLets imagine an index with a mix of  \"A\", \"B\" and \"C\" doc types organised as follows:\ndocId    docType\n=====  =======\n1            A\n2            B\n3            C\n4            A\n5            C\n6            C\n\nThe mapping arrays for docType \"C\" would look as follows\nBar.java\nint [ ] globalDocIdToTypeCLookUp = {-1,-1,0,-1,1,2}        // sparse, sized 0-> num docs in overall index\nint [ ] typeCToGlobalDocIdLookUp = {0,1,2}          // dense, sized 0-> num type C docs in overall index\n\n\n\nYour cached filters would be created as follows:\nBar.java\nmyTypeCBitset=new OpenBitSet(numberOfTypeCDocs);  //this line is hopefully where you save RAM!\n//for all matching type C docs...\nmyTypeCBitSet.setBit(globalDocIdToTypeCLookUp[realDocId];\n\n\n\nYour filters can then be used by dereferencing the child doc IDs as follows:\nBar.java\nint nextRealDocId=typeCToGlobalDocIdLookUp [myTypeCBitSet.getNextSetBit()];\n\n\n\nClearly the mapping arrays come at a cost of 4bytes*num docs which is non trivial. The sparse globalDocIdToTypeCLookUp array shown here could be avoided by reading TermDocs and counting at cached-Filter-create time . ",
            "author": "Mark Harwood",
            "id": "comment-12878617"
        },
        {
            "date": "2010-06-14T20:58:30+0000",
            "content": "That's an interesting strategy.  The size of these arrays is no big deal to me since there's only a couple of them.  My concern with this strategy is that I wonder if potentially many places in Solr would have to be become aware of this scheme which might make this strategy untenable to implement even though its theoretically sound.\n\nAnother nice thing about the parallel index is that the idf relevancy factor stays clean since it will only consider \"real\" documents.\n\nI want to investigate these options closer ASAP since this feature you've implemented is something I need.  Before I saw this issue, I was going to try something with SpanNearQuery and the masking-field variant. ",
            "author": "David Smiley",
            "id": "comment-12878741"
        },
        {
            "date": "2010-06-14T22:36:39+0000",
            "content": "I wonder if potentially many places in Solr would have to be become aware of this scheme \n\nSupporting multiple doc types in this way can be a big modelling change. Not everyone needs it so I suspect that may be a hard sell to the Solr crowd.\n\nAnother nice thing about the parallel index is that the idf relevancy factor stays clean\n\nI have an \"IDF compensating\" wrapper query that takes care of that. It wraps a child query to then wrap the Similarity class in use and adjusts IDF calculations to be based on the number of documents of the required type. I'll attach it here when I get a chance.\n\nBefore I saw this issue, I was going to try something with SpanNearQuery and the masking-field variant.\n\nI went through a similar thought process around using position info to make this stuff work. This child-doc approach used here seems the cleanest by far. ",
            "author": "Mark Harwood",
            "id": "comment-12878782"
        },
        {
            "date": "2010-06-24T14:07:58+0000",
            "content": "This is amazing feature!\n\nCan this help in searching over multiple child/nested documents? Is there a sample code avaialble that demonstrates how to achieve this?\n\nWe have requirement wherein search result need to carry fields from child documents. Can this be achieved? ",
            "author": "Amit Kulkarni",
            "id": "comment-12882163"
        },
        {
            "date": "2010-06-26T23:12:29+0000",
            "content": "Can this help in searching over multiple child/nested documents?\n\nYes, a typical use case is to use \"NestedDocumentQuery\" to fetch the top 10 parents then do a second query to fetch the children using a mandatory clause which lists the primary keys of the selected parents (assuming the children have an indexed field with the parent primary key).\nThe \"PerParentLimitedQuery\" can be used to limit the number of child docs returned per parent if there are many e.g. pages in a book. Both these classes are in the zipped attachment to this issue. ",
            "author": "Mark Harwood",
            "id": "comment-12882899"
        },
        {
            "date": "2010-07-15T19:48:51+0000",
            "content": "I tried this solution and works perfectly for smaller indexes with (either less number of Documents or Document size is small) However for larger indexes that span across multiple segments it only matches the the parent document acurately for the 1st segment. I think this is due to the way the parent docs are marked using a bit array for the ENTIRE index but actual traversing for matching criteria done by the Scorer is segment-by-segment (i.e. in nextDoc() and advance() methods) .  Have you considered this situation? ",
            "author": "Buddika Gajapala",
            "id": "comment-12888896"
        },
        {
            "date": "2010-07-15T20:25:04+0000",
            "content": "The 2nd comment above talks about this and the need for Lucene to offer more control over flush policy.\n\nit only matches the the parent document acurately for the 1st segment. I think this is due to the way the parent docs are marked using a bit array for the ENTIRE index\n\nBut aren't filters held and evaluated the within the context of each sub reader? Are you sure the issue isn't limited to a parent/child combo that is split across segments?  ",
            "author": "Mark Harwood",
            "id": "comment-12888908"
        },
        {
            "date": "2010-07-16T09:03:14+0000",
            "content": "Attached Junit confirms issue with multiple segments (thanks, Buddika).\nPrevious tests masked the error.\nI'm looking into a fix now.\n\nCheers,\nMark ",
            "author": "Mark Harwood",
            "id": "comment-12889078"
        },
        {
            "date": "2010-07-16T09:15:22+0000",
            "content": "Maybe we should add an addDocuments call to IW?  To add more than one document, \"atomically\", so that any flush must happen before or after them? ",
            "author": "Michael McCandless",
            "id": "comment-12889088"
        },
        {
            "date": "2010-07-16T09:43:43+0000",
            "content": "Updated package with fix for multi-segment issue and improved Junit test ",
            "author": "Mark Harwood",
            "id": "comment-12889099"
        },
        {
            "date": "2010-07-16T09:48:03+0000",
            "content": "Maybe we should add an addDocuments call to IW? To add more than one document, \"atomically\", so that any flush must happen before or after them? \n\nThat would be nice. \nAnother way of modelling this would be to introduce Document.add(Document childDoc) but I think that is a more fundamental and wide-reaching change. ",
            "author": "Mark Harwood",
            "id": "comment-12889104"
        },
        {
            "date": "2010-07-16T15:46:59+0000",
            "content": "Mark, that was fast \n\nBTW another scenario, when there are lot of data, there is a posibility of having parent docuemnt and matching child document in two different segments causing to miss some matches. I made a minor modification your approch by making it do a \"Forward-scan\" instead of reverse scan for parent docs and have the parent document inserted AFTER the child docs are inserted and in case of parent doc is located outside the scop of current doc, it's docid is preserved at the \"Weight Object\" level and nextDoc() modified to check fo that for the very 1st nextDoc call to new segment. ",
            "author": "Buddika Gajapala",
            "id": "comment-12889215"
        },
        {
            "date": "2010-07-16T16:11:53+0000",
            "content": "I made a minor modification your approch by making it do a \"Forward-scan\" instead of reverse scan\n\nInteresting, but I'm not sure what guarantees Lucene will make about:\n\n\n\tSequencing of calls on scorer.nextDoc (i.e. are calls to all scorers involved guaranteed to be in doc-insert order ?)\n\tIndex-time merging of segments (i.e. are all segments merged together in an order that keeps the parent doc in one segment next to the child doc from the next segment?)\n\n\n\nThat seems like a fragile set of dependencies. \nAlso, don't things get tricky when reporting matches from NestedDocumentQuery and PerParentLimitedQuery back to the collector? During the query process the IndexSearcher resets the docId context (Collector.setNextReader) as it moves from one Scorer/segment to another. If we are delaying the assessment/reporting of matches until we've crossed a segment boundary it is too late to report a match on a child doc id from a previous segment as the collector has already changed context. Unfortunately \"PerParentLimitedQuery\" needs to do this when selecting the \"best\" children for a single parent.\n ",
            "author": "Mark Harwood",
            "id": "comment-12889217"
        },
        {
            "date": "2011-02-27T17:08:36+0000",
            "content": "How about an implementation for strict hierarchies that uses two fields per document, in the following way:\n\nThe two fields each contain a single (indexed) token that indicates the node in the nesting hierarchy, one field meaning that the document is a child of that node, and the other that the document is the representative of that node. Any number of levels could be allowed, but no cycles of course.\nThese fields are then used by a merge policy to keep the documents ordered postorder, that is the children immediately followed by the representative for each node.\nCollecting scores at any node in the hierarchy could then be done by using term filters, one for each involved scorer, to provide the representative for the current doc by advancing.\n\n\nFor example, in index order:\n\nuserDocId nodeMemberField nodeReprField\n\ndoc1 nodeA1 .\ndoc2 nodeA1 .\ndoc3 nodeA nodeA1\ndoc4 nodeA2 .\ndoc5 nodeA2 .\ndoc6 nodeA nodeA2\n\nThe node representatives for scoring could then be obtained by a term filter for nodeA.\n\n\nI think this could work for the scoring part, basically along the lines of the code already posted here.\n\nCould someone with more experience in segment merge policies comment on this? This is quite restrictive for merging as the only freedom that is left in the document order is the order of the children for each node.\n\nFor example, adding a leaf document doc7 for nodeA1 could result in the following index order:\n\ndoc4 nodeA2 .\ndoc5 nodeA2 .\ndoc6 nodeA nodeA2\ndoc7 nodeA1 .\ndoc1 nodeA1 .\ndoc2 nodeA1 .\ndoc3 nodeA nodeA1\n\n ",
            "author": "Paul Elschot",
            "id": "comment-13000000"
        },
        {
            "date": "2011-02-28T11:44:45+0000",
            "content": "Hi Paul,\nI'm not sure I currently have an issue with merges as they just concatenate established segments without interleaving their documents. This operation should retain the order that is crucial to maintaining the parent/child/grandchild relationships (unless something has changed in merge logic which would certainly be an issue!). My main cause for concern is robust control over flushes so parent/child docs don't end up being separated into different segments at the point of arbitrary flushes.\n\nI think your proposal here is related to a new (to me) use case where clients can add a single new \"child\" document and the index automagically reorganises to assemble all prior related documents back into a structure where they are grouped as contiguous documents held in the same segment? Please correct me if I am wrong.\nPreviously I have always seen this need for reorganisation as an application's responsibility and a single child document addition required the app to delete the associated parent and all old child docs, then add a new batch of documents representing the parent, old children plus the new child addition. Given the implied deletes and inserts required to maintain relationship integrity that seems like an operation that needs to be done under the control of Lucene's transaction management APIs rather than some form of special MergePolicy which are really intended for background efficiency tidy-ups not integrity maintenance.\n\nAs for the fields you outline for merging , generally speaking in applications using NestedDocumentQuery and PerParentLimitedQuery I have found that for searching purposes I already need to store: \n1) A globally unique ID as an indexed \"primary key\" field on the top-level container document\n2) An indexed field with the same unique ID held in a different \"foreign key\" field on child documents\n3) An indexed field indicating the document type e.g \"root\" or \"resume\" and \"level1Child\" or \"employmentRecord\"\n\n\nI could be a little confused about your intentions - maybe should we start with what problem we are trying to solve before addressing how we achieve it?\n\nCheers\nMark ",
            "author": "Mark Harwood",
            "id": "comment-13000239"
        },
        {
            "date": "2011-02-28T17:22:06+0000",
            "content": "I think your proposal here is related to a new (to me) use case where clients can add a single new \"child\" document and the index automagically reorganises to assemble all prior related documents back into a structure where they are grouped as contiguous documents held in the same segment?\n\nIndeed.\n\nThe first two fields match the ones I intended.\nThe third field for the document type would be quite useful for searching, but it may not be necessary to maintain the document order.\n\nThe intention is quite simple: allow a set of documents to be used to provide a single score value during query searching. AFAICT that fits most of the use cases described here.\n\nTo allow conjunctions inside such a set, it is necessary to advance() a scorer into a set, and for that it might be better to put the set representative before the children. The document order would then be pre-order instead of post-order, which would not really make any difference in difficulty to keep the docs in order.\nWith the representative before the children, an extra operation (sth like previousDocId()) would be needed on the iterator of the filter.\n\nI don't know about flushes during merging. One operation that would recur during index maintenance is appending a sequence of documents from one segment to another segment, see docs 1, 2 and 3 above.\nThis is indeed what needs to be done when a new child is added, or when an existing one is changed, i.e. deleted and added.\nI'm not familiar with the merging code, but I would suppose something very close to appending a sequence of documents from an existing segment is already available. Anyway this is costly, but that is the price to pay.\n\nDuring searching, the term filters used for the node representatives might use some optimizations. Since one term filter is needed for every document scorer involved in searching the query and these term filters are all based on the same term, they could share index information, for example in a filter cache.\nA bit set is not always optimal for such filters, perhaps a more tree like structure could be more compact and faster. But bit sets could be used to get this going.\n\nThe good news so far for me is that this seems to be feasible, thanks.\n ",
            "author": "Paul Elschot",
            "id": "comment-13000413"
        },
        {
            "date": "2011-02-28T18:15:58+0000",
            "content": "The intention is quite simple: allow a set of documents to be used to provide a single score value during query searching\n\nThat's what the existing NestedDocumentQuery code attached to this issue already provides. As far as I am concerned the search side works fine and I have it installed in several live installations (along with a bug fix for \"skip\" that I must remember to upload here). \"Parent\" filters as you suggest benefit from caching and I typically use the XMLQueryParser with a <CachedFilter> tag to take care of  that (I need to upload the XMLQueryParser extensions for this Nested stuff too).\n\nThe new \"intention\" that I think you added in your last post was more complex and is related to indexing, not searching and introduced the idea that adding a new child doc on its own should somehow trigger some automated repair of the index contents. This repair would involve ensuring that related documents from previous adds would be reorganised such that all related documents still remained physically next to each other in the same segment. \nI don't think a custom choice of \"MergePolicy\" is the class to perform this operation - they are simply consulted as an advisor to pick which segments are ripe for a background merge operation conducted elsewhere. The more complex merge task you need to be performed here requires selective deletes of related docs from existing segments and addition of the same documents back into a new segment. This is a task I have always considered something the application code should do rather than relying on Lucene to second-guess what index reorganisation may be required. We could try make core Lucene understand and support parent/child relationships more fully but I'd settle for this existing approach with some added app-control over flushing as a first step.\n\n\n ",
            "author": "Mark Harwood",
            "id": "comment-13000444"
        },
        {
            "date": "2011-02-28T20:37:18+0000",
            "content": "So the missing basic operation is a copy/append of a range of existing index docs.\nAfter that operation, the original docs can be deleted, but that is trivial.\n\nI'll have a look at IndexWriter for this over the coming days. Any quick hints? ",
            "author": "Paul Elschot",
            "id": "comment-13000503"
        },
        {
            "date": "2011-02-28T21:47:43+0000",
            "content": "I'm not sure the auto-repair is that trivial.\nLet's say the parent/child docs are resumes and nested docs for employment positions (as in the attached example).\nAn update may not just be adding a new \"employment position\" doc but editing an existing one, deleting an old one etc.\nYour auto-updater is going to need to do a lot of figuring out to work out which existing docs need copying over from earlier segments and patching in to the new segment with the updated parts of the resume. This gets worse if we start to consider multiple levels to the hierarchy.\nIt all feels like a lot of work for the IndexWriter to take on? ",
            "author": "Mark Harwood",
            "id": "comment-13000541"
        },
        {
            "date": "2011-02-28T23:06:38+0000",
            "content": "Updated version with fix for Scorer.advance issue and added XMLQueryBuilder support ",
            "author": "Mark Harwood",
            "id": "comment-13000594"
        },
        {
            "date": "2011-03-21T09:34:35+0000",
            "content": "Mark, do you have any plans for including this feature into the Lucene trunk?\nI think that this is a \"must have\" feature since tree structures are so common!\nThank you in advance. ",
            "author": "RynekMedyczny.pl",
            "id": "comment-13009071"
        },
        {
            "date": "2011-03-21T12:25:37+0000",
            "content": "Mark, do you have any plans for including this feature into the Lucene trunk?\n\nThat is my intention in providing it here. I had to work hard to convince my employer to let me release this as open source in the interests of seeing it updated/tested as core Lucene APIs change - and hopefully receive some improved support in IndexWriter \"flush\" control. \nUnfortunately it seems not everyone shares the pain when it comes to modelling richer data structures and seem content with the \"flat\" model we have in Lucene today. Code like this ends up in trunk when there is concensus so your support is welcome.\n\nWhile core Lucene adoption is a relatively simple technical task, Solr adoption feels like a much more disruptive change.\n ",
            "author": "Mark Harwood",
            "id": "comment-13009111"
        },
        {
            "date": "2011-03-21T14:33:36+0000",
            "content": "Mark,\n\nFor my project this is a must have feature that could decide the adoption of SOLR.  What do think is the best way to help ensure this gets incorporated into SOLR?\n\nThank you,\nJamal ",
            "author": "Jamal Natour",
            "id": "comment-13009141"
        },
        {
            "date": "2011-03-21T15:12:10+0000",
            "content": "Lucene does not dictate a schema and so using this approach to index design/querying is not a problem with base Lucene.\n\nSolr, however does introduce a schema and much more that assumes a \"flat\" model.  In the opening chapters of the \"Solr 1.4 Enterprise Search Server\" book the authors take the time to discuss the modelling limitations of this flat model and acknowledge this as an issue. The impact of adopting \"nested\" documents in Solr at this stage would be very large.\nThere may be ways you can overcome some of your issues without requiring nested documents (using phrase/span queries or combining tokens from multiple fields in Solr)  but in my experience these are often poor alternatives if richer structures are important.\n\nCheers\nMark ",
            "author": "Mark Harwood",
            "id": "comment-13009163"
        },
        {
            "date": "2011-03-23T03:50:21+0000",
            "content": "Solr, however does introduce a schema and much more that assumes a \"flat\" model.\n\nIn SOLR-1566 we could add a DocList as a field within a SolrDocument \u2013 this would at least allow the output format to return a nested structure.\n\nI have not looked this patch so this comment may be off base. ",
            "author": "Ryan McKinley",
            "id": "comment-13009985"
        },
        {
            "date": "2011-03-23T13:20:32+0000",
            "content": "I have not looked this patch so this comment may be off base.\n\nThe slideshare deck gives a good overview: http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene\n\nAs a simple Lucene-focused addition I'd prefer not to explore all the possible implications for Solr adoption here. The affected areas in Solr are extensive and would include schema definitions, query syntax, facets/filter caching, result-fetching, DIH etc etc. Probably best discussed elsewhere.\n ",
            "author": "Mark Harwood",
            "id": "comment-13010110"
        },
        {
            "date": "2011-03-29T14:20:07+0000",
            "content": "\nCode like this ends up in trunk when there is concensus so your support is welcome.\n\nOf course! How can we help you?\n\n\nWhile core Lucene adoption is a relatively simple technical task\n\nWe are eagerly waiting for incorporating your work into Lucene Core! ",
            "author": "RynekMedyczny.pl",
            "id": "comment-13012501"
        },
        {
            "date": "2011-05-17T10:52:47+0000",
            "content": "I think this is a very important addition to Lucene, so let's get this\ndone!\n\nI just opened LUCENE-3112, to add IW.add/updateDocuments, which would\natomically add Document produced by an iterator, and ensure they all\nwind up in the same segment.  I think this is the only core change\nnecessary for this feature?  Ie, all else can be built on top of Lucene\nonce LUCENE-3112 is committed? ",
            "author": "Michael McCandless",
            "id": "comment-13034702"
        },
        {
            "date": "2011-05-17T11:33:48+0000",
            "content": "I think this is the only core change necessary for this feature?\n\nYup. A same-segment indexing guarantee is all that is required. ",
            "author": "Mark Harwood",
            "id": "comment-13034726"
        },
        {
            "date": "2011-05-23T16:23:30+0000",
            "content": "Attached patch, pulling out just NestedDocumentQuery from the zip\nfile:\n\n\n\tI put NestedDocumentQuery and its test case in contrib/queries,\n    under oal.search.nested\n\n\n\n\n\tChanged scoreMode to final int that's passed to ctor\n\n\n\n\n\tBrought everything up to current trunk (scoring is per-segment,\n    etc.).this also moves parentBits (previously mutable on the\n    original query) into the scorer\n\n\n\n\n\tSimplified the test case \u2013 instead of parsing XHTML resumes it\n    just directly builds up a toy example of parent+child docs.\n\n\n\n\n\tRemoved @author tags, fixed code style (indent to 2 spaces, {}s,\n    newlines, etc.), etc.\n\n\n\nI put a few nocommits in that still need resolving...\n\nMy patch doesn't include XMLQueryParser integration; I think we should\ndo this as separate issue?\n\nI think PerParentLimitedQuery is the same functionality as grouping,\nright?  Except, it only supports relevance withinGroup sort, but it\noperates only w/ a parent filter (whereas our grouping impls so far\nrequire a more-RAM-costly DocTermsIndex FC entry of the field you are\ngrouping on).  I think we should somehow merge this w/ the single pass\ngrouping collector (LUCENE-3129)? ",
            "author": "Michael McCandless",
            "id": "comment-13038022"
        },
        {
            "date": "2011-05-23T16:26:25+0000",
            "content": "One idea: maybe the parent doc should be indexed last, in the block,\ninstead of first?  Ie, so that NestedDocumentQuery.getParentDoc\ncould use obs.nextSetBit()? ",
            "author": "Michael McCandless",
            "id": "comment-13038024"
        },
        {
            "date": "2011-05-24T06:36:45+0000",
            "content": "Hi.\n\nGreat feature...\nI have some difficulties understanding the semantics/flow of document creation.\nDo you have to add the parent and child levels in any correct sequence? Or can you insert all parents and then insert all child levels later.\nThe reason I as is that in my case I look for a one->many relation style insertion. I had hoped that I could add more child levels later.\n\nCheers ",
            "author": "Thomas Guttesen",
            "id": "comment-13038415"
        },
        {
            "date": "2011-05-24T08:59:44+0000",
            "content": "Thanks for the patch work, Mike. I'll need to check LUCENE-3129 for equivalence with PerParentLimitQuery. It's certainly a central part of what I typically deploy for nested queries - pass 1 is usually a NestedDocumentQuery to get the best parents and pass 2 uses PerParentLimitQuery to get the best children for these best parents. Of course some apps can simply fetch ALL children for the top parents but in some cases summarising children is required (note: this is potentially a great solution for performance issues on highlighting big docs e.g. entire books).\n\nI haven't benchmarked nextSetBit vs the existing \"rewind\" implementation but I imagine it may be quicker. Parent- followed-by-children seems more natural from a user's point of view however. I guess you could always keep the parent-then-child insertion order but flip the bitset (then cache) for query execution if that was faster. Benchmarking rewind vs nextSetbit vs flip then nextSetBit would reveal all.\n\nThomas - maintaining a strict order of parent/child docs is important and the recently-committed LUCENE-3112 should help with this. ",
            "author": "Mark Harwood",
            "id": "comment-13038460"
        },
        {
            "date": "2011-05-26T10:15:57+0000",
            "content": "I'll need to check LUCENE-3129 for equivalence with PerParentLimitQuery. It's certainly a central part of what I typically deploy for nested queries - pass 1 is usually a NestedDocumentQuery to get the best parents and pass 2 uses PerParentLimitQuery to get the best children for these best parents.\n\nHmm, so I wonder if we could do this in one pass?  Ie, like grouping,\nif you indexed your docs as blocks, you can use the faster single-pass\ncollector; but if you didn't, you can use the more general but slower\nand more-RAM-consuming two pass collector.\n\nIt seems like we should be able to do something similar with joins,\nsomehow... ie Solr's join impl is a start at the \"fully general\"\ntwo-pass solution.\n\nBut I agree the \"join child to parent\" and then \"grouping of child\ndocs\" go hand in hand for searching...\n\nWhat do you do for facet counting in these apps...?  Post-grouping\nfaceting also ties in here.\n\nOf course some apps can simply fetch ALL children for the top parents but in some cases summarising children is required\n\nRight...\n\n(note: this is potentially a great solution for performance issues on highlighting big docs e.g. entire books).\n\nI think it'd be compelling to index book/articles with each\npage/section/chapter being a new doc, and then group them under their\nbook/article.\n\nI haven't benchmarked nextSetBit vs the existing \"rewind\" implementation but I imagine it may be quicker.\n\nI think it should be much faster \u2013 obs.nextSetBit looks heavily\noptimized, since it can operate a word at a time.  Though, if the\ngroups are smallish, so that nextSetBit is often maybe 2 or 3 bits\naway, I'm not sure it'd be faster...\n\nParent- followed-by-children seems more natural from a user's point of view however.\n\nBut is it really so bad to ask the app to put parent doc last?\n\nI mean, the docs have to be indexed w/ the new doc block APIs in IW\nanyway, which will often be eg a List<Document>, at which point\nputting parent last seems a minor imposition?\n\nSince this is an expert API I think it's OK to put [minor] impositions\non its usage if this can simplify the impl / make it faster / less\nrisky.  That said, I'm not yet sure on the impl (single pass query +\ncollector vs generic two-pass join that solr now has), so it's\nprobably premature to worry about this...\n\nI guess you could always keep the parent-then-child insertion order but flip the bitset (then cache) for query execution if that was faster.\n\nTrue but this adds some hair into the impl (we must also \"flip\" coming\nback from nextSetBit)...\n\nBenchmarking rewind vs nextSetbit vs flip then nextSetBit would reveal all.\n\nTrue, though it'd be best to do this in the context of the actual join impl... ",
            "author": "Michael McCandless",
            "id": "comment-13039623"
        },
        {
            "date": "2011-05-26T10:56:09+0000",
            "content": "I see no test cases for required terms in a nested document.\nThis may be non trivial in that advance() should advance into the first doc of the nested doc.\nFor example, assume the parents p1 and p2 are the first docs in the nested docs, and that the query\nrequires a and b to be present:\n\ndocId\n0   p1\n1   a\n2   b\n3   p2\n4   b\n5   a\n\n\nIn this situation, p2 may be missed when advance() on a required scorer for \"b\" is given docId 5 (containing \"a\")\nas a target. It should be given target docId 3 to advance into the nested doc p2 containing \"a\".\n\nI quickly read the code here, but I could not easily determine whether this is done correctly or not.\nShall I add a test case here, or would it be better to open another issue after this one is closed, or can someone reassure me that this is not in an issue?\n ",
            "author": "Paul Elschot",
            "id": "comment-13039641"
        },
        {
            "date": "2011-05-26T15:16:31+0000",
            "content": "Mike - having thought about it moving the per-parent grouping logic to a collector removes some of the functionality I am used to.\nI often use multiple PerParentLimitedQuery clauses in the same query to limit different aspects of the structures I query e.g. for each best parent document I may want the best 5 children of type A and the best 10 children of type B. Type \"A\" docs are of a different size and shape to type \"B\" - hence the need for different limits. I realise this can be done with multiple searches using different collectors each time but I use distributed servers that service all XML-based queries the same way. In these sorts of environments it is desirable to maintain a common query API (e.g. the XML syntax) and not introduce the need for special collectors to service different types of queries. \n\nPaul, not sure on the exact details of your query (a parent with a child containing A and B or a parent with a child containing A and a potentially different child containing B?) I suspect a test case would help.\n ",
            "author": "Mark Harwood",
            "id": "comment-13039733"
        },
        {
            "date": "2011-05-26T18:14:35+0000",
            "content": "Mark, this may occur when one child contains A and another contains B, and at least one scorer (the one for A and/or the one for B) uses advance() to get to the nested doc.\nI'll add a test case here, but that could take a few days. ",
            "author": "Paul Elschot",
            "id": "comment-13039848"
        },
        {
            "date": "2011-06-04T16:20:55+0000",
            "content": "OK I opened LUCENE-3171 to explore the single-pass approach. ",
            "author": "Michael McCandless",
            "id": "comment-13044326"
        },
        {
            "date": "2011-06-05T19:14:33+0000",
            "content": "I finally had some time to start taking a look at the grouping module and again at the patch here.\nThere is too much code there for me to come up with a test case soon.\nSo please don't wait for me to commit this.\n\nAn easy way to test this would be to have a boolean query with required term and an optional term,\nwith the optional term occurring in a document group in a document before (i.e. with a lower docId than)\na document in the same group with a required term.\n\nIn case I run into this I'll open a separate issue. ",
            "author": "Paul Elschot",
            "id": "comment-13044608"
        },
        {
            "date": "2011-06-06T13:05:37+0000",
            "content": "Below are 2 example tests searching employment resumes - both using the same optional and mandatory clauses but in subtly different ways.\nQuestion 1 is \"who has Mahout skills and preferably used them at Lucid?\" while the other question is \"who has Mahout skills and preferably has been employed by Lucid?\". The questions and the answers are different. Below is the XML test script I used to illustrate the data/queries used, define expected results and run as an executable test. \nHopefully you can make sense of this:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"test.xsl\"?>\n<Test description=\"NestedQuery tests\">\n\t<Data>\n\t\t<Index name=\"ResumeIndex\">\n\t\t\t<Analyzers class=\"org.apache.lucene.analysis.WhitespaceAnalyzer\">\n\t\t\t</Analyzers>\n\t\t\t<Shard name=\"shard1\">\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t<Document pk=\"1\">\n\t\t\t\t\t<Field name=\"name\">grant</Field>\n\t\t\t\t\t<Field name=\"docType\">resume</Field>\n\t\t\t\t</Document>\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t\t\t<Document pk=\"2\">\n\t\t\t\t\t\t\t<Field name=\"employer\">lucid</Field>\n\t\t\t\t\t\t\t<Field name=\"docType\">employment</Field>\n\t\t\t\t\t\t\t<Field name=\"skills\">java lucene</Field>\n\t\t\t\t\t\t</Document>\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t\t\t<Document pk=\"3\">\n\t\t\t\t\t\t\t<Field name=\"employer\">somewhere else</Field>\n\t\t\t\t\t\t\t<Field name=\"docType\">employment</Field>\n\t\t\t\t\t\t\t<Field name=\"skills\">mahout and more mahout</Field>\n\t\t\t\t\t\t</Document>\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t<Document pk=\"4\">\n\t\t\t\t\t<Field name=\"name\">sean</Field>\n\t\t\t\t\t<Field name=\"docType\">resume</Field>\n\t\t\t\t</Document>\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t\t\t<Document pk=\"5\">\n\t\t\t\t\t\t\t<Field name=\"employer\">foo bar</Field>\n\t\t\t\t\t\t\t<Field name=\"docType\">employment</Field>\n\t\t\t\t\t\t\t<Field name=\"skills\">java</Field>\n\t\t\t\t\t\t</Document>\n\t\t\t\t<!--  =============================================================== -->\n\t\t\t\t\t\t<Document pk=\"6\">\n\t\t\t\t\t\t\t<Field name=\"employer\">some co</Field>\n\t\t\t\t\t\t\t<Field name=\"docType\">employment</Field>\n\t\t\t\t\t\t\t<Field name=\"skills\">mahout mahout and more mahout</Field>\n\t\t\t\t\t\t</Document>\n\t\t\t</Shard>\n\t\t</Index>\n\t</Data>\n\t<Tests>\n\t\t<Test description=\"Who knows Mahout and preferably used it *while employed at Lucid*?\">\n\t\t\t<Query>\n\t            <NestedQuery> \n\t            \t<!-- testing properties of individual child employment docs -->\n\t               <Query>\n\t                  <BooleanQuery>\n\t                  \t\t<Clause occurs=\"must\">\n\t                  \t\t\t<TermsQuery fieldName=\"skills\">mahout</TermsQuery>\n\t                  \t\t</Clause>\n\t                  \t\t<Clause occurs=\"should\">\n\t                  \t\t\t<TermsQuery fieldName=\"employer\">lucid</TermsQuery>\n\t                  \t\t</Clause>\n\t                  </BooleanQuery>\n\t               </Query>\n\t               <ParentsFilter>\t\n\t                    <TermsFilter fieldName=\"docType\">resume</TermsFilter>                  \t\t \n\t               </ParentsFilter>\t\n\t            </NestedQuery>\n\t\t\t</Query>\n\t\t\t<ExpectedResults why=\"Grant's tenure at Lucid is overlooked for scoring purposes \n\t\t\t                       because it did not involve the required Mahout. Sean has more Mahout experience\">\n\t\t\t\t\t\t\t<Result fieldName=\"pk\">4</Result>\n\t\t\t\t\t\t\t<Result fieldName=\"pk\">1</Result>\n\t\t\t</ExpectedResults>\n\t\t</Test>\n\n\t\t<!-- ==================================================================================== -->\n\t\t\n\t\t<Test description=\"Different question - who knows Mahout and preferably has been employed by Lucid?\">\n\t\t\t<Query>\n                <BooleanQuery>\n                  \t\t<Clause occurs=\"must\">\n\t\t\t\t            <NestedQuery> \n\t\t\t\t            \t<!-- testing properties of one child employment docs -->\n\t\t\t\t               <Query>\n\t\t\t\t                  \t<TermsQuery fieldName=\"skills\">mahout</TermsQuery>\n\t\t\t\t               </Query>\n\t\t\t\t               <ParentsFilter>\t\n\t\t\t\t                    <TermsFilter fieldName=\"docType\">resume</TermsFilter>                  \t\t \n\t\t\t\t               </ParentsFilter>\t\n\t\t\t\t            </NestedQuery>\n                  \t\t</Clause>\n                  \t\t<Clause occurs=\"should\">\n\t\t\t\t            \t<!-- Another NestedQuery testing properties of *potentially different* child employment docs -->\n\t\t\t\t            <NestedQuery> \n\t\t\t\t               <Query>\n\t\t                  \t\t\t<TermsQuery fieldName=\"employer\">lucid</TermsQuery>\n\t\t\t\t               </Query>\n\t\t\t\t               <ParentsFilter>\t\n\t\t\t\t                    <TermsFilter fieldName=\"docType\">resume</TermsFilter>                  \t\t \n\t\t\t\t               </ParentsFilter>\t\n\t\t\t\t            </NestedQuery>\n                  \t\t</Clause>\n                  \t</BooleanQuery>\n\t\t\t</Query>\n\t\t\t<ExpectedResults why=\"Grant has the required Mahout skills plus the optional Lucid engagement\">\n\t\t\t\t\t\t\t<Result fieldName=\"pk\">1</Result>\n\t\t\t\t\t\t\t<Result fieldName=\"pk\">4</Result>\n\t\t\t</ExpectedResults>\n\t\t</Test>\n\t\t<!-- ==================================================================================== -->\n\t</Tests>\n</Test>\n\n\t ",
            "author": "Mark Harwood",
            "id": "comment-13044828"
        },
        {
            "date": "2011-06-07T08:06:30+0000",
            "content": "That is very nicely readable XML.\n\nThe problem might occur when a document with an optional term occurs before a document in the same group with a required term.\nSo the second question is the one for which the problem might occur.\nThe score value Grant's resume should then be higher than the score value for Sean's.\nTesting only for the set of expected results is not enough for this particular query.\n\nThe problem might occur in another disguise when requiring both terms and then the set of expected results is enough to test,\nbut this is not as easily tested because one does not know beforehand the order in which the terms are going to be advance()d.\nThe case with an optional term is simpler to test because the optional term is certain to be advance()d to compute the score value after the required term determines that there is a match (see ReqOptSumScorer.score()), and then to be certain of the correct advance() on the optional term one needs to test the score value. ",
            "author": "Paul Elschot",
            "id": "comment-13045314"
        },
        {
            "date": "2011-06-07T08:21:07+0000",
            "content": "Looking at the structure of the BooleanQuery, I would expect this to work correctly.  The ParentsFilter on the unfiltered scorer of required term (\"mahout\") should return the docId of the parent (\"resume\") when the unfiltered scorer is at the document containing the required term. ",
            "author": "Paul Elschot",
            "id": "comment-13045319"
        },
        {
            "date": "2011-06-07T09:23:23+0000",
            "content": "Looking at the structure of the BooleanQuery, I would expect this to work correctly.\n\nI've found it to be robust so far - you just need to be clear about directing criteria at only one child or potentially different children. \nThe main challenge in using this functionality is allowing users to articulate the nuances of such queries and Lucene-3133 is a holding place for this.\n\nUnder the covers using the same cached filter for parent filters certainly helps with performance and I typically wrap the ParentFilter tag in the XML queries with a <CachedFilter> tag to achieve this ",
            "author": "Mark Harwood",
            "id": "comment-13045334"
        },
        {
            "date": "2011-06-07T18:08:22+0000",
            "content": "So one concern that is left is performance for parent testing.\nI'll open an issue for OpenBitSet.prevSetBit(), LUCENE-3179 ",
            "author": "Paul Elschot",
            "id": "comment-13045561"
        },
        {
            "date": "2011-06-18T09:26:48+0000",
            "content": "Tried the current patch here to make use prevSetBit, but ran into a problem with the query weight that could be related to LUCENE-3208.\n\nWhen fixing the patch here so that NestedDocumentQuery.java looks like this:\n\n  public Weight createWeight(IndexSearcher searcher) throws IOException {\n    return new NestedDocumentQueryWeight(childQuery.createWeight(searcher));\n  }\n\n\n\nthe TestNestedDocumentQuery from the patch here fails with an UnsupportedOperationException.\n\nAfter adding the class name to Query.java constructing this exception the test fails by:\n\nUnsupportedOperationException: org.apache.lucene.search.NumericRangeQuery\n\nThat means that probably the above fix to the patch is wrong.\nAny comments on how to continue this?\n ",
            "author": "Paul Elschot",
            "id": "comment-13051484"
        },
        {
            "date": "2011-06-18T09:35:00+0000",
            "content": "I suspect the NestedDocumentQuery must impl rewrite, and rewrite the childQuery.  I hit this on LUCENE-3171, too. ",
            "author": "Michael McCandless",
            "id": "comment-13051486"
        },
        {
            "date": "2011-06-18T10:41:38+0000",
            "content": "NestedDocumentQuery already implements rewrite() by returning this, just as in 3171.\n\nThis is a more complete traceback of the exception:\n\n\n    [junit] java.lang.UnsupportedOperationException: org.apache.lucene.search.NumericRangeQuery\n    [junit] \tat org.apache.lucene.search.Query.createWeight(Query.java:91)\n    [junit] \tat org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:177)\n    [junit] \tat org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:358)\n    [junit] \tat org.apache.lucene.search.nested.NestedDocumentQuery.createWeight(NestedDocumentQuery.java:65)\n    [junit] \tat org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:177)\n    [junit] \tat org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:358)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:676)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:292)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\n    [junit] \tat org.apache.lucene.search.TestNestedDocumentQuery.testSimple(TestNestedDocumentQuery.java:92)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1414)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1332)\n\n\n\nCould BooleanWeight be the offendor?\n\nOr should NestedDocumentQuery rewrite by rewriting its child, as you suggested?\nBut rewrite to what then?\n ",
            "author": "Paul Elschot",
            "id": "comment-13051495"
        },
        {
            "date": "2011-06-18T12:48:12+0000",
            "content": "NestedDocumentQuery already implements rewrite() by returning this, just as in 3171.\n\nI put another patch up on LUCENE-3171 earlier, fixing this...\n\nMaybe we should impl MTQ.createWeight, to throw a \"you forgot to rewrite me first\" exception? ",
            "author": "Michael McCandless",
            "id": "comment-13051508"
        },
        {
            "date": "2011-06-18T22:33:28+0000",
            "content": "At Query, the javadocs of both createWeight() and rewrite() start with a word of warning.\nI'll probably need at least a few days to wrap my head around it, so in case anyone meanwhile can provide more help... ",
            "author": "Paul Elschot",
            "id": "comment-13051611"
        },
        {
            "date": "2011-06-19T09:41:44+0000",
            "content": "With these rewrite and createWeight methods TestNestedDocumentQuery passes:\n\n\n+  @Override\n+  public Query rewrite(IndexReader reader) throws IOException {\n+    Query rewrittenChildQuery = childQuery.rewrite(reader);\n+    return (rewrittenChildQuery == childQuery) ? this\n+      : new NestedDocumentQuery(rewrittenChildQuery, parentsFilter, scoreMode);\n+  }\n+\n+  @Override\n+  public Weight createWeight(IndexSearcher searcher) throws IOException {\n+    return new NestedDocumentQueryWeight(childQuery.createWeight(searcher));\n+  }\n+\n\n\n\nI'll continue adding the use of prevSetBit.\n\nWould modules/grouping meanwhile be a better place for this than lucene/contrib/queries?\n\n ",
            "author": "Paul Elschot",
            "id": "comment-13051662"
        },
        {
            "date": "2011-06-19T11:26:08+0000",
            "content": "Patch of 19 June 2011.\n\nIn NestedDocumentQuery.java:\nAdded use of prevSetBit with an assert for the presence of a parent.\nAdded/changed rewrite/createWeight.\nAlso added equals/hashCode.\n\nTestNestedDocument.java is unchanged in this patch and passes.\n\nIn the patch both java files are in lucene/contrib/queries.\n\nThere is still one nocommit for the enum for the score mode. ",
            "author": "Paul Elschot",
            "id": "comment-13051671"
        },
        {
            "date": "2011-06-19T11:37:04+0000",
            "content": "The assert on the parent was an IllegalArgumentException in the previous patch.\nSuch and unconditional exception would probably be better than an assert, because when the assert is switched off a mistake in the parent filter would not be detected early. ",
            "author": "Paul Elschot",
            "id": "comment-13051673"
        },
        {
            "date": "2011-06-20T20:07:56+0000",
            "content": "Would modules/grouping meanwhile be a better place for this than lucene/contrib/queries?\n\nI think modules/join is the right place?  When we factor out Solr's\ngeneric join impl it can go there too...\n\nI have some concerns about the current approach here (this is why I\nopened LUCENE-3171):\n\n\n\tprevSetBit is called for each child doc, which is an O(N^2) cost\n    (N = number of child docs for one parent) I think?  Admittedly,\n    \"typically\" N is probably small...\n\n\n\n\n\tIt uses 2 passes if you also want to collect child docs per\n    parent\n\n\n\n\n\tPerParentLimitedQuery is also O(N^2) cost, both on insert of a new\n    child and on popping the child docs per group: I think it should\n    use a PQ to find the lowest child to evict per parent doc?\n\n\n\n\n\tI think \"typically\" an app will want to collect the top N groups\n    (parent docs and their children), so it's more efficient to gather\n    those top N and only in the end sort the each set of children\n    per-parent?  (This is similar to how 2nd pass grouping collector\n    works).\n\n\n\n\n\tPerParentLimitedQuery only supports relevance sort w/in each\n    parent.\n\n\n\n\n\tYou don't get the parent/child structure back, from\n    PerParentLimitedQuery (but now we have TopGroups which is a great\n    match for representing each parent and its children).\n\n\n\nIf you always only use PerParentLimitedQuery on the top parents from\nthe first pass, eg you AND/filter it against those parent docs, then\nthe O(N^2) cost is less severe since it'll have a small constant in\nfront, but since it's a Query I imagine users will use it w/o that\nfilter, which is bad... I think using a TopN Collector is a better match\nhere. ",
            "author": "Michael McCandless",
            "id": "comment-13052194"
        },
        {
            "date": "2011-06-20T21:10:24+0000",
            "content": "prevSetBit is called for each child doc\n\nYou could call nextSetBit on the first child to know the \"safe\" range of child docs attributable to the same parent but you would be taking a gamble that this was worth the call i.e. there were many possible children per parent to be tested.\n\nIt uses 2 passes if you also want to collect child docs per parent\n\nI tend to work with distributed indexes so it involves a 2 pass op anyway - one to understand best parents across the multiple shards first then the perparentlimitedquery to ensure we only pay the retrieve costs for those parents that make the final cut.\n\nI think it should use a PQ to find the lowest child to evict per parent doc?\n\nCareful object reuse would need to be factored in to avoid excessive GC - each parent would fill a PQ full of child-match object instances that could/should be reused in assessing the next parent\n ",
            "author": "Mark Harwood",
            "id": "comment-13052223"
        },
        {
            "date": "2011-06-21T07:46:15+0000",
            "content": "This overlaps with the BlockJoinQuery of LUCENE-3171, this issue might even be closed as duplicate of that one. Which one is preferred?\n\nOn using prev/nextSetBit in a safe range, this safe range starts with the parent and ends with the largest known child. A variant of prevSetBit could take this largest known child as an argument to limit its search, and then from the return value one has either a new parent, or one is certain that the current parent is the right one. This would also limit the worst case number of inspected bits for the group to the group size.\n\nWith or without that variant, I think it would be good to add a remark in the javadocs about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.\n ",
            "author": "Paul Elschot",
            "id": "comment-13052409"
        },
        {
            "date": "2011-06-21T08:52:59+0000",
            "content": "This overlaps with the BlockJoinQuery of LUCENE-3171, this issue might even be closed as duplicate of that one. Which one is preferred?\n\nWe need to look at the likely use cases. 2454 was created to service a use case which I expect to be a very common pattern and I'm not sure if LUCENE-3171 satisfies this need. Apps commonly need to return a selection of both matching and non-matching children along with the \"best\" parents. Why? - it's a very similar rationale to the way that highlighting returns a summary of text - it doesn't just return the matched words, it also returns surrounding text as useful context when displaying results to users. However, some texts can be very large and there's a need to limit what context is brought back.\nIf we apply this logic to 2454 we can see that for the top parents it is common to also want some non-matching children (e.g. for a resume return a person's employment history - not just the employments that matched the original search) but it is also necessary to summarize some parent's history (e.g. the contractor who listed a gazillion positions in his employment history needs summarising). A common pattern is for solutions to ask for the best 11 children for the best parents and display only 10 - that way the app knows that for certain parents there is more data available (i.e. those with 11 matches) and can offer a \"more\" button to retrieve the extra children for parents of interest. 2454 satisfies this use case as follows:\n\n\tUse a NestedDocumentQuery to get best parents with child criteria expressed as a \"must\"\n\tUse a PerParentLimitedQuery to get a selection of children per top parent where MUST belong to a top parent (tested using primary key) and use the child criteria again but this time as a \"SHOULD\" clause to relevance rank the selection of children returned\n\n\n\nIt's worth considering this sort of use case carefully before making any code decisions.\n ",
            "author": "Mark Harwood",
            "id": "comment-13052436"
        },
        {
            "date": "2011-06-21T10:32:57+0000",
            "content": "\nIt uses 2 passes if you also want to collect child docs per parent\n\nI tend to work with distributed indexes so it involves a 2 pass op anyway - one to understand best parents across the multiple shards first then the perparentlimitedquery to ensure we only pay the retrieve costs for those parents that make the final cut.\n\nThe distributed case can still be done single pass, using LUCENE-3171,\nie each shard returns the top groups and then they are merged in the\nfront.  This should be substantially faster than doing a 2nd pass out\nto all shards.\n\nAlso, we now have TopDocs.merge/TopGroups.merge to support this use\ncase.\n\nThis overlaps with the BlockJoinQuery of LUCENE-3171, this issue might even be closed as duplicate of that one. Which one is preferred?\n\nI think they are likely dups of one another and I agree we need to\nmake sure all important use cases are covered.\n\nApps commonly need to return a selection of both matching and non-matching children along with the \"best\" parents.\n\nLUCENE-3171 can do this as well, with the same approach as here, ie\ndoing 2 passes with two different child queries.\n\nHowever, I think for both this issue and for LUCENE-3171, this means\neach child doc must have the parent's PK indexed against it, right?\nIe, for that 2nd query you need some way to return all child docs\nunder any of the top parents, so the child query is \"parentID MUST be\nin XX, YY, ZZ\" and \"childDoc SHOULD XYZ\".\n\nIn fact, we could make this a single pass capability with LUCENE-3171\nand without requireing each child doc index its parent PK, ie also\npull & sort all other non-matching children under any top parent,\nbecause collction within each parent is done when you retrieve the\nTopGroups, but this can be a later enhancement. ",
            "author": "Michael McCandless",
            "id": "comment-13052459"
        },
        {
            "date": "2011-06-21T16:20:02+0000",
            "content": "A variant of prevSetBit could take this largest known child as an argument to limit its search,\n\nI think we should not require the app to \"know\" the max number of children per parent?  (Ie, we should just grow buffers, etc., on demand as we collect).\n\nI mean, if this information is easily available we could optimize for that case, but for some apps it's a good amount of work to record this and update it so I don't think it should be a required arg when creating the query/collectors, even though it's tempting  ",
            "author": "Michael McCandless",
            "id": "comment-13052644"
        },
        {
            "date": "2011-06-21T16:34:09+0000",
            "content": "A common pattern is for solutions to ask for the best 11 children for the best parents and display only 10 - that way the app knows that for certain parents there is more data available (i.e. those with 11 matches) and can offer a \"more\" button to retrieve the extra children for parents of interest\n\nWith LUCENE-3171, you should be able to just ask for 10 here, and then check if the TopDocs.totalHits is > 10 to decide whether to offer the \"more\" button. ",
            "author": "Michael McCandless",
            "id": "comment-13052648"
        },
        {
            "date": "2011-06-21T16:53:38+0000",
            "content": "Sounds encouraging. I think the only thing 3171 may be missing from my original use cases then is that I can use multiple PerParentLimitedQueries in one query to get a limit of children of different types e.g. for each parent resume, max 10 results from \"employment detail\" children and max 10 results from \"education background\" children.\nNot sure that behaviour is possible in the collector-based approach? ",
            "author": "Mark Harwood",
            "id": "comment-13052657"
        },
        {
            "date": "2011-06-21T17:39:36+0000",
            "content": "I think the only thing 3171 may be missing from my original use cases then is that I can use multiple PerParentLimitedQueries in one query to get a limit of children of different types e.g. for each parent resume, max 10 results from \"employment detail\" children and max 10 results from \"education background\" children.\n\nI think LUCENE-3171 can handle this, or something very similar: the\ncollector tracks all of the BlockJoinQuerys involved in the top query.\n\nSo, you'd have 1 BJQ matching \"employment detail\" child docs and\nanother matching \"education bg\" child docs.  The BJC collects the\ntop parent docs, then you can retrieve separate TopGroups for each\nBJQ.\n\nIn the end you have a TopGroups for the \"employment detail\" child docs\nand another TopGroups for the \"education bg\" child docs.\n\nCould that work for your use case? ",
            "author": "Michael McCandless",
            "id": "comment-13052696"
        },
        {
            "date": "2011-06-22T09:17:56+0000",
            "content": "Could that work for your use case?\n\nSounds like it, that's great \nDo you think there any efficiencies to be gained on the document retrieve side of things if you know that the documents commonly being retrieved are physically nearby i.e. an app will often retrieve a parent's fields and then those from child docs which are required to be physically located adjacent to the parent's data. Would existing lower-level caching in Directory or the OS mean there's already a good chance of finding child data in cached blocks or could a change to file structures and/or doc retrieve APIs radically boost parent-plus-child retrieve performance?\n ",
            "author": "Mark Harwood",
            "id": "comment-13053142"
        },
        {
            "date": "2011-06-23T05:33:21+0000",
            "content": "This is exactly what I am looking for, hope this becomes part of core.\n\nHow to make this work with Lucene 3.2? I downloaded the zip file and I was able to run the test with lucene 3.0, but I would like to use the addDocuments() method added to Lucene 3.2. The patches seems to be specific to Lucene 4.0. ",
            "author": "Srinivas Raj",
            "id": "comment-13053663"
        },
        {
            "date": "2011-06-29T22:52:15+0000",
            "content": "Duplicate of LUCENE-3171. ",
            "author": "Michael McCandless",
            "id": "comment-13057535"
        },
        {
            "date": "2011-06-29T22:54:59+0000",
            "content": "Do you think there any efficiencies to be gained on the document retrieve side of things if you know that the documents commonly being retrieved are physically nearby\n\nGood question!  I think OS level caching should mostly solve this? ",
            "author": "Michael McCandless",
            "id": "comment-13057538"
        }
    ]
}