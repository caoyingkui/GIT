{
    "id": "LUCENE-579",
    "title": "TermPositionVector offsets incorrect if indexed field has multiple values and one ends with non-term chars",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.9",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "If you add multiple values for a field with term vector positions and offsets enabled and one of the values ends with a non-term then the offsets for the terms from subsequent values are wrong. For example (note the '.' in the first value):\n\n        IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true);\n\n        Document doc = new Document();\n\n        doc.add(new Field(\"\", \"one.\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        doc.add(new Field(\"\", \"two\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n\n        writer.optimize();\n\n        writer.close();\n\n        IndexSearcher searcher = new IndexSearcher(directory);\n\n        Hits hits = searcher.search(new MatchAllDocsQuery());\n\n        Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(),\n            new QueryScorer(new TermQuery(new Term(\"\", \"camera\")), searcher.getIndexReader(), \"\"));\n\n        for (int i = 0; i < hits.length(); ++i) {\n\n            TermPositionVector v = (TermPositionVector) searcher.getIndexReader().getTermFreqVector(\n                hits.id, \"\");\n\n            StringBuilder str = new StringBuilder();\n\n            for (String s : hits.doc.getValues(\"\")) \n{\n\n                str.append(s);\n                str.append(\" \");\n            }\n\n            System.out.println(str);\n\n            TokenStream tokenStream = TokenSources.getTokenStream(v, false);\n\n            String[] terms = v.getTerms();\n            int[] freq = v.getTermFrequencies();\n\n            for (int j = 0; j < terms.length; ++j) {\n\n                System.out.print(terms[j] + \":\" + freq[j] + \":\");\n\n                int[] pos = v.getTermPositions(j);\n\n                System.out.print(Arrays.toString(pos));\n\n                TermVectorOffsetInfo[] offset = v.getOffsets(j); \n\n                for (int k = 0; k < offset.length; ++k) \n{\n                    \n                    System.out.print(\":\");\n                    System.out.print(str.substring(offset[k].getStartOffset(), offset[k].getEndOffset()));\n                }\n\n                System.out.println();\n            }\n        }\n\n        searcher.close();\n\nIf I run the above I get:\n        one:1:[0]:one\n        two:1:[1]: tw\n\nNote that the offsets for the second term are off by 1.\n\nIt seems to be that the length of the value that is stored is not taken into account when calculating the offset for the fields of the next value.\n\nI noticed ths problem when using the highlight contrib package which can make use of term vectors for highlighting. I also noticed that the offset for the second string is +1 the end of the previous value, so when concatenating the fields values to pass to the hgighlighter I add to append a ' ' character after each string...which is quite useful, but not documented anywhere.",
    "attachments": {
        "offsets.patch": "https://issues.apache.org/jira/secure/attachment/12396950/offsets.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-07-14T21:41:18+0000",
            "content": "DocumentWriter seems to be the culprit in adding 1 to the previous token's endOffset. It may not be possible to provide token offsets that \"undo\" this operation since it is not possible to determine the order in which tokens are handled as they are grouped by field which doesn't necessarily correspond to document-order. This would also interfere with custom synonym tokens since custom token offsets are no longer guaranteed.\n\nI suggest that there be a flag in Fieldable or IndexWriter which allows exact provided offsets to be stored rather than increased by one. There does not seem to be any sanity checks on offset values during reading/writing the term vector.\n\nA current workaround to this issue is to store the correct startOffset, but leave the endOffset as -1. This has the effect of undoing the +1 of the previous token's endOffset but prevents endOffset information from being available without retokenizing/reparsing the original document.\n ",
            "author": "Shahan Khatchadourian",
            "id": "comment-12512743"
        },
        {
            "date": "2007-07-14T21:46:21+0000",
            "content": "The documentation is also not correct with respect to the current offset implementation. ",
            "author": "Shahan Khatchadourian",
            "id": "comment-12512744"
        },
        {
            "date": "2007-07-18T15:12:07+0000",
            "content": "Can you provide a unit test for this? ",
            "author": "Grant Ingersoll",
            "id": "comment-12513604"
        },
        {
            "date": "2008-12-31T03:27:13+0000",
            "content": "I've attached a patch to 2.4's DocInverterPerField.java that fixes this. The problem is in line 160, which stores the starting offset for the next value of the same field:\n\n\n\tif a field value has delimiter text after its last token this is ignored.\n\tIf there is no extra delimiter text after the last token, the offsets are off by +1 for the tokens in the second value, +2 for the third value and so on.\n\tThe problem is hidden when there is exactly one delimiter character after each value.\n\n\n\nThe patch removes the +1 completely and uses the length of the string to adjust offsets for fields with a string value. Fields with reader or token stream values can't easily be fixed but can't be stored either so are much less likely to affect anyone. ",
            "author": "Andrew Duffy",
            "id": "comment-12660017"
        },
        {
            "date": "2008-12-31T12:16:00+0000",
            "content": "Thank you for the patch Andrew.\n\nI think this issue is a dup of LUCENE-1448, where the plan is to effectively add a getFinalOffset() method to TokenStream, which for the core/contrib analyzers would in fact default to the total number of characters read from their Reader inputs. ",
            "author": "Michael McCandless",
            "id": "comment-12660073"
        },
        {
            "date": "2008-12-31T15:54:47+0000",
            "content": "It is a duplicate of LUCENE-1448; the fix proposed in that issue will fix the problem in a very comprehensive way. ",
            "author": "Andrew Duffy",
            "id": "comment-12660088"
        }
    ]
}