{
    "id": "LUCENE-4345",
    "title": "Create a Classification module",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [],
        "type": "New Feature",
        "fix_versions": [
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Lucene/Solr can host huge sets of documents containing lots of information in fields so that these can be used as training examples (w/ features) in order to very quickly create classifiers algorithms to use on new documents and / or to provide an additional service.\nSo the idea is to create a contrib module (called 'classification') to host a ClassificationComponent that will use already seen data (the indexed documents / fields) to classify new documents / text fragments.\nThe first version will contain a (simplistic) Lucene based Naive Bayes classifier but more implementations should be added in the future.",
    "attachments": {
        "LUCENE-4345.patch": "https://issues.apache.org/jira/secure/attachment/12543248/LUCENE-4345.patch",
        "LUCENE-4345_2.patch": "https://issues.apache.org/jira/secure/attachment/12543551/LUCENE-4345_2.patch",
        "SOLR-3700_2.patch": "https://issues.apache.org/jira/secure/attachment/12540478/SOLR-3700_2.patch",
        "SOLR-3700.patch": "https://issues.apache.org/jira/secure/attachment/12540004/SOLR-3700.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-08-09T10:15:10+0000",
            "content": "first raw patch, I'll work more on this in 3 weeks. ",
            "author": "Tommaso Teofili",
            "id": "comment-13431715"
        },
        {
            "date": "2012-08-09T14:53:16+0000",
            "content": "in trainVocabulary:\n\nTopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);\n\n\n\nThats gonna be really slow if the goal is simply to iterate over all documents.\nI would just grab indexSearcher.getIndexReader() and loop until maxDoc(), rather\nthan scoring in a massive priority queue.\n\nin tokenizeDoc:\n\nTokenStream tokenStream = analyzer.tokenStream(textFieldName, new StringReader(doc));\nwhile (tokenStream.incrementToken()) {\n  CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class);\n  result.add(charTermAttribute.toString());\n}\n\n\n\nthis should use the mandatory reset() etc methods, otherwise its not safe, so something like:\n\nTokenStream tokenStream = analyzer.tokenStream(textFieldName, new StringReader(doc));\nCharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);\ntokenStream.reset();\nwhile (tokenStream.incrementToken()) {\n  result.add(charTermAttribute.toString());\n}\ntokenStream.end();\ntokenStream.close();\n\n\n\nIf you change the test's analyzer from WhitespaceTokenizerFactory to MockTokenizerFactory,\nit should check for these things and currently fail, so thats an easy way to test  ",
            "author": "Robert Muir",
            "id": "comment-13431873"
        },
        {
            "date": "2012-08-10T12:35:50+0000",
            "content": "new patch incorporating Robert's suggestions (plus added a couple more TODOs) ",
            "author": "Tommaso Teofili",
            "id": "comment-13432723"
        },
        {
            "date": "2012-08-30T09:35:55+0000",
            "content": "the suggested snippet for calculating the frequency of terms (from the 'content' field) in docs with a certain class is ok apart that the terms should be extracted from the text field instead of the class field and the docFreq should be counted on the class field:\n\n    Terms terms = MultiFields.getTerms(atomicReader, textFieldName);\n    long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n    double avgNumberOfUniqueTerms = numPostings / (double) terms.getDocCount(); // avg # of unique terms per doc\n    int docsWithC = atomicReader.docFreq(classFieldName, new BytesRef(c));\n    return avgNumberOfUniqueTerms * docsWithC; // avg # of unique terms in text field per doc * # docs with c\n\n\ncomparing the previous (slow) ranked search giving an output of 92 this gives an estimated output of ~98.6 which seems reasonable. ",
            "author": "Tommaso Teofili",
            "id": "comment-13444827"
        },
        {
            "date": "2012-08-30T09:49:40+0000",
            "content": "Is there any reason not to develop it as a Lucene module? I haven't looked at the patch, but if it's not Solr-specific, or depends on Solr API, perhaps we can make this issue a LUCENE-#### one?\n\nI see no reason such module will be available for Solr users only, unless you plan to depend on Solr API, in which case I will not slow down your development by insisting it becomes a Lucene module. ",
            "author": "Shai Erera",
            "id": "comment-13444837"
        },
        {
            "date": "2012-08-30T10:01:52+0000",
            "content": "Is there any reason not to develop it as a Lucene module? I haven't looked at the patch, but if it's not Solr-specific, or depends on Solr API, perhaps we can make this issue a LUCENE-#### one?\n\n+1 ",
            "author": "Chris Male",
            "id": "comment-13444841"
        },
        {
            "date": "2012-08-30T10:08:50+0000",
            "content": "The patch is Solr specific just because it uses a Solr RequestHandler to expose the Classifier interface, also the idea is that more classifier implementations (e.g. based on MLT) may be plugged in so I thought Solr was a good place for it, however it's ok for me to put this into a Lucene module and then add only the needed Solr specific bindings to use it in Solr. ",
            "author": "Tommaso Teofili",
            "id": "comment-13444843"
        },
        {
            "date": "2012-08-30T10:36:34+0000",
            "content": "however it's ok for me to put this into a Lucene module and then add only the needed Solr specific bindings to use it in Solr\n\nif it doesn't complicate matters for you, then it will be great if you can do that ! ",
            "author": "Shai Erera",
            "id": "comment-13444854"
        },
        {
            "date": "2012-08-30T15:33:36+0000",
            "content": "ok, now this is moved to Lucene ",
            "author": "Tommaso Teofili",
            "id": "comment-13445020"
        },
        {
            "date": "2012-08-31T10:03:38+0000",
            "content": "updated patch for Lucene ",
            "author": "Tommaso Teofili",
            "id": "comment-13445811"
        },
        {
            "date": "2012-08-31T10:43:55+0000",
            "content": "docsWithClassSize should ideally be terms.getDocCount() for the field as well\nrather than maxDoc.\n\ndocCount() should not do a search, instead I think it should just return IR.docFreq(term) ?\n\nOne more piece: if classCount is just a Map<UniqueValues,DocFreq>,\nit would be a lot better to just compute this with a TermsEnum,\njust iterating over the terms for the field.\n\nIt seems the \"value\" part is not used, so for now it could be\njust a hashset as well?\n\nThis would remove the stored fields loop (replacing it with a termsenum\nloop), but I think we can probably remove the loop entirely too as\na second step.\n\nI don't like that assignClass has a loop over all possible terms in the\nfield, re-tokenizing the doc for each one! \n\nit seems we dont need this classCount map at all, nor the priors map?\n\nInstead we would just tokenize each doc a single time, and compute the prior of the terms\nwe find on the fly (it seems to just be IDF anyway really).\n\nAnd we wouldnt need any maps for that. ",
            "author": "Robert Muir",
            "id": "comment-13445830"
        },
        {
            "date": "2012-08-31T12:23:56+0000",
            "content": "docsWithClassSize should ideally be terms.getDocCount() for the field as well rather than maxDoc.\n\nyep, the early assumption here was that all the docs have a value for the class field but your suggestion is good.\n\ndocCount() should not do a search, instead I think it should just return IR.docFreq(term) ?\n\ncorrect\n\nit seems we dont need this classCount map at all, nor the priors map?\n\nyes and no, having the priors map slows the training phase (each time it needs to recompute the priors for all the classes), but fasten the classification task with the unseen text (it's a cache in the end), wrt the classCount I agree with you it could be easily replaced (with TermsEnum).\n\nInstead we would just tokenize each doc a single time, and compute the prior of the terms\nwe find on the fly (it seems to just be\n\nyou mean because of the likelihood calculation tokenizing the same doc multiple times (|terms in the class field|), right? That'd be surely good, I'll work on improving that.\n\nThanks Robert  ",
            "author": "Tommaso Teofili",
            "id": "comment-13445867"
        },
        {
            "date": "2012-08-31T12:54:26+0000",
            "content": "\nyes and no, having the priors map slows the training phase (each time it needs to recompute the priors for all the classes), but fasten the classification task with the unseen text (it's a cache in the end), wrt the classCount I agree with you it could be easily replaced (with TermsEnum).\n\nMy concern here is that if the # of terms is large, its a lot of ram too. We can see though, I think tokenizing the doc so many times today is\nactually the slowest part. But we can move to termsenum as a step, just an iteration \n\n\nyou mean because of the likelihood calculation tokenizing the same doc multiple times (|terms in the class field|), right? That'd be surely good, I'll work on improving that\n\nExactly, basically i was thinking in the short term lets remove the extra loop, as an iteration.\n\nlong term I think we would not need the maps and just call docFreq on the terms from the term dictionary on the fly here.\nWhile this sounds like a lot of docFreq calls, i am not so sure. it seems the larger formula is looking for a max() here?\n\nSo we could consider performance-driven heuristics/approximations like MoreLikeThis does based on things like local\nterm frequency within the document/term length, whatever to save on docFreq() calls, if it makes sense (i have to look at the formula in more detail here).\n\nIn that case instead of consuming the tokenStream as an array, it probably makes more sense to consume it into a Map<string,freq>\nso we have a little 'inverted index' for the doc. the current code, given a word that appears many times in the document,\nwill do many computations when instead we could really just work across the unique terms within the document. ",
            "author": "Robert Muir",
            "id": "comment-13445884"
        },
        {
            "date": "2012-09-02T22:57:29+0000",
            "content": "Nice! I've found that filtering for nouns & verbs makes another NLP task (latent semantic indexing) work much better. This will benefit from parts-of-speech filtering. ",
            "author": "Lance Norskog",
            "id": "comment-13447052"
        },
        {
            "date": "2012-09-03T12:06:19+0000",
            "content": "This will benefit from parts-of-speech filtering.\n\nsure, and that can be done by passing the correct Analyzer to the Classifier#train() method. ",
            "author": "Tommaso Teofili",
            "id": "comment-13447240"
        },
        {
            "date": "2012-09-03T12:12:35+0000",
            "content": "So we could consider performance-driven heuristics/approximations like MoreLikeThis does based on things like local term frequency within the document/term length, whatever to save on docFreq() calls, if it makes sense (i have to look at the formula in more detail here).\n\nThe generic formula is C = argmax( P(doc|class) * P(class) ) , I agree it makes sense to incrementally see if we can find good heuristics / approximations which low the computational cost of this calculation.\n\nthe current code, given a word that appears many times in the document, will do many computations when instead we could really just work across the unique terms within the document.\n\nanother good point where we can improve, thanks \n\nI managed to remove all the Maps from the code, I'll attach the patch shortly. I'll then work on removing the tokenizeDoc() loop. ",
            "author": "Tommaso Teofili",
            "id": "comment-13447243"
        },
        {
            "date": "2012-09-03T12:44:30+0000",
            "content": "attaching the updated patch (without Maps) ",
            "author": "Tommaso Teofili",
            "id": "comment-13447249"
        },
        {
            "date": "2012-09-03T14:33:16+0000",
            "content": "Nice! I've found that filtering for nouns & verbs makes another NLP task (latent semantic indexing) work much better. This will benefit from parts-of-speech filtering.\n\nmy former comment is partially correct as the Analyzer is currently used only on the unseen text rather than on the whole set of docs too, using it (or other Analyzers) with the existing docs' text would make training slower but it could be useful to improve accuracy. Maybe a subclass of the current one which is capable of doing that would be a nice addition. ",
            "author": "Tommaso Teofili",
            "id": "comment-13447293"
        },
        {
            "date": "2012-09-04T00:00:29+0000",
            "content": "would make training slower but it could be useful to improve accuracy\nIf you use index data which is already analyzed with the same analyzer as your test (unseen) documents, you can use a lot more documents as input. More is better. As the training data increases, signal drives out noise. Once you add the ability to store & load models, training speed becomes less important. \n\nLook at the Mahout project for ideas about text classifiers. The ConfusionMatrix class and the html page it prints are really handy for summarizing and probing the classifier's performance. ",
            "author": "Lance Norskog",
            "id": "comment-13447429"
        },
        {
            "date": "2012-09-11T13:03:23+0000",
            "content": "Thanks Lance for your useful insights, I'll definitely have a look  .\n\nIf you use index data which is already analyzed with the same analyzer as your test (unseen) documents, you can use a lot more documents as input. More is better. As the training data increases, signal drives out noise.\n\nI agree, we could leverage this for sure.\n\nOnce you add the ability to store & load models, training speed becomes less important.\n\nRegarding storing and loading models, the base intuition (at least my intuition ) in the case of Lucene is that the index itself plays that role. ",
            "author": "Tommaso Teofili",
            "id": "comment-13452980"
        },
        {
            "date": "2012-09-11T13:04:41+0000",
            "content": "by the way, if no one objects I plan to commit this shortly so that we can improve things directly by patching the trunk. ",
            "author": "Tommaso Teofili",
            "id": "comment-13452987"
        },
        {
            "date": "2012-09-11T17:35:54+0000",
            "content": "Can we remove the ClassificationException? It only seems to box IOException... we can just throw IOException directly instead? ",
            "author": "Robert Muir",
            "id": "comment-13453208"
        },
        {
            "date": "2012-09-12T04:26:18+0000",
            "content": "What is the scale that you expect this bayesian classifier to handle? How many training documents does it need?  ",
            "author": "Lance Norskog",
            "id": "comment-13453694"
        },
        {
            "date": "2012-09-12T09:16:33+0000",
            "content": "Can we remove the ClassificationException? It only seems to box IOException... we can just throw IOException directly instead?\n\nsure, we can keep IOException for now\n\nWhat is the scale that you expect this bayesian classifier to handle? How many training documents does it need?\n\nI'm doing some benchmarking in these days therefore I should be able to say something about this shortly. ",
            "author": "Tommaso Teofili",
            "id": "comment-13453853"
        },
        {
            "date": "2012-09-12T10:51:58+0000",
            "content": "side note: it seems a bit old but I just realized something similar had been done in LUCENE-1039, maybe both impl could be then added in the future. ",
            "author": "Tommaso Teofili",
            "id": "comment-13453899"
        },
        {
            "date": "2012-09-13T08:26:42+0000",
            "content": "hey tommaso, \n\nI just briefly skimmed through your latest patch and I have a bunch of comments:\n\n\n\tI agree with robert you should build a small inverted index instead of retokenizing. I'd use a BytesRefHash with a parallel array as we use during indexing, if you have trouble with this I am happy to update your patch and give you an example.\n\tI suggest to move the termsEnum.next() into the while() part like while((next = termsEnum.next) != null) for consistency (in assignClass)\n\tCan you use BytesRef for fieldNames to safe the conversion everytime.\n\tInstead of specifying the document as a String you should rather use IndexableField and in turn pull the tokenstream from IndexableField#tokenStream(Analyzer)\n\tI didn't see a reason why you use Double instead of double (primitive) as return values, I think the boxing is unnecessary\n\tin assignClass can't you reuse the BytesRef returned from the termsEnum for further processing instead of converting it to a string?\n\tin getWordFreqForClass you might want to use TotalHitCountCollector since you are only interested in the number of hits. That collector will not score or collect any documents at all and is way less complex that the default TopDocsCollector\n\tI have trouble to understand why the interface expects an atomic reader here. From my perspective you should handle per-segment aspect internally and instead just use IndexReader in the interface.\n\tThe interface you defined has some problems with respect to Multi-Threading IMO. The interface itself suggests that this class is stateful and you have to call methods in a certain order and at the same you need to make sure that it is not published for read access before training is done. I think it would be wise to pass in all needed objects as constructor arguments and make the references final so it can be shared across threads and add an interface that represents the trained model computed offline? In this case it doesn't really matter but in the future it might make sense. We can also skip the model interface entirely and remove the training method until we have some impls that really need to be trained.\n\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-13454729"
        },
        {
            "date": "2012-09-13T10:53:49+0000",
            "content": "I agree with robert you should build a small inverted index instead of retokenizing. I'd use a BytesRefHash with a parallel array as we use during indexing, if you have trouble with this I am happy to update your patch and give you an example.\n\n+1, not trouble but actually not much time in the latest days to work on it, however if you have time that'd be surely nice (I've committed this on trunk at r1384219 so it should be easier to update it).\n\nI suggest to move the termsEnum.next() into the while() part like while((next = termsEnum.next) != null) for consistency (in assignClass)\n\nsure\n\nCan you use BytesRef for fieldNames to safe the conversion everytime.\n\nactually this depends on \"who\"'s calling the Classifier, if, for example, it's Solr then it makes sense to keep Strings but since this lives in Lucene it may make sense to use ByteRefs, however from an API point of view I'm not sure what could be better.\n\nInstead of specifying the document as a String you should rather use IndexableField and in turn pull the tokenstream from IndexableField#tokenStream(Analyzer)\n\nI think this is not always ok as often the document to be classified is not supposed to be in the index immediately but may get indexed right after the classification, however we could provide that with as an additional method. \n\nI didn't see a reason why you use Double instead of double (primitive) as return values, I think the boxing is unnecessary\n\nyes, I agree.\n\nin assignClass can't you reuse the BytesRef returned from the termsEnum for further processing instead of converting it to a string?\n\nactually, after reading your comment above I realized converting to a String is not a good idea, so I'll change the methods (#calculateLikelihood and #calculatePrior) to use ByteRef rather than String.\n\nin getWordFreqForClass you might want to use TotalHitCountCollector since you are only interested in the number of hits. That collector will not score or collect any documents at all and is way less complex that the default TopDocsCollector\n\nvery good point, thanks \n\nI have trouble to understand why the interface expects an atomic reader here. From my perspective you should handle per-segment aspect internally and instead just use IndexReader in the interface.\n\nas a first implementation I thought it made sense to keep the complexity of explicitly doing distributed probabilities calculations out, also AtomicReaders expose more internals that can be leveraged in a classification algorithm.\n\nThe interface you defined has some problems with respect to Multi-Threading IMO. The interface itself suggests that this class is stateful and you have to call methods in a certain order and at the same you need to make sure that it is not published for read access before training is done. \n\nit'd raise an exception if #assignClass() is called before #train()\n\nI think it would be wise to pass in all needed objects as constructor arguments and make the references final so it can be shared across threads and add an interface that represents the trained model computed offline? In this case it doesn't really matter but in the future it might make sense. We can also skip the model interface entirely and remove the training method until we have some impls that really need to be trained.\n\nI'm +1 for making the references final while I put the #train() method so that a Classifier could be trained multiple times. In this implementation that doesn't make much difference but it may not be the case for other implementations.\nTherefore we could (maybe should) mark this API @experimental and just evolve it form the different implementations we have so finally moving parameters to the constructor may be a nice idea here.\nOn the contrary removing the #train() method from the API would remove any reference to Lucene APIs in the Classifier interface leading to question if that's too much generic. ",
            "author": "Tommaso Teofili",
            "id": "comment-13454802"
        },
        {
            "date": "2012-09-14T19:22:56+0000",
            "content": "I recently did some related research in text analysis and found that limiting terms to nouns&verbs was a 10-15% increase in all variations of the test.\n\nSo, filtering terms from Parts-of-Speech annotation will be very helpful. In my OpenNLP patch is a FilterPayloadsFilter which keeps or rips out terms based on a list of text payloads.\n\nhttp://ultrawhizbang.blogspot.com/2012/09/document-summarization-with-lsa-1.html ",
            "author": "Lance Norskog",
            "id": "comment-13456053"
        },
        {
            "date": "2012-09-15T14:13:13+0000",
            "content": "I don't think this should be using payloads to pull POS tags: the purpose of payloads\nis when you need something stored in the actual index (and should be limited to e.g. a single byte),\nits not type-safe but application-specific.\n\nInstead such taggers should expose a type-safe PartOfSpeechAttribute as suggested in the\no.a.l.analysis package javadocs. If they want to put POS into the index for e.g. payload-based queries,\nthats a separate concern, they should have a separate tokenfilter that encodes the POS attribute\ninto the payload so this is optional (as it has tradeoffs in the index). See TypeAsPayloadFilter etc\nas an example of what I mean. But for this module we don't need anything in the index.\n\nIf we think its useful for classifiers to limit the analysis to certain POS categories, then\ninstead we should factor out a minimal POSAttribute sub-interface with something very generic\nlike isNominal()/isVerbal() that can actually be implemented by different taggers with different tag sets\nacross different languages.\n\nThen things like kuromoji's POSAttribute, openNLP's POSAttribute, or even your custom home-grown one,\nor some commercial one could extend this sub-interface and plug into it.\n\nAt least i think this is possible with our attributes API  ",
            "author": "Robert Muir",
            "id": "comment-13456415"
        },
        {
            "date": "2012-09-15T14:27:57+0000",
            "content": "another simpler idea, you just handle this yourself in the Analyzer you pass to the thing.\n\nThis is currently how Kuromoji works, it has a POS-based stopfilter. these are trivial to write. ",
            "author": "Robert Muir",
            "id": "comment-13456418"
        },
        {
            "date": "2012-09-17T02:05:43+0000",
            "content": "I don't think this should be using payloads to pull POS tags: the purpose of payloads\nis when you need something stored in the actual index (and should be limited to e.g. a single byte),\nits not type-safe but application-specific.\nYes, some NLP applications want actual payloads. For entity resolution you can have a UI add little icons for person, place, etc. In the OpenNLP patch it just seemed silly to add another Attribute type.\n\nIf we think its useful for classifiers to limit the analysis to certain POS categories, then instead we should factor out a minimal POSAttribute sub-interface with something very generic like isNominal()/isVerbal() that can actually be implemented by different taggers with different tag sets across different languages.\nThere is a generic subset with mapping lists for most common tagsets for different languages. They map these tags down to 12 POS tags. Adding this mapper to the OpenNLP patch is on my large TODO list. They even have a mapping set for the Twitter Parts-of-Speech tagger.\n\nThis is currently how Kuromoji works, it has a POS-based stopfilter. these are trivial to write. I also added a filter to remove payloads. If you use a different Attribute for the analysis chain, then you need a 'change POSAttribute to PayloadAttribute' at the bottom of the analysis chain.\nYes, I added one also. Some of the Kuromoji Attributes should be pulled up into the generic set. ",
            "author": "Lance Norskog",
            "id": "comment-13456734"
        },
        {
            "date": "2012-10-23T16:22:37+0000",
            "content": "I've just committed some slight improvements to testing and a basic MLT based kNearestNeighbor classifier (with a bunch of TODOs), comments are welcome  ",
            "author": "Tommaso Teofili",
            "id": "comment-13482448"
        },
        {
            "date": "2012-10-24T10:13:13+0000",
            "content": "The builds have been failing because some methods are missing javadocs:\n\n-documentation-lint:\n     [echo] Checking for broken links...\n     [exec]\n     [exec] Crawl/parse...\n     [exec]\n     [exec] Verify...\n     [echo] Checking for missing docs...\n     [exec]\n     [exec] build/docs/classification/org/apache/lucene/classification/ClassificationResult.html\n     [exec]   missing Constructors: ClassificationResult(java.lang.String, double)\n     [exec]   missing Methods: getAssignedClass()\n     [exec]   missing Methods: getScore()\n     [exec]\n     [exec] build/docs/classification/org/apache/lucene/classification/KNearestNeighborClassifier.html\n     [exec]   missing Constructors: KNearestNeighborClassifier(int)\n     [exec]\n     [exec] Missing javadocs were found!\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13483117"
        },
        {
            "date": "2012-10-24T14:01:41+0000",
            "content": "thanks Michael, it should be fixed now. ",
            "author": "Tommaso Teofili",
            "id": "comment-13483234"
        },
        {
            "date": "2012-11-29T08:18:12+0000",
            "content": "[trunk commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1415074\n\nLUCENE-4345: Fix forbidden APIs and make the test more predicatable\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13506317"
        },
        {
            "date": "2012-12-10T08:22:09+0000",
            "content": "[trunk commit] Tommaso Teofili\nhttp://svn.apache.org/viewvc?view=revision&revision=1419258\n\nLUCENE-4345 - improved DS performance by doing commits only once ",
            "author": "Commit Tag Bot",
            "id": "comment-13527765"
        },
        {
            "date": "2012-12-22T07:08:45+0000",
            "content": "I think I'll resolve this and make further improvements / additions in different more fine grained issues. ",
            "author": "Tommaso Teofili",
            "id": "comment-13538691"
        },
        {
            "date": "2013-02-17T02:29:15+0000",
            "content": "Tommaso, is there any reason this can't be backported to branch_4x? ",
            "author": "Steve Rowe",
            "id": "comment-13580103"
        },
        {
            "date": "2013-02-17T07:36:23+0000",
            "content": "Hi Steve. While it was not the case when this was started, surely it can be backported now, I'm just not sure it can be safely merged back (w/ svn merge) so maybe I'll just create a patch for branch_4x in a separate issue from the trunk version and commit that. ",
            "author": "Tommaso Teofili",
            "id": "comment-13580133"
        },
        {
            "date": "2014-10-31T19:54:22+0000",
            "content": "Hi, Tommaso:\n\nAppreciate your great work and contribution to the community \n\nBut I can't find solr/contrib/classification in dev/trunk. Is it not checked in?\n\nAlso I read your presentation:\nhttp://archive.apachecon.com/eu2012/presentations/08-Thursday/L1R-Lucene/aceu-2012-text-categorization-with-lucene-and-solr.pdf\n\nand like your autmatic text caetgoration druing index: the CategorizationUpdateRequestProcessorFactory\n\nIs it possible to also check in it to Solr?\n\nThanks. ",
            "author": "jefferyyuan",
            "id": "comment-14192350"
        },
        {
            "date": "2014-11-06T13:48:23+0000",
            "content": "But I can't find solr/contrib/classification in dev/trunk. Is it not checked in?\n\ncorrect, that was not checked in as this only referred to the stuff to go in Lucene\n\nIs it possible to also check in it to Solr?\n\nthis would have to be discussed in a separate (Solr) issue I think, also the code that I have for that is 2 years old so it'd probably need some cleaning / refactoring, however that should be easy. ",
            "author": "Tommaso Teofili",
            "id": "comment-14200174"
        }
    ]
}