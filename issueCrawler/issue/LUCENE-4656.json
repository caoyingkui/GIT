{
    "id": "LUCENE-4656",
    "title": "Fix IndexWriter working together with EmptyTokenizer and EmptyTokenStream (without CharTermAttribute), fix BaseTokenStreamTestCase",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "4.0",
        "priority": "Trivial",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "TestRandomChains can fail because EmptyTokenizer doesn't have a CharTermAttribute and doesn't compute the end offset (if the offset attribute was added by a filter).",
    "attachments": {
        "LUCENE-4656-IW-fix.patch": "https://issues.apache.org/jira/secure/attachment/12563091/LUCENE-4656-IW-fix.patch",
        "LUCENE-4656_bttc.patch": "https://issues.apache.org/jira/secure/attachment/12563095/LUCENE-4656_bttc.patch",
        "LUCENE-4656-IW-bug.patch": "https://issues.apache.org/jira/secure/attachment/12563085/LUCENE-4656-IW-bug.patch",
        "LUCENE-4656.patch": "https://issues.apache.org/jira/secure/attachment/12563070/LUCENE-4656.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-01-03T12:55:11+0000",
            "content": "I think this might be a broken assert in BaseTokenStreamTestCase?\n\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n\nWhy do we have that? This assert can be safely removed i think. ",
            "author": "Robert Muir",
            "id": "comment-13542896"
        },
        {
            "date": "2013-01-03T12:55:33+0000",
            "content": "Patch. I wasn't sure whether to add a CharTermAttribute to EmptyTokenizer or to try fixing BaseTokenStreamTestCase but I couldn't think of a non-trivial tokenizer that wouldn't have a CharTermAttribute so I left the assertion that checks that a token stream always has a CharTermAttribute. ",
            "author": "Adrien Grand",
            "id": "comment-13542897"
        },
        {
            "date": "2013-01-03T12:58:45+0000",
            "content": "Why do we have that?\n\nIt feels strange to me that a non-trivial TokenStream could have no CharTermAttribute? ",
            "author": "Adrien Grand",
            "id": "comment-13542899"
        },
        {
            "date": "2013-01-03T12:58:45+0000",
            "content": "I think we should fix BaseTokenStreamTestCase but your patch fixes other things about this Tokenizer?\n\nLike it fixes its end() to work and makes it consume from its reader. ",
            "author": "Robert Muir",
            "id": "comment-13542900"
        },
        {
            "date": "2013-01-03T13:02:46+0000",
            "content": "\nIt feels strange to me that a non-trivial TokenStream could have no CharTermAttribute?\n\nI dont think analysis components should add attributes they dont really need: it should only \nbe the minimal ones they need to work. so I think that extends to this empty one?\n ",
            "author": "Robert Muir",
            "id": "comment-13542902"
        },
        {
            "date": "2013-01-03T13:09:17+0000",
            "content": "I am happy with both solutions. I agree, that only the really used attributes should be added. There are other tokenstream that dont have a CTA, e.g. NumericTokenStream. For the indexer, only BytesRefAttribute is mandatory (I am not sure it is really mandatory, but it is the only one that it is consumed to get the term text). ",
            "author": "Uwe Schindler",
            "id": "comment-13542908"
        },
        {
            "date": "2013-01-03T13:12:02+0000",
            "content": "We should at least test EmptyTokenizer with IndexWrite.\n\nBy the way: EmptyTokenizer is wrong name, as it needs no input reader. It should only extend TokenStream. That's the real bug here! ",
            "author": "Uwe Schindler",
            "id": "comment-13542910"
        },
        {
            "date": "2013-01-03T13:13:18+0000",
            "content": "There is an EmptyTokenStream in src/java that is correct.\n\nEmptyTokenizer is in test-framework, for some crazy test reason. If its not being used lets remove it! ",
            "author": "Robert Muir",
            "id": "comment-13542913"
        },
        {
            "date": "2013-01-03T15:11:59+0000",
            "content": "Alternative patch that fixes BaseTokenStreamTestCase. I needed to add a quick hack to add a TermToBytesRefAttribute when the tokenstream doesn't have one so that TermsHashPerField doesn't complain that it can't find this attribute when indexing. ",
            "author": "Adrien Grand",
            "id": "comment-13542974"
        },
        {
            "date": "2013-01-03T15:27:13+0000",
            "content": "I needed to add a quick hack to add a TermToBytesRefAttribute when the tokenstream doesn't have one so that TermsHashPerField doesn't complain that it can't find this attribute when indexing.\n\nBut this is a bug. If you use the non-test EmptyTokenStream from the analysis-common package and add it to a document it will fail, right? So we should fix IndexWriter to handle that case? ",
            "author": "Uwe Schindler",
            "id": "comment-13542987"
        },
        {
            "date": "2013-01-03T15:36:14+0000",
            "content": "So we should fix IndexWriter to handle that case?\n\nHow would IndexWriter handle token streams with no TermToBytesRefAttribute?\n\n\tfail if the tokens stream happens to have tokens? (incrementToken returns true at least once)\n\tindex empty terms?\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13542995"
        },
        {
            "date": "2013-01-03T15:51:19+0000",
            "content": "I'm not really certain what it should do.\n\nit does seem a little funky that even though incrementToken returned false we start() consumer anyway though. ",
            "author": "Robert Muir",
            "id": "comment-13543009"
        },
        {
            "date": "2013-01-03T15:56:24+0000",
            "content": "The problem is here that the attributes are initialized after construction of the Tokenizer before the consumer starts to consume the tokens. The bug in IndexWriter is that it fails, when the initial getAttribute fails. Maybe it should just initialize the bytesRef attribute to be NULL and fail later if really tokens are emitted.\n\nLucene 3.x indexed empty terms. ",
            "author": "Uwe Schindler",
            "id": "comment-13543017"
        },
        {
            "date": "2013-01-03T16:13:52+0000",
            "content": "Here a patch showing the bug in the public class EmptyTokenStream from analysis-common working together with IndexWriter:\n\n\n[junit4:junit4] ERROR   0.33s | TestEmptyTokenStream.testIndexWriter_LUCENE4656 <<<\n[junit4:junit4]    > Throwable #1: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute'.\n[junit4:junit4]    >    at __randomizedtesting.SeedInfo.seed([3B209861053849AF:D7B239E3D4067832]:0)\n[junit4:junit4]    >    at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:303)\n[junit4:junit4]    >    at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:119)\n[junit4:junit4]    >    at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:109)\n[junit4:junit4]    >    at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:272)\n[junit4:junit4]    >    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:250)\n[junit4:junit4]    >    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376)\n[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1131)\n[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1112)\n[junit4:junit4]    >    at org.apache.lucene.analysis.miscellaneous.TestEmptyTokenStream.testIndexWriter_LUCENE4656(TestEmptyTokenSt\n\n\nIt also has a test that assertTokenStreamContents actually works, which it doesnt at the moment, because it asserts that the CTA is available. But NumericTokenStream and this one both dont have this attribute. ",
            "author": "Uwe Schindler",
            "id": "comment-13543025"
        },
        {
            "date": "2013-01-03T16:49:12+0000",
            "content": "Here the fix that solves the DocInverterPerField issue (it also removes the horrible for(; loop where the first clause is a \"if ... break\".\n\nNow only BaseTokenStreamTestCase should be able to handle the missing attribute. It should only complain when actually tokens are emitted. ",
            "author": "Uwe Schindler",
            "id": "comment-13543058"
        },
        {
            "date": "2013-01-03T16:52:38+0000",
            "content": "Better patch, ueses do...while, which is more readable. ",
            "author": "Uwe Schindler",
            "id": "comment-13543062"
        },
        {
            "date": "2013-01-03T17:16:10+0000",
            "content": "here's a patch for BaseTokenStreamTestCase. I think it should work for this EmptyTokenizer too. ",
            "author": "Robert Muir",
            "id": "comment-13543085"
        },
        {
            "date": "2013-01-03T17:17:21+0000",
            "content": "New patch merged with Adrien's. I am not sure if the Fix in BaseTokenStreamTestCase is correct, because if you pass the String[] you expect tokens and the fix is different like the one for offsets or positionincrements. ",
            "author": "Uwe Schindler",
            "id": "comment-13543086"
        },
        {
            "date": "2013-01-03T17:24:20+0000",
            "content": "Thanks Robert! I will merge again and I took the issue! ",
            "author": "Uwe Schindler",
            "id": "comment-13543095"
        },
        {
            "date": "2013-01-03T17:28:49+0000",
            "content": "Patch merged with Robert's. ",
            "author": "Uwe Schindler",
            "id": "comment-13543101"
        },
        {
            "date": "2013-01-03T17:32:51+0000",
            "content": "Add a check that the document is really in IW after indexing. ",
            "author": "Uwe Schindler",
            "id": "comment-13543107"
        },
        {
            "date": "2013-01-03T17:37:30+0000",
            "content": "Slightly related to the BaseToken changes, i think its confusing how we use output.length (from the String[]) also as the number of expected tokens.\n\nwe could clear this up with something like:\n\n@@ -114,21 +114,32 @@\n   public static void assertTokenStreamContents(...\n     assertNotNull(output);\n+    final int numExpected = output.length;\n\n\n\nand then use this in the for loop and such.\n\nadditionally i've often sent the wrong number of parameters when writing tests because you are passing huge parallel arrays.\nso something like this could save some trouble:\n\n\n     TypeAttribute typeAtt = null;\n     if (types != null) {\n       assertTrue(\"has no TypeAttribute\", ts.hasAttribute(TypeAttribute.class));\n       typeAtt = ts.getAttribute(TypeAttribute.class);\n+      assertEquals(\"wrong number of types\", numExpected, types.length);\n     }\n\n\n\nWe don't have to do these changes here. it just reminded me of it looking at this stuff. ",
            "author": "Robert Muir",
            "id": "comment-13543111"
        },
        {
            "date": "2013-01-03T17:51:37+0000",
            "content": "I would also say that we dont need EmptyTokenizer in test-framework. \nIts only there because 2 places use it, and both in a bogus way (in my opinion):\n1. core/TestDocument\n2. queryparsers\n\nwe should first fix TestDocument, its test does not care if the tokenstream is empty or anything:\n\nIndex: src/test/org/apache/lucene/document/TestDocument.java\n===================================================================\n--- src/test/org/apache/lucene/document/TestDocument.java\t(revision 1428441)\n+++ src/test/org/apache/lucene/document/TestDocument.java\t(working copy)\n@@ -20,7 +20,7 @@\n import java.io.StringReader;\n import java.util.List;\n \n-import org.apache.lucene.analysis.EmptyTokenizer;\n+import org.apache.lucene.analysis.CannedTokenStream;\n import org.apache.lucene.analysis.MockAnalyzer;\n import org.apache.lucene.index.DirectoryReader;\n import org.apache.lucene.index.IndexReader;\n@@ -318,7 +318,7 @@\n   // LUCENE-3616\n   public void testInvalidFields() {\n     try {\n-      new Field(\"foo\", new EmptyTokenizer(new StringReader(\"\")), StringField.TYPE_STORED);\n+      new Field(\"foo\", new CannedTokenStream(), StringField.TYPE_STORED);\n       fail(\"did not hit expected exc\");\n     } catch (IllegalArgumentException iae) {\n       // expected\n\n\n\nThe queryparser test looks outdated, like its some test about when an Analyzer returns null?\nMaybe the test can just be removed, but if we apply this patch, we could move EmptyTokenizer \nfrom test-framework/src/java to queryparser/src/test at least as an improvement, since it is kinda funky. ",
            "author": "Robert Muir",
            "id": "comment-13543125"
        },
        {
            "date": "2013-01-03T17:52:45+0000",
            "content": "I would really like to remove that horrible piece of sh*  ",
            "author": "Uwe Schindler",
            "id": "comment-13543127"
        },
        {
            "date": "2013-01-03T18:01:57+0000",
            "content": "Uwe, I just ran all Lucene tests with your patch and they passed, so +1. +1 to removing EmptyTokenizer too. ",
            "author": "Adrien Grand",
            "id": "comment-13543133"
        },
        {
            "date": "2013-01-03T18:03:13+0000",
            "content": "In trunk it is not even used in core! Only in 4.x's TestDocument! ",
            "author": "Uwe Schindler",
            "id": "comment-13543134"
        },
        {
            "date": "2013-01-03T18:05:24+0000",
            "content": "Patch with Tokenizer completely removed!\n\nIn trunk its completely useless in core (not used), in moudles/qp it is just used to supply a Tokenizer that returns no tokens at all (not null, just empty tokenstream). MockTokenizer is fine for this, too. ",
            "author": "Uwe Schindler",
            "id": "comment-13543137"
        },
        {
            "date": "2013-01-03T18:13:18+0000",
            "content": "Sorry was wrong patch. ",
            "author": "Uwe Schindler",
            "id": "comment-13543144"
        },
        {
            "date": "2013-01-03T18:29:03+0000",
            "content": "Correct patch. TestDocument actually used it, too - sorry.\nThis patch also makes the QueryParser Analyzer correctly reuse. The trick is to simply override initReader() in the Analyzer to return an empty one and close the original reader. ",
            "author": "Uwe Schindler",
            "id": "comment-13543159"
        },
        {
            "date": "2013-01-03T18:30:13+0000",
            "content": "I also cleaned up some imports in affected files. I will commit this later and backport. ",
            "author": "Uwe Schindler",
            "id": "comment-13543161"
        },
        {
            "date": "2013-01-03T23:45:03+0000",
            "content": "I committed the latest patch. Thanks Adrien and Robert! ",
            "author": "Uwe Schindler",
            "id": "comment-13543424"
        },
        {
            "date": "2013-01-05T20:27:49+0000",
            "content": "[branch_4x commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1428675\n\nMerged revision(s) 1428671 from lucene/dev/trunk:\nLUCENE-4656: Fix regression in IndexWriter to work with empty TokenStreams that have no TermToBytesRefAttribute (commonly provided by CharTermAttribute), e.g., oal.analysis.miscellaneous.EmptyTokenStream. Remove EmptyTokenizer from test-framework. ",
            "author": "Commit Tag Bot",
            "id": "comment-13544830"
        },
        {
            "date": "2013-01-05T20:28:17+0000",
            "content": "[trunk commit] Uwe Schindler\nhttp://svn.apache.org/viewvc?view=revision&revision=1428671\n\nLUCENE-4656: Fix regression in IndexWriter to work with empty TokenStreams that have no TermToBytesRefAttribute (commonly provided by CharTermAttribute), e.g., oal.analysis.miscellaneous.EmptyTokenStream. Remove EmptyTokenizer from test-framework. ",
            "author": "Commit Tag Bot",
            "id": "comment-13544860"
        }
    ]
}