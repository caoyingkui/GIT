{
    "id": "SOLR-3561",
    "title": "Error during deletion of shard/core",
    "details": {
        "affect_versions": "4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "multicore",
            "replication (java)",
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Implemented"
    },
    "description": "Running several Solr servers in Cloud-cluster (zkHost set on the Solr servers).\nSeveral collections with several slices and one replica for each slice (each slice has two shards)\nBasically we want let our system delete an entire collection. We do this by trying to delete each and every shard under the collection. Each shard is deleted one by one, by doing CoreAdmin-UNLOAD-requests against the relevant Solr\n\nCoreAdminRequest request = new CoreAdminRequest();\nrequest.setAction(CoreAdminAction.UNLOAD);\nrequest.setCoreName(shardName);\nCoreAdminResponse resp = request.process(new CommonsHttpSolrServer(solrUrl));\n\n\n\nThe delete/unload succeeds, but in like 10% of the cases we get errors on involved Solr servers, right around the time where shard/cores are deleted, and we end up in a situation where ZK still claims (forever) that the deleted shard is still present and active.\n\nForm here the issue is easilier explained by a more concrete example:\n\n\t7 Solr servers involved\n\tSeveral collection a.o. one called \"collection_2012_04\", consisting of 28 slices, 56 shards (remember 1 replica for each slice) named \"collection_2012_04_sliceX_shardY\" for all pairs in \n{X:1..28}\nx\n{Y:1,2}\n\tEach Solr server running 8 shards, e.g Solr server #1 is running shard \"collection_2012_04_slice1_shard1\" and Solr server #7 is running shard \"collection_2012_04_slice1_shard2\" belonging to the same slice \"slice1\".\n\n\n\nWhen we decide to delete the collection \"collection_2012_04\" we go through all 56 shards and delete/unload them one-by-one - including \"collection_2012_04_slice1_shard1\" and \"collection_2012_04_slice1_shard2\". At some point during or shortly after all this deletion we see the following exceptions in solr.log on Solr server #7\n\nAug 1, 2012 12:02:50 AM org.apache.solr.common.SolrException log\nSEVERE: Error while trying to recover:org.apache.solr.common.SolrException: core not found:collection_2012_04_slice1_shard1\n\nrequest: http://solr_server_1:8983/solr/admin/cores?action=PREPRECOVERY&core=collection_2012_04_slice1_shard1&nodeName=solr_server_7%3A8983_solr&coreNodeName=solr_server_7%3A8983_solr_collection_2012_04_slice1_shard2&state=recovering&checkLive=true&pauseFor=6000&wt=javabin&version=2\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.solr.common.SolrExceptionPropagationHelper.decodeFromMsg(SolrExceptionPropagationHelper.java:29)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:445)\nat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:264)\nat org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:188)\nat org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:285)\nat org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:206)\n\nAug 1, 2012 12:02:50 AM org.apache.solr.common.SolrException log\nSEVERE: Recovery failed - trying again...\nAug 1, 2012 12:02:51 AM org.apache.solr.cloud.LeaderElector$1 process\nWARNING:\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\nat java.util.ArrayList.RangeCheck(ArrayList.java:547)\nat java.util.ArrayList.get(ArrayList.java:322)\nat org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:96)\nat org.apache.solr.cloud.LeaderElector.access$000(LeaderElector.java:57)\nat org.apache.solr.cloud.LeaderElector$1.process(LeaderElector.java:121)\nat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:531)\nat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:507)\nAug 1, 2012 12:02:51 AM org.apache.solr.cloud.LeaderElector$1 process\n\n\nIm not sure exactly how to interpret this, but it seems to me that some recovery job tries to recover collection_2012_04_slice1_shard2 on Solr server #7 from collection_2012_04_slice1_shard1 on Solr server #1, but fail because Solr server #1 answers back that it doesnt run collection_2012_04_slice1_shard1 (anymore).\n\nThis problem occurs for serveral (in this conrete test for 4) of the 28 slice pairs. For those 4 shards the end result is that /node_states/solr_server_X:8983_solr in ZK still contains information about the shard being running and active. E.g. /node_states/solr_server_7:8983_solr still contains\n\n{ \n \"shard\":\"slice1\",\n \"state\":\"active\",\n \"core\":\"collection_2012_04_slice1_shard2\",\n \"collection\":\"collection_2012_04\",\n \"node_name\":\"solr_server_7:8983_solr\",\n \"base_url\":\"http://solr_server_7:8983/solr\"\n} \n\n\nand that CloudState therefore still reports that those shards are running and active - but thay are not. A.o. I have noticed that \"collection_2012_04_slice1_shard2\" HAS been removed from solr.xml on Solr server #7 (we are running with persistent=\"true\")\n\nAny chance that this bug is fixed in a later revision (than one from 29/2-2012) of 4.0-SNAPSHOT?\nIf not we need to get it fixed, I believe.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-13412077",
            "date": "2012-07-11T22:25:56+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13429813",
            "date": "2012-08-07T03:43:16+0000",
            "content": "rmuir20120906-bulk-40-change "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13451128",
            "date": "2012-09-07T23:22:57+0000",
            "content": "Assigning to Mark Miller to assess if this is still a problem that needs addressed for 4.0-final\n\nPer Steffensen: it would be helpful if you can provide more details about whether you can still reproduce this in the 4.0-Beta or any subsequent snapshot builds. "
        },
        {
            "author": "Rob Speer",
            "id": "comment-13474421",
            "date": "2012-10-11T18:45:47+0000",
            "content": "I'm still seeing this error, consistently.\n\nI'm currently running two Solr processes on one machine to test sharding. If I ever delete all the cores of a collection (and even if I explicitly delete the collection using the cloud admin), it shows an error like this first:\n\n\nINFO: Unregistering core testdb-shard6-rep2 from cloudstate.\nOct 11, 2012 2:42:11 PM org.apache.solr.core.SolrCore close\nINFO: [testdb-shard6-rep2]  CLOSING SolrCore org.apache.solr.core.SolrCore@7a0ec60b\nOct 11, 2012 2:42:11 PM org.apache.solr.core.SolrCore closeSearcher\nINFO: [testdb-shard6-rep2] Closing main searcher on request.\nOct 11, 2012 2:42:11 PM org.apache.solr.update.DirectUpdateHandler2 close\nINFO: closing DirectUpdateHandler2{commits=14,autocommits=0,soft autocommits=0,optimizes=0,rollbacks=0,expungeDeletes=0,docsPending=0,adds=0,deletesById=0,deletesByQuery=0,errors=0,cumulative_adds=44,cumulative_deletesById=0,cumulative_deletesByQuery=8,cumulative_errors=0}\nOct 11, 2012 2:42:11 PM org.apache.solr.cloud.RecoveryStrategy close\nWARNING: Stopping recovery for core testdb-shard6-rep2 zkNodeName=panama:8983_solr_testdb-shard6-rep2\nOct 11, 2012 2:42:11 PM org.apache.solr.cloud.LeaderElector$1 process\nWARNING: \njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.ArrayList.rangeCheck(ArrayList.java:571)\n        at java.util.ArrayList.get(ArrayList.java:349)\n        at org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:95)\n        at org.apache.solr.cloud.LeaderElector.access$000(LeaderElector.java:57)\n        at org.apache.solr.cloud.LeaderElector$1.process(LeaderElector.java:125)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:526)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)\n\n\n\nAfterward it repeats this error over and over:\n\n\nSEVERE: Error while trying to recover.                                                                                                                                                                                       java.lang.RuntimeException: No registered leader was found, collection:lumi-test_pipeline-test slice:shard2\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderProps(ZkStateReader.java:428)\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderProps(ZkStateReader.java:414)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:297)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:211)\nOct 11, 2012 11:30:11 AM org.apache.solr.cloud.RecoveryStrategy doRecovery\nSEVERE: Recovery failed - trying again...\nOct 11, 2012 11:30:11 AM org.apache.solr.cloud.RecoveryStrategy doRecovery\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13484629",
            "date": "2012-10-26T01:06:38+0000",
            "content": "It's very likely this could have been SOLR-3939. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13484711",
            "date": "2012-10-26T04:57:06+0000",
            "content": "And/Or SOLR-3994 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13484712",
            "date": "2012-10-26T04:58:17+0000",
            "content": "\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.ArrayList.rangeCheck(ArrayList.java:571)\n        at java.util.ArrayList.get(ArrayList.java:349)\n        at org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:95)\n\n\n\nThis is actually likely fine and unrelated - it's something that can happen on shutdown and should not be a problem. I've updated it so that a more appropriate message is logged. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717206",
            "date": "2013-07-23T18:47:28+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971337",
            "date": "2014-04-16T12:57:48+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Cao Manh Dat",
            "id": "comment-15534920",
            "date": "2016-09-30T03:44:54+0000",
            "content": "So can this issue be closed? "
        },
        {
            "author": "Cao Manh Dat",
            "id": "comment-15544742",
            "date": "2016-10-04T08:38:13+0000",
            "content": "Alexandre Rafalovitch Based on comments of Mark Miller I think we can close this ticket now. "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-15546190",
            "date": "2016-10-04T18:13:03+0000",
            "content": "I originally created the ticket. I am not against closing it. I do not know if the problem still exists (in some shape), but a lot of things has changed since, so someone will have to bring up the problem again if it is still a problem. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15547154",
            "date": "2016-10-05T00:18:08+0000",
            "content": "Old issue that may have been resolved by other issues mentioned. If a similar problems shows up later, a new issue can be opened with more-specific details. "
        }
    ]
}