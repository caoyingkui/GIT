{
    "id": "LUCENE-5589",
    "title": "release artifacts are too large.",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [],
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": []
    },
    "description": "Doing a release currently products 600MB of artifacts. This is unwieldy...",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13965564",
            "author": "Shawn Heisey",
            "content": "My big concern with the release artifacts is user download time.  The Solr binary download is HUGE ... whenever I need to download a binary release to update my custom projects, I dread doing so when I'm at home where bandwidth is limited.\n\nSolr's competition includes ElasticSearch.  Their .zip download is 21.6MB and the .tar.gz is even smaller.  Solr's .war file is larger than either, and that's just the tip of the iceberg.  There's a lot more 'stuff' in a Solr download, but the majority of users don't need that stuff.  Why should they download it unless they need it? ",
            "date": "2014-04-10T17:22:54+0000"
        },
        {
            "id": "comment-13965960",
            "author": "Hoss Man",
            "content": "Robert noted in LUCENE-5590...\n\n... Our release process generates 600MB of \"stuff\". The size just keeps getting bigger and bigger, and is already difficult to work with (e.g. upload across the internet). ...\n\nWhile at apachecon the other day, rmuir mentioned his frustration with trying to publish a 4.7.2 RC over the slow conference network, which led to this parent issue.  \n\nLater, while rmuir wasn't around, sarowe and i were discussing that perhaps the root problem isn't so much how big all of the release artifacts are combined (since most users aren't downloading more then one of the artifacts per version) but how to minimize the amount of net transfer involved in preparing & smoke testing an RC.  \n\nThis lead me to spit ball the following idea for what an \"ideal\" release process might look like by taking more advantage of both jenkins & the svn \"release\" repo (which can be used for RCs as well as the final release artifacts)...\n\n\n\n\tsomeone steps up to be an RM\n\tif this is a major or minor release:\n\t\n\t\tthe RM creates the new branch in SVN,\n\t\tthe RM clicks some buttons in jenkins to create a new \"prep-release\" job on the new branch\n\t\n\t\n\ta week or so goes by...\n\t\n\t\tpeople merge things into the release branch as needed\n\t\tthe \"prep-release\" job in jenkins is constantly doing almost everything the current buildAndPushRelease.py, with the notable exception of signing the artifacts or pushing the data anywhere\n\t\tthe \"prep-release\" job will also build up a directory containing all of the docs & jdocs we intend to publish on the website for this release once it's official\n\t\n\t\n\twhen the RM is ready, they SSH into the lucene jenkins machine, CD into the latest artifacts dir of the \"prep-release\" job and:\n\t\n\t\trun some kind of \"stage-maven-jars.py\" that handles the maven side of staging the RC\n\t\tsvn commits the non-maven artifacts into https://dist.apache.org/repos/dist/dev/lucene/...\n\t\t\n\t\t\tNOTE: this is the dir setup by infra specifically for release candidates\n\t\t\tthis could be fully scripted, or there could be a script similar to the way we do ref-guide publishing that moves files around as needed & then echo's out hte svn command's to cut/paste for actually committing the files\n\t\t\n\t\t\n\t\trun's some sript that stashes away the docs+jdocs for this RC in a more permanant local dir so they won't be deleted automatically by jenkins\n\t\n\t\n\ton the RM's local machine, he runs some \"smoke-test-release.py --no-gpg-check\" script pointed at the SVN URL of the release candidate\n\t\n\t\tthis smoke-test-release.pl script will do an svn checkout, followed by all of the normal things that our existing release smoke checker does\n\t\t(the \"--no-gpg-check\" is because the RM hasn't signed anything yet)\n\t\n\t\n\tif the smoke test script passes, then the RM, (on his local machine) runs a \"sign-releases.py\" script on the artifacts & svn commits the newly created *.asc files (or maybe the script does that automatically)\n\tthe RM then sends out the email calling a vote on the RC\n\t\n\t\teverybody else can run the same \"smoke-test-release.py\" on the svn RC URL, but w/o using the \"--no-gpg-check\" since now the release is signed.\n\t\tpeople cast their votes as normal\n\t\n\t\n\twhen the vote passes, the RM runs a \"publish-rc.py\" script, pointing at the SVN URL for the RC...\n\t\n\t\tthis script will automatically do whatever magic is needed to promote the \"staged\" maven artifacts into \"real\" maven artifacts\n\t\tthis script will then do a remove \"svn mv\" from the RC's svn URL to the final place the release files should live (in https://dist.apache.org/repos/dist/releases/lucene/...)\n\t\t\n\t\t\tor, similar to how we deal with the ref guide: maybe it just echo's out the SVN commands for the RM to cut/paste and run manually\n\t\t\n\t\t\n\t\n\t\n\tonce the dist mirror network has caught up:\n\t\n\t\tthe RM ssh's back to the jenkins machine and cd's to the directory where the docs+jdocs for the RC got stashed\n\t\tthe RM runs some \"publish-javadocs.py\" script that executes (or echos so the RM can cut/paste to manually run...) the needed SSH commands to publish the javadocs ono lucene.apache.org\n\t\n\t\n\n\n\nThe net gains here would be:\n\n\tless manual steps for the RM - jenkins does most of the heavy lifting\n\tthe RM never has to \"upload\" any release artifacts (or big directories of javadocs) from their local machine to any remote server - all large transfers are:\n\t\n\t\tjenkins -> dist.apache.org\n\t\tdist.apache.org -> dist.apache.org (remote svn mv)\n\t\tjenkins -> lucene.apache.org\n\t\n\t\n\tthe RM only has to \"download' the RC like everyone else\n\t\n\t\tthe only files the RM \"uploads\" is the signature files as part of an svn commit\n\t\n\t\n\n\n\n\n\nWhile i certainly agree that it would be nice to find ways to make the individual release artifacts smaller (to facilitate download time for end users) perhaps something like this idea would be a good solution for the aggregate of all the artifacts of a release being large & cumbersome for the RM?  ",
            "date": "2014-04-10T22:54:59+0000"
        },
        {
            "id": "comment-13965991",
            "author": "Robert Muir",
            "content": "I'm not sure that really helps: we already do this in a way in jenkins, with nightly-smoke task (but its rarely run, and we discard the artifacts).\n\nAnd at the end of the day, you still have the network transfer as i cant imagine not testing an RC before calling a vote.\n\nSo I think we should still be cautious about the size of the artifacts we are releasing for a number of reasons.  ",
            "date": "2014-04-10T23:25:57+0000"
        },
        {
            "id": "comment-13966017",
            "author": "Hoss Man",
            "content": "And at the end of the day, you still have the network transfer as i cant imagine not testing an RC before calling a vote.\n\nFor most people, their network \"download\" speed is much faster then the network \"upload\" speed. An approach like i'm suggesting would leverage the ASF infra hardware & network to do \"server -> server\" file copies whenever possible instead of requiring that a lot of \"laptop -> server\" copying like our current process involves.\n\nby my count the RM currently has to \"upload\" the large release artifacts from their local net a minimum of 3 times: \n\n\tpush the whole RC to people.apache.org\n\tpush the maven jars to the maven staging repo (not sure if the final maven publish involves a local copy or if we currently re-push)\n\tpush the non-maven artifacts to dist.apache.org\n...i'm suggesting that that can al be eliminated.\n\n\n\n\nI never suggested that the RM wouldn't personal test out the RC before calling the vote \u2013 that would of course still be the way we do things, and was covered in the steps i mentioned...\n\n\n\n\ton the RM's local machine, he runs some \"smoke-test-release.py --no-gpg-check\" script pointed at the SVN URL of the release candidate\n\t\n\t\tthis smoke-test-release.pl script will do an svn checkout, followed by all of the normal things that our existing release smoke checker does\n\t\t(the \"--no-gpg-check\" is because the RM hasn't signed anything yet)\n\t\tif the smoke test script passes, then the RM, (on his local machine) runs a \"sign-releases.py\" script on the artifacts & svn commits the newly created *.asc files (or maybe the script does that automatically)\n\t\n\t\n\tthe RM then sends out the email calling a vote on the RC\n\n ",
            "date": "2014-04-10T23:45:34+0000"
        },
        {
            "id": "comment-13966026",
            "author": "Robert Muir",
            "content": "I guess it depends on how the RM does releasing.\n\nFor example I probably created like 10 release candidates this week, running the smoketester (file:/// URL, no network transfer required), doing other tests, etc etc. Once i've iterated until i'm happy, then i do the upload and call a vote.\n\nSo its somewhat like 'compile-test-debug'. I am concerned with a \"jenkins\" doing that that it would be slower: it puts network transfer right in the middle of this iteration loop. ",
            "date": "2014-04-10T23:52:12+0000"
        }
    ]
}