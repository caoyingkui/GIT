{
    "id": "SOLR-2059",
    "title": "Allow customizing how WordDelimiterFilter tokenizes text.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "By default, WordDelimiterFilter assigns 'types' to each character (computed from Unicode Properties).\nBased on these types and the options provided, it splits and concatenates text.\n\nIn some circumstances, you might need to tweak the behavior of how this works.\nIt seems the filter already had this in mind, since you can pass in a custom byte[] type table.\nBut its not exposed in the factory.\n\nI think you should be able to customize the defaults with a configuration file:\n\n# A customized type mapping for WordDelimiterFilterFactory\n# the allowable types are: LOWER, UPPER, ALPHA, DIGIT, ALPHANUM, SUBWORD_DELIM\n# \n# the default for any character without a mapping is always computed from \n# Unicode character properties\n\n# Map the $, %, '.', and ',' characters to DIGIT \n# This might be useful for financial data.\n$ => DIGIT\n% => DIGIT\n. => DIGIT\n\\u002C => DIGIT",
    "attachments": {
        "SOLR-2059.patch": "https://issues.apache.org/jira/secure/attachment/12452520/SOLR-2059.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Peter Karich",
            "id": "comment-12902588",
            "date": "2010-08-25T19:15:10+0000",
            "content": "Robert,\n\nthanks for this work! I have a different application for this patch: in a twitter search # and @ shouldn't be removed. Instead I will handle them like ALPHA, I think.\n\nWould you mind to update the patch for the latest version of the trunk? I got a problem with WordDelimiterIterator at line 254 if I am using https://svn.apache.org/repos/asf/lucene/dev/trunk/solr and a file is missing problem (line 37) for http://svn.apache.org/repos/asf/solr "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12902593",
            "date": "2010-08-25T19:26:29+0000",
            "content": "Hi Peter:\n\nthats a great example. For my use case it was actually not the example either, but I was just trying to give a good general example.\n\nWhat do you think of the file format, is it ok for describing these categories? \nThis format/parser is just stolen the one from MappingCharFilterFactory, it seemed unambiguous and is already in use.\n\nAs far as applying the patch, you need to apply it to https://svn.apache.org/repos/asf/lucene/dev/trunk, not https://svn.apache.org/repos/asf/lucene/dev/trunk/solr\n\nThis is because it has to modify a file in modules, too. "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12902600",
            "date": "2010-08-25T19:45:17+0000",
            "content": "Ups, my mistake ... this helped!\n\n> What do you think of the file format, is it ok for describing these categories? \n\nI think it is ok. I even had a more simpler patch before stumbling over yours: handleAsChar=\"@#\" which is now more powerful IMHO:\n\n \n@ => ALPHA\n# => ALPHA\n\n  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12902891",
            "date": "2010-08-26T15:08:55+0000",
            "content": "Thanks for the feedback. I'd like to commit (to trunk and 3x) in a few days if no one objects. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12903897",
            "date": "2010-08-28T22:05:01+0000",
            "content": "Committed revision 990451 (trunk) 990456 (3x) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13013149",
            "date": "2011-03-30T15:45:42+0000",
            "content": "Bulk close for 3.1.0 release "
        }
    ]
}