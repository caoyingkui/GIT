{
    "id": "LUCENE-4398",
    "title": "\"Memory Leak\" in TermsHashPerField memory tracking",
    "details": {
        "components": [],
        "fix_versions": [
            "3.6.2"
        ],
        "affect_versions": "3.4",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I am witnessing an apparent leak in the memory tracking used to determine when a flush is necessary.\n\nOver time, this will result in every single document being flushed into its own segment as the memUsage will remain above the configured buffer size, causing a flush to be triggered after every add/update.\n\nBest I can figure, this is being caused by TermsHashPerField's tracking of memory usage for postingsHash and/or postingsArray combined with multi-threaded feeding.\n\nI suspect that the TermsHashPerField's postingsHash is growing in one thread, then, when a segment is flushed, a single, different thread will merge all TermsHashPerFields in FreqProxTermsWriter and then call shrinkHash(). I suspect this call of shrinkHash() is seeing an old postingsHash array, and subsequently not releasing all the memory that was allocated.\n\nIf this is the case, I am also concerned that FreqProxTermsWriter will not write the correct terms into the index, although I have not confirmed that any indexing problem occurs as of yet.\n\nNOTE: i am witnessing this growth in a test by subtracting the amount or memory allocated (but in a \"free\" state) by perDocAllocator/byteBlockAllocator/charBlocks/intBlocks from DocumentsWriter.memUsage.get() in IndexWriter.doAfterFlush()\nI will see this stay at a stable point for a while, then on some flushes, i will see this grow by a couple of bytes, and all subsequent flushes will never go back down the the previous state\n\n\nI will continue to investigate and post any additional findings",
    "attachments": {
        "LUCENE-4398.patch": "https://issues.apache.org/jira/secure/attachment/12545498/LUCENE-4398.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-09-17T17:03:45+0000",
            "content": "More information:\n\nI started tracking the memory usage internally in TermsHashPerField\nthis resulted in an internal AtomicLong that held the amount of memory that was \"held\" by this class\n\ni then added a finalize() method that dumped the memory held to stdout\n\nresult:\nas soon as i witnessed the memory grow, i forced garbage collection (via yourkit profiler)\ni then saw the finalize methods were called and the memory held by all garbage collected TermsHashPerField instances equaled the amount of memory that was leaked\n\n\nlooks like the DocumentsWriter is releasing thread states without freeing the bytesUsed()?\n\nNOTE: this puts my concerns about thread safety/improper indexing to rest\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13457151"
        },
        {
            "date": "2012-09-17T17:57:43+0000",
            "content": "Not good!  Would be nice to boil this down to a small test case...\n\nHave you tested 3.6 to see if it also shows the leak? ",
            "author": "Michael McCandless",
            "id": "comment-13457195"
        },
        {
            "date": "2012-09-17T17:58:00+0000",
            "content": "Looks like the culprit is DocFieldProcessorPerThread.trimFields()\n\nthis method \"releases\" fields that were not seen recently\nfor each field, this leaks 16 bytes from DocumentsWriter.bytesUsed's memory accounting ",
            "author": "Tim Smith",
            "id": "comment-13457196"
        },
        {
            "date": "2012-09-17T18:38:04+0000",
            "content": "Found a easy \"fix\" for this:\ncommenting out the \"bytesUsed(postingsHashSize * RamUsageEstimator.NUM_BYTES_INT)\" line from TermsHashPerField's constructor does the trick\n\nThis results in not accounting for 16 bytes for each field for each thread, this being the same 16 bytes that were not being reclaimed by trimFields()\n\nI suppose a more robust means to fix this would be to add a \"destroy()\" method to the PerField interfaces that would release this memory (however that would be a rather large patch)\n\n\nAlso found a relatively easy way to reproduce this:\nFeed N documents with fields A-M\nforce flush\nFeed N documents with fields N-Z\nforce flush\nRepeat\n\nit will take a long time to actually consume all the memory (more fields used in test should accelerate things)\n\n\n\n\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13457214"
        },
        {
            "date": "2012-09-17T19:22:13+0000",
            "content": "NOTE: 16 bytes of \"unaccounted\" space in the postingsHash is actually much less than the object header fields require for TermsHashPerField\n\nso, i would argue that not accounting for this 16 bytes is a valid low-profile fix to this\n\nthe only gotcha would be if trimFields() is ever called on a TermsHashPerField that has not been shrunk down to size due to a flush\nis this possible?\neven if possible, i expect this only occurs in the rare case of deep-down exceptions?\nin that case, if abort() is called, i suppose the abort() method can be updated to shrink down the hash as well (if this is safe to do)\n ",
            "author": "Tim Smith",
            "id": "comment-13457244"
        },
        {
            "date": "2012-09-18T00:05:24+0000",
            "content": "Patch w/ test case and fix.\n\nThe issue doesn't affect 4.x/5.x because on flush we completely clear the slate / allocate a new DWPT.  I'll commit the test case to be sure...\n\nI added a InvertedDocConsumerPerField.close() method, and implemented it in TermsHashPerField to account for freeing up the RAM. ",
            "author": "Michael McCandless",
            "id": "comment-13457492"
        },
        {
            "date": "2012-09-19T16:19:17+0000",
            "content": "Thanks Tim! ",
            "author": "Michael McCandless",
            "id": "comment-13458802"
        },
        {
            "date": "2013-03-22T16:37:18+0000",
            "content": "[branch_4x commit] Michael McCandless\nhttp://svn.apache.org/viewvc?view=revision&revision=1386921\n\nLUCENE-4398: add test case ",
            "author": "Commit Tag Bot",
            "id": "comment-13610815"
        },
        {
            "date": "2013-05-10T10:33:39+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13653989"
        }
    ]
}