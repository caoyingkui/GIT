{
    "id": "SOLR-667",
    "title": "Alternate LRUCache implementation",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The only available SolrCache i.e LRUCache is based on LinkedHashMap which has get() also synchronized. This can cause severe bottlenecks for faceted search. Any alternate implementation which can be faster/better must be considered.",
    "attachments": {
        "SOLR-667-alternate.patch": "https://issues.apache.org/jira/secure/attachment/12393171/SOLR-667-alternate.patch",
        "SOLR-667-updates.patch": "https://issues.apache.org/jira/secure/attachment/12393062/SOLR-667-updates.patch",
        "ConcurrentLRUCache.java": "https://issues.apache.org/jira/secure/attachment/12387164/ConcurrentLRUCache.java",
        "SOLR-667.patch": "https://issues.apache.org/jira/secure/attachment/12388051/SOLR-667.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12618072",
            "date": "2008-07-30T06:15:40+0000",
            "content": "A POC implementation based on ConcurrentHashMap\n\n\n\tGets are free\n\tPuts are free till it touches the high water mark . It is expensive (very expensive) after the high watermark .\n\tTo lighten the load on put an extra thread is employed to do a concurrent mark and sweep\n\tthere is a high-water-mark and a low-water-mark . The extra thread cleans anything if low-water-mark is crossed. Put must removes if level crosses high-water-mark\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618708",
            "date": "2008-07-31T14:56:11+0000",
            "content": "If somebody can review the implementation we can add another cache implementation to Solr. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12618715",
            "date": "2008-07-31T15:17:12+0000",
            "content": "Gets are free\n\nNot entirely... there are a few memory barriers that make things thread safe, so I wouldn't call it \"free\" (since this branched off of another issue where some had the idea that one could get away without any sort of locks or memory barriers).\n\nIt's a good approach in genera, and should scale better with many CPUs under very high lookup load.  But I'm not sure that it should use a separate cleaner thread... and if it does, I don't think it should be scheduled.\n\nAfter we got those details worked out, then we'd need a SolrCache implementation that uses it.  Given where we are in the release cycle (and that the cache contention issue is only affecting 1 person that I've seen), I think this should want until after 1.3 "
        },
        {
            "author": "Fuad Efendi",
            "id": "comment-12618750",
            "date": "2008-07-31T16:27:13+0000",
            "content": "...safety, where nothing bad ever happens to an object. \nWhen SOLR adds object to cache or remove it from cache it does not change it, it manipulates with internal arrays of pointers to objects (which are probably atomic, but I don't know such JVM & GC internals in-depth...)\n\nLooks heavy with TreeSet... "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618768",
            "date": "2008-07-31T17:35:33+0000",
            "content": "It's a good approach in genera, and should scale better with many CPUs under very high lookup load. But I'm not sure that it should use a separate cleaner thread... and if it does, I don't think it should be scheduled.\n\nThanks for the comments. I agree with you . I devised this approach because some user reported that heavy cache lookups are slowing things down for him. The cost benefit analysis is yet to be studied. Separate cleaner thread is optional in the implementation . I am yet to study the cost of sorting over a hundred thousand entries.  Do you recommend that the cleaner thread just keep running forever? That is fine, so there should be a sleep for the thread? \n\nBTW is the executorservice very expensive?\n\nI did not provide a SolrCache implementation because that is going to be drown the approach in details. \n\nI think this should want until after 1.3\n\nTrue. This is not marked for 1.3. But this can definitely live as a patch and anyone who needs it would benefit from it "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618773",
            "date": "2008-07-31T17:46:32+0000",
            "content": "\nFuad. You cannot trivialize concurrent programming so easily. Whatever\nwe have commented are from our experience (and wisdom)  . There is a\nprice to pay it. Java could have easily eliminated the\njava.util.concurrent package by using 'volatile' everywhere and no\nneed of AtomicInteger,AtomimcLong etc. So they are there for a reason\n\n\n\nBTW. Using TreeSet is not 'heavy' . It is the right tool for right\npurpose. If you need a sorted set that is best\n\n\n\nOn Thu, Jul 31, 2008 at 9:58 PM, Fuad Efendi (JIRA) <jira@apache.org> wrote:\n\n\n\n\u2013 \n--Noble Paul "
        },
        {
            "author": "Fuad Efendi",
            "id": "comment-12618805",
            "date": "2008-07-31T18:40:53+0000",
            "content": "Paul, \n\n\nI have never ever suggested to use 'volatile'  'to avoid synchronization' for concurrent programming. I only noticed some extremely stupid code where SOLR uses _double_synchronization and AtomicLong inside:\n\n\n  public synchronized Object put(Object key, Object value) {\n    if (state == State.LIVE) {\n      stats.inserts.incrementAndGet();\n    }\n\n    synchronized (map) {\n      // increment local inserts regardless of state???\n      // it does make it more consistent with the current size...\n      inserts++;\n      return map.put(key,value);\n    }\n  }\n\n\n\nEach tool has an area of applicability, and even ConcurrentHashMap just slightly intersects with SOLR needs; SOLR does not need 'consistent view at a point in time' on cached objects.\n\n'volatile' is part of Java Specs, and implemented differently by different vendors. I use volatile (instead of more expensive AtomicLong) only and only to prevent JVM HotSpot Optimizer from some not-applicable staff... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12618812",
            "date": "2008-07-31T19:03:57+0000",
            "content": "I only noticed some extremely stupid code where SOLR uses _double_synchronization and AtomicLong inside:\n\nA simple typo I think... a remnant from way back when changing what object was being synchronized on.  That's why I like explicit synchronization rather than adding it to a method signature (easier to miss).   I just fixed this to be\n\n  public Object put(Object key, Object value) {\n    synchronized (map) {\n      if (state == State.LIVE) {\n        stats.inserts.incrementAndGet();\n      }\n\n      // increment local inserts regardless of state???\n      // it does make it more consistent with the current size...\n      inserts++;\n      return map.put(key,value);\n    }\n  }\n\n "
        },
        {
            "author": "Fuad Efendi",
            "id": "comment-12618824",
            "date": "2008-07-31T19:28:06+0000",
            "content": "Thanks Yonik, I even guess that in some cases synchronization is faster than sun.misc.Unsafe.compareAndSwapLong(this, valueOffset, expect, update);\n\n\n    public final long incrementAndGet() {\n        for (;;) {\n            long current = get();\n            long next = current + 1;\n            if (compareAndSet(current, next))\n                return next;\n        }\n    }\n\n\n\n\n\textremal level of safety with some level of concurrency... Do we need exact value for 'stats.inserts' (if it is not synchronized)?\n\n\n\nIt can be 'long' inside synchronized block... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12618826",
            "date": "2008-07-31T19:36:29+0000",
            "content": "If you don't need the synchronized block, then an atomic variable for \"inserts\" (for example) would be a big win.\nBut if you have the synchronized block anyway, it's probably faster to just expand it's scope if the operations to be done are simple. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618934",
            "date": "2008-08-01T05:21:25+0000",
            "content": "my findings on a simple perf test with no contention (single thread)\n\nThe code is there in the main()\n\ncache size 1 million\n\nwith Hashmap\n\n\ttime taken  for 1 million inserts = 2019ms\n\ttime taken for 1 million gets = 625\n\ttime taken  for cleanup  = 345ms\n\n\n\nwith ConcurrenthashHashMap\n\n\n\ttime taken  for 1 million inserts  = 2437(roughly 20% slower than hashmap but small in absolute numbers)\n\ttime taken for 1 million gets  = 393ms (actually faster than simple HashMap )\n\ttime taken  for cleanup = 298ms (actually faster)\n\n\n\nother observations \nThe extra thread may not be be necessary . The unlucky put() may take around .25 secs to .5secs for a cache size of 1 million .\nIf we keep the value of (highHaterMark -lowWaterMark) value very high cleanups will be infrequent\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12621779",
            "date": "2008-08-12T11:30:55+0000",
            "content": "Full SolrCache implementation \nnot tested \n\nthe initialization parameters are same us the current LRUCache.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12621827",
            "date": "2008-08-12T14:13:33+0000",
            "content": "this is the right patch "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12623656",
            "date": "2008-08-19T12:42:29+0000",
            "content": "with testcase "
        },
        {
            "author": "Antony Bowesman",
            "id": "comment-12627592",
            "date": "2008-09-02T07:05:45+0000",
            "content": "I have also been considering a concurrent LRU cache for my own application and seeing this isse made me think about it again.  Wouldn't one option be to use a ReentrantReadWriteLock to synchronise the map rather than complete synchronisation on the map for both readers and writers.  Although that does not give a free get() it would at least allow concurrent get and still be able to use the LinkedHashMap and would not require the extra thread.  Not sure if SOLR is java 1.5, but if not you could still use Doug Lea's concurrent package for pre 1.5 Java. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12627598",
            "date": "2008-09-02T08:00:00+0000",
            "content": "The patch contains a implementation which uses the java 5 features (ConcurrentHashMap) .It is better than using a separate Lock "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12634373",
            "date": "2008-09-25T04:39:35+0000",
            "content": "name change and some refactoring "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12635032",
            "date": "2008-09-26T21:28:34+0000",
            "content": "Here is a prototype of an idea I've had for a while for an efficient concurrent LRU cache.\nIt is completely untested... consider it more \"code as design\".  It should feature faster cleaning - O( n ) when it works well.\n\nIn addition to low and high water marks, it adds the concept of an \"acceptable\" water mark.  A cleaning phase will try to go to the low water mark, but will be considered successful if it hits the acceptable water mark.\n\nThis is coupled with a multi-pass cleaning phase.  From the comments:\n\n    // if we want to keep at least 1000 entries, then timestamps of\n    // current through current-1000 are guaranteed not to be the oldest!\n    // Also, if we want to remove 500 entries, then\n    // oldestEntry through oldestEntry+500 are guaranteed to be\n    // removed.\n\n\n\nThe oldestEntry and newestEntry in the set of entries currently being considered is recorded for each phase.  Entries that are new enough such that they are guaranteed to be kept are immediately removed from consideration, and entries that are old enough such that they are guaranteed to be removed are immediately removed (no sorting necessary).  After 2 phases of this (configurable) and we still haven't removed enough entries, a priority queue is used to find the oldest entries out of those remaining.\n\nThere are undoubtedly some other tricks we can use, but this was the best I could come up with for now. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12635108",
            "date": "2008-09-27T06:50:41+0000",
            "content": "Looks good. This has a lot in common with my approach. The doClean() is done far more efficiently in your implementation . I can improve mine with your cleanup code (if you think it is fine) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12635159",
            "date": "2008-09-27T15:17:40+0000",
            "content": "I can improve mine with your cleanup code (if you think it is fine)\n\n+1\n\nI'd also include the manual tracking of size() that mine did... the ConcurrentHashMap.size() doesn't look fast.\n\nAnother thing to think about : pass an optional Executor in the constructor instead of creating a cleaning thread... and if it's null, it means \"do it in the foreground\".  That would add flexibility and the ability to avoid one thread per cache if desired. "
        },
        {
            "author": "Fuad Efendi",
            "id": "comment-12635221",
            "date": "2008-09-28T13:36:47+0000",
            "content": "Paul, Yonik,  thanks for your efforts; BTW 'Concurrent'HashMap uses spinloops for 'safe' updates in order to avoid synchronization (instead of giving up CPU cycles); there are always cases when it is not faster that simple HashMap with synchronization.\n\nLingPipe uses different approach, see last comment at SOLR-665.\n\nAlso, why are you in-a-loop with LRU? LFU is logically better.\n\n+1 and thanks for sharing. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12635230",
            "date": "2008-09-28T15:06:46+0000",
            "content": "My implementation just uses a Map<K,V> internally. If we can get a Map implemenatation that is faster than  ConcurrentHashMap (and concurrent) we can replace it (after seeing the performance). \n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12637029",
            "date": "2008-10-06T07:29:43+0000",
            "content": "borrowed some ideas from yonik's impl.\nProbably this is a good enough first cut.   "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12639791",
            "date": "2008-10-15T10:32:33+0000",
            "content": "Yonik \u2013 do you think this is good enough to go in now? Probably some users can try it out and report their experiences if we commit it early. I can take this up if you want. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12643190",
            "date": "2008-10-28T12:11:15+0000",
            "content": "\n\tAdded comments in the code\n\tFixed a few concurrency issues\n\n\n\nI'll commit this shortly. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12643328",
            "date": "2008-10-28T20:14:16+0000",
            "content": "Committed revision 708656.\n\nThanks Fuad, Noble and Yonik! "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12643520",
            "date": "2008-10-29T15:45:32+0000",
            "content": "Huge thanks on this one. This was one of the bottlenecks I've seen previously. For apples to apples load tests, this more then doubled my overall throughput.\n\nI do notice a sort of \"pulsing\" in the responses. It appears that everything flies along, but on occasion everything piles up for a second, then starts going again. This leads to a few response times that are over 1 second, but the average is way down closer to 20ms. Is this the cleanup involved when hitting a high-water mark?\n\nOverall, it's a huge improvement. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12643535",
            "date": "2008-10-29T17:31:09+0000",
            "content": "This leads to a few response times that are over 1 second, but the average is way down closer to 20ms.\nyeah you are right.\nthere is one more feature in ConcurrentLRUCache whcih enables cleanups to be done in a new thread .FastLRUCache is not using it yet .I'll give a patch soon . This will take care of that 1 sec delay. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12643536",
            "date": "2008-10-29T17:31:26+0000",
            "content": "For apples to apples load tests, this more then doubled my overall throughput. \nVery good to hear that!\n\nIs this the cleanup involved when hitting a high-water mark?\nYes, a cleanup is attempted when the size crosses the high watermark ('size' config parameter). It is done in two stages. In the first stage, least recently used items are evicted. If, after the first stage, the cache size is still greater than 'acceptableSize' config parameter (default is 0.95*maxSize), the second stage takes over. The second stage is more intensive and tries to bring down the cache size to the 'minSize' config parameter (default is 0.9*maxSize).\n\nNote that the cleanup is done in the same thread which calls a put on the cache, hence the 'pulsing' that you are seeing. The cache implementation supports using a separate cleanup thread too, however it is not used currently. We still need to evaluate the best way to use it and how much it can help. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12643555",
            "date": "2008-10-29T17:54:17+0000",
            "content": "run cleanup in new thread "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12643604",
            "date": "2008-10-29T18:58:10+0000",
            "content": "I'm not sure if that helped out. I haven't run a profiler yet ,but I think the \"pulsing\" (for lack of a better term) is caused by something else.\n\nHere's why... I used the new FastLRUCache only for my Document cache in my latest test. The document cache size is large enough to hold all of the documents in the test set, helping focus on cache behavior. The warming query is enough to get all documents into the cache on startup. So, the cache is essentially as full as it's gonna get. 100% hit rate, and not growing. Yet, it still exhibits this pulsing. Could it be associated with the overhead of maintaining the least recently used entries?\n\nThe introduction of the background thread didn't address this. It also didn't appear to speed things up, in fact it dropped overall throughput a bit, though still better then they old LRUCache. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12643738",
            "date": "2008-10-30T03:49:41+0000",
            "content": "Could it be associated with the overhead of maintaining the least recently used entries?\nThe overhead is ~= 0. It just has to increment an AtomicLong everytime you do a get() .I suspect the 'pulsing' may be because of GC pauses. enable GC logging and you will know "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12643968",
            "date": "2008-10-30T14:01:45+0000",
            "content": "Todd, what's the used size of your Document cache?\nI'll review this latest incarnation of ConcurrentLRUCache to see if there's anything that might cause this. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12643980",
            "date": "2008-10-30T14:27:25+0000",
            "content": "Some minor updates:\n\n\tfix thread saftey issue in finalizer (background thread may never see stop being set)\n\tfix tracking of size in put() to only increment if oldValue != null\n\toptimize cleaning check in put() since it will be called for every put until the size is back down.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12643984",
            "date": "2008-10-30T15:00:32+0000",
            "content": "Todd: have you verified that the hit rate stays at 100% (no new inserts, no new evictions, etc)?  If so, it might just be GC as Noble suggests.  Bigger heaps often increase GC pauses. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12643988",
            "date": "2008-10-30T15:10:25+0000",
            "content": "I'll hook up profiling and see if my GC overhead has changed, or is seeing big peaks. Should be later today.\n\nMy document cache is only 4000 for that test, but only about 3300 documents are in it. I wanted to focus on the overhead of getting things out of LRUCache, as in production we get >97% hit rates. I verified it's at 100% hit rate (warming fills it). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12644008",
            "date": "2008-10-30T16:07:05+0000",
            "content": "Committed revision 709188.\n\nThanks Yonik! "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644023",
            "date": "2008-10-30T16:34:21+0000",
            "content": "The markAndSweep logic in the current code didn't replicate the logic I gave in my example ConcurrentLRUCache, and it can remove a lot more than it should.\n\nSpecifically, my algorithm broke the results into 3 groups:\n\n\tthose documents that are guaranteed to be in the top group (most recently accessed)\n\tthose documents guaranteed to be in the bottom group (immediately discard)\n\tthose documents where you can't tell.\n\n\n\nThe current code reversed this logic, assuming that one can remove everything that is not in the top group.  This isn't valid though, as lastAccess isn't uniform (and thus the size of the top group could be as small as 1). "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12644093",
            "date": "2008-10-30T20:38:04+0000",
            "content": "I ran with a profiler and I'm not seeing any bursts of garbage collection. It's at a steady ~2%, with no major collections occurring (which is great!). However, the use of the profiler also slows things down about 10-20 % which seems to be enough that the pulsing goes away. I believe the pulsing may be some sort of limit of the testing I'm doing on a limited local environment. I'll include the patch in my current Solr app and do a more accurate comparison with our production level load tests to see if it still exists. Though it may be a while before I can provide feedback from that one, as getting machines allocated for the heavy load testing can take a bit.\n\nAs I said before, this is a huge improvement. Thanks for all the work on this one. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644097",
            "date": "2008-10-30T20:44:28+0000",
            "content": "I'll take a crack at updating the patch such that too many entries aren't removed. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12644195",
            "date": "2008-10-31T01:47:55+0000",
            "content": "The current code reversed this logic,\n\nI missed the point . I guess this post made it clear . As you mentioned it should be a 3 step cleanup "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12644216",
            "date": "2008-10-31T05:35:43+0000",
            "content": "I forgot to add license headers to the three source files for this issue. I'll hold off adding them lest it breaks patches that you guys are working on. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12644222",
            "date": "2008-10-31T06:13:04+0000",
            "content": "yonik's suggestions implemented "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12644235",
            "date": "2008-10-31T08:15:30+0000",
            "content": "creating an array[map.size()] for every markAndSweep() is expensive. Iterator should be better "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644321",
            "date": "2008-10-31T15:45:12+0000",
            "content": "Thanks Noble, we had a mid-air implementation collision \nI'm doing some quick performance testing the version I wrote now...I'll try it against your version after and then we can go from there. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644381",
            "date": "2008-10-31T18:04:25+0000",
            "content": "OK, here's the implementation based on my previous pseudo code, along with a very quick performance test.\nThe test uses random keys over a slightly bigger range than the table.\nIt also uses an upper water mark 10% higher than the lower water mark, and an acceptable water mark half way inbetween.  I haven't experimented to see what the best acceptable water mark is for either impl.  If anyone wants to do a more realistic test with real queries, they are welcome to it.\n\nI did 4 runs and took the lowest number for each sub-test.  Java6 -server, WinXP, P4.  Times in milliseconds.\n\n    doPerfTest(2000000, 100000, 200000); // big cache\nnoble=17063  yonik=9157\n    doPerfTest(2000000, 100000, 120000);  // smaller key space increases distance between \noldest, newest and makes the first passes less effective.\nnoble=8391  yonik=5812\n    doPerfTest(6000000, 1000, 2000);    // small cache, smaller hit rate\nnoble=17578  yonik=12515\n    doPerfTest(6000000, 1000, 1200);    // small cache, bigger hit rate\nnoble=11500  yonik=8219\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644389",
            "date": "2008-10-31T18:21:08+0000",
            "content": "Nobe, your latest patch contains code like this:\n\n    if (!markAndSweepLock.tryLock()) return;\n    long oldestItem = this.oldestItem;\n    [...]\n    markAndSweepLock.unlock();\n    this.oldestItem = oldestItem;\n\n\n\noldestItem isn't volatile (and doesn't need to be if accessed correctly).\nThe lock will also serve as a read barrier, so the first part is OK.\nThe unlock will serve as a write barrier, but the set of oldestItem comes after it (not OK... another thread may not see the value).\nChanging the order (the write to oldestItem before the unlock) will ensure that the next thread that crosses a read barrier will see the new value. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644532",
            "date": "2008-11-01T19:20:52+0000",
            "content": "Added some minor changes, making sure that minLimit >= 1 and limit >minLimit (needed for rounding with small cache sizes).\nAlso added test code for LRUCache vs FastLRUCache.\nIt appears that LRUCache is faster (at least on my single proc PC) when the hit ratio is low, and FastLRUCache is faster when the hit ratio is high.\nShould FastLRUCache be made the default in the example schema for the filterCache?\n\n\ntime=2937 impl=LRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=2266 impl=FastLRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=3594 impl=LRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075\ntime=1484 impl=FastLRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=3203 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225\ntime=4593 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.751506\ntime=3781 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.834685\ntime=2656 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8232835000000001\ntime=3234 impl=LRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.523398\ntime=5047 impl=FastLRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.3831675\ntime=4125 impl=LRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.511871\ntime=3969 impl=FastLRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.6665975\ntime=3390 impl=LRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.1445725\ntime=5687 impl=FastLRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10041049999999996\ntime=4750 impl=LRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10340150000000004\ntime=6875 impl=FastLRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.22233749999999997\ntime=1343 impl=LRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=860 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=1547 impl=LRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=703 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=1610 impl=LRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.833648\ntime=2406 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.7404839999999999\ntime=2078 impl=LRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8334255\ntime=859 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.998974\ntime=1922 impl=LRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5003285\ntime=2875 impl=FastLRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.3516785\ntime=2422 impl=LRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5002055000000001\ntime=1203 impl=FastLRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.821195\ntime=2297 impl=LRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10054949999999996\ntime=2969 impl=FastLRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.05416350000000003\ntime=3078 impl=LRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10003499999999999\ntime=3000 impl=FastLRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10475299999999999\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644540",
            "date": "2008-11-01T21:07:41+0000",
            "content": "Latest patch:\n\n\tfixes an off-by-one (it was possible to go below minSize)\n\tfixes tests to not expect a specific number of evictions.\n\tmakes FastLRUCache the default for filterCache in the example schema and main test schema.\n\n\n\nI'll commit soon if there are no objections. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12644554",
            "date": "2008-11-02T02:27:12+0000",
            "content": "\nCacheEntry[] eset = new CacheEntry[sz];\nint eSize = 0;\n\n\n\nIsn't it too expensive to create a potentially huge array every time we do a clean? (too much work for GC) .May be we do not even need it if the first loop is enough. Moreover this one thing has added more code.\n\nI didn't use lucene PriorityQueue because if somebody wished to lift the code they can easily do so if there is no other dependency. When I posted the requirement in google collections there was a lot of interest in such  a component. Can TreeSet do the trick?\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644594",
            "date": "2008-11-02T15:33:24+0000",
            "content": "Isn't it too expensive to create a potentially huge array every time we do a clean? (too much work for GC) \n\nThat's what benchmarking is for \n\nIt's a single short-lived allocation that allows us to  greatly reduce the number of elements we need to evaluate on successive passes.  Inserts into a TreeSet may have a higher GC cost given it's an allocation per insert.\n\nMay be we do not even need it if the first loop is enough.\n\nRight... although in my testing, it seemed like the first loop was rarely sufficient (although the second often was). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12644656",
            "date": "2008-11-03T07:24:58+0000",
            "content": "Here's the performance test from the patch on a more recent machine \u2013 Intel Quad Core, RHEL 64-bit, Java HotSpot(TM) 64-Bit Server VM (build 1.5.0_11-b03, mixed mode):\n\n\ntime=1456 impl=LRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=1041 impl=FastLRUCache nThreads= 1 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=3256 impl=LRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=754 impl=FastLRUCache nThreads= 2 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=1234 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225\ntime=1564 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.751506\ntime=3728 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835006\ntime=1384 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.798109\ntime=1357 impl=LRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.523398\ntime=1894 impl=FastLRUCache nThreads= 1 size=100000 maxKey=200000 gets=2000000 hitRatio=0.3831675\ntime=4556 impl=LRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.512785\ntime=1514 impl=FastLRUCache nThreads= 2 size=100000 maxKey=200000 gets=2000000 hitRatio=0.4682115\ntime=1614 impl=LRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.1445725\ntime=1837 impl=FastLRUCache nThreads= 1 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10041049999999996\ntime=4710 impl=LRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10963999999999996\ntime=1816 impl=FastLRUCache nThreads= 2 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.11144399999999999\ntime=339 impl=LRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=292 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=2511 impl=LRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=351 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=383 impl=LRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.833648\ntime=580 impl=FastLRUCache nThreads= 1 size=1000 maxKey=1200 gets=2000000 hitRatio=0.7404839999999999\ntime=2716 impl=LRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8337875\ntime=805 impl=FastLRUCache nThreads= 2 size=1000 maxKey=1200 gets=2000000 hitRatio=0.79799\ntime=570 impl=LRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5003285\ntime=794 impl=FastLRUCache nThreads= 1 size=1000 maxKey=2000 gets=2000000 hitRatio=0.3516785\ntime=3676 impl=LRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.49959549999999997\ntime=1685 impl=FastLRUCache nThreads= 2 size=1000 maxKey=2000 gets=2000000 hitRatio=0.436728\ntime=712 impl=LRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10054949999999996\ntime=1022 impl=FastLRUCache nThreads= 1 size=1000 maxKey=10000 gets=2000000 hitRatio=0.05416350000000003\ntime=4395 impl=LRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.100526\ntime=2562 impl=FastLRUCache nThreads= 2 size=1000 maxKey=10000 gets=2000000 hitRatio=0.08556600000000003\n\n\n\nWith more number of threads this time (4 and 16):\n\ntime=1794 impl=LRUCache nThreads= 4 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981608\ntime=594 impl=FastLRUCache nThreads= 4 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075\ntime=1737 impl=LRUCache nThreads= 16 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981607\ntime=602 impl=FastLRUCache nThreads= 16 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981602\ntime=2387 impl=LRUCache nThreads= 4 size=100000 maxKey=120000 gets=2000000 hitRatio=0.830956\ntime=866 impl=FastLRUCache nThreads= 4 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8892465\ntime=1793 impl=LRUCache nThreads= 16 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8274485\ntime=706 impl=FastLRUCache nThreads= 16 size=100000 maxKey=120000 gets=2000000 hitRatio=0.9586865\ntime=2233 impl=LRUCache nThreads= 4 size=100000 maxKey=200000 gets=2000000 hitRatio=0.5025255\ntime=1228 impl=FastLRUCache nThreads= 4 size=100000 maxKey=200000 gets=2000000 hitRatio=0.654153\ntime=1905 impl=LRUCache nThreads= 16 size=100000 maxKey=200000 gets=2000000 hitRatio=0.500583\ntime=883 impl=FastLRUCache nThreads= 16 size=100000 maxKey=200000 gets=2000000 hitRatio=0.9067965\ntime=5336 impl=LRUCache nThreads= 4 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10182199999999997\ntime=1780 impl=FastLRUCache nThreads= 4 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.25870800000000005\ntime=2911 impl=LRUCache nThreads= 16 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10132300000000005\ntime=1941 impl=FastLRUCache nThreads= 16 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.508488\ntime=687 impl=LRUCache nThreads= 4 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=421 impl=FastLRUCache nThreads= 4 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=782 impl=LRUCache nThreads= 16 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=452 impl=FastLRUCache nThreads= 16 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=813 impl=LRUCache nThreads= 4 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8333735\ntime=678 impl=FastLRUCache nThreads= 4 size=1000 maxKey=1200 gets=2000000 hitRatio=0.9988885\ntime=794 impl=LRUCache nThreads= 16 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8331635\ntime=503 impl=FastLRUCache nThreads= 16 size=1000 maxKey=1200 gets=2000000 hitRatio=0.977526\ntime=1554 impl=LRUCache nThreads= 4 size=1000 maxKey=2000 gets=2000000 hitRatio=0.500093\ntime=928 impl=FastLRUCache nThreads= 4 size=1000 maxKey=2000 gets=2000000 hitRatio=0.802332\ntime=1102 impl=LRUCache nThreads= 16 size=1000 maxKey=2000 gets=2000000 hitRatio=0.5002759999999999\ntime=566 impl=FastLRUCache nThreads= 16 size=1000 maxKey=2000 gets=2000000 hitRatio=0.954131\ntime=1543 impl=LRUCache nThreads= 4 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10062899999999997\ntime=1039 impl=FastLRUCache nThreads= 4 size=1000 maxKey=10000 gets=2000000 hitRatio=0.7582409999999999\ntime=1372 impl=LRUCache nThreads= 16 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10031000000000001\ntime=604 impl=FastLRUCache nThreads= 16 size=1000 maxKey=10000 gets=2000000 hitRatio=0.935282\n\n\n\nNow with 8 and 32 threads:\n\ntime=2109 impl=LRUCache nThreads= 8 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816075\ntime=608 impl=FastLRUCache nThreads= 8 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981606\ntime=1502 impl=LRUCache nThreads= 32 size=100000 maxKey=100000 gets=2000000 hitRatio=0.9816045\ntime=648 impl=FastLRUCache nThreads= 32 size=100000 maxKey=100000 gets=2000000 hitRatio=0.981592\ntime=3876 impl=LRUCache nThreads= 8 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8267995\ntime=748 impl=FastLRUCache nThreads= 8 size=100000 maxKey=120000 gets=2000000 hitRatio=0.915961\ntime=2176 impl=LRUCache nThreads= 32 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8271935\ntime=694 impl=FastLRUCache nThreads= 32 size=100000 maxKey=120000 gets=2000000 hitRatio=0.9652565\ntime=2038 impl=LRUCache nThreads= 8 size=100000 maxKey=200000 gets=2000000 hitRatio=0.5005305\ntime=1088 impl=FastLRUCache nThreads= 8 size=100000 maxKey=200000 gets=2000000 hitRatio=0.789179\ntime=2147 impl=LRUCache nThreads= 32 size=100000 maxKey=200000 gets=2000000 hitRatio=0.4997505\ntime=884 impl=FastLRUCache nThreads= 32 size=100000 maxKey=200000 gets=2000000 hitRatio=0.926915\ntime=2343 impl=LRUCache nThreads= 8 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10397699999999999\ntime=2207 impl=FastLRUCache nThreads= 8 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.34063\ntime=3440 impl=LRUCache nThreads= 32 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.10123850000000001\ntime=2087 impl=FastLRUCache nThreads= 32 size=100000 maxKey=1000000 gets=2000000 hitRatio=0.5367375\ntime=909 impl=LRUCache nThreads= 8 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=443 impl=FastLRUCache nThreads= 8 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=682 impl=LRUCache nThreads= 32 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=447 impl=FastLRUCache nThreads= 32 size=1000 maxKey=1000 gets=2000000 hitRatio=0.9998065\ntime=1189 impl=LRUCache nThreads= 8 size=1000 maxKey=1200 gets=2000000 hitRatio=0.832726\ntime=605 impl=FastLRUCache nThreads= 8 size=1000 maxKey=1200 gets=2000000 hitRatio=0.919104\ntime=1463 impl=LRUCache nThreads= 32 size=1000 maxKey=1200 gets=2000000 hitRatio=0.8337005\ntime=489 impl=FastLRUCache nThreads= 32 size=1000 maxKey=1200 gets=2000000 hitRatio=0.9845845\ntime=1256 impl=LRUCache nThreads= 8 size=1000 maxKey=2000 gets=2000000 hitRatio=0.500149\ntime=678 impl=FastLRUCache nThreads= 8 size=1000 maxKey=2000 gets=2000000 hitRatio=0.907774\ntime=1013 impl=LRUCache nThreads= 32 size=1000 maxKey=2000 gets=2000000 hitRatio=0.49962399999999996\ntime=503 impl=FastLRUCache nThreads= 32 size=1000 maxKey=2000 gets=2000000 hitRatio=0.976796\ntime=1504 impl=LRUCache nThreads= 8 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10030550000000005\ntime=754 impl=FastLRUCache nThreads= 8 size=1000 maxKey=10000 gets=2000000 hitRatio=0.9151345\ntime=1245 impl=LRUCache nThreads= 32 size=1000 maxKey=10000 gets=2000000 hitRatio=0.10028899999999996\ntime=499 impl=FastLRUCache nThreads= 32 size=1000 maxKey=10000 gets=2000000 hitRatio=0.978823\n\n\n\nWhen the number of threads are increased, FastLRUCache is true to its name  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12644730",
            "date": "2008-11-03T14:31:45+0000",
            "content": "That benchmark really isn't valid for a high number of threads though: notice the difference in hitRatio.\nIf you have many threads quickly adding items and only one thread at a time removing items, the FastLRUCache goes over it's target size and thus increases it's hitRatio, making it artificially faster.\n\nThis isn't a concern for it's use in Solr though, since the generation of a cache value will be much slower than clearing the cache. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12644738",
            "date": "2008-11-03T15:48:53+0000",
            "content": "Is there an easy way to get this patched into 1.3.0?\n\nRight now, I think I have to grab 7 patches and apply them in order. Will that give me the correct content? Is there an easier way to do this from the repository? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12644889",
            "date": "2008-11-04T04:04:29+0000",
            "content": "There is another number which we have ignored. If the cleanup is done in a separate thread , FastLRUCache consistently outperforms the legacy one. (shalin forgot to put the numbers). For a very large cache size , the cleanup takes ~200-300 ms. Which means a request can end up paying a huge price. \n\nWe need to add a new  'newCleanupThread' option to FastLRUCache (it is there in my old patch). I guess with that we can make FastLRUcache the default with newCleanupThread=true. \n\nIs there an easy way to get this patched into 1.3.0? \n\nIf you apply yonik's latest patch on trunk you get two extra files . You can straightaway copy those two files to 1.3 and use it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12644942",
            "date": "2008-11-04T10:55:02+0000",
            "content": "I ran it again with a new thread for each cleanup. Time taken for markAndSweep is printed:\n\ntime=1550 impl=LRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835225\nMarkAndSweepTime = 138\nMarkAndSweepTime = 35\nMarkAndSweepTime = 36\nMarkAndSweepTime = 39\nMarkAndSweepTime = 41\nMarkAndSweepTime = 42\nMarkAndSweepTime = 42\nMarkAndSweepTime = 44\nMarkAndSweepTime = 43\nMarkAndSweepTime = 43\nMarkAndSweepTime = 44\nMarkAndSweepTime = 43\nMarkAndSweepTime = 43\nMarkAndSweepTime = 44\nMarkAndSweepTime = 43\nMarkAndSweepTime = 44\nMarkAndSweepTime = 44\nMarkAndSweepTime = 43\ntime=1378 impl=FastLRUCache nThreads= 1 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8130459999999999\n\ntime=3942 impl=LRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.835045\nMarkAndSweepTime = 58\nMarkAndSweepTime = 165\nMarkAndSweepTime = 32\nMarkAndSweepTime = 34\nMarkAndSweepTime = 37\nMarkAndSweepTime = 37\nMarkAndSweepTime = 46\nMarkAndSweepTime = 40\nMarkAndSweepTime = 61\nMarkAndSweepTime = 53\nMarkAndSweepTime = 51\nMarkAndSweepTime = 44\nMarkAndSweepTime = 47\nMarkAndSweepTime = 47\nMarkAndSweepTime = 48\nMarkAndSweepTime = 48\nMarkAndSweepTime = 46\ntime=1062 impl=FastLRUCache nThreads= 2 size=100000 maxKey=120000 gets=2000000 hitRatio=0.8560415\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12645302",
            "date": "2008-11-05T19:35:42+0000",
            "content": "I just committed a fix to the setting of acceptableSize - it was always being set to maxSize, which would normally cause the cleaning routine to return after the first phase (and thus be called more often than normal). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12646094",
            "date": "2008-11-09T16:23:30+0000",
            "content": "added a new boolean attribute newCleanThread . Default is set to false "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12647587",
            "date": "2008-11-14T11:21:09+0000",
            "content": "Yonik, what do you think about using a new cleanup thread? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12647657",
            "date": "2008-11-14T16:42:41+0000",
            "content": "The ability to use a separate cleanup thread is interesting... but I'm not sure that having the ability to spin off a new thread for each cleanup is something one would ever want to do.  The cleanup thread logic should probably be fixed too (no sleeping and polling... it should wait until notified that a cleanup is needed) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12647798",
            "date": "2008-11-15T04:54:48+0000",
            "content": "OK that is a good idea. But this is an important functinality.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12648076",
            "date": "2008-11-17T04:17:37+0000",
            "content": "\n\tcleanup thread does wait() and get notified when needed\n\tConcurrentLRUCache is generified\n\tA new interface added to ConcurrentLRUCache called EvictionListener .This gets callback for each entry that is evicted\n\tFastLRUcache has a new configuration 'cleanupThread' . default is set to 'false' . I believe it should be true by default\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12648918",
            "date": "2008-11-19T06:26:21+0000",
            "content": "made CacheEntry non static  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12649342",
            "date": "2008-11-20T11:43:21+0000",
            "content": "Yonik, not trying to be pushy but can this patch be committed? \n\nI want to create a build for internal use out of trunk with this feature. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12649982",
            "date": "2008-11-22T22:26:37+0000",
            "content": "Does this have a thread leak?  Where is FastLRUCache.destroy() ever called?\nIt's called from the finalizer (yuck), but that finalizer will never be called because the cleaning thread references the cache (the definition of liveness).  Issues with having the cache deal with the thread lifecycle is why I previously recommended exploring the use of an Executor that the user passes in. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12650031",
            "date": "2008-11-23T16:22:52+0000",
            "content": " Yonik, Nice catch . There was a thread leak.\n\nI hope this patch fixes that. The cleanup thread now holds a WeakReference to the cache \nThe close() of Solrcache ensures that it is destroyed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12650035",
            "date": "2008-11-23T17:12:49+0000",
            "content": "Thanks Noble, looks like that solution should work.\n\nFunny thing with the latest patch though - I get compile errors with \"ant test\" from the command line:\n\ncompile-common:\n    [mkdir] Created dir: f:\\code\\solr\\build\\common\n    [javac] Compiling 39 source files to f:\\code\\solr\\build\\common\n    [javac] f:\\code\\solr\\src\\java\\org\\apache\\solr\\common\\util\\ConcurrentLRUCache.java:201: generic array creation\n    [javac]       CacheEntry[] eset = new CacheEntry[sz];\n    [javac]                           ^\n    [javac] f:\\code\\solr\\src\\java\\org\\apache\\solr\\common\\util\\ConcurrentLRUCache.java:379: non-static class org.apache.solr.common.util.ConcurrentLRUCache.Cache\nEntry cannot be referenced from a static context\n    [javac]       return ((CacheEntry)b).lastAccessedCopy < ((CacheEntry)a).lastAccessedCopy;\n   [...]\n\n\n\nIt looks like the compiler thinks that the method is static.  IntelliJ doesn't flag any errors, and I can't see anything wrong after a quick glance at the code.  Does \"ant test\" from the command line work for you? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12650055",
            "date": "2008-11-23T21:44:24+0000",
            "content": "OK, committed latest patch after some minor logic changes (including changing CacheEntry from an inner class to a static inner class, which solved the compilation errors). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12650103",
            "date": "2008-11-24T04:30:53+0000",
            "content": "\n\tShould we keep the cleanupThread= true default for FastLRUCache?\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12650104",
            "date": "2008-11-24T04:42:48+0000",
            "content": "I think the default should remain cleanupThread=false.  It's simpler behavior, and for normal cache sizes, the pause an individual request may see is less than what one would see from a GC pause. "
        }
    ]
}