{
    "id": "SOLR-7255",
    "title": "Index Corruption on HDFS whenever online bulk indexing (from Hive)",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.3",
        "status": "Resolved",
        "resolution": "Duplicate",
        "priority": "Blocker"
    },
    "description": "When running SolrCloud on HDFS and using the LucidWorks hadoop-lws-job.jar to index a Hive table (620M rows) to Solr it runs for about 1500 secs and then gets this exception:\n\nException in thread \"Lucene Merge Thread #2191\" org.apache.lucene.index.MergePolicy$MergeException: org.apache.lucene.index.CorruptIndexException: codec header mismatch: actual header=1494817490 vs expected header=1071082519 (resource: BufferedChecksumIndexInput(_r3.nvm))\n        at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:549)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:522)\nCaused by: org.apache.lucene.index.CorruptIndexException: codec header mismatch: actual header=1494817490 vs expected header=1071082519 (resource: BufferedChecksumIndexInput(_r3.nvm))\n        at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:136)\n        at org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.<init>(Lucene49NormsProducer.java:75)\n        at org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.normsProducer(Lucene49NormsFormat.java:112)\n        at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:127)\n        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:108)\n        at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:145)\n        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(BufferedUpdatesStream.java:282)\n        at org.apache.lucene.index.IndexWriter._mergeInit(IndexWriter.java:3951)\n        at org.apache.lucene.index.IndexWriter.mergeInit(IndexWriter.java:3913)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3766)\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:409)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:486)\n\n\nSo I deleted the whole index, re-create it and re-ran the job to send Hive table contents to Solr again and it returned exactly the same exception the first time after trying to send a lot of updates to Solr.\n\nI moved off HDFS to a normal dataDir backend and then re-indexed the full table in 2 hours successfully without index corruptions.\n\nThis implies that this is some sort of stability issue on the HDFS DirectoryFactory implementation.\n\nRegards,\n\nHari Sekhon\nhttp://www.linkedin.com/in/harisekhon",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-03-17T13:33:19+0000",
            "author": "Mark Miller",
            "content": "Depends on the version and options you are using. The write side of the hdfs block cache causes index corruption and has been disabled by default for a few releases now. ",
            "id": "comment-14365104"
        },
        {
            "date": "2015-03-17T13:37:21+0000",
            "author": "Mark Miller",
            "content": "4.10.3\n\nIf you indeed are on 4.10.3, I'd inspect the config and make sure the write side of the block cache is not on (it doesn't really give much or any benefit anyway IMO). The above exception pattern is a very common result if it is. ",
            "id": "comment-14365111"
        },
        {
            "date": "2015-03-18T11:34:31+0000",
            "author": "Hari Sekhon",
            "content": "Yes it was enabled, I've disabled it and re-ran the ingest which got further without index corruption... however the indexing speed on HDFS is so bad compared to local disk that the bulk ingest I'm doing that used to take 2 hours for 620M rows from Hive now runs for 16 hours and then fails with a broken pipe to the server... but that's a separate issue.\n\nBack to this setting - I believe solr.hdfs.blockcache.write.enabled is still set to true by default according to this page:\n\nhttps://cwiki.apache.org/confluence/display/solr/Running+Solr+on+HDFS\n\nDefault behaviour should probably be changed to false if this is buggy, then fixed and re-enabled when it works properly.\n\nIs there another ticket documenting work to fix this HDFS block write cache corruption issue (ie should we close this jira as duplicate)? ",
            "id": "comment-14366991"
        },
        {
            "date": "2015-03-22T19:00:38+0000",
            "author": "Mark Miller",
            "content": "There is an issue somewhere - as a stop gap the default was changed AFAIR. It should also print a warning in the logs if it is on. The doc should hopefully be out of date. ",
            "id": "comment-14375112"
        },
        {
            "date": "2015-11-10T20:19:13+0000",
            "author": "Gregory Chanan",
            "content": "There is an issue somewhere - as a stop gap the default was changed AFAIR. It should also print a warning in the logs if it is on. The doc should hopefully be out of date.\n\nI did a quick check \u2013 it doesn't look like anything was changed in 4.10 (default is true).  On trunk, it doesn't look like you can even set it to true anymore.\n\nI think Hari has a point; we should explain this better in the docs IMO. ",
            "id": "comment-14999268"
        },
        {
            "date": "2015-11-10T20:56:10+0000",
            "author": "Mark Miller",
            "content": "On trunk, there is really nothing to explain. This broken feature never should have existed, no reason to explain about it not being there. \n\nIt shouldn't be mentioned in the doc. Anyone can update or comment on that page of they want. \n\nI see this as a dupe report of that feature not working. If you want to take ownership and change it to a doc issue, feel free. Given its age, it wasn't benefiting me anymore.  ",
            "id": "comment-14999341"
        },
        {
            "date": "2015-11-13T00:45:45+0000",
            "author": "Gregory Chanan",
            "content": "If you want to take ownership and change it to a doc issue, feel free\n\nSOLR-8286. ",
            "id": "comment-15003299"
        }
    ]
}