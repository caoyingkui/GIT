{
    "id": "LUCENE-75",
    "title": "Problem searching with WildCards and quotes",
    "details": {
        "labels": "",
        "priority": "Critical",
        "components": [
            "core/queryparser"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Hello,\n\nWhen I'm trying to search in an index, and the query has wildcards and quotes \n(see example), Lucene doesn't find hits. It is needed for example to search for \na String that has white spaces in it.\nExaple:\nMy index contains Document with field which its name is the string \"0\", and \nvalue is the string \"1234\".\nThe following code works fine, and finds 1 hit:\nCode:\n\nString queryStr = \"123*\";\nQuery query = QueryParser.parse(queryStr, \"0\", new StandardAnalayzer());\nHits hits = myIndexSearcher.search(query);\nSystem.out.println(\"Hits length: \" + hits.length());\n\nHowever, if the first line in the code will be:\nString queryStr = \"\\\"123*\\\"\";\nand not the original one (the string will have quotes in it (the first and last \ncharacters are the character \")), the search won't find any hit!!!!\nI've tried to put the * after the closing quote, or use parenthesis, but it \ndidn't help.\n\nThe quotes are important becasue I need the search with wildcards for strings \nthat have more than one word (for example \"hello world\"), and without quotes it \nis impossible to search for it. However, with quotes (\"\\\"hello w*\\\"\"), Lucene \ndoesn't find any hit. But then I've noticed that the problem isn't with the \nwhite spaces, because you can see from my example that also without the white \nspace, there is no any hit.\n\nDo you have a solution for it?\n\nYuval Vardi.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--example.zip": "https://issues.apache.org/jira/secure/attachment/12312207/ASF.LICENSE.NOT.GRANTED--example.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2003-01-16T22:08:51+0000",
            "content": "I'll be bold here and mark this one as INVALID.  It has to do with your Analyzer being used.  StandardAnalyzer tokenizes by breaking at whitespace (among many other rules) and so you aren't indexing tokens with quotes.  I'm sure a better description of this can be used here, but in essence you need to look at how you are indexing your documents and how the Analyzer is breaking it up into tokens/terms.You could use the near operator to get closer to the query you want, perhaps. ",
            "author": "Erik Hatcher",
            "id": "comment-12321171"
        },
        {
            "date": "2003-01-19T16:01:15+0000",
            "content": "i have the same bug!!!\ni have written my own Analyzer and it looks like this:\n--start code--\npublic class testAnalyzer extends Analyzer\n{\n   public TokenStream tokenStream(String fieldName, Reader reader)\n   {\n      return newtestTokenizer(reader);\n   }\n}\n--end code--\n\n--start code--\npublic class testTokenizer extends CharTokenizer\n{\n   public testTokenizer(Reader reader)\n   {\n      siper(reader);\n   }\n   protected boolean isTokenChar(char c)\n   {\n      return true;\n   }\n}\n--end code--\n\ni index the documents with this analyzer and queryparse with the same analyzer.\nwhen i search for \"12*\" i get a hit but when i search for \"\\\"12*\\\"\" i get no \nhits. so yes indeed this is a bug and if not please tell me what i am doing \nwrong.\n\nthanks,\n     Amit ",
            "author": "amit",
            "id": "comment-12321172"
        },
        {
            "date": "2003-01-19T21:54:46+0000",
            "content": "Again marking as INVALID.  Several points:- Your code is not compilable as you've put it here - be sure to provide a working example of a problem.  Just providing the TokenStream implementation is not enough to see the issue.  What is the document you're indexing?  Please provide a standalone complete example.- Your tokenizer is simply tokenizing the entire string its fed, and not breaking it.  Here's an example:        TokenStream ts = new TestTokenizer(new StringReader(\"12abc xyz foo bar\"));        while (true) {            Token token = ts.next();            if (token == null) \n{                break;            }\n            System.out.print(\"[\" + token.termText() + \"] \");        }This is using a corrected version of your TestTokenizer.  The output is:[12abc xyz foo bar]As you can see, the entire string is one token, not separate ones as would be typical with other implementations.It seems there is a misunderstanding of how the tokenizer and QueryParser are working.  Try constructing your own Query's rather than using QueryParser to see things in action better. ",
            "author": "Erik Hatcher",
            "id": "comment-12321173"
        },
        {
            "date": "2003-01-19T23:58:15+0000",
            "content": "Created an attachment (id=4489)\nexample source file ",
            "author": "amit",
            "id": "comment-12321174"
        },
        {
            "date": "2003-01-19T23:59:14+0000",
            "content": "well i dont quiet understand i have made a working code so you can see for \nyourself what i am talking about. why shouldnt i use the queryparser???\ni dont want to build a prefix query myself because if i get a query \nlike: \"123* OR 222\" then i dont want to play with it and the queryparser \nhandles it just fine.\nattached is the code of my sample.\nPLEASE HELP!!!!!!!!\n\nthanks,\n     Amit\n ",
            "author": "amit",
            "id": "comment-12321175"
        },
        {
            "date": "2003-01-20T07:14:57+0000",
            "content": "Ok, this is probably the last time I'll post on this issue.  Thank you for supplying working code.  The issue is QueryParser, and I think you'll find that the Lucene committers are not that interested in making QueryParser handle everyone's own needs.  Since you've already written your own Analyzer and Tokenizer, how about simply writing your own QueryParser now too?  Just clone Lucene's JavaCC-based parser and write your own.  Or pre-process the string passed to remove quotes before handing it to the parser.  I'm marking as WONTFIX this time.  I'm not a committer, but I believe the sentiment is that QueryParser does more than enough for you already.  If you re-open it, I suggest you submit a patch at least, or in some way make a stronger case for why you feel this is a problem that should be corrected in the common codebase. ",
            "author": "Erik Hatcher",
            "id": "comment-12321176"
        }
    ]
}