{
    "id": "LUCENE-5989",
    "title": "Allow StringField to take BytesRef value, to index a single binary token",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.2",
            "6.0"
        ]
    },
    "description": "5 years ago (LUCENE-1458) we \"enabled\" fully binary terms in the\nlowest levels of Lucene (the codec APIs) yet today, actually adding an\narbitrary byte[] binary term during indexing is far from simple: you\nmust make a custom Field with a custom TokenStream and a custom\nTermToBytesRefAttribute, as far as I know.\n\nThis is supremely expert, I wonder if anyone out there has succeeded\nin doing so?\n\nI think we should make indexing a single byte[] as simple as indexing\na single String.\n\nThis is a pre-cursor for issues like LUCENE-5596 (encoding IPv6\naddress as byte[16]) and LUCENE-5879 (encoding native numeric values\nin their simple binary form).",
    "attachments": {
        "LUCENE-5989.patch": "https://issues.apache.org/jira/secure/attachment/12672988/LUCENE-5989.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14159482",
            "author": "Michael McCandless",
            "content": "Patch, adding BinaryField... the field types are the same as StringField so\nwe could maybe instead just add a ctor for StringField taking\nBytesRef...  ",
            "date": "2014-10-05T09:05:41+0000"
        },
        {
            "id": "comment-14159500",
            "author": "Shai Erera",
            "content": "we could maybe instead just add a ctor for StringField taking BytesRef...\n\nWe could also rename StringField to KeywordField, making it more obvious that this field isn't tokenized. Then a KeywordField can take a String or BytesRef in ctors. ",
            "date": "2014-10-05T10:48:33+0000"
        },
        {
            "id": "comment-14159507",
            "author": "Uwe Schindler",
            "content": "We could also rename StringField to KeywordField, making it more obvious that this field isn't tokenized. Then a KeywordsField can take a String or BytesRef in ctors.\n\n+1\n\nPatch, adding BinaryField\n\nI don't like the violation that clear() is a no-op in BytesTermAttribute. In a correct world, this should null the bytesref and the TokenStream should set the BytesRef after clearAttributes.\n\nThis is not urgent here, but it violates the contract. I know NumericTermAttribute does similar things...  ",
            "date": "2014-10-05T11:25:30+0000"
        },
        {
            "id": "comment-14159511",
            "author": "Jack Krupansky",
            "content": "rename StringField to KeywordField, making it more obvious that this field isn't tokenized. Then a KeywordsField can take a String or BytesRef in ctors.\n\nBoth Lucene and Solr are suffering from a conflation of the two concepts of treating an input stream as a single token (\"a keyword\") and as a sequence of tokens (\"sequence of keywords\"). We have the \"KeywordTokenizer\" that does NOT tokenize the input stream into \"a sequence of keywords\". The term \"keyword search\" is commonly used to describe the ability of search engines to find \"individual keywords\" in extended streams of \"text\" - a clear reference to \"keyword\" in a tokenized stream.\n\nSo, I don't understand how it is claimed that naming StringField to KeywordField is making anything \"obvious\" - it seems to me to be adding to the existing confusion rather than clarifying anything. I mean, the term \"keyword\" should be treated more as a synonym for \"token\" or \"term\", NOT as synonym for \"string\" or \"raw character sequence\".\n\nI agree that we need a term for \"raw, uninterpreted character sequence\", but it seems to me that \"string\" is a more \"obvious\" candidate than \"keyword\".\n\nThere has been some grumbling at the Solr level that KeywordTokenizer should be renamed to... something, anything, but just not KeywordTokenizer, which \"obviously\" implied that the input stream will be tokenized into a sequence of keywords, which it does not.\n\nIn an effort to try to resolve this ongoing confusion, can somebody provide from historical background as to how KeywordTokenizer got its name, and how a subset of people continue to refer to an uninterpreted sequence of characters as a \"keyword\" rather than a string. I checked the Javadoc, Jira, and even the source code, but came up empty.\n\nIn short, it is a real eye-opener to see a claim that the term \"keyword\" in any way makes it \"obvious\" that input is not tokenized!!\n\nMaybe we could fix this for 5.0 to have a cleaner set of terminology going forward. At a minimum, we should have some clarifying language in the Javadoc. And hopefully we can refrain from making the confusion/conflation worse by renaming StringField to KeywordField.\n\nThen a KeywordsField can take a String\n\nIs that simply a typo or is the intent to have both a KeywordField (singular) and a KeywordsField (plural)? I presume it is a typo, but... maybe it's a Freudian slip and highlights this semantic difficulty that persists in the Lucene terminology (and hence infects Solr terminology as well.) ",
            "date": "2014-10-05T12:20:24+0000"
        },
        {
            "id": "comment-14159515",
            "author": "Robert Muir",
            "content": "\nThis is supremely expert, I wonder if anyone out there has succeeded\nin doing so?\n\nSo the solution is to proceed and make matters worse by requiring the user to also deal with the .document API? \n\nIf the user wants their field to work with various query-time features (queryparser, morelikethis, whatever), then they must deal with the tokenstream side anyway, so adding *Field doesn't help anything. It just adds yet another place they must plug in \"schema\" information (as opposed to only being once in Analyzer). Sure, its easier to get past indexwriter maybe, but you win the battle and lose the war.\n\nI'm not going to try to block the change, just please, please, please think about it. ",
            "date": "2014-10-05T12:47:41+0000"
        },
        {
            "id": "comment-14159528",
            "author": "Shai Erera",
            "content": "Is that simply a typo\n\nYes, fixed .\n\nThe term 'keyword' is of course overloaded here. When I propose KeywordField, I am following the existing Keyword* classes that we have: KeywordTokenizer, KeywordAnalyzer, KeywordAttribute. And from what I remember, when users ask how to parse 'keywords' they indexed as StringFields, we often tell them to use PerFieldAnalyzerWrapper with a KeywordAnalyzer for that field. That's why I feel that KeywordField fits better with the overall Keyword* tokenstream API. ",
            "date": "2014-10-05T13:55:54+0000"
        },
        {
            "id": "comment-14159825",
            "author": "David Smiley",
            "content": "This is supremely expert, I wonder if anyone out there has succeeded in doing so?\n\norg.apache.lucene.spatial.prefix.CellTokenStream     Though this doesn't count since it's in Lucene.\n\n+1 to make this easier via a BinaryField.  With BinaryField and auto-prefixing, CellTokenStream won't be needed for indexing a point.  But it's needed for other shapes and to support heat-map style faceting.\n\nJack's opinion about the \"Keyword\" name being far from obvious really resonated with me.  Despite Shai's reasonable explanation, it doesn't seem to me that changing the status-quo to anything non-obvious is helpful.  And it wouldn't seem like the text equivalent of BinaryField \u2013 for that the current name is perfect, I think.  But I do like the idea of simply having StringField taking a byte[] too such that there is no BinaryField.  Either way. ",
            "date": "2014-10-06T03:03:39+0000"
        },
        {
            "id": "comment-14160101",
            "author": "Michael McCandless",
            "content": "I have also never understood the origin of \"keyword\" meaning the\nentire string is treated as one token.  I don't think it's obvious.\nIt is consistent with the existing KeywordAnalyzer/Tokenizer, but I\ndon't think that's a good justification to further propagate non-obvious\nnaming.  I would rather rename KeywordTokenizer/Analyzer to\nsomething else...\n\nI guess net/net I would prefer here that we not add BinaryField and\ninstead keep the name StringField, just giving it another ctor to take\nbyte[]/BytesRef.  Added classes have an API cost higher than just an\nadded ctor, and the \"purpose\" of these two is exactly the same...\n\nI don't like the violation that clear() is a no-op in BytesTermAttribute. In a correct world, this should null the bytesref and the TokenStream should set the BytesRef after clearAttributes.\n\nThanks Uwe, I'll add a nocommit to somehow fix it ... seems like\nByteTermAttributeImpl.clear must null out its copy of the bytes, and\nthen BinaryTokenStream.reset must re-instate the next one (pulling it\nvia the previous setValue call?).  I guess I must add\nBinaryTokenStream.bytes too?  Our analysis APIs are ... challenging.\n\nSo the solution is to proceed and make matters worse by requiring the user to also deal with the .document API?\n\nBut if you can't even figure out how to get your IPv6 byte[]\n(LUCENE-5596) or your numeric value encoded as byte[4] or byte[8]\n(LUCENE-5879) into Lucene's IndexWriter in the first place, how will\nyou even have any hope of querying it? ",
            "date": "2014-10-06T08:37:54+0000"
        },
        {
            "id": "comment-14160259",
            "author": "Robert Muir",
            "content": "Why jump to the conclusion that a user would have a byte[] already for an IP address? Thats a horrible representation Why wouldn't they pass java.net.Inet6Address?\n\nI'm just saying that if they then go do the following in queryparser, why can't it please work? (ranges too)\n\n\n... AND address:\"1.2::3.4\" \n\n\n\nOtherwise, if we don't want to make binary/numeric/etc fields \"first class\", and only treat them as bastardizations of text fields, then please, do this consistently everywhere, parse them as text everywhere, so that they will work correctly everywhere. ",
            "date": "2014-10-06T12:51:20+0000"
        },
        {
            "id": "comment-14161141",
            "author": "Michael McCandless",
            "content": "Why jump to the conclusion that a user would have a byte[] already for an IP address? Thats a horrible representation Why wouldn't they pass java.net.Inet6Address?\n\nI agree: taking Inet6Address would be great, but under the hood it should be indexed as a byte[] (which this issue is trying to enable), right?  I'll go reopen LUCENE-5596...\n\nI'm just saying that if they then go do the following in queryparser, why can't it please work? (ranges too)\n\n+1, but that's really out of scope here?  I mean I don't think we can solve all of Lucene's \"schema\" issues here.  Seems like LUCENE-5596 should make it easy to do IP address range querying...\n ",
            "date": "2014-10-06T22:37:16+0000"
        },
        {
            "id": "comment-14481102",
            "author": "Michael McCandless",
            "content": "I think now that LUCENE-5879 is in, as a dark/unusable feature, we should add BinaryField (this issue) to at least shed a bit of light on it.\n\nWith BinaryField, apps can efficiently prefix and range search anything they can convert to/from byte[], e.g. BigInteger/Decimal, InetAddress (LUCENE-5596), int/long/float/double, etc.  On the LUCENE-6005 branch there is also a half-precision float (2 bytes).\n\nI don't think Lucene's lack of schema is a justifiable reason to block progress here. ",
            "date": "2015-04-06T10:03:03+0000"
        },
        {
            "id": "comment-14481265",
            "author": "Michael McCandless",
            "content": "I'll switch to just adding a StringField ctor that takes a BytesRef ... less API. ",
            "date": "2015-04-06T14:36:35+0000"
        },
        {
            "id": "comment-14481268",
            "author": "Robert Muir",
            "content": "If we fix this .document api to allow a StringField to have a binary value, maybe it could help with merge code.\n\nCurrently the StoredFieldsVisitor returns strings as java.lang.String, which is wasteful for the default merge implementation (it must decode/re-encode). If we could remove this and let the visitor deal with it, default merge could avoid this decode/re-encode and we might be able to even nuke some specialized bulk merge logic that we have solely for reasons like this (at the least we will speed up the worst case). I tried to look at this recently and the .document api stopped me. \n\nNot something we have to fix here, but just something related to think about when looking at how to change it. ",
            "date": "2015-04-06T14:42:26+0000"
        },
        {
            "id": "comment-14482902",
            "author": "Michael McCandless",
            "content": "If we fix this .document api to allow a StringField to have a binary value, maybe it could help with merge code.\n\nThis would be very nice ... I struggled some with it, but got stuck with StorableField.stringValue() returning String.  I think we need to keep that because that's also the API apps use to retrieve their stored fields.\n\nBut the default merging operates on StorableDocument/StorableField, so I'm not sure how to separate the two.  Really there are two concepts: the \"schema\" for this doc (did it store a binary or string value for this field), and what's used to represent a string value (byte[] vs String), and both concepts are being smooshed together into this API.\n\nMaybe we could baby step here, and just change StoredFieldVisitor.stringField to take byte[]?  I know this doesn't help all the stupid work we do during default merge to decode/encode but at least it's a start ... ",
            "date": "2015-04-07T09:28:31+0000"
        },
        {
            "id": "comment-14482982",
            "author": "Michael McCandless",
            "content": "New patch, just adding another ctor to StringField taking BytesRef.\n\nI also cutover StoredFieldVisitor.stringField from String -> byte[] and put TODOs to try to eliminate the conversion that default stored fields merge impl does.\n\nI tried to fix BinaryTokenStreams attr to be \"proper\" as Uwe Schindler but ran into problems because this BytesRef is pre-shared up front to consumers, so we can't null it in clear...\n\nI think it's ready. ",
            "date": "2015-04-07T10:43:37+0000"
        },
        {
            "id": "comment-14483021",
            "author": "Robert Muir",
            "content": "\nMaybe we could baby step here, and just change StoredFieldVisitor.stringField to take byte[]? I know this doesn't help all the stupid work we do during default merge to decode/encode but at least it's a start ...\n\nThanks for looking into it. Maybe we can remove the smooshing in a separate issue. I think its really bad that our default merge impl creates so many strings. ",
            "date": "2015-04-07T11:11:23+0000"
        },
        {
            "id": "comment-14483065",
            "author": "Uwe Schindler",
            "content": "I tried to fix BinaryTokenStreams attr to be \"proper\" as Uwe Schindler but ran into problems because this BytesRef is pre-shared up front to consumers, so we can't null it in clear...\n\nI don't think this is a problem here, because the TokenStream is only used internally and is never visible to the outside (isn't it?). Another thing is that Attribute's  copyTo() does not deep clone, but this is also not an issue (because nobody has the chance to copy this tokenstream anywhere else). Shai Erera and I fixed TokensStreams in another issue, where payloads were not cloned (see changelog, don't have issue number).\n\nIn general we should fix the TermToBytesRefAttribute and remove the horrible fillBytesRef, which was needed in Lucene 4.x because of some early Lucene 3 compatibility. But it makes it hard to use, so we should get rid of it. TermToBytesRefAttribute should only have a single method: getBytesRef() that returns the BytesRef.\n\nGenerally I am fine. The issues Robert mentioned should be done in a separate issue. ",
            "date": "2015-04-07T11:55:09+0000"
        },
        {
            "id": "comment-14490469",
            "author": "ASF subversion and git services",
            "content": "Commit 1672781 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1672781 ]\n\nLUCENE-5989: allow passing BytesRef to StringField to make it easier to index arbitrary binary tokens ",
            "date": "2015-04-10T22:24:48+0000"
        },
        {
            "id": "comment-14490851",
            "author": "ASF subversion and git services",
            "content": "Commit 1672842 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1672842 ]\n\nLUCENE-5989: allow passing BytesRef to StringField to make it easier to index arbitrary binary tokens ",
            "date": "2015-04-11T07:46:41+0000"
        },
        {
            "id": "comment-14490852",
            "author": "ASF subversion and git services",
            "content": "Commit 1672843 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1672843 ]\n\nLUCENE-5989: fix CHANGES entry ",
            "date": "2015-04-11T07:47:13+0000"
        },
        {
            "id": "comment-14586798",
            "author": "Anshum Gupta",
            "content": "Bulk close for 5.2.0. ",
            "date": "2015-06-15T21:42:41+0000"
        }
    ]
}