{
    "id": "LUCENE-3022",
    "title": "DictionaryCompoundWordTokenFilter Flag onlyLongestMatch has no affect",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "2.9.4,                                            3.1",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "When using the DictionaryCompoundWordTokenFilter with a german dictionary, I got a strange behaviour:\nThe german word \"streifenbluse\" (blouse with stripes) was decompounded to \"streifen\" (stripe),\"reifen\"(tire) which makes no sense at all.\nI thought the flag onlyLongestMatch would fix this, because \"streifen\" is longer than \"reifen\", but it had no effect.\nSo I reviewed the sourcecode and found the problem:\n[code]\nprotected void decomposeInternal(final Token token) {\n    // Only words longer than minWordSize get processed\n    if (token.length() < this.minWordSize) \n{\n      return;\n    }\n    \n    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.buffer());\n    \n    for (int i=0;i<token.length()-this.minSubwordSize;++i) {\n        Token longestMatchToken=null;\n        for (int j=this.minSubwordSize-1;j<this.maxSubwordSize;++j) {\n            if(i+j>token.length()) {\n                break;\n            }\n            if(dictionary.contains(lowerCaseTermBuffer, i, j)) {\n                if (this.onlyLongestMatch) {\n                   if (longestMatchToken!=null) {\n                     if (longestMatchToken.length()<j) {\n                       longestMatchToken=createToken(i,j,token);\n                     }\n                   } else {\n                     longestMatchToken=createToken(i,j,token);\n                   }\n                } else {\n                   tokens.add(createToken(i,j,token));\n                }\n            } \n        }\n        if (this.onlyLongestMatch && longestMatchToken!=null) {\n          tokens.add(longestMatchToken);\n        }\n    }\n  }\n[/code]\n\nshould be changed to \n\n[code]\nprotected void decomposeInternal(final Token token) {\n    // Only words longer than minWordSize get processed\n    if (token.termLength() < this.minWordSize) {      return;    }\n\n    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.termBuffer());\n\n    Token longestMatchToken=null;\n    for (int i=0;i<token.termLength()-this.minSubwordSize;++i) {\n\n        for (int j=this.minSubwordSize-1;j<this.maxSubwordSize;++j) {\n            if(i+j>token.termLength()) \n{\n                break;\n            }\n            if(dictionary.contains(lowerCaseTermBuffer, i, j)) {\n                if (this.onlyLongestMatch) {\n                   if (longestMatchToken!=null) {\n                     if (longestMatchToken.termLength()<j) \n{\n                       longestMatchToken=createToken(i,j,token);\n                     }\n                   } else \n{\n                     longestMatchToken=createToken(i,j,token);\n                   }\n                } else \n{\n                   tokens.add(createToken(i,j,token));\n                }\n            }\n        }\n    }\n    if (this.onlyLongestMatch && longestMatchToken!=null) \n{\n        tokens.add(longestMatchToken);\n    }\n  }\n[/code]\n\nSo, that only the longest token is really indexed and the onlyLongestMatch Flag makes sense.",
    "attachments": {
        "LUCENE-3022.patch": "https://issues.apache.org/jira/secure/attachment/12476311/LUCENE-3022.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-04-14T00:39:59+0000",
            "content": "This sounds like a bug, do you want to try your hand at contributing a patch?\n\nSee http://wiki.apache.org/lucene-java/HowToContribute for some instructions. ",
            "author": "Robert Muir",
            "id": "comment-13019637"
        },
        {
            "date": "2011-04-14T09:36:20+0000",
            "content": "Patch fixing this issue\nincluding JUnitTest ",
            "author": "Johann H\u00f6chtl",
            "id": "comment-13019752"
        },
        {
            "date": "2011-04-14T16:05:58+0000",
            "content": "Hi Johann, in my opinion your patch is completely correct, thanks for fixing this.\n\nI noticed though, that a solr test failed because its factory defaults to this value being \"on\" (and the previous behavior was broken!!!)\n\nBecause of this, I propose we default this behavior to \"off\" in the Solr factory and add an upgrading note. Previously decompounding in solr defaulted to buggy behavior, but I think by default we should index all compound components (since that seems to be what the desired intended behavior was, which mostly worked, only because of the bug!)\n\nI'll leave the issue open for a few days to see if anyone objects to this plan. ",
            "author": "Robert Muir",
            "id": "comment-13019885"
        },
        {
            "date": "2011-04-20T17:12:05+0000",
            "content": "Hi, it seems we need to figure out some things about this compound filter:\n\nWhat is the onlyLongestMatch option intended to do? Its description is a bit vague, is it:\n\n\tSupposed to only extract the longest-matching subword dictionary matches?\n\tSupposed to only extract the longest subword from a compound?\n\n\n\nAs an example, consider a dictionary containing: so, ft, soft, ball\nHow should \"softball\" be decomposed based on this option?\n\nIf onlyLongestMatch is interpreted to mean only the longest-matching subwords, then i think it should emit \"softball, soft, ball\". \"so\" and \"ft\" would not be emitted, because you set this option.\n\nHowever, if its interpreted to mean only the longest matching subword from the compound (like this patch), then it should only emit \"softball, soft\" and stop.\n\n\n\n\nonlyLongestMatch\ntrunk\nLUCENE-3022\nLUCENE-3038\n\n\nfalse\nsoftball, so, soft, ft, ball\nsoftball, so, soft, ft, ball\nsoftball, so, soft, ft, ball\n\n\ntrue\nsoftball, soft, ft, ball\nsoftball, soft\nsoftball, soft, ft, ball\n\n\n\n\n\nIts definitely clear that the current behavior is wrong, but what should the correct behavior be? Maybe we need two separate options here? ",
            "author": "Robert Muir",
            "id": "comment-13022232"
        },
        {
            "date": "2011-04-20T20:27:42+0000",
            "content": "Let's stay with this example:\nDict: \n{\"soft\",\"so\",\"ft\"}\nWord: \"softball\"\n\nThe first option onlyLongestMatch, which behaves in the way, that only the longest matching dictionary entry should be returned. (Should this option be modified to keep all of the longest matches? length(soft) == length(ball)?)\nOutput: \"soft\" if true; \"so\",\"ft\",\"soft\" if false\n\nThe second option should be keepRemain, which makes a term out of the remain after substracting the longestMatch (makes only sense with onlyLongestMatch!?)\nOutput: \"soft\",\"ball\" if keepRemain==onlyLongestMatch==true\n\nWith this second option you could keep the remains, which are not in your dictionary (reduces the complexity of the required dictionary and can improve the compound-splitting-logic) ",
            "author": "Johann H\u00f6chtl",
            "id": "comment-13022351"
        },
        {
            "date": "2011-06-03T16:40:40+0000",
            "content": "bulk move 3.2 -> 3.3 ",
            "author": "Robert Muir",
            "id": "comment-13043542"
        },
        {
            "date": "2013-07-23T18:44:51+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717076"
        },
        {
            "date": "2014-04-16T12:54:45+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970869"
        }
    ]
}