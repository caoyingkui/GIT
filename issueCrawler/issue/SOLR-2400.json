{
    "id": "SOLR-2400",
    "title": "FieldAnalysisRequestHandler; add information about token-relation",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.3",
            "4.0-ALPHA"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The XML-Output (simplified example attached) is missing one small information .. which could be very useful to build an nice Analysis-Output, and that's \"Token-Relation\" (if there is special/correct word for this, please correct me).\n\nMeaning, that is actually not possible to \"follow\" the Analysis-Process (completly) while the Tokenizers/Filters will drop out Tokens (f.e. StopWord) or split it into multiple Tokens (f.e. WordDelimiter).\n\nWould it be possible to include this Information? If so, it would be possible to create an improved Analysis-Page for the new Solr Admin (SOLR-2399) - short scribble attached",
    "attachments": {
        "field.xml": "https://issues.apache.org/jira/secure/attachment/12477255/field.xml",
        "SOLR-2400-revision1.patch": "https://issues.apache.org/jira/secure/attachment/12482333/SOLR-2400-revision1.patch",
        "110303_FieldAnalysisRequestHandler_output.xml": "https://issues.apache.org/jira/secure/attachment/12472604/110303_FieldAnalysisRequestHandler_output.xml",
        "SOLR-2400.patch": "https://issues.apache.org/jira/secure/attachment/12477254/SOLR-2400.patch",
        "110303_FieldAnalysisRequestHandler_view.png": "https://issues.apache.org/jira/secure/attachment/12472605/110303_FieldAnalysisRequestHandler_view.png"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Uwe Schindler",
            "id": "comment-13002292",
            "date": "2011-03-03T22:50:18+0000",
            "content": "The \"position\" is used e.g. in analysis.jsp to do exactly what you want to have. It is the token position. If no \"broken\" TokenFilters are used that do not correctly modify the posIncr attribute, you can simply use it for alignment. "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13002313",
            "date": "2011-03-03T23:20:05+0000",
            "content": "Uwe, that was the first thing i thought myself, yes - but .. let's take \"flat\" (starting on position 4) and follow it. Passing StopFilter, still position 4; Arriving at WordDelimiter, it's position 6 - the dash was dropped out while beeing an StopWord and VA902B gets splitted up in three Tokens.\n\nSo, what i guess, that it's missing .. is some type of information, that for example the original Token on position 2 (VA902B) is splitted an know (partial) placed on position 3 through 6 .. also for example, that flat is no longer position 4, because it's moved to 6.\n\nOr did i just miss something really simple? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13002491",
            "date": "2011-03-04T07:41:21+0000",
            "content": "Stefan, this is a general issue of TokenStreams adding Tokens. TokenStreams that remove Tokens should automatically preserve position, but not even all of those do that correctly (we were fixing some of them lately). The way of how the Lucene analysis works makes it impossible to guarantee any corresponence of the position numbers. Because for the indexer it's only important what comes out at the end, the steps inbetween are not interesting. AnalysisReqHandler on the other hand does some bad \"hacks\" to look \"inside\" the analysis (by using temporary TokenStreams that buffer tokens), which are not the general use-case of TokenStreams.\n\nI wonder a little bit about your xml file, it only contains text and position, but it should also contain rawTerm, startOffset, endOffset. When I call analysis i get all of those attributes not only two of them. Is this a hand-made file or what is the problem? Which Solr version?\n\nOne possibility to handle the thing might be the char offset in the original text, because that the req handler may use the character offset of begin and end of the token in the original stream instead of the token position, but this is likely to break for lots of TokenFilters (WordDelimiterFilter would work as long as you don't do stemming before...). The problem is incorrect handling of offset calculation (also leading to bugs in highlighting) when the inserted terms are longer than their originals.\n\nAlltogether: Its unlikely that you can implement that and it will work for all combinations of TokenStream components. "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13002533",
            "date": "2011-03-04T10:48:11+0000",
            "content": "Uwe, thanks for your reply.\n\nI wonder a little bit about your xml file, it only contains text and position, but it should also contain rawTerm, startOffset, endOffset. When I call analysis i get all of those attributes not only two of them. Is this a hand-made file or what is the problem? Which Solr version?\nMy fault, indeed it's not the original output - i thought it would be enough to demonstrate the point which i was talking about, sorry for that.\n\nMy Solr 4.x nighlty-build from last week only has the following output; there is no rawTerm - which would be extremly helpful, because with this information it should be possible to establish to relation i talked about earlier. \n\n\n<!-- .. -->\n<arr name=\"org.apache.lucene.analysis.standard.StandardTokenizer\">\n  <lst>\n    <str name=\"text\">this</str>\n    <str name=\"type\"><ALPHANUM></str>\n    <int name=\"start\">0</int>\n    <int name=\"end\">4</int>\n    <int name=\"position\">1</int>\n  </lst>\n  <!-- .. -->\n</arr>\n<!-- .. -->\n\n\nMay i miss an important configuration-setting for having rawTerm in Analysis-Output? "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13016502",
            "date": "2011-04-06T20:05:25+0000",
            "content": "I've checked out the current trunk-Revision .. but could not see any change on that, especially the raw-Term thing. Did i miss something else? Special Setting required for getting this property? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13016580",
            "date": "2011-04-06T22:17:50+0000",
            "content": "Hi Stefan,\n\nsorry for missing your last response.\n\nAbout the raw term: The raw term is only shown by solr currently, if the term is only binary (like numerics) or similar (when the FieldType does some transformation like with the deprecated Sortable*) fields. I just mentioned it as example that I was missing some attributes in your example output. To solve your problem it is of no use.\n\nI already mentioned:\nOne possibility to handle the thing might be the char offset in the original text, because that the req handler may use the character offset of begin and end of the token in the original stream instead of the token position, but this is likely to break for lots of TokenFilters (WordDelimiterFilter would work as long as you don't do stemming before...). The problem is incorrect handling of offset calculation (also leading to bugs in highlighting) when the inserted terms are longer than their originals.\n\nThis might be your only chance (using the OffsetAttribute), but it is likely to break. What you want to have is not possible with the analysis API of Lucene, as some information is missing (as not needed during analysis - the absolute positions are not important for the indexer, so TokenStreams don't preserve them.\n\nA possibility to preserve the original positions would be a trick in the analysis RequestHandler: It could insert a Fake TokenFilter directly after the Tokenizer, that adds an additional Attribute with the absolute position (incremented on each call to input.incrementToken()). This could be a hack to achieve what you want.\n\nMaybe I can help you, but that needs some refactoring in AnalysisRequestHandlers, but might be a good idea. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13024613",
            "date": "2011-04-24T11:05:07+0000",
            "content": "Hi Stefan\n\nyou seem to work again on this admin interface. How about my last proposal: Adding an internal TokenFilter in the FieldAnalysisRequestHandler that is inserted directly after the Tokenizer before the first TokenFilter? This one could simply count the tokens emitted by the Tokenizer and add it as a special attribute. By this every Token emitted by Tokenizer would get a unique ID (a integer). If some TokenFilter later splits a token, both would get the same ID. Please note: This only works for the first Tokenizer and all TokenFilters together. If another TokenFilter later again splits Tokens produced by a TokenFilter before, all those would get the original ID of the Tokenizer.\n\nAny comments? This should be quite simple to implement. "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13024614",
            "date": "2011-04-24T11:13:33+0000",
            "content": "Hi Uwe,\n\nsorry, missed your last comment :/\n\nAny comments? This should be quite simple to implement.\nSounds great! Sample/Static/Sample Output of the Analysis-Handler should be enough for the first setp to check, if we could (easily, more or less *g) integrate that.\n\nThanks\nStefan "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13024615",
            "date": "2011-04-24T11:22:56+0000",
            "content": "After thinking a little bit more, I think it would even be possible to add this Filter after each Step to track tokens. The resulting Attribute would then contain the whole tracking of positions:\n\n\tAfter Tokenizer this attribute would contains \"0\", \"1\", \"2\",...\n\tAfter the first TokenFilter: \"0.0\", \"1.1\", \"1.2\", \"1.3\", \"2.2\" (while the second token (1) emitteded by the Tokenizer was split into 3 Tokens). I think this would help? Additionally the Filter could use PositionIncrement to track same position tokens - or this could be left to the consumer (so if 1.2 and 1.3 have posIncr 0, the consumer knows that they all are at same position). If the TokenFilter would use the PosIncr to increment the unique IDs, then this would be solved (so 1.x tokens would always get \"1.1\" as ID if at same position).\n\n\n\nI will think about it an supply a patch that enriches the FieldAnalysisContentHandler by this extra attribute.\n\nWe can then iterate. But today is Easter Holiday, so little bit later... "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13024624",
            "date": "2011-04-24T13:48:46+0000",
            "content": "I will think about it an supply a patch that enriches the FieldAnalysisContentHandler by this extra attribute.\nGoes better an better - the more information we have, the more we could display :> everything that's possible and help to understand the Analysis would be great!\n\nWe can then iterate. But today is Easter Holiday, so little bit later...\nwhenever you'll find the time, i'll continue work on another topic. ty anway Uwe. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13024678",
            "date": "2011-04-24T21:23:43+0000",
            "content": "Here a first & quick patch for TRUNK (may not apply to 3.x).\n\nThe FieldAnalysisRequestHandler behaves as before, only tht it adds an additional property \"positionHistory\" to the named lists with attributes. This property contains all positions this token had before, the last one ist the actual position repeated. \"2.2.4.4\" means that this token had position 2 after Tokenizer, after first filter still 2, but then changed to 4 after second filter. The actual position after 3rd filter is 4.\n\nBy the way, this also fixes a bug in the RequestHandler: The list of tokens is sorted on printout (by position) and the original list is modified by that. Later Filters will then see the Tokens in the new order, which is a bug. The new code copies the List to an array first to dont touch the tokens. This bug only affects strange TokenStreams with negative position increments, so we can fix this together with this issue (once it is committed).\n\nAn example output is:\nhttp://localhost:8983/solr/analysis/field?analysis.fieldtype=text&analysis.fieldvalue=moo-moo+dontstems+foo-bar+and+this+fucking+token\n\n(default schema, Solr trunk)\n\nHope that helps. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13024679",
            "date": "2011-04-24T21:26:08+0000",
            "content": "Here the output of above analysis in XML. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13024681",
            "date": "2011-04-24T21:32:19+0000",
            "content": "Updated patch (the deep clone in the attribute was not needed) "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13025958",
            "date": "2011-04-27T19:04:08+0000",
            "content": "Yes =) Ty Uwe, applied the Patch: works perfectly! I've tried splitting on Words, also removing of Stopwords - both are looking good.\nWill see how we could integrate this \u2013 actually for the normal languages an their analysis .. afterwords for the Japanase one  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13040056",
            "date": "2011-05-27T05:02:24+0000",
            "content": "Hi Stefan,\n\ndo you have any addition requirements to this patch? So it might be a good idea to also commit that one, so you can produce a full-featured analysis GUI in your great new admin interface, showing all token relations and their attributes.\n\nThat would really be an improvement over analysis.jsp!\n\nBy the way, to test out custom attributes, you can simply show tokens of a numeric field type like \"tint\", it will add some additional attributes (like shift...)!\n\nI would like to only change one part of my patch: The separator for the hierarchy levels is currently \".\", I would prefer \"/\" (like a fs path), any other ideas from the other committers? "
        },
        {
            "author": "Stefan Matheis (steffkes)",
            "id": "comment-13040152",
            "date": "2011-05-27T09:44:01+0000",
            "content": "Uwe,\n\ndo you have any addition requirements to this patch?\nNo, it's perfect \u2013 thank you \n\nSo it might be a good idea to also commit that one, so you can produce a full-featured analysis GUI in your great new admin interface, showing all token relations and their attributes.\nYes, that would be good. Actually the analysis page is not yet using it, but will integrate them while working on Otis Feedback from SOLR-2399\n\nBy the way, to test out custom attributes, you can simply show tokens of a numeric field type like \"tint\", it will add some additional attributes (like shift...)!\nAh cool, will do so\n\nI would like to only change one part of my patch: The separator for the hierarchy levels is currently \".\", I would prefer \"/\" (like a fs path), any other ideas from the other committers?\nGo for it, fine with me\n\nStefan "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13040324",
            "date": "2011-05-27T17:03:10+0000",
            "content": "I would prefer \"/\" (like a fs path), any other ideas from the other committers?\n\nSounds good to me "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13040631",
            "date": "2011-05-28T16:59:21+0000",
            "content": "Updated patch with '/' as history separator and updated to trunk.\n\nThere is still a good test missing (test coverage of FieldAnalysisReqHandler is already bad...) "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13047885",
            "date": "2011-06-11T11:42:56+0000",
            "content": "Here a new patch with test case for the positionHistory. It also adds another test for WDF in combination with FieldAnalysisReqHandler, as its more complicated there to track token position history.\n\nI think that is ready to commit! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13047949",
            "date": "2011-06-11T16:58:57+0000",
            "content": "Committed trunk revision: 1134685\nCommitted 3.x revision: 1134692 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13048547",
            "date": "2011-06-13T12:59:35+0000",
            "content": "I reopen this issue, as I don't really like the position history as String.\n\nI revised the patch to return a int[] (serialized as Integer[]) for positionHistory. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13048549",
            "date": "2011-06-13T13:03:08+0000",
            "content": "Will commit soon. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13048554",
            "date": "2011-06-13T13:15:57+0000",
            "content": "Small improvement to cache array when sorting. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13048623",
            "date": "2011-06-13T16:15:58+0000",
            "content": "Committed trunk revision: 1135154\nCommitted 3.x branch revision: 1135156 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13058963",
            "date": "2011-07-02T02:43:14+0000",
            "content": "Bulk close for 3.3 "
        }
    ]
}