{
    "id": "SOLR-7511",
    "title": "Unable to open searcher when chaosmonkey is actively restarting solr and data nodes",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.3",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "I have a working chaos-monkey setup which is killing (and restarting) solr and data nodes in a round-robin fashion periodically. I wrote a simple Solr client to periodically index and query bunch of documents. After executing the test for some time, Solr returns incorrect number of documents. In the background, I see following errors,\n\norg.apache.solr.common.SolrException: Error opening new searcher\n        at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1577)\n        at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1689)\n        at org.apache.solr.core.SolrCore.<init>(SolrCore.java:856)\n        ... 8 more\nCaused by: java.io.EOFException: read past EOF\n        at org.apache.solr.store.blockcache.CustomBufferedIndexInput.refill(CustomBufferedIndexInput.java:186)\n        at org.apache.solr.store.blockcache.CustomBufferedIndexInput.readByte(CustomBufferedIndexInput.java:46)\n        at org.apache.lucene.store.BufferedChecksumIndexInput.readByte(BufferedChecksumIndexInput.java:41)\n        at org.apache.lucene.store.DataInput.readInt(DataInput.java:98)\n        at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:134)\n        at org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoReader.read(Lucene46SegmentInfoReader.java:54)\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:358)\n        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:454)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:906)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:792)\n        at org.apache.solr.update.SolrIndexWriter.<init>(SolrIndexWriter.java:77)\n        at org.apache.solr.update.SolrIndexWriter.create(SolrIndexWriter.java:64)\n\nThe issue here is that the index state for one of the replica is corrupt (verified using Lucene CheckIndex tool). Hence Solr is not able to load the core on that particular instance. \n\nInterestingly when the other sane replica comes online, it tries to do a peer-sync to this failing replica and gets an error, it also moves to recovering state. As a result this particular shard is completely unavailable for read/write requests. Here is a sample log entries on this sane replica,\n\nError opening new searcher,trace=org.apache.solr.common.SolrException: SolrCore 'customers_shard1_replica1' is not available due to init failure: Error opening new searcher\n        at org.apache.solr.core.CoreContainer.getCore(CoreContainer.java:745)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:303)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:211)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.solr.servlet.SolrHadoopAuthenticationFilter$2.doFilter(SolrHadoopAuthenticationFilter.java:288)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:277)\n\n\n2015-05-07 12:41:49,954 INFO org.apache.solr.update.PeerSync: PeerSync: core=customers_shard1_replica2 url=http://ssl-systests-3.ent.cloudera.com:8983/solr DONE. sync failed\n2015-05-07 12:41:49,954 INFO org.apache.solr.cloud.SyncStrategy: Leader's attempt to sync with shard failed, moving to the next candidate\n2015-05-07 12:41:50,007 INFO org.apache.solr.cloud.ShardLeaderElectionContext: There may be a better leader candidate than us - going back into recovery\n2015-05-07 12:41:50,007 INFO org.apache.solr.cloud.ElectionContext: canceling election /collections/customers/leader_elect/shard1/election/93773657844879326-core_node6-n_0000001722\n2015-05-07 12:41:50,020 INFO org.apache.solr.update.DefaultSolrCoreState: Running recovery - first canceling any ongoing recovery\n2015-05-07 12:41:50,020 INFO org.apache.solr.cloud.ActionThrottle: The last recovery attempt started 2685ms ago.\n2015-05-07 12:41:50,020 INFO org.apache.solr.cloud.ActionThrottle: Throttling recovery attempts - waiting for 7314ms\n\nI am able to reproduce this problem consistently.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-05-07T20:14:35+0000",
            "author": "Hrishikesh Gadre",
            "content": "I think we can improve the error handling in Solr in following ways\n\n(a) If Solr is unable to load the core due to index corruption, we can still allow the core loading to succeed (but may be in recovery mode i.e. unavailable for read/write requests).\n(b) If Solr with such corrupt index gets an peer-sync request, it could inform the other replica that it's index state is corrupt so that the other replica can take over the leader role and send the updates back to the first one.\n\nAny thoughts? ",
            "id": "comment-14533302"
        },
        {
            "date": "2015-05-19T13:01:39+0000",
            "author": "Mark Miller",
            "content": "Yeah, we need to improve this. I think if a node detects corruption (eg, we can't open a reader), we should try and either purge the data and tlog dirs or roll over to a new index dir and purge the tlogs and then enter recovery. ",
            "id": "comment-14550389"
        },
        {
            "date": "2015-05-19T13:18:14+0000",
            "author": "Shawn Heisey",
            "content": "There are people who run a single Solr instance in production, even though we strongly recommend high availability practices.  I think some of those people are also using their single Solr instance as a primary data store.\n\nRolling over to a new index directory definitely seems smarter \u2013 keep the corrupt index around in case the user wants to attempt data recovery.  If we just purge it, somebody is going to complain loudly that we deleted all their data. ",
            "id": "comment-14550417"
        },
        {
            "date": "2015-05-19T14:34:37+0000",
            "author": "Mark Miller",
            "content": "If you don't have an active replica to recover from, there is no reason to attempt any of this. ",
            "id": "comment-14550541"
        },
        {
            "date": "2015-05-19T14:36:57+0000",
            "author": "Mark Miller",
            "content": "\u2013 keep the corrupt index around in case the user wants to attempt data recovery. \n\n+1 on the idea - but then, this can be hundreds of gig of index - who cleans these up? What happens when it fills the drive after a few random corruptions? At the least, should be a configuration. ",
            "id": "comment-14550543"
        },
        {
            "date": "2017-06-22T10:40:38+0000",
            "author": "Mihaly Toth",
            "content": "One strategy could be to decide based on the replication factor. If it is like 3 it should be safe enough to delete a corrupted index. If it is 1 new index should be created if there is enough space.\n\nAlso, such strategy (keep old/purge/conditional) may be made configurable. \n\nThe other part of the coin is how a bad node is handled from the leader candidate. Would not it make sense to close out nodes with which replication fails? ",
            "id": "comment-16059138"
        }
    ]
}