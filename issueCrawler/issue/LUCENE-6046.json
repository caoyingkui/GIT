{
    "id": "LUCENE-6046",
    "title": "RegExp.toAutomaton high memory use",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "4.10.1",
        "components": [
            "core/queryparser"
        ],
        "labels": "",
        "fix_versions": [
            "4.10.3",
            "5.0",
            "6.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "When creating an automaton from an org.apache.lucene.util.automaton.RegExp, it's possible for the automaton to use so much memory it exceeds the maximum array size for java.\n\nThe following caused an OutOfMemoryError with a 32gb heap:\n\n\nnew RegExp(\"\\\\[\\\\[(Datei|File|Bild|Image):[^]]*alt=[^]|}]{50,200}\").toAutomaton();\n\n\n\nWhen increased to a 60gb heap, the following exception is thrown:\n\n\n  1> java.lang.IllegalArgumentException: requested array size 2147483624 exceeds maximum array in java (2147483623)\n  1>     __randomizedtesting.SeedInfo.seed([7BE81EF678615C32:95C8057A4ABA5B52]:0)\n  1>     org.apache.lucene.util.ArrayUtil.oversize(ArrayUtil.java:168)\n  1>     org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:295)\n  1>     org.apache.lucene.util.automaton.Automaton$Builder.addTransition(Automaton.java:639)\n  1>     org.apache.lucene.util.automaton.Operations.determinize(Operations.java:741)\n  1>     org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:62)\n  1>     org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:51)\n  1>     org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:477)\n  1>     org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:426)",
    "attachments": {
        "LUCENE-6046.patch": "https://issues.apache.org/jira/secure/attachment/12678952/LUCENE-6046.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14194395",
            "author": "Michael McCandless",
            "date": "2014-11-03T09:50:53+0000",
            "content": "Hmm, two bugs here.\n\nFirst off, RegExp.toAutomaton is an inherently costly method: wasteful of RAM and CPU, doing minimize after each recursive operation, in order to build a DFA in the end. It's unfortunately quite easy to concoct regular expressions that make it consume ridiculous resources.  I'll look at this example and see if we can improve it, but in the end it will always have its \"adversarial cases\" unless we give up on making the resulting automaton deterministic, which would be a very big change.\n\nMaybe we should add adversary defenses to it, e.g. you set a limit on the number of states it's allowed to create, and it throws a RegExpTooHardException if it would exceed that?\n\nSecond off, ArrayUtil.oversize has the wrong (too large) value for MAX_ARRAY_LENGTH, which is a bug from LUCENE-5844.  Which JVM did you run this test on? "
        },
        {
            "id": "comment-14194397",
            "author": "Dawid Weiss",
            "date": "2014-11-03T09:56:01+0000",
            "content": "Just a note \u2013 Russ Cox wrote a series of excellent articles about different approaches of implementing regexp scanners. \nhttp://swtch.com/~rsc/regexp/regexp1.html\n\n(There is no clear winner \u2013 both DFAs and NFA have advantages.) "
        },
        {
            "id": "comment-14194400",
            "author": "Lee Hinman",
            "date": "2014-11-03T10:00:52+0000",
            "content": "Michael McCandless I ran it with the following JVM:\n\n\njava version \"1.8.0_20\"\nJava(TM) SE Runtime Environment (build 1.8.0_20-b26)\nJava HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode)\n\n "
        },
        {
            "id": "comment-14194406",
            "author": "Michael McCandless",
            "date": "2014-11-03T10:18:40+0000",
            "content": "Russ Cox wrote a series of excellent articles about different approaches of implementing regexp scanners. \n\nThanks Dawid, these are great.\n\nSwitching to NFA based matching would be a very large change ... I don't think we should pursue it here.  Terms.intersect implementation for block tree is already very complex ... though I suppose of we could hide the \"on the fly subset construction\" (and convert regexp to a Thompson NFA) under an API, then Terms.intersect implementation wouldn't have to change much.\n\nStill, there will always be adversarial cases no matter which approach we choose.  I think for this issue we should allow passing in a \"how much work are you willing to do\" to RegExp.toAutomaton, and it throws an exc when it would exceed that. "
        },
        {
            "id": "comment-14194412",
            "author": "Michael McCandless",
            "date": "2014-11-03T10:24:54+0000",
            "content": "Michael McCandless I ran it with the following JVM:\n\nThanks Lee Hinman.\n\nI was wrong about the first bug: there is no bug in ArrayUtil.oversize.  That exception just means RegExp is trying to create a too-big array ... so just the one bug  "
        },
        {
            "id": "comment-14194413",
            "author": "Dawid Weiss",
            "date": "2014-11-03T10:25:27+0000",
            "content": "I didn't mean to imply we should change the regexp implementation!  This was just a pointer in case somebody wished to understand why regexps can explode. I actually wish there was an NFA-based regexp implementation for Java (with low-memory footprint) because this would make concatenating thousands of regexps (e.g., for pattern detection) much easier. "
        },
        {
            "id": "comment-14194436",
            "author": "Lee Hinman",
            "date": "2014-11-03T11:03:58+0000",
            "content": "I think for this issue we should allow passing in a \"how much work are you willing to do\" to RegExp.toAutomaton, and it throws an exc when it would exceed that.\n\nFor what it's worth, I think this would be a good solution for us, much better than silently (from the user's perspective) freezing and then hitting an OOME. "
        },
        {
            "id": "comment-14194484",
            "author": "Nik Everett",
            "date": "2014-11-03T12:01:46+0000",
            "content": "I'm working on a first cut of something that does that.  Better regex implementation would be great but the biggest thing to me is being able to limit the amount of work the determinize operation performs.  Its such a costly operation that I don't think it should ever be really abstracted from the user.  Something like having determinize throw a checked exception when it performs too much work would make you keenly aware whenever you might be straying into exponential territory. "
        },
        {
            "id": "comment-14194585",
            "author": "Michael McCandless",
            "date": "2014-11-03T14:40:35+0000",
            "content": "OK I boiled down the adversarial regexp to this simpler still-adversarial version: [ac]*a[ac]{50,200}\n\nI suspect this is a legitimate adversary and not a bug in our RegExp/automaton impl, i.e. the number of states in the DFA for this is exponential as a function of the 50/200. "
        },
        {
            "id": "comment-14194592",
            "author": "Nik Everett",
            "date": "2014-11-03T14:50:39+0000",
            "content": "Oh yeah, its totally running into 2^n territory legitiately here.  This is totally something that'd be rejected by a framework to prevent explosive growth during determination. "
        },
        {
            "id": "comment-14194703",
            "author": "Nik Everett",
            "date": "2014-11-03T16:36:30+0000",
            "content": "First cut at a patch.  Adds maxDeterminizedStates to Operations.determinize and pipes it through to tons of places.  I think its important never to hide when determinize is called because of how potentially heavy it is.  Forcing callers of MinimizationOperations.minimize, Operations.reverse, Operations.minus etc to specify maxDeterminizedStates makes it pretty clear that the automaton might be determinized during those processes.\n\nI added an unchecked exception for when the Automaton can't be determinized within the specified number of state but I'm really tempted to change it to a checked exception to make it super duper obvious when determinization might occur. "
        },
        {
            "id": "comment-14194716",
            "author": "Nik Everett",
            "date": "2014-11-03T16:48:21+0000",
            "content": "Oh - I'm still running the solr tests against this.  I imagine they'll pass as they've been running fine for 30 minutes now but I should throw that out there in case someone gets them to fail with this before I do. "
        },
        {
            "id": "comment-14195024",
            "author": "Michael McCandless",
            "date": "2014-11-03T20:18:47+0000",
            "content": "Patch, tests pass.  I added a required \"int maxStates\" to\nRegExp.toAutomaton, and it threads this down to determinize, and\nthrows RegExpTooHardExc if determinize would need to exceed that\nlimit.\n\nI didn't make it a checked exc; I had started that way but it\npercolates up high, e.g. into query parsers, and I think that's too\nmuch.  The exception message itself should make it quite clear what\nwent wrong at query time.\n\nI also added this as an optional param to RegexpQuery default ctor,\nand defaulted it to 10000 states, and to QueryParserBase, with the same\ndefault. "
        },
        {
            "id": "comment-14195033",
            "author": "Nik Everett",
            "date": "2014-11-03T20:25:24+0000",
            "content": "Oh no!  I wrote a very similar patch!  Sorry to duplicate effort there.  \n\nI found that 10,000 states wasn't quite enough to handle some of the tests so I went with 1,000,000 as the default.  Its pretty darn huge but it does get the job done. "
        },
        {
            "id": "comment-14195047",
            "author": "Michael McCandless",
            "date": "2014-11-03T20:35:26+0000",
            "content": "Woops, sorry, I didn't see you had a patch here!  Thank you.\n\nI like your patch: it's good to make all hidden usages of determinize visible.  Let's start from your patch and merge anything from mine in?  E.g. I think we can collapse minimizeHopcroft into just minimize...\n\nI found that 10,000 states wasn't quite enough to handle some of the tests so I went with 1,000,000 as the default. Its pretty darn huge but it does get the job done.\n\nWhoa, which tests needed 1M max states?  I worry about passing a 1M state automaton to minimize... "
        },
        {
            "id": "comment-14195053",
            "author": "Michael McCandless",
            "date": "2014-11-03T20:42:07+0000",
            "content": "I like the test simplifications, and removing dead code from Operations.determinize.\n\nCan we fix the exc thrown from Regexp to include the offending regular expression, and fix the test to confirm the message contains it?  Maybe also add RegExp.toStringTree?  I found it very useful while debugging the original regexp \n\nI think QueryParserBase should also have a set/get for this option? "
        },
        {
            "id": "comment-14195056",
            "author": "Nik Everett",
            "date": "2014-11-03T20:43:35+0000",
            "content": "TestDeterminizeLexicon wants to make an automata that accepts 5000 random strings.  So 10,000 isn't enough states for it.  I'll drop the default limit to 10,000 again and just feed a million to that test case.  "
        },
        {
            "id": "comment-14195065",
            "author": "Nik Everett",
            "date": "2014-11-03T20:53:01+0000",
            "content": "I'll certainly add the regexp string to the exception message.  And I'll merge the toStringTree from your patch into mine if you'd like.\n\nYeah - QueryParserBase should have this option too.\n\nThe thing I found most useful for debugging this was to call toDot on the automata before and after normalization.  I just looked at it and went, oh, of course you have to do it that way.  No wonder the states explode.  And then I read https://en.wikipedia.org/wiki/Powerset_construction and remembered it from my rusty CS degree. "
        },
        {
            "id": "comment-14195625",
            "author": "Nik Everett",
            "date": "2014-11-04T02:59:51+0000",
            "content": "Next version with fixes based on Mike's feedback. "
        },
        {
            "id": "comment-14196070",
            "author": "Nik Everett",
            "date": "2014-11-04T13:03:43+0000",
            "content": "A couple of updates:\nThis affects version 4.9 as well.  Probably all versions.  But its impact is likely minor enough to only be worth adding to the 4.10 line.\n\nA found a few test cases that need lots and lots of states.  Any time you feed a couple hundred random unicode words to the automata you'll end up needing more than ten thousand states.  I've updated those tests to ask for a million states and they caught a few places where I hadn't been as diligent in piping maxDeterminizedStates through. "
        },
        {
            "id": "comment-14196295",
            "author": "Michael McCandless",
            "date": "2014-11-04T16:11:50+0000",
            "content": "Thanks Nik, your patch looks great; I'll fold in some more minor things from my patch and commit! "
        },
        {
            "id": "comment-14196723",
            "author": "ASF subversion and git services",
            "date": "2014-11-04T20:03:12+0000",
            "content": "Commit 1636716 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1636716 ]\n\nLUCENE-6046: add maxDeterminizedStates to determinize to prevent exhausting CPU/RAM when the automaton is too difficult to determinize "
        },
        {
            "id": "comment-14196770",
            "author": "ASF subversion and git services",
            "date": "2014-11-04T20:38:26+0000",
            "content": "Commit 1636728 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1636728 ]\n\nLUCENE-6046: add maxDeterminizedStates to determinize to prevent exhausting CPU/RAM when the automaton is too difficult to determinize "
        },
        {
            "id": "comment-14196947",
            "author": "ASF subversion and git services",
            "date": "2014-11-04T22:16:03+0000",
            "content": "Commit 1636758 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1636758 ]\n\nLUCENE-6046: fix test failure, add maxDeterminizedStates to AutomatonQuery and WildcardQuery too "
        },
        {
            "id": "comment-14196949",
            "author": "ASF subversion and git services",
            "date": "2014-11-04T22:16:35+0000",
            "content": "Commit 1636759 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1636759 ]\n\nLUCENE-6046: fix test failure, add maxDeterminizedStates to AutomatonQuery and WildcardQuery too "
        },
        {
            "id": "comment-14196981",
            "author": "ASF subversion and git services",
            "date": "2014-11-04T22:29:30+0000",
            "content": "Commit 1636762 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1636762 ]\n\nLUCENE-6046: add maxDeterminizedStates to determinize to prevent exhausting CPU/RAM when the automaton is too difficult to determinize "
        },
        {
            "id": "comment-14196982",
            "author": "Michael McCandless",
            "date": "2014-11-04T22:29:34+0000",
            "content": "Thanks Nik! "
        },
        {
            "id": "comment-14197970",
            "author": "ASF subversion and git services",
            "date": "2014-11-05T09:17:57+0000",
            "content": "Commit 1636830 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1636830 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14197972",
            "author": "ASF subversion and git services",
            "date": "2014-11-05T09:18:45+0000",
            "content": "Commit 1636831 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1636831 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14197974",
            "author": "ASF subversion and git services",
            "date": "2014-11-05T09:19:51+0000",
            "content": "Commit 1636832 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1636832 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14200018",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T09:22:00+0000",
            "content": "Commit 1637054 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1637054 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14200019",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T09:23:02+0000",
            "content": "Commit 1637055 from Robert Muir in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1637055 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14200021",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T09:24:06+0000",
            "content": "Commit 1637056 from Robert Muir in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1637056 ]\n\nLUCENE-6046: let this test determinize massive automata "
        },
        {
            "id": "comment-14200079",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T11:17:00+0000",
            "content": "Commit 1637078 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1637078 ]\n\nLUCENE-6046: remove det state limit for all AutomatonTestUtil.randomAutomaton since they can become biggish "
        },
        {
            "id": "comment-14200086",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T11:21:29+0000",
            "content": "Commit 1637080 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1637080 ]\n\nLUCENE-6046: remove det state limit for all AutomatonTestUtil.randomAutomaton since they can become biggish "
        },
        {
            "id": "comment-14200090",
            "author": "ASF subversion and git services",
            "date": "2014-11-06T11:26:12+0000",
            "content": "Commit 1637082 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1637082 ]\n\nLUCENE-6046: remove det state limit for all AutomatonTestUtil.randomAutomaton since they can become biggish "
        },
        {
            "id": "comment-14333006",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:03:03+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}