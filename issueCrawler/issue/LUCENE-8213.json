{
    "id": "LUCENE-8213",
    "title": "Cache costly subqueries asynchronously",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "7.2.1",
        "status": "Open",
        "type": "Improvement",
        "components": [
            "core/query/scoring"
        ],
        "fix_versions": []
    },
    "description": "IndexOrDocValuesQuery allows to combine\u00a0costly range queries with a\u00a0selective\u00a0lead iterator in an optimized way. However,\u00a0the range query at some point gets cached by a querying thread in LRUQueryCache, which negates the optimization of IndexOrDocValuesQuery for that specific query.\n\nIt would be nice to see an asynchronous caching implementation in such cases, so that queries involving IndexOrDocValuesQuery would have consistent performance characteristics.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16403701",
            "date": "2018-03-17T19:47:04+0000",
            "content": "This problem applies to most queries actually. Creating cache entries asynchronously to prevent query caching from hurting latency is an interesting idea. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16403901",
            "date": "2018-03-18T07:32:10+0000",
            "content": "Adrien Grand, maxCostFactor can be used to decide whether to cache synchronously or asynchronously. Its default value can then be lowered from 100 to something close to 1.\n\nWDYT? ",
            "author": "Amir Hadadi"
        },
        {
            "id": "comment-16403993",
            "date": "2018-03-18T11:59:50+0000",
            "content": "It causes a latency hit regardless: using a threadpool doesnt matter. I don't think we should introduce additional threadpools because servers using lucene already use FAR too many threads, sorry.\n\nIf the user passes an executor to IndexSearcher, they've already said they don't have much throughput and they'd rather trade it for latency. So in that case i think its fine to use that threadpool for this. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16404005",
            "date": "2018-03-18T13:21:10+0000",
            "content": "\"It causes a latency hit regardless\"\n\nRobert Muir\u00a0I will clarify the issue. Assume you have these queries:\n\nq1:\u00a0term query that matches 1 doc\n\nq2:\u00a0range query that matches 10M docs\n\nAnd you query for \"q1 AND q2\"\n\nq2 would be an IndexOrDocValuesQuery and would use doc values to do a range check on the single doc that matches q1, so the BKD tree won't be scanned for 10M documents.\n\nHowever, when\u00a0q2 gets cached, the bit set for the entire 10M docs\u00a0is cached, which causes \"q1 AND q2\"\u00a0to suddenly spike in latency.\n\n\n\tactually the recent introduction of\u00a0maxCostFactor will\u00a0prevent caching 10M docs since q1 matches only one term,\u00a0but\u00a0as\u00a0maxCostFactor is 100 by default, a latency hit can occur in\u00a0similar cases.\n\n\n\n\u00a0 ",
            "author": "Amir Hadadi"
        },
        {
            "id": "comment-16404387",
            "date": "2018-03-19T05:11:15+0000",
            "content": "Your description sounds like maybe a different problem, where IndexOrDocValuesQuery chooses the wrong execution path to fill the \"filter\" bitset? If it uses docvalues instead of a BKD tree, because it must cache a large range that definitely seems like its bad. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16404750",
            "date": "2018-03-19T12:42:53+0000",
            "content": "Robert Muir the issue is that the execution path for (q1 AND q2) depends on whether q2 gets cached or not.\n When q2 does not get cached, doc values are used to execute q2 and only the single document matching q1 is evaluated against the range.\n When q2 gets cached, it gets cached as a query that stands by itself, i.e. not in the context of (q1 AND q2).\n So the entire 10M documents that q2 matches are scanned in the BKD tree and get cached to a bit set.\n To protect against the caching of q2 causing the latency of (q1 AND q2) to be too high, Adrien Grand\u00a0added maxCostFactor.\n This factor checks whether the cost of caching q2 is higher by more than maxCostFactor than the cost of evaluating (q1 AND q2).\n This is the relevant code from LRUQueryCache:\n\n\u00a0\n\ndouble costFactor = (double) inSupplier.cost() / leadCost;\nif (costFactor >= maxCostFactor) {\n  // too costly, caching might make the query much slower\n  return inSupplier.get(leadCost);\n}\n\n\u00a0\n\nMy suggestion is to always evaluate (q1 AND q2) using the optimal execution path, and cache q2 asynchrounously.\n A refinement is to cache q2 synchronously if the cost of caching it is not too high. ",
            "author": "Amir Hadadi"
        },
        {
            "id": "comment-16406027",
            "date": "2018-03-20T09:41:40+0000",
            "content": "I didn't remember pushing this maxCostFactor prototype I had been working on and indeed I had pushed it inadvertently with changes on LUCENE-8008. It has one issue that I didn't like which is that you can't use the bulk scorer for caching anymore, which matters for disjunctions.\n\nRegarding this issue, maybe one way to make it use the executor from IndexSearcher would be to add an executor parameter to QueryCache.doCache so that IndexSearcher can pass its executor to the cache. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16406908",
            "date": "2018-03-20T19:37:35+0000",
            "content": "Adrien Grand\u00a0Currently if an executor is used\u00a0in IndexSearcher, it means that segments should be queried in parallel.\n\nI guess you wouldn't want to conflate parallel segments\u00a0querying\u00a0and async caching. ",
            "author": "Amir Hadadi"
        },
        {
            "id": "comment-16407632",
            "date": "2018-03-21T09:05:13+0000",
            "content": "Actually I think we want this: both parallel segments querying and async caching are about trading throughput for latency? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16407861",
            "date": "2018-03-21T12:59:13+0000",
            "content": "Indeed both are\u00a0about trading throughput for latency.\n\nHowever, there is a quantitative difference:\n\nin parallel segments querying you would slice your index to e.g. 5 slices on each and every query.\n\nasync caching would happen only when caching is needed, and even then, only when the ratio between the caching cost and the leading query\u00a0cost is big enough to justify async execution.\n\nI would expect the additional async tasks\u00a0triggered by async caching to be 100x less than parallel segments querying tasks.\n\nCoupling these features together would mean that if someone is not willing to pay the overhead of parallel segments querying, he will not be able to use async caching. ",
            "author": "Amir Hadadi"
        },
        {
            "id": "comment-16408260",
            "date": "2018-03-21T17:13:46+0000",
            "content": "You can still create an IndexSearcher with a single slice and/or wrap the cache to use a custom threadpool instead of the one from IndexSearcher, this is fine. I'd just like to make common use-cases, such as trading throughput for latency, simple. ",
            "author": "Adrien Grand"
        }
    ]
}