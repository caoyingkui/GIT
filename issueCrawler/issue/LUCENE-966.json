{
    "id": "LUCENE-966",
    "title": "A faster JFlex-based replacement for StandardAnalyzer",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.3"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.",
    "attachments": {
        "jflex-analyzer-r562378-patch.txt": "https://issues.apache.org/jira/secure/attachment/12363108/jflex-analyzer-r562378-patch.txt",
        "jflex-analyzer-patch.txt": "https://issues.apache.org/jira/secure/attachment/12362606/jflex-analyzer-patch.txt",
        "jflex-analyzer-r562378-patch-nodup.txt": "https://issues.apache.org/jira/secure/attachment/12363116/jflex-analyzer-r562378-patch-nodup.txt",
        "AnalyzerBenchmark.java": "https://issues.apache.org/jira/secure/attachment/12362607/AnalyzerBenchmark.java",
        "jflex-analyzer-r560135-patch.txt": "https://issues.apache.org/jira/secure/attachment/12362672/jflex-analyzer-r560135-patch.txt",
        "jflex-analyzer-r561292-patch.txt": "https://issues.apache.org/jira/secure/attachment/12362865/jflex-analyzer-r561292-patch.txt",
        "jflex-analyzer-r561693-compatibility.txt": "https://issues.apache.org/jira/secure/attachment/12362954/jflex-analyzer-r561693-compatibility.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-07-26T13:12:47+0000",
            "content": "Here comes a somewhat rough (needing refactorings, see below) patch adding a JFlex-based repalcement for StandardAnalyzer called FastAnalyzer.  A few comments:\n\n\n\tall tests from TestStandardAnalyzer pass\n\ttokenizer generation added to Ant build scripts\n\n\n\nBecause FastAnalyzer shares some code with StandardAnalyzer, it might be worthwhile to do some refactorings:\n\n\n\thave a common test for both analyzers (only if we want to keep them in sync)\n\thave a common superclass for FastFilter and StandardFilter\n\thave a common superclass for FastAnalyzer and StandardAnalyzer\n\n ",
            "author": "Stanislaw Osinski",
            "id": "comment-12515655"
        },
        {
            "date": "2007-07-26T13:15:55+0000",
            "content": "Here is a very simple benchmark I used to test the performance of StandardAnalyzer, FastAnalyzer and WhitespaceAnalyzer. I ran it on a number of JVMs and got the following results:\n\nInput: Reuters collection, the one used by contrib/benchmark, only documents\nlonger than 100 bytes\n\nMachine: AMD Sempron 2600+, 2G RAM, Windows XP\n\nSun 1.4.2 Server\norg.apache.lucene.analysis.standard.StandardAnalyzer: 15172 ms, 139667 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 2438 ms, 869170 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 781 ms, 3547585 tokens/s\n\nSun 1.4.2 Client\norg.apache.lucene.analysis.standard.StandardAnalyzer: 24187 ms, 87610 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 3157 ms, 671218 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1453 ms, 1906857 tokens/s\n\nSun 1.5.0 Server\norg.apache.lucene.analysis.standard.StandardAnalyzer: 16062 ms, 131928 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 2641 ms, 802361 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 750 ms, 3694218 tokens/s\n\nSun 1.5.0 Client\norg.apache.lucene.analysis.standard.StandardAnalyzer: 23891 ms, 88696 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 3641 ms, 581993 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1437 ms, 1928089 tokens/s\n\nSun 1.6.0 Server\norg.apache.lucene.analysis.standard.StandardAnalyzer: 13719 ms, 154460 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 2484 ms, 853074 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 750 ms, 3694218 tokens/s\n\nSun 1.6.0 Client\norg.apache.lucene.analysis.standard.StandardAnalyzer: 22312 ms, 94972 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 2750 ms, 770558 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1297 ms, 2136209 tokens/s\n\nIBM 1.4.2\norg.apache.lucene.analysis.standard.StandardAnalyzer: 11922 ms, 177741 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 3218 ms, 658495 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1407 ms, 1969199 tokens/s\n\nIBM 1.5.0\norg.apache.lucene.analysis.standard.StandardAnalyzer: 11797 ms, 179625 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 2968 ms, 713961 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1000 ms, 2770664 tokens/s\n\nBEA 1.4.2\norg.apache.lucene.analysis.standard.StandardAnalyzer: 16234 ms, 130530 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 3344 ms, 633683 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 1343 ms, 2063040 tokens/s\n\nBEA 1.5.0 (looks really slow)\norg.apache.lucene.analysis.standard.StandardAnalyzer: 33891 ms, 62525 tokens/s\norg.apache.lucene.analysis.fast.FastAnalyzer: 12703 ms, 166813 tokens/s\norg.apache.lucene.analysis.WhitespaceAnalyzer: 4860 ms, 570095 tokens/s\n ",
            "author": "Stanislaw Osinski",
            "id": "comment-12515658"
        },
        {
            "date": "2007-07-26T15:05:37+0000",
            "content": "Thanks Staszek, very nice!\nIf FastTokenizer is compatible with StandardTokenizer, it should probably just replace it.\nHave you tried this replacement and running all the lucene tests? ",
            "author": "Yonik Seeley",
            "id": "comment-12515726"
        },
        {
            "date": "2007-07-26T16:35:50+0000",
            "content": "Great patch! And a very quick turnaround to boot!\n\nI am seeing a HUGE speed increase!\n\nI also think this should certainly become the new StandardAnalyzer.\n\nI am running into only one issue...when rebuilding, after a clean, FastTokenizer does not seem to get generated \u2013 FastTokenizerImpl is there, but no FastTokenizer interface; it appears to be wiped out in the clean but not regenerated in the build. If I put it in manually, everything works great.\n\n\n\tMark\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12515768"
        },
        {
            "date": "2007-07-26T21:49:08+0000",
            "content": "We did pretty much the same thing here at Aconex,   The tokenization mechanism in the old javacc-based analyser is woeful compared to what JFlex outputs.\n\nNice work!   ",
            "author": "Paul Smith",
            "id": "comment-12515882"
        },
        {
            "date": "2007-07-27T08:05:43+0000",
            "content": "Here's another patch (against r560135) which:\n\n\n\treplaces JavaCC-based StandardTokenizer with a JFlex-based one (all tests pass), as suggested by Yonik\n\tupdates build scripts accordingly and fixes the issue of deleting one file to many, as reported by Mark\n\n\n\nAdditionally, I noticed that ChainedFilterTest from contrib/miscellaneous would fail on machines with non-US locale, so this patch fixes that too. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12515986"
        },
        {
            "date": "2007-07-31T10:05:20+0000",
            "content": "Here is another (this time let's call it \"final\"  patch for this issue which comes with a simplified (but test-wise equivalent) grammar for numeric expressions that gives about 5% performance increase. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12516665"
        },
        {
            "date": "2007-07-31T17:12:22+0000",
            "content": "I took the patch from here (to use jflex for StandardAnalyzer) and\nmerged it with the patch from LUCENE-969 (re-use Token & TokenStream)\nto measure the net performance gains.\n\nI measure the time to just tokenize all of Wikipedia using\nStandardAnalyzer using contrib/benchmark plus patch from LUCENE-967\n(test details are described in LUCENE-969).\n\nWith the jflex patch it takes 646 sec (best of 2 runs); when I then\nmerge in the patch from LUCENE-969 it takes 455 sec.  Subtracting off\nthe time to just load all Wikipedia docs (= 112 sec) that gives net\nadditional speedup of 36% (534 sec -> 343 sec) when using LUCENE-969\nin addition to jflex.\n\nA couple other things I noticed:\n\n\n\tThe init cost of jflex (StandardTokenizerImpl) seems to be fairly\n    high: when I repeat the above test with smallish docs (100 tokens\n    each) instead, the gain is around 84%.  I think this just makes\n    the new reusableTokenStream() in LUCENE-969 important to commit.\n\n\n\n\n\tI'm seeing differing token counts with the jflex StandardAnalyzer\n    vs the current one; I think there is some difference here.  I will\n    track down which tokens differ and post back...\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12516745"
        },
        {
            "date": "2007-07-31T17:36:07+0000",
            "content": "I tracked down at least some differences between the JavaCC vs JFlex\nversions of StandardAnalyzer.\n\nI think we should resolve these before committing.\n\nI just printed all tokens for the first 20 Wikipedia docs and diff'd\nthe outputs.\n\nHere are the categories of differences that I saw:\n\n\n\tOnly the type differs on a filename-like token:\n\n\n\n      OLD: (2004.jpg,34461,34469,type=<HOST>)\n      NEW: (2004.jpg,34461,34469,type=<NUM>)\n\n    In this case the old StandardAnalyzer called \"2004.jpg\" a HOST and\n    the new one calls it a NUM.  Seems like neither one is right!\n\n\n\tOnly the type differs on a number token:\n\n\n\n      OLD: (62.46,37004,37009,type=<HOST>)\n      NEW: (62.46,37004,37009,type=<NUM>)\n\n    The new tokenizer looks right here.  I guess the decimal point\n    confuses the JavaCC (old) one.\n\n\n\tDifferent number of tokens produced for number-like-token:\n\n\n\n      OLD: (978-0-94045043-1,86408,86424,type=<NUM>)\n      NEW: (978-0-94045043,86408,86422,type=<NUM>)\n           (1,86423,86424,type=<ALPHANUM>)\n\n    The new one split off the final \"-1\" as its own token, and called\n    it ALPHANUM not NUM.  I think the old behavior is correct.\n\n\n\tDifferent number of tokens produced for filename:\n\n\n\n      OLD: (78academyawards/rules/rule02.html,7194,7227,type=<NUM>)\n      NEW: (78academyawards/rules/rule02,7194,7222,type=<NUM>)\n           (html,7223,7227,type=<ALPHANUM>)\n\n    I think the old one is better, though it should not be called a\n    NUM (maybe we need a new \"FILENAME\" token type?).\n\n\n\tSame as above, but split on final '_' instead of '.' ('-' also\n    shows this behavior):\n\n\n\n      OLD: (2006-03-11t082958z_01_ban130523_rtridst_0_ozabs,2076,2123,type=<NUM>)\n      new: (2006-03-11t082958z_01_ban130523_rtridst_0,2076,2117,type=<NUM>)\n           (ozabs,2118,2123,type=<ALPHANUM>) ",
            "author": "Michael McCandless",
            "id": "comment-12516752"
        },
        {
            "date": "2007-07-31T18:14:52+0000",
            "content": "Thanks for spotting the differences, I'll add them to the unit tests and will correct the tokenizer accordingly.\n\nOne doubt I have is about the filename-like tokens, e.g.:\n\n      OLD: (2004.jpg,34461,34469,type=<HOST>)\n      NEW: (2004.jpg,34461,34469,type=<NUM>) \n\nTo be honest, both variants seem \"almost\" correct. If you try 2007.org \u2013 this is a correct domain name (and has a funny website on it , so, given the fact that we don't check for typical suffixes, such as \".com\", <HOST> doesn't seem wrong. On the other hand, 2004.jpg may well have been some sort of numerical code or a product number, so <NUM> is not totally irrelevant either.\n\nFor the JFlex-based tokenizer, I put the <NUM> rule matching first, as it gives some nice performance benefits. We can put HOST first, and then we'll get compliance with the old version.\n\nAnother option we might consider is:\n\n\n\tadding a new token type for file names (get a list of common extensions or even assume that an extension is simply three alphanumerical characters)\n\tchecking for common domain names in hosts (something along the lines of: \"mil\" | \"info\" | \"gov\" | \"edu\" | \"biz\" | \"com\" | \"org\" | \"net\" |  \"arpa\" | \n{LETTER}\n{2}\n)\n\n\n\nI'm not sure how this will affect the performance of the tokenizer, but my rough guess is that if we don't come up with very complex/ backtracking-prone rules there should not be too much of a difference. On the other hand, if 100% compatibility with the old tokenizer is a priority, adding new token types is not a good idea, I guess.\n\nFinally, when it comes to the initialization time of the new tokenizer \u2013 according to the JFlex documentation, some time is required to unpack the transition tables. But the unpacking takes place during the initialization of static fields, so once the class is loaded the overhead should be negligible. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12516762"
        },
        {
            "date": "2007-07-31T18:36:49+0000",
            "content": "I think it is important that we do not modify the StandardAnalyzer and that this improved version has results identical to the original. I believe that the StandardAnalyzer is pretty heavily used \u2013 these performance increases could give many users huge gains. By changing this Analyzer at all we will be forcing users to effectively lose terms in their indexes if they want to upgrade core Lucene for use on an older index.\n\nI think any enhanced StandardAnalzyer should probably be released as a new Analyzer. This one should mimic the old though, as the performance gains will be very beneficial to a lot of current users.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12516767"
        },
        {
            "date": "2007-07-31T19:03:29+0000",
            "content": "\nI agree, let's try to perfectly match the tokens of the old\nStandardAnalyzer so we have a way-faster drop-in replacement.\n\nThe speedups of JFlex are amazing: based on a quick test, with JFlex +\npatch from LUCENE-969, the new StandardAnalyzer is only 2.09X slower\nthan WhitespaceAnalyzer even though it's doing so much more ...\n\n> Finally, when it comes to the initialization time of the new\n> tokenizer \u2013 according to the JFlex documentation, some time is\n> required to unpack the transition tables. But the unpacking takes\n> place during the initialization of static fields, so once the class\n> is loaded the overhead should be negligible.\n\nYeah I'm baffled why it's that much slower, but on 100 token docs I\ndefinitely see LUCENE-969 making things 84% faster but \"only\" 36%\nfaster if I use the full Wikipedia doc (which are much larger than 100\ntokens on average).  If we tested even smaller docs I think the gains\nwould be even more.\n\nWhen I ran under the profiler it was the StandardTokenizerImpl\n<init>(java.io.Reader) way on the top.  Maybe it's the cost of new'ing\nthe 16 KB buffer each time?\n\nIn any event I think it's OK, so long as we get LUCENE-969 in, and\ndocument the importance of using reusableTokenStream() API for better\nperformance. ",
            "author": "Michael McCandless",
            "id": "comment-12516775"
        },
        {
            "date": "2007-07-31T19:03:57+0000",
            "content": "It is important that the same sequence of token text is produced, but I think we could live with different token types in some cases, if we must.  Few applications depend on token types, no?\n\nProvided the token text issues can be resolved, I'd like to see StandardTokenizer replaced with this.  Performance is important, and ideally folks shouldn't have to change their applications to see performance improvements. ",
            "author": "Doug Cutting",
            "id": "comment-12516776"
        },
        {
            "date": "2007-08-01T07:53:20+0000",
            "content": "When digging deeper into the issues of compatibility with the original StandardAnalyzer, I stumbled upon something strange. Take the following text:\n\n78academyawards/rules/rule02.html,7194,7227,type\n\nwhich was tokenized by the original StandardAnalyzer as one <NUM>. If you look at the definition of the <NUM> token:\n\n// every other segment must have at least one digit\n<NUM: (<ALPHANUM> <P> <HAS_DIGIT>\n\n\n\n <HAS_DIGIT> <P> <ALPHANUM>\n\n\n <ALPHANUM> (<P> <HAS_DIGIT> <P> <ALPHANUM>)+\n\n\n <HAS_DIGIT> (<P> <ALPHANUM> <P> <HAS_DIGIT>)+\n\n\n <ALPHANUM> <P> <HAS_DIGIT> (<P> <ALPHANUM> <P> <HAS_DIGIT>)+\n\n\n <HAS_DIGIT> <P> <ALPHANUM> (<P> <HAS_DIGIT> <P> <ALPHANUM>)+\n        )\n\n\n\n\n\nyou'll see that, as explained in the comment, every other segment must have at least one digit. But actually, according to my understanding, this rule should not match the above text as a whole (and with JFlex it doesn't , actually). Below is the text split by punctuation characters, and it looks like there is no way of splitting this text into alternating segments, every second of which must have a digit (A = ALPHANUM, H = HAS_DIGIT):\n\n78academyawards   /   rules   /   rule02   .   html   ,   7194   ,   7227   ,   type\n                H                  P      A     P       H       P     A     P     H      P      A     P    H?*     (starting from the beginning)\n                                                                              H?*    P     A      P      H     P     A       (starting from the end)\n\n\n\t(would have to be H, but no digits in substring \"type\" or \"html\")\n\n\n\nI have no idea why JavaCC matched the whole text as a <NUM>, JFlex behaved \"more correctly\" here. \n\nNow I can see two solutions:\n\n\n\ttry to patch the JFlex grammar to emulate JavaCC quirks (though I may not be aware of most of them...)\n\trelax the <NUM> rule a little bit (JFlex notation):\n\n\n\n// there must be at least one segment with a digit\nNUM = (\n{P} ({HAS_DIGIT} | {ALPHANUM}))* {HAS_DIGIT} ({P}\n (\n{HAS_DIGIT}\n | \n{ALPHANUM}\n))*\n\nWith this definition, again, all StandardAnalyzer tests pass, plus all texts along the lines of:\n\n2006-03-11t082958z_01_ban130523_rtridst_0_ozabs,2076,2123,type\n78academyawards/rules/rule02.html,7194,7227,type\n978-0-94045043-1,86408,86424,type\n62.46,37004,37009,type    (this one was parsed as <HOST> by the original analyzer)\n\nget parsed as a whole as one <NUM>, which is equivalent to what JavaCC-based version would do. I will attach a corresponding patch in a second.\n ",
            "author": "Stanislaw Osinski",
            "id": "comment-12516893"
        },
        {
            "date": "2007-08-01T08:33:18+0000",
            "content": "A patch for better compatibility with the StandardAnalyzer containing:\n\n\n\trelaxed definition of the <NUM> token\n\tnew test cases in TestStandardAnalyzer\n\n\n\nI noticed that with this patch org.apache.lucene.benchmark.quality.TestQualityRun.testTrecQuality fails, but I'm not sure if this is related to the tokenizer. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12516895"
        },
        {
            "date": "2007-08-01T15:50:15+0000",
            "content": "Oddly, the patch for TestStandardAnalyzer failed to apply for me (but\nthe rest did), so I manually merged those changes in.  Oh, I see: it\nwas the \"Korean words\" test \u2013 somehow the characters got mapped to\n?'s in your patch.  This is why the patch didn't apply, I think?\nMaybe you used a diffing tool that wasn't happy with unicode or\nsomething?\n\nI also see the quality test failing in contrib benchmark.  I fear\nsomething about the new StandardAnalyzer is in fact causing this test\nto fail (it passes on a clean checkout).  That test uses\nStandardAnalyzer.\n\nKO I re-tested the old vs new StandardAnalyzer on Wikipedia and I\nstill found some differences, I think only on these very large\nURL-like tokens.  Here's one:\n\n  OLD\n    (money.cnn.com,1382,1395,type=<HOST>)\n    (magazines,1396,1405,type=<ALPHANUM>)\n    (fortune,1406,1413,type=<ALPHANUM>)\n    (fortune,1414,1421,type=<ALPHANUM>)\n    (archive/2007/03/19/8402357,1422,1448,type=<NUM>)\n    (index.htm,1449,1458,type=<HOST>)\n\n  NEW\n    (/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm,1381,1458,type=<NUM>)\n\nI like the NEW behavior better but I fear we should try to match the\nold one?\n\n\nHere's another one:\n\n  OLD\n    (mid-20th,2436,2444,type=<NUM>)\n\n  NEW\n    (mid,2436,2439,type=<ALPHANUM>)\n    (-20th,2439,2444,type=<NUM>)\n\nI like the old behavior better here.\n\nAnother one:\n\n  OLD\n    (safari-0-sheikh,12011,12026,type=<NUM>)\n    (zayed,12027,12032,type=<ALPHANUM>)\n    (grand,12033,12038,type=<ALPHANUM>)\n    (mosque.jpg,12039,12049,type=<HOST>)\n\n  NEW\n    (safari,12011,12017,type=<ALPHANUM>)\n    (0-sheikh-zayed-grand-mosque.jpg,12018,12049,type=<NUM>)\n\nAnother one:\n\n  OLD\n    (semitica-01.png,616,631,type=<NUM>)\n\n  NEW\n    (-semitica-01.png,615,631,type=<NUM>)\n ",
            "author": "Michael McCandless",
            "id": "comment-12517003"
        },
        {
            "date": "2007-08-01T22:23:27+0000",
            "content": "The search quality test failure can be caused by the standard \nanalyzer generating different tokens than before. (has nothing \nto do with token types.)\n\nThis is because the test's topics (queries) and qrels (expected matches) \nwere created by examining an index that was created using the current \nstandard analyzer. Now, running this test with an analyzer that creates \nother tokens is likely to fail. \n\nIt is not difficult to update this test for a modified analyzer, but it seems \nbetter to me to preserve the original standard analyzer behavior. ",
            "author": "Doron Cohen",
            "id": "comment-12517097"
        },
        {
            "date": "2007-08-02T08:44:35+0000",
            "content": "Thanks for more test cases. I guess the biggest problem here is that the scanner generated by JavaCC doesn't seem to strictly follow the specification (see https://issues.apache.org/jira/browse/LUCENE-966#action_12516893), so I'd need to emulate possible JavaCC \"bugs\" I'm not aware of at the moment (I'm not an expert on lexical scanner generation either, not yet at least . I can add some workarounds to the grammar to make the known incompatibility examples work, but this won't guarantee consistency in general.\n\nAs a side note, it's a shame there's no trace of the version of JavaCC that was used to generate the scanner for the original StandardAnalyzer. I'm also curious if the results of the current JavaCC grammar would be the same with the newest version of the generator (4.0 I guess) \u2013 I'll try to check that.\n\nAnyway, I'll take a look at the problem in more depth once again. And in the worst case scenario, we can keep the StandardAnalyzer as it was and add the new one next to it so that people can have a choice (on the other hand, this might be a problem for the quality tests). ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517184"
        },
        {
            "date": "2007-08-02T11:56:05+0000",
            "content": "If it really is down to emulating the bugs/oddities in JavaCC then I\nthink it's not worth polluting the new tokenizer with these legacy\nbugs, unless one or two cases can match perfectly and not degrade\nperformance too badly?\n\nAnd maybe what we should do is make this a new tokenizer, calling it\nStandardAnalyzer2, and then deprecate the existing StandardAnalyzer?\nThen remove any & all JavaCC bug emulation from the new one.\n\nThis way people relying on the precise bugs in JavaCC tokenization are\nnot hurt on upgrading to 2.3 and are given a chance to migrate to the\nnew one (with 1 release of deprecated StandardAnalyzer).  And new\npeople will use the new faster one. ",
            "author": "Michael McCandless",
            "id": "comment-12517222"
        },
        {
            "date": "2007-08-02T12:09:35+0000",
            "content": "To be precise \u2013 I'm not 100% sure that this is a bug in JavaCC (I'll try to browse/ask on their mailing list to find out), but it looks like the scanner generated by JavaCC does not really strictly follow the grammar. I discovered this when you gave the examples of different token produced by JFlex- and JavaCC-based scanners generated from equivalent grammars \u2013 JFlex seemed to behave \"correctly\", hence different tokens.\n\nI was about to start trying to hack the grammar to produce results simiar to StandardAnalyzer (based on a finite set of test cases . I'll see how it compares performance-wise too. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517233"
        },
        {
            "date": "2007-08-02T18:55:23+0000",
            "content": "+1 for deprecating StandardAnalyzer, although it is a little weird to have a StandardAnalyzer2, which is it the standard or not?  Maybe we could call it the DefaultAnalyzer or something like that.  I never much cared for tacking on numbers on the end of class names.  \n\nI agree with Mike M's last comment and approach, though, if that proves to be the case for the differences. ",
            "author": "Grant Ingersoll",
            "id": "comment-12517339"
        },
        {
            "date": "2007-08-02T19:08:53+0000",
            "content": "I like the name DefaultAnalyzer. ",
            "author": "Michael McCandless",
            "id": "comment-12517342"
        },
        {
            "date": "2007-08-02T19:32:25+0000",
            "content": "These issues seem odd.\n\nBoth JavaCC and Flex match with the same rules:\n1. Longest match first\n2. If match size is the same, use the first in the grammar\n\nOLD\n    (money.cnn.com,1382,1395,type=<HOST>)\n    (magazines,1396,1405,type=<ALPHANUM>)\n    (fortune,1406,1413,type=<ALPHANUM>)\n    (fortune,1414,1421,type=<ALPHANUM>)\n    (archive/2007/03/19/8402357,1422,1448,type=<NUM>)\n    (index.htm,1449,1458,type=<HOST>)\n\n  NEW\n    (/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm,1381,1458,type=<NUM>)\n\nThe old is correct and the NEW should not match <NUM>. <NUM> should break on '/' and '.' and every other token from the break should have a digit for a NUM match to occur. This is not the case.\n\n  OLD\n    (mid-20th,2436,2444,type=<NUM>)\n\n  NEW\n    (mid,2436,2439,type=<ALPHANUM>)\n    (-20th,2439,2444,type=<NUM>)\n\nSomething is wrong with the NEW one. <NUM> is certainly a valid longer match.\n\n  OLD\n    (safari-0-sheikh,12011,12026,type=<NUM>)\n    (zayed,12027,12032,type=<ALPHANUM>)\n    (grand,12033,12038,type=<ALPHANUM>)\n    (mosque.jpg,12039,12049,type=<HOST>)\n\n  NEW\n    (safari,12011,12017,type=<ALPHANUM>)\n    (0-sheikh-zayed-grand-mosque.jpg,12018,12049,type=<NUM>)\n\nAgain, something seems wrong with the NEW.  (safari-0-sheikh,12011,12026,type=<NUM>) is a correct and longer match than  (safari,12011,12017,type=<ALPHANUM>)\n\nIt would be nice to have the source text for these comparisons.\n\nAlso, a hard vote against StandardAnalyzer2 <g> Default is arguable as well, as this wouldn't be the default analyzer you should use in many cases (don't like standard because of that either).\n\nFrom the latest samples, I would say something is off with the NEW and OLD appears mostly correct.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12517348"
        },
        {
            "date": "2007-08-02T20:02:04+0000",
            "content": "By the way...you can see one of the issues here:\n\nOLD\n    (money.cnn.com,1382,1395,type=<HOST>)\n    (magazines,1396,1405,type=<ALPHANUM>)\n    (fortune,1406,1413,type=<ALPHANUM>)\n    (fortune,1414,1421,type=<ALPHANUM>)\n    (archive/2007/03/19/8402357,1422,1448,type=<NUM>)\n    (index.htm,1449,1458,type=<HOST>)\n\n  NEW\n    (/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm,1381,1458,type=<NUM>) \n\nJavaCC StandardAnalyzer would never output a token that starts with a '/'. It would be cut off. The issues may be involved with how JFlex skips characters that are not part of a match compared to how JavaCC is doing it. Or perhaps the JFlex version is considering '/' and '.' to be ALPHANUM's.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12517358"
        },
        {
            "date": "2007-08-02T20:42:50+0000",
            "content": ">When digging deeper into the issues of compatibility with the original StandardAnalyzer, I stumbled upon something strange. Take the following >text:\n>\n>78academyawards/rules/rule02.html,7194,7227,type\n>\n>which was tokenized by the original StandardAnalyzer as one <NUM>. If you look at the definition of the <NUM> token:\n>\n>// every other segment must have at least one digit\n><NUM: (<ALPHANUM> <P> <HAS_DIGIT>\n>       | <HAS_DIGIT> <P> <ALPHANUM>\n>       | <ALPHANUM> (<P> <HAS_DIGIT> <P> <ALPHANUM>)+\n>       | <HAS_DIGIT> (<P> <ALPHANUM> <P> <HAS_DIGIT>)+\n>       | <ALPHANUM> <P> <HAS_DIGIT> (<P> <ALPHANUM> <P> <HAS_DIGIT>)+\n>       | <HAS_DIGIT> <P> <ALPHANUM> (<P> <HAS_DIGIT> <P> <ALPHANUM>)+\n>        )\n>\n>you'll see that, as explained in the comment, every other segment must have at least one digit. But actually, according to my understanding, this >rule should not match the above text as a whole (and with JFlex it doesn't , actually). Below is the text split by punctuation characters, and it looks >like there is no way of splitting this text into alternating segments, every second of which must have a digit (A = ALPHANUM, H = HAS_DIGIT):\n>\n>78academyawards / rules / rule02 . html , 7194 , 7227 , type\n>                H P A P H P A P H P A P H?* (starting from the beginning)\n>                                                                              H?* P A P H P A (starting from the end)\n>\n>* (would have to be H, but no digits in substring \"type\" or \"html\")\n>\n>I have no idea why JavaCC matched the whole text as a <NUM>, JFlex behaved \"more correctly\" here. \n\nI think that JavaCC is correct here. Every other segment must have a digit: 78academyawards / rules / rule02 . html \nIt looks to me like every other segment does have a digit. First segment has a digit, second does not, third does, fourth does not. Matches NUM correctly. ",
            "author": "Mark Miller",
            "id": "comment-12517368"
        },
        {
            "date": "2007-08-02T21:08:58+0000",
            "content": "Okkkk \u2013 only now I realized I made a really silly mistake  When using Mark's examples I somehow took the \",type\" substring as part of the token image, which made the JavaCC tokenizer look \"buggy\"...  Apologies for the confusion, tomorrow in the morning I'll correct my tests and will see what's happening.\n\nOne more important clarification \u2013 the tokenizer from the last patch (jflex-analyzer-r561693-compatibility.txt) has a completely different definition of the <NUM> token \u2013 it allows digits in any segment, hence the totally different results. If we want to be compatible with the StandardAnalyzer, we should forget about that patch.\n\nMark \u2013 have you tried the jflex-analyzer-r560135-patch.txt patch with your wikipedia diff test? That's the early one whose grammar was \"dot for dot\" translated from the original JavaCC spec \u2013 for further patches I did some \"optimizations\", which seem to have broken the compatibility...\n\nIncidentally, what was the motivation for requiring the <NUM> token to have numbers only in every second segment and not in any segment?\n ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517371"
        },
        {
            "date": "2007-08-03T08:19:20+0000",
            "content": "I've attached another patch with:\n\n\n\tgrammar translated \"dot for dot\" from JavaCC (no \"optimizations\" this time)\n\ttests extended with Mark's latest compatibility examples\n\n\n\nI hope this time the patch correctly applies to the test class (previously I used Eclipse to create the patch which may not have been a good idea). All unit tests pass, so I hope this time I finally got it right. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517483"
        },
        {
            "date": "2007-08-03T12:28:03+0000",
            "content": "The TestStandardAnalyzer.java patch still fails to apply \u2013 looks like\nthe same thing as before (the unicode chars under \"Korean words\" were\nreplaced by ?'s somewhere along the way).\n\nAnd, somehow, many of the files are dup'd (appear twice) in the\npatch (eg StandardTokenizerImpl.jflex, StandardFilter.java, etc.).\nI'm not sure whether first or 2nd one is the one to keep. ",
            "author": "Michael McCandless",
            "id": "comment-12517513"
        },
        {
            "date": "2007-08-03T12:33:45+0000",
            "content": "> The TestStandardAnalyzer.java patch still fails to apply \u2013 looks like\n> the same thing as before (the unicode chars under \"Korean words\" were\n> replaced by ?'s somewhere along the way). \n\nWOOPS!  This was my bad.  I was clicking on the patch and then\ncopy/pasting the text out of the browser into a local file, and this\nstep (ugh) introduces the ?'s.  If I instead right-click on the link\nand have browser directly save the file then it works perfectly!\n\nSorry for the confusion. ",
            "author": "Michael McCandless",
            "id": "comment-12517514"
        },
        {
            "date": "2007-08-03T12:34:53+0000",
            "content": "This time I used Tortoise, but it made things worse  I'll create the patch the usual way and e-mail it directly to you to see if it's not JIRA's fault (I don't see the '?' characters on my local machine). ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517515"
        },
        {
            "date": "2007-08-03T12:37:51+0000",
            "content": "One more try \u2013 this time without duplicated entries (no idea why Tortoise did this...). ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517518"
        },
        {
            "date": "2007-08-03T12:43:00+0000",
            "content": "Super, I will compare tokens on Wikipedia... ",
            "author": "Michael McCandless",
            "id": "comment-12517520"
        },
        {
            "date": "2007-08-03T14:12:39+0000",
            "content": "First 1000 documents in wikipedia show absolutely no difference \u2013 I think this one is it! ",
            "author": "Michael McCandless",
            "id": "comment-12517542"
        },
        {
            "date": "2007-08-03T16:51:35+0000",
            "content": "Good news \u2013 thanks for the test! ",
            "author": "Stanislaw Osinski",
            "id": "comment-12517577"
        },
        {
            "date": "2007-08-03T16:55:41+0000",
            "content": "OK I can commit this.  Does anyone see any reason not to?  I will wait a few days then commit...\n\nThank you Stanislaw! ",
            "author": "Michael McCandless",
            "id": "comment-12517581"
        },
        {
            "date": "2007-08-08T15:01:57+0000",
            "content": "\nI was gearing up to commit this, but realized the copyright on top of\nStandardTokenizer.java isn't the standard Apache header; it's this\none:\n\n/*\n\n\tCarrot2 project.\n *\n\tCopyright (C) 2002-2007, Dawid Weiss, Stanis\u0142aw Osi\u0144ski.\n\tPortions (C) Contributors listed in \"carrot2.CONTRIBUTORS\" file.\n\tAll rights reserved.\n *\n\tRefer to the full license file \"carrot2.LICENSE\"\n\tin the root folder of the repository checkout or at:\n\thttp://www.carrot2.org/carrot2.LICENSE\n */\n\n\n\nI believe (according to http://www.apache.org/legal/src-headers.html),\nwe need to replace the above with the standard Apache header and then\nmaybe move this one into the NOTICE.txt.  Is that right?  Stanislaw,\nis that OK?\n\nI would also add the Apache header into StandardAnalyzerImpl.jflex. ",
            "author": "Michael McCandless",
            "id": "comment-12518463"
        },
        {
            "date": "2007-08-08T17:50:19+0000",
            "content": "Absolutely \u2013 the header was there only because I used Carrot2's grammar to start with and forgot to remove it. You can put the Apache header on all the files, NOTICE.txt is not necessary either. Apologies for confusion  ",
            "author": "Stanislaw Osinski",
            "id": "comment-12518521"
        },
        {
            "date": "2007-08-08T17:54:57+0000",
            "content": "Super, thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12518523"
        },
        {
            "date": "2007-08-08T22:29:30+0000",
            "content": "OK I committed this!  Thank you Stanislaw!\n\nI ran a quick perf test on Wikipedia (first 50K docs only) and found\nthe new StandardTokenizer is ~6X faster \u2013 awesome \n\nI made these small additional changes over the final patch before\ncommitting:\n\n\n\tI removed StandardAnalyzer.html \"grammar doc\" generation from\n    build.xml since it was using jjdoc.  Stanislaw, is there something\n    in jflex that can generated a BNF description of the grammar as\n    HTML?\n\n\n\n\n\tI removed the @author tag from StandardTokenizer.java: we are\n    removing all such tags and instead giving credit in CHANGES.txt.\n\n\n\n\n\tI removed the whitespace-only diffs from common-build.xml &\n    build.xml.\n\n\n\n\n\tI put back the big comment that describes this tokenizer in\n    StandardTokenizer.java.\n\n\n\n\n\tPut standard Apache copyright headers in all sources.\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12518566"
        },
        {
            "date": "2007-08-09T06:53:14+0000",
            "content": ">  * I removed StandardAnalyzer.html \"grammar doc\" generation from\n>    build.xml since it was using jjdoc. Stanislaw, is there something\n>    in jflex that can generated a BNF description of the grammar as\n>    HTML? \n\nI've had a look at the JFlex docs and there doesn't seem to be such a tool for JFlex, I'm afraid. ",
            "author": "Stanislaw Osinski",
            "id": "comment-12518636"
        },
        {
            "date": "2007-08-09T08:21:53+0000",
            "content": "> > * I removed StandardAnalyzer.html \"grammar doc\" generation from\n> > build.xml since it was using jjdoc. Stanislaw, is there something\n> > > in jflex that can generated a BNF description of the grammar as\n> > > HTML?\n\n> I've had a look at the JFlex docs and there doesn't seem to be such\n> a tool for JFlex, I'm afraid.\n\nOK, I think we can just live without this.  Thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12518661"
        }
    ]
}