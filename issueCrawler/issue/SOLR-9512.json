{
    "id": "SOLR-9512",
    "title": "CloudSolrClient's cluster state cache can break direct updates to leaders",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "6.4"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "This is the root cause of SOLR-9305 and (at least some of) SOLR-9390.  The process goes something like this:\n\nDocuments are added to the cluster via a CloudSolrClient, with directUpdatesToLeadersOnly set to true.  CSC caches its view of the DocCollection.  The leader then goes down, and is reassigned.  Next time documents are added, CSC checks its cache again, and gets the old view of the DocCollection.  It then tries to send the update directly to the old, now down, leader, and we get ConnectionRefused.",
    "attachments": {
        "SOLR-9512.patch": "https://issues.apache.org/jira/secure/attachment/12829174/SOLR-9512.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-09-14T15:57:35+0000",
            "author": "Alan Woodward",
            "content": "Possibly related: SOLR-9090, SOLR-6312.\n\nI think the best case here is to catch a ConnectionRefusedException on a routed update and expire the collection cache entry before retrying? ",
            "id": "comment-15490788"
        },
        {
            "date": "2016-09-15T13:27:05+0000",
            "author": "Noble Paul",
            "content": "\n\tCatch the Connectionrefused Exception\n\tCheck how old is the state. if it is older than a few seconds invalidate the cache and retry\n\tif it is fresh, throw the error back\n\n ",
            "id": "comment-15493315"
        },
        {
            "date": "2016-09-16T08:36:20+0000",
            "author": "Alan Woodward",
            "content": "Having played with this a bit, I think adding extra retry logic to CloudSolrClient isn't the best solution; instead, I think we should make directUpdatesToLeaders a hint, rather than a directive, and just make sure that the leader is the first URL in the list passed to the load-balancer.  We can then check in the response if the leader was in fact the shard that served that particular request, and if not, then we invalidate the collection cache.  Christine Poerschke you worked on SOLR-9090, does this make sense to you? ",
            "id": "comment-15495751"
        },
        {
            "date": "2016-09-17T07:45:17+0000",
            "author": "Noble Paul",
            "content": "It's just hiding the problem. The fact that there is no leader for that shard means that the request will fail eventually.  ",
            "id": "comment-15498485"
        },
        {
            "date": "2016-09-17T07:50:25+0000",
            "author": "Alan Woodward",
            "content": "I don't think so - there is a leader, it's just that we locally have the wrong one cached.  And because we're sending our updates only to the previous (and now down) leader, we get a failure, when instead we ought to be sending this update on to the next leader. ",
            "id": "comment-15498495"
        },
        {
            "date": "2016-09-17T07:55:16+0000",
            "author": "Noble Paul",
            "content": "there is a leader, it's just that we locally have the wrong one cached\n\nWhen we get an error , we must invalidate the cache. That's part of the solution. But the larger problem is that leader election takes a while and the state.json will have that information after sometime. The solution of sending the doc to another replica in the shard is just kicking the can down the road ",
            "id": "comment-15498506"
        },
        {
            "date": "2016-09-17T09:22:28+0000",
            "author": "Alan Woodward",
            "content": "It's kicking the can to a live server, though, which will be able to tell us either a) yes, everything's fine now, you just need to invalidate your cache, or b) there's no leader for this shard, so fail the update.  Either way, we get useful information, whereas at the moment we're trying to find out what's happening from a dead server. ",
            "id": "comment-15498629"
        },
        {
            "date": "2016-09-17T09:23:13+0000",
            "author": "Alan Woodward",
            "content": "(apologies for the terrible metaphor there, it's early on a Saturday) ",
            "id": "comment-15498631"
        },
        {
            "date": "2016-09-17T09:53:36+0000",
            "author": "Noble Paul",
            "content": "So in your solution here is what happens\n\n\n\tInstead of just passing one server , we pass all the nodes to LBHttpSolrClient (LBHSC). The shard leader should be the first in the list\n\tLBHSC knows that the leader is a dead node (or it will soon know that). So it would pick up the next server in the list and makes a request there\n\tThe request would come back with an error (no leader)\n\tCSC returns the call with an error \"no leader\"\n\n\n\nIs that right? ",
            "id": "comment-15498680"
        },
        {
            "date": "2016-09-19T08:25:38+0000",
            "author": "Alan Woodward",
            "content": "Right, there are two scenarios:\n1. The leader is down, and there's no replacement voted for yet, in which case things happen much as you describe above\n2. The old leader is down, a new leader has been selected, but the cache hasn't updated yet.  In this case the update actually succeeds, as it's passed to the next node in the list and then forwarded on to the relevant leader.\nIn both cases, we need to invalidate the cache.\n\nSeparately, there's a bit of cleanup we can do in the directUpdate() method call - at the moment we have two paths, dependent on whether or not we're using parallel updates or not, and they end up doing things like throwing slightly different exceptions for the same failure types.  I'll open up another JIRA for that. ",
            "id": "comment-15502696"
        },
        {
            "date": "2016-09-19T09:41:05+0000",
            "author": "Noble Paul",
            "content": "The old leader is down, a new leader has been selected, but the cache hasn't updated yet. In this case the update actually succeeds, as it's passed to the next node in the list and then forwarded on to the relevant leader.\n\nThis is already taken care of. When SolrJ makes a request to the node, it responds with a flag to invalidate the cache. So the next request will get the updated state ",
            "id": "comment-15502871"
        },
        {
            "date": "2016-09-19T10:29:36+0000",
            "author": "Alan Woodward",
            "content": "Patch, changing the 'buildUrlMap()' method to always include a full list of replica URLs, with the leader in front.  In reality, this makes 'directUpdateToLeadersOnly' a no-op, so maybe we should just deprecate and remove it?\n\nWhen SolrJ makes a request to the node, it responds with a flag to invalidate the cache. So the next request will get the updated state\n\nI don't think this is the case for update requests?  At the least, when running the test in the patch, a breakpoint in ZkStateReader.compareStateVersions is never triggered.  Looking at HttpSolrCall it appears that it's only used in /select requests, which may be a bug by itself, of course. ",
            "id": "comment-15503035"
        },
        {
            "date": "2016-09-19T10:32:30+0000",
            "author": "Noble Paul",
            "content": "Looking at HttpSolrCall it appears that it's only used in /select requests,\n\nIt must be a serious bug. Without proper invalidation,  the caching is useless ",
            "id": "comment-15503044"
        },
        {
            "date": "2016-09-19T14:31:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit f96017d9e10c665e7ab6b9161f2af08efc491946 in lucene-solr's branch refs/heads/branch_6x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f96017d ]\n\nSOLR-9512: CloudSolrClient tries other replicas if a cached leader is down ",
            "id": "comment-15503651"
        },
        {
            "date": "2016-09-19T14:31:31+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 3d130097b7768a8d753476ffe26b83db070c8e20 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3d13009 ]\n\nSOLR-9512: CloudSolrClient tries other replicas if a cached leader is down ",
            "id": "comment-15503653"
        },
        {
            "date": "2016-09-19T18:01:05+0000",
            "author": "Noble Paul",
            "content": "Alan Woodward The patch contains way more changes than you mentioned. You committed it without any review from the people who are collaborating with you. \n\nIf there are other people collaborating on a ticket, the general protocol is that you submit a patch with the changes explained and give some review time before committing stuff.\n\nYou submitted a patch and 3 hours later you committed it ",
            "id": "comment-15504167"
        },
        {
            "date": "2016-09-19T18:17:30+0000",
            "author": "Noble Paul",
            "content": "The following is not the way we should invalidate the cache. \n\n if (response.getServer().equals(url) == false) {\n            // we didn't hit our first-preference server, which means that our cached\n            // collection state is no longer valid\n            invalidateCollectionState(collection);\n          }\n\n ",
            "id": "comment-15504227"
        },
        {
            "date": "2016-09-19T19:58:16+0000",
            "author": "Alan Woodward",
            "content": "The patch contains way more changes than you mentioned.\n\nI ... don't think it does?  It makes the changes I described above to CloudSolrClient, and adds a test case.  What else is there?\n\nThe following is not the way we should invalidate the cache.\n\nHow so? ",
            "id": "comment-15504502"
        },
        {
            "date": "2016-09-20T03:05:01+0000",
            "author": "Noble Paul",
            "content": "How so?\n\nYou invalidate the cache if the first server did not serve the request. That's a problem. When the next request comes , it gets the fresh state which is exactly the same as the entry that was just invalidated because the new leader is not elected yet and the state. json is not yet updated in ZK.  As we discussed before, the cache must be invalidated when a server says the version is stale ",
            "id": "comment-15505419"
        },
        {
            "date": "2016-09-20T07:18:30+0000",
            "author": "Alan Woodward",
            "content": "because the new leader is not elected yet\n\nThe invalidation clause above is only called when the request was successful, ie if a new leader is up.  If leader election hasn't happened yet, then LBHttpSolrClient will throw an exception and the whole request will fail.  The test case should illustrate this. ",
            "id": "comment-15505849"
        },
        {
            "date": "2016-09-20T07:22:03+0000",
            "author": "Noble Paul",
            "content": "The invalidation clause above is only called when the request was successful,\n\nIt shouldn't need to. The response must have the flag to invalidate the collection. if that code is not working as expected we should fix that.  ",
            "id": "comment-15505858"
        },
        {
            "date": "2016-09-20T08:02:23+0000",
            "author": "Alan Woodward",
            "content": "OK, but lets deal with that in a separate issue, as I think it's a bit more complicated than just fixing cache invalidation for /update requests.  The current invalidation and retry logic happens at the level of the whole request, but CSC is splitting updates up into several sub-requests, and we probably don't want to redo the whole thing if a single subshard has changed leader. ",
            "id": "comment-15505951"
        },
        {
            "date": "2016-09-20T08:04:23+0000",
            "author": "Noble Paul",
            "content": "Let's first discuss what is the fix and don't rush into solutions.If I were you I would revert the commit and discuss the solution and get a consensus ",
            "id": "comment-15505956"
        },
        {
            "date": "2016-09-20T08:28:38+0000",
            "author": "Christine Poerschke",
            "content": "Hi Alan Woodward and Noble Paul - am only now joining this ticket late here since i have been on vacation.\n\nThe SOLR-9090 directUpdatesToLeadersOnly motivation/intention was for the flag to be not a hint but a directive and for updates to 'fail fast' if there is (temporarily or otherwise) no shard leader. Fail fast (and let the caller of the CloudSolrClient handle alarming and retries as it sees fit) as opposed to sending or retry-sending to a non-leader which would then forward to the leader (and potentially still fail eventually, eventually/not-fast-slowly).\n\nMarvin Justice and I worked together on SOLR-9090 - Marvin, any thoughts on this ticket here? ",
            "id": "comment-15506019"
        },
        {
            "date": "2016-09-20T13:36:14+0000",
            "author": "ASF subversion and git services",
            "content": "Commit a4293ce7c4e849b171430a34f36b830a84927a93 in lucene-solr's branch refs/heads/branch_6x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4293ce ]\n\nRevert \"SOLR-9512: CloudSolrClient tries other replicas if a cached leader is down\"\n\nThis reverts commit f96017d9e10c665e7ab6b9161f2af08efc491946. ",
            "id": "comment-15506551"
        },
        {
            "date": "2016-09-20T13:36:17+0000",
            "author": "ASF subversion and git services",
            "content": "Commit bd3fc7f43ff54a174660b7ad51f031d2104f84b5 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd3fc7f ]\n\nRevert \"SOLR-9512: CloudSolrClient tries other replicas if a cached leader is down\"\n\nThis reverts commit 3d130097b7768a8d753476ffe26b83db070c8e20. ",
            "id": "comment-15506552"
        },
        {
            "date": "2016-09-20T13:42:08+0000",
            "author": "Alan Woodward",
            "content": "OK, have reverted.  For the moment, I'll set directUpdatesToLeaders=false on the two regularly failing test cases. ",
            "id": "comment-15506575"
        },
        {
            "date": "2016-09-20T14:14:52+0000",
            "author": "Noble Paul",
            "content": "Does it make any sense to have the explicit flag directUpdatesToLeaders ? IMHO it should be the only supported behavior. Why would we ever send a request ever to another node when the shard leader is down?\n\nMy proposal is as follows.\n\n\n\tThe LBHttpSolrClient is aware of down servers. So, if the leader for the shard is down we already know it and we can fail fast\n\tIf we have to fail because the shard leader is dead, we should try to explicitly read the state.json for that collection provided the cached state is older than a certain threshold (say 1 sec?). This means, the client may fire a request to ZK to every second to refresh the state. For such refreshes, check the version first to optimize ZK reads\n\n ",
            "id": "comment-15506675"
        },
        {
            "date": "2016-09-21T10:16:38+0000",
            "author": "ASF subversion and git services",
            "content": "Commit bae66f7cca8cff796d142eb19585d8e79fae34f8 in lucene-solr's branch refs/heads/branch_6x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bae66f7 ]\n\nSOLR-9305, SOLR-9390: Don't use directToLeaders updates in partition tests (see SOLR-9512) ",
            "id": "comment-15509483"
        },
        {
            "date": "2016-09-21T10:16:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit d326adc8bfa33432d50293402a39454d60e070e4 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d326adc ]\n\nSOLR-9305, SOLR-9390: Don't use directToLeaders updates in partition tests (see SOLR-9512) ",
            "id": "comment-15509487"
        },
        {
            "date": "2016-10-14T19:02:21+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Noble and I discussed this offline. Here is a summary of the problem and the solution:\n\nThere are five cases that we need to tackle. Assuming replica x is leader:\n\n\tCase 1: x is disconnected from zk, y becomes leader\n\t\n\t\tcurrently \u2014 x throws error on indexing, client fails and keeps trying to send requests to x and fail. This will continue until X re-connects and the client gets a stale state flag in the response.\n\t\n\t\n\tCase 2: x is dead, y becomes leader\n\t\n\t\tcurrently - client gets connect exception or NoResponseException (for in-flight requests) and client keeps retrying request to x. This will continue until x comes back online.\n\t\n\t\n\tCase 3: x is disconnected from zk, no one is leader\n\t\n\t\tcurrently \u2013 client keeps sending requests to x which fail because x is disconnected from leader. This will continue until X re-connects and the client gets a stale state flag in the response.\n\t\n\t\n\tCase 4: x is dead, no one is leader yet\n\t\n\t\tcurrently - client gets connect exception or NoResponseException (for in-flight requests) and client keeps retrying request to x. This will continue until x comes back online.\n\t\n\t\n\tCase 5: x is alive but now y is leader\n\t\n\t\tcurrently \u2013 client gets a stale state flag from x and refreshes cluster state to see y as the new leader. All further indexing requests are sent to y.\n\t\n\t\n\tCase 6: client is disconnected from zk\n\t\n\t\tcurrently \u2013 client keeps indexing to x. If it receives a stale state error, it will try to refresh cluster state, fail and continue to send further requests to x, keep failing and keep trying to read from zk and be stuck in a cycle.\n\t\n\t\n\n\n\nCases 1-5 are solved by a single solution \u2013 On ConnectException, NoHttpResponseException, Leader disconnected from zk error, client should fetch state from zk again. If client fetches from zk and does not get a new version then this should be marked in a flag and subsequent retries should only happen after N seconds are elapsed or if we know for a fact that version has changed since the last zk fetch was made. N could be something small as 2 seconds or so.\n\nCase 6 is more difficult. Either we can keep failing the indexing requests or we can ask a random Solr instance to return the latest cluster state. This is kinda dangerous because it can open us up to very difficult to debug bugs so I am inclined to punt on this for now. ",
            "id": "comment-15576189"
        },
        {
            "date": "2016-10-17T20:36:11+0000",
            "author": "Christine Poerschke",
            "content": "Thanks Shalin and Noble for analysing and summarising this.\n\nThe proposed cases 1-5 solution sounds good to me (though i have not actually looked at the code concerned to see what the implementation of that solution might look like).\n\nCase 6: do i understand it right that we would keep failing the indexing requests but 'only' until eventually the client manages to reconnect to zk? I agree that asking a random solr instance for its latest cluster state could be problematic. ",
            "id": "comment-15583374"
        },
        {
            "date": "2016-10-18T23:55:45+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Case 6: do i understand it right that we would keep failing the indexing requests but 'only' until eventually the client manages to reconnect to zk?\n\nYes, that is correct. ",
            "id": "comment-15587088"
        },
        {
            "date": "2016-11-14T15:19:28+0000",
            "author": "Noble Paul",
            "content": "no tests yet ",
            "id": "comment-15664168"
        },
        {
            "date": "2016-11-22T12:02:04+0000",
            "author": "Noble Paul",
            "content": "Added tests ",
            "id": "comment-15686548"
        },
        {
            "date": "2016-11-24T18:57:43+0000",
            "author": "ASF subversion and git services",
            "content": "Commit e309f9058985375076cac0ed982a158dd865b86a in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e309f90 ]\n\nSOLR-9784: Refactor CloudSolrClient to eliminate direct dependency on ZK\nSOLR-9512: CloudSolrClient's cluster state cache can break direct updates to leaders ",
            "id": "comment-15694027"
        },
        {
            "date": "2016-11-24T19:23:07+0000",
            "author": "ASF subversion and git services",
            "content": "Commit d87ffa4bf82c30e9a6f0bbb6b8c0087a5c07f9d6 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d87ffa4 ]\n\nSOLR-9784: Refactor CloudSolrClient to eliminate direct dependency on ZK\nSOLR-9512: CloudSolrClient's cluster state cache can break direct updates to leaders ",
            "id": "comment-15694063"
        },
        {
            "date": "2016-11-24T19:23:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 5650939a8d41b7bad584947a2c9dcedf3774b8de in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5650939 ]\n\nSOLR-9784: Refactor CloudSolrClient to eliminate direct dependency on ZK\nSOLR-9512: CloudSolrClient's cluster state cache can break direct updates to leaders ",
            "id": "comment-15694066"
        },
        {
            "date": "2016-11-25T02:48:23+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 142461b395506efa01ec2509346bae755f1b2726 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=142461b ]\n\nSOLR-9512: removed unused import ",
            "id": "comment-15694664"
        },
        {
            "date": "2016-11-25T03:40:57+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 3a3d6afef08e3c49c29cc9a015f4e7cca40cf52d in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a3d6af ]\n\nSOLR-9512: removed unused import ",
            "id": "comment-15694736"
        }
    ]
}