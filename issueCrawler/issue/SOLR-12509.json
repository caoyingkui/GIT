{
    "id": "SOLR-12509",
    "title": "Improve SplitShardCmd performance and reliability",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "fix_versions": [
            "7.5"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "SplitShardCmd is currently quite complex.\n\nShard splitting occurs on active shards, which are still being updated, so the splitting has to involve several carefully orchestrated steps, making sure that new sub-shard placeholders are properly created and visible, and then also applying buffered updates to the split leaders and performing recovery on sub-shard replicas.\n\nThis process could be simplified in cases where collections are not actively being updated or can tolerate a little downtime - we could put the shard \"offline\", ie. disable writing while the splitting is in progress (in order to avoid users' confusion we should disable writing to the whole collection).\n\nThe actual index splitting\u00a0could\u00a0perhaps be improved to use HardLinkCopyDirectoryWrapper for creating a copy of the index by hard-linking existing index segments, and then applying deletes to the documents that don't belong in a sub-shard. However, the resulting index slices that replicas would have to pull would be the same size as the whole shard.",
    "attachments": {
        "SOLR-12509.patch": "https://issues.apache.org/jira/secure/attachment/12932255/SOLR-12509.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-07-19T16:13:53+0000",
            "content": "This patch implements a new method for shard splitting that uses HardLinkCopyDirectoryWrapper. The old method is still available and used by default, and the new method may be selected by using splitMethod=link request parameter (the old method can be explicitly selected with splitMethod=rewrite).\n\nThere's also support for a new timing parameter - when set to true the SPLITSHARD command returns a \"timing\" section with elapsed times for each internal phase of the command execution.\n\nI've been testing the new implementation locally and on a cluster of 5 physical nodes, using collections ranging from 2 mln up to 22 mln documents (15 GB index size). The new method consistently outperforms the old method by a factor of 3 to 5, depending on the index size and number of replicas.\n\nThe downside of the new method is that the resulting sub-shards initially have the same size as the original shard - on the shard leader these files are hard-linked so they don't consume additional space, but replica nodes still need to fetch all that data, which affects the network IO and the initial disk consumption on replica nodes, and consequently replica recovery time. The upside is that the total time is much shorter and the CPU / IO load on the shard leader is negligible, unlike the old method that is very IO- and CPU-intensive.\n\nHere are example timings for the old method (note that the SPLITSHARD command returns before the remaining replicas have recovered - the parent shard is switched to sub-shards only when all replicas are recovered so the total time is a sum of the SPLITSHARD and the time for all replicas to recover):\n\n  \"timing\":{\n    \"time\":1547111.0,\n    \"checkDiskSpace\":{\n      \"time\":14.0},\n    \"fillRanges\":{\n      \"time\":2.0},\n    \"createSubSlicesAndLeadersInState\":{\n      \"time\":4439.0},\n    \"waitForSubSliceLeadersAlive\":{\n      \"time\":1009.0},\n    \"splitParentCore\":{\n      \"time\":1538986.0},\n    \"applyBufferedUpdates\":{\n      \"time\":7.0},\n    \"identifyNodesForReplicas\":{\n      \"time\":1.0},\n    \"createReplicaPlaceholders\":{\n      \"time\":7.0},\n    \"createCoresForReplicas\":{\n      \"time\":2173.0},\n    \"finalCommit\":{\n      \"time\":462.0}},\n\n\nAfter that, sub-shards recovered in 220753 ms, so the total time was ca. 1770 sec.\n\n\u00a0\n And the timings for the new method, with exactly the same initial data layout, hardware, etc:\n\n  \"timing\":{\n    \"time\":15633.0,\n    \"checkDiskSpace\":{\n      \"time\":5.0},\n    \"fillRanges\":{\n      \"time\":2.0},\n    \"createSubSlicesAndLeadersInState\":{\n      \"time\":4411.0},\n    \"waitForSubSliceLeadersAlive\":{\n      \"time\":2.0},\n    \"splitParentCore\":{\n      \"time\":9005.0},\n    \"identifyNodesForReplicas\":{\n      \"time\":0.0},\n    \"createReplicaPlaceholders\":{\n      \"time\":2.0},\n    \"createCoresForReplicas\":{\n      \"time\":2105.0},\n    \"finalCommit\":{\n      \"time\":95.0}},\n\n\nAfter that, sub-shards recovered in 443350 ms, so the total time was ca. 460 sec. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16549474"
        },
        {
            "date": "2018-07-20T11:16:36+0000",
            "content": "Awesome speedups!\n\nA few minor issues:\n\n\tSolrIndexSplitter.findDocsToDelete uses the wrong key to lookup inside the synchronized block \u2013 docsToDelete.get(readerContext.ord);\n\tThere is a new DefaultSolrCoreState.getIndexWriterLock method which isn't used anywhere?\n\tTypo changepostd in ReplicaMutator\n\tWe should rename index.split to follow the index.<timestamp> convention otherwise dangling \"index.split\" directories won't be cleaned up by DirectoryFactory.cleanupOldIndexDirectories\n\n ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16550662"
        },
        {
            "date": "2018-07-20T15:53:43+0000",
            "content": "Thanks Shalin for the review! I attached a new patch that fixes these issues. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16550905"
        },
        {
            "date": "2018-08-01T14:31:09+0000",
            "content": "Commit 1133bf98a5fd075173efecfb75a51493fceb62b3 in lucene-solr's branch refs/heads/master from Andrzej Bialecki\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1133bf9 ]\n\nSOLR-12509: Improve SplitShardCmd performance and reliability. ",
            "author": "ASF subversion and git services",
            "id": "comment-16565410"
        },
        {
            "date": "2018-08-01T17:38:48+0000",
            "content": "Commit 7faa803a7c9699f38b8a6b3ddd3a88c4729c5e5f in lucene-solr's branch refs/heads/branch_7x from Andrzej Bialecki\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7faa803 ]\n\nSOLR-12509: Improve SplitShardCmd performance and reliability. ",
            "author": "ASF subversion and git services",
            "id": "comment-16565690"
        },
        {
            "date": "2018-08-02T15:11:19+0000",
            "content": "Reproducing SolrIndexSplitterTest.testSplitAlternatelyLink() failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/2469/:\n\n\nChecking out Revision 600c15d14e73274d4152e8ef1b8c0d0aae69fd18 (refs/remotes/origin/branch_7x)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=SolrIndexSplitterTest -Dtests.method=testSplitAlternatelyLink -Dtests.seed=2EC831F1D9B21D7D -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=pl -Dtests.timezone=CTT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 1.02s J1 | SolrIndexSplitterTest.testSplitAlternatelyLink <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: split index1 has wrong number of documents expected:<5> but was:<6>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([2EC831F1D9B21D7D:9992FDD95CED0639]:0)\n   [junit4]    > \tat org.apache.solr.update.SolrIndexSplitterTest.doTestSplitAlternately(SolrIndexSplitterTest.java:272)\n   [junit4]    > \tat org.apache.solr.update.SolrIndexSplitterTest.testSplitAlternatelyLink(SolrIndexSplitterTest.java:247)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n[...]\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=Memory)}, docValues:{_version_=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Asserting)}, maxPointsInLeafNode=419, maxMBSortInHeap=6.5586874621200195, sim=RandomSimilarity(queryNorm=false): {}, locale=pl, timezone=CTT\n   [junit4]   2> NOTE: Linux 4.15.0-29-generic amd64/Oracle Corporation 1.8.0_172 (64-bit)/cpus=8,threads=1,free=160622208,total=536870912\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16566931"
        },
        {
            "date": "2018-08-02T18:48:42+0000",
            "content": "Thanks Steve Rowe - I found the bug (a side-effect of refactoring that somehow didn't affect tests on master), fix is coming. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16567355"
        },
        {
            "date": "2018-08-02T19:10:51+0000",
            "content": "Commit 724a65a60ab7537ab9f0c49cf0a93d2504553ae1 in lucene-solr's branch refs/heads/branch_7x from Andrzej Bialecki\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=724a65a ]\n\nSOLR-12509: Fix a bug when using round-robin doc assignment. ",
            "author": "ASF subversion and git services",
            "id": "comment-16567380"
        },
        {
            "date": "2018-08-02T19:22:10+0000",
            "content": "Commit b5ed6350a0ea444553242ef2b141161c0fc3830b in lucene-solr's branch refs/heads/master from Andrzej Bialecki\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b5ed635 ]\n\nSOLR-12509: Fix a bug when using round-robin doc assignment. ",
            "author": "ASF subversion and git services",
            "id": "comment-16567389"
        }
    ]
}