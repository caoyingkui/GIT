{
    "id": "SOLR-2564",
    "title": "Integrating grouping module into Solr 4.0",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Since work on grouping module is going well. I think it is time to wire this up in Solr.\nBesides the current grouping features Solr provides, Solr will then also support second pass caching and total count based on groups.",
    "attachments": {
        "LUCENE-2564.patch": "https://issues.apache.org/jira/secure/attachment/12481404/LUCENE-2564.patch",
        "SOLR-2564_performance_loss_fix.patch": "https://issues.apache.org/jira/secure/attachment/12483352/SOLR-2564_performance_loss_fix.patch",
        "SOLR-2564.patch": "https://issues.apache.org/jira/secure/attachment/12481011/SOLR-2564.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Martijn van Groningen",
            "id": "comment-13041892",
            "date": "2011-05-31T23:13:09+0000",
            "content": "Initial patch. All tests, but the random test pass. The goal is that all existing grouping test must pass without any changes to the TestGroupingSearch class.\n\nThis patch also includes collectors for function queries.\n\nLUCENE-3099 must be applied first before using the patch. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13043048",
            "date": "2011-06-02T21:10:11+0000",
            "content": "Added updated patch. All tests are passing now. I think that when LUCENE-3099 is committed, this patch can also be committed.\n\nAlso\u00a0included a small fix to the CachingCollector. During replaying the scorer is only set once. This must be done for every segment. This fix resolves that issue "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044028",
            "date": "2011-06-03T20:39:11+0000",
            "content": "\nPatch looks great!\n\nAttached new patch with minor fixes:\n\n\n\tI had to fixup Solr's common-build.xml (it was referencing\n    lucene/contrib/grouping from 3.x).  After that all tests pass.\n\n\n\n\n\tFixed jdocs for GroupParams.GROUP_CACHE (we now default to true\n    not false, right?)\n\n\n\nI think GroupParams.GROUP_FACETS (group.docSet request param) is\nunused?  Should we remove it?  (This is for when we do LUCENE-3097, I\nthink?)\n\nSo this cutover improves Solr trunk grouping's impl by adding option\nto get total group count (group.totalCount request param) and using\nthe CachingCollector for fast replaying of hits for 2nd pass collector\n(adds group.cache/group.cache.maxSizeMB request params).  The\ndouble-RAM issue is fixed, thanks to LUCENE-3099.\n\nSo I think this is ready to be committed!! "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044043",
            "date": "2011-06-03T20:55:28+0000",
            "content": "Fixed jdocs for GroupParams.GROUP_CACHE (we now default to true not false, right?)\nYes we do.\n\n\nI think GroupParams.GROUP_FACETS (group.docSet request param) is\nunused? Should we remove it? (This is for when we do LUCENE-3097, I\nthink?)\nI 'reserved' this parameter for LUCENE-3097. Should we keep it? Once LUCENE-3097 is committed, I will quickly integrate it into Solr. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044080",
            "date": "2011-06-03T21:53:51+0000",
            "content": "I haven't been following this... I took a quick look at the patch, and at first blush it's hard to tell what changes are cleanup and what changes are cut-over.\n\n\n\tin the QueryComponent, why the change to set the GET_SCORES flag based on the sort(s)?\n\tI'm not a fan of this new style for matching request parameters to enums... solr does a lot of lookups on a typical request, and a switch to this style everywhere could definitely have an impact (the whole upper-casing the request param so we can match it to the enum name).\n\t\"Accuracy\" seems a bit mis-named?  It seems to imply an accuracy trade-off, but both methods are 100% accurate here, they just do different things to serve different usecases.  At least the name doesn't seem to have made it's way into the external API though.\n\tThe parameter \"group.totalCount\" I would expect to return the total count of something, not control the pre/post faceting thing?  (or are the comments just wrong?)  If it's to return the number of groups, then perhaps the name should be \"group.groupCount\" as totalCount is unit-less.\n\tWhat does \"group.docSet\" do?  The comments don't quite make sense to me, but the param suggests it's sort of like group.totalCount?\n\tI'm not sure we should default group.cache to true... there's a downside to the memory usage, and it's fragile: things may be working just fine, and the user may add a few more documents to the index and then the limit is hit and it just stops working (but still consumes much memory and extra log warnings per request).\n\tin the interest of reducing the number of parameters, we could dump group.cache and have a single group.cacheMB parameter that uses 0 as no cache, -1 as maximum needed (solr uses -1 in this manner in other places too), and other values as literal number of MB (which I'd discourage people from using personally).\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044095",
            "date": "2011-06-03T22:24:07+0000",
            "content": "FYI: there's a nocommit in there misspelled as \"No commit\" "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044122",
            "date": "2011-06-03T23:21:41+0000",
            "content": "In QueryComponent, is it possible to remove this?\n\n+        if (limitDefault < 0) {\n+          limitDefault = searcher.maxDoc();\n+        }\n\n\n\nIt wasn't necessary before, and there are advantages to preserving information (like the fact that someone said \"no limit\" vs a specific number) until as late as possible.  That was previously handled by getMax() in Grouping.java, and I still see it being called... so it should be OK? "
        },
        {
            "author": "Bill Bell",
            "id": "comment-13044187",
            "date": "2011-06-04T03:52:41+0000",
            "content": "Do we have a group.facet = after ? I still like the old collapse component better because it had the collapse.facet=after.\n\nI think there is another ticket, but not sure if it is being worked on? "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044253",
            "date": "2011-06-04T10:15:39+0000",
            "content": "I haven't been following this... \n\nThank you for the review Yonik!\n\nI took a quick look at the patch, and at first blush it's hard to tell what changes are cleanup and what changes are cut-over.\n\nIt's definitely a big refactoring.\n\nin the QueryComponent, why the change to set the GET_SCORES flag based on the sort(s)?\n\nI assume this is because if you sort or groupSort by a Sort that\ncontains SortField.SCORE, you need a scorer.  Not sure why Solr trunk\ngets away with not doing this, though... Martijn do you know?  Oh, and\nlikely because the CachingCollector needs to know up front if scores\nare needed.\n\nI'm not a fan of this new style for matching request parameters to enums... solr does a lot of lookups on a typical request, and a switch to this style everywhere could definitely have an impact (the whole upper-casing the request param so we can match it to the enum name).\n\nThe .upper() calls (twice per request, only if the request has\ngroup=true) are negligible here (true, other situations might be\ndifferent, though we should fix them to lookup/check once).\n\nAnd, this approach gives us strong checking of the enum fields.\nContrast that with what's on trunk now:\n\n\n  String format = params.get(GroupParams.GROUP_FORMAT);\n  Grouping.Format defaultFormat = \"simple\".equals(format) ? Grouping.Format.Simple : Grouping.Format.Grouped; \n\n\n\nIf you have a typo in \"simple\", you'll unexpectedly get\nGrouping.Format.Grouped.\n\nI think we should strongly check all of our request params?\n\n\n\"Accuracy\" seems a bit mis-named? It seems to imply an accuracy trade-off, but both methods are 100% accurate here, they just do different things to serve different usecases. At least the name doesn't seem to have made it's way into the external API though.\n\nThe parameter \"group.totalCount\" I would expect to return the total count of something, not control the pre/post faceting thing? (or are the comments just wrong?) If it's to return the number of groups, then perhaps the name should be \"group.groupCount\" as totalCount is unit-less.\n\nI agree.  This setting just controls what the \"total hit count\" should\ncount \u2013 unique groups vs docs.\n\nHow about TotalCount.GROUPS and TotalCount.DOCS?\n\nWhat does \"group.docSet\" do? The comments don't quite make sense to me, but the param suggests it's sort of like group.totalCount?\n\nI think we should remove this from this patch (see my comment above \u2013\nit applies to LUCENE-3097).\n\nin the interest of reducing the number of parameters, we could dump group.cache and have a single group.cacheMB parameter that uses 0 as no cache, -1 as maximum needed (solr uses -1 in this manner in other places too), and other values as literal number of MB (which I'd discourage people from using personally).\n\n+1, that makes sense.\n\nI'm not sure we should default group.cache to true... there's a downside to the memory usage, and it's fragile: things may be working just fine, and the user may add a few more documents to the index and then the limit is hit and it just stops working (but still consumes much memory and extra log warnings per request).\n\nEnabling caching can make a huge improvement in QPS, especially for\nqueries that are costly to execute but don't match too many docs.\n\nMaybe instead of a fixed MB we could make it a percentage of the\nmaxDoc?  This would make it less fragile..  So you could say you're\nwilling to cache up to 50% of the total docs in the index.\n\nFYI: there's a nocommit in there misspelled as \"No commit\"\n\nMartijn can you fix this one...?  And in general try to spell it as\n\"nocommit\"  This is what our build catches.  (And, fear not, you're\nnot the only person to have trouble spelling nocommit!!). "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044255",
            "date": "2011-06-04T10:17:46+0000",
            "content": "\nDo we have a group.facet = after ? I still like the old collapse component better because it had the collapse.facet=after.\n\nI think there is another ticket, but not sure if it is being worked on?\nNo, not in this issue.  This issue is just about cutting Solr over to the grouping module.\n\nLUCENE-3097 is the issue for post-grouping facet counts. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044263",
            "date": "2011-06-04T10:42:22+0000",
            "content": "Hi Yonik,\n\nIt is good to know that you took a look at the patch!\n\nin the QueryComponent, why the change to set the GET_SCORES flag based on the sort(s)?\nYes I did this because I used to set Grouping.needScores with this flag. The needScores I also used whether to indicate if the scores need to be cached. However I have changed this in the updated patch and basically this check isn't done with setting GET_SCORES flag. \n\nI'm not a fan of this new style for matching request parameters to enums...\nWe can choose to leave out the upper-casing. Solr users would then need make sure that parameter options are spelled correctly. Would that be allright? \n\n\"Accuracy\" seems a bit mis-named?\nMaybe another name is more descriptive. Maybe style or method?\n\nThe parameter \"group.totalCount\" I would expect to return the total count of something, not control the pre/post faceting thing?\nThe jdoc is mixed up with group.docSet. I also think that group.groupCount is a better name. I changed this in the new patch \n\nWhat does \"group.docSet\" do?\nCurrently nothing. I plan to use it when I finish LUCENE-3097. Basically it will decide whether the docset (for FacetComponent and StatsComponent) is based on plain documents or groups. Since you can have more than one Command (Field / Function / Query), it will then select the first CommandField or CommandFunction. I'm not sure how we should handle multiple command when having more than one command. \n\nI'm not sure we should default group.cache to true\nThe query time can really be reduced with this option, but yes it requires more memory. If the cache collector threshold is met they array is immediately set to null during the search, so gc might be able to clean it up during the search. Also Solr users get a message in the response. Somehow I forget to move that from SOLR-2524, but it is in the updated patch now.\n\nwe could dump group.cache and have a single group.cacheMB parameter that uses 0 as no cache, -1 as maximum needed (solr uses -1 in this manner in other places too)\nMakes sense, grouping then at least consistent with the rest of Solr. I made it default to -1 for now.\n\nFYI: there's a nocommit in there misspelled as \"No commit\"\nI have removed that.\n\n\nIt wasn't necessary before, and there are advantages to preserving information (like the fact that someone said \"no limit\" vs a specific number) until as late as possible. That was previously handled by getMax() in Grouping.java, and I still see it being called... so it should be OK?\nI've removed this if statement and made sure that getMax(...) is used wherever it is needed. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044267",
            "date": "2011-06-04T11:00:41+0000",
            "content": "How about TotalCount.GROUPS and TotalCount.DOCS?\n+1 I think we can with that name for the total counts.  "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044283",
            "date": "2011-06-04T12:39:39+0000",
            "content": "We can choose to leave out the upper-casing. Solr users would then need make sure that parameter options are spelled correctly. Would that be allright?\n\nIt seems like \"typically\" the request param values must be lower-case?\n(Which I think is good...).\n\nSo, maybe... we make an exception to enum casing (normally ALL CAPS)\nand use lowercase for enums that corresopnd to valid values for\nrequest params?  Then we can just do .valueOf() directly, and we get\nthe strong checking to catch mis-spellings. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044322",
            "date": "2011-06-04T16:08:37+0000",
            "content": "So, maybe... we make an exception to enum casing (normally ALL CAPS) and use lowercase for enums that corresopnd to valid values for request params? \n\n+1 - seems like a good tradeoff, and the fact that it's scoped by the classname still makes it stand out. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044535",
            "date": "2011-06-05T14:11:48+0000",
            "content": "Thanks for theupdate Martijn, it's looking good.\n\nJust a note that the following optimization will no longer be valid once we have \"post collapse faceting\" or whatever we're calling it, or\nwhen we have an option to return the total number of groups.\nBut hopefully our random testing will catch that in the future.\n\n\n    protected Collector createFirstPassCollector() throws IOException {\n      // Ok we don't want groups, but do want a total count\n      if (actualGroupsToFind <= 0) {\n        fallBackCollector = new TotalHitCountCollector();\n        return fallBackCollector;\n      }\n\n\n\nHowever I have changed this in the updated patch and basically this check isn't done with setting GET_SCORES flag.\n\nThanks... GET_SCORES does have a different meaning: scores must be returned to the caller (which can still be false even if scores are used for sorting)\n\nI also think that group.groupCount is a better name.\n\ngroupCount is now GROUPED or UNGROUPED, and is used to set \"Accuracy\" (more on that later \nSeems like this parameter should be a boolean that says if the total number of groups should be returned?\nIf true, we can add a \"ngroups\" or \"groupCount\" element at the same level as \"matches\".  We should probably just name the parameter the same thing as the variable that gets returned... i.e. group.ngroups=true would cause \"ngroups\" to be populated (or groupCount if we decide that's a better name).\n\nMaybe another name is more descriptive. Maybe style or method?\n\"method\" should probably be reserved for the algorithm used for collapsing (as we do for faceting).\n\nBackground for others: This feature has been called many things like \"post collapse faceting\", etc.  But it's really much more than that.  Normal grouping simply groups documents and presents them in a different way, but does not change what documents match the base query + filters.  The other use-case is more like field collapsing and does change what documents match (basically, only the first documents in each group, up to limit, \"match\").\n\nMaybe just use a word from the original name for this whole feature... \"group.collapse=true\"?\n\nThere are some other interesting semantics to work out for group.collapse=true, such as if the collapsing happens before or after filters are applied.  Perhaps either could make sense depending on the use case?  Here's one use case I can think of: using field collapsing for only showing the latest version of a document.  In this case, one would only want collapsing to apply to the base query (with filtering happening after that) because you don't want to get into the position of having a filter that filters out the most recent version of a document and thus shows an older version.\n\nHowever for now if it's easier, we could treat group.collapse=true to apply to the base query and all filters, and handle the use case I mentioned above via a qparser in the future. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044573",
            "date": "2011-06-05T16:36:10+0000",
            "content": "\ngroupCount is now GROUPED or UNGROUPED, and is used to set \"Accuracy\" (more on that later \nSeems like this parameter should be a boolean that says if the total number of groups should be returned?\nIf true, we can add a \"ngroups\" or \"groupCount\" element at the same level as \"matches\". We should probably just name the parameter the same thing as the variable that gets returned... i.e. group.ngroups=true would cause \"ngroups\" to be populated (or groupCount if we decide that's a better name).\nI like that. I'll update the patch soon, so it includes ngroups together with matches, if group.ngroups=true is specified. \n\n\"method\" should probably be reserved for the algorithm used for collapsing (as we do for faceting).\nMaybe we can use this for letting the user choose one of the Term|Block|Function|*GroupCollectors. The collectors define the way how groups are created.\n\nHowever for now if it's easier, we could treat group.collapse=true to apply to the base query and all filters, and handle the use case I mentioned above via a qparser in the future\nI agree let's commit this first and then start to enhance the collapse feature. The group.collapse should be included when LUCENE-3097 is finished which shouldn't take that long!  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044647",
            "date": "2011-06-05T23:28:34+0000",
            "content": "Attached an updated patch.\n\n\tRenamed group.groupCount to group.ngroups\n\tWhen group.ngroups=true ngroups is added with matches in the response.\n\tIncluded group.ngroups option into random testing. (all grouping tests succeed)\n\n "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044820",
            "date": "2011-06-06T12:43:40+0000",
            "content": "Patch looks great Martijn!\n\nOnly thing I noticed is cacheSizeMB is computed incorrectly from\nmaxDoc (for the -1 case), because that's all int math I think?  Ie\nit'll be truncated from eg 13.7 MB -> 13.  But: why not just use\nDouble.MAX_VALUE? "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044821",
            "date": "2011-06-06T12:44:51+0000",
            "content": "The other use-case is more like field collapsing and does change what documents match (basically, only the first documents in each group, up to limit, \"match\").\n\nI'm not sure it's that simple, ie that we can so cleanly model\ncollapsing as reducing the docs to consider and then running faceting\non that reduced set.\n\nEG, the use case of getting correct facet counts for a field that has\ndifferent values within the group, can't be handled by this approach?\nThis is the count=2 for size=S in my example at\nhttps://issues.apache.org/jira/browse/LUCENE-3097?focusedCommentId=13038605&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13038605\n\nI think to do that properly, the faceting impl needs to see all docs\nin the group, not just the \"lead doc\" per group.\n\nI think another way to visualize/model this that we really need to be\nable to configure \"which field counts\" (ID_FIELD) for the schema.\nThis field would then decide all counts \u2013 total \"hit count\", facet\ncounts, etc., ie each of these counts is count(unique(ID_FIELD)) of\nthe docs falling in that facet/result set.  The default is Lucene's docid,\nbut the app should be able to state any other ID_FIELD. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13044867",
            "date": "2011-06-06T14:18:37+0000",
            "content": "> The other use-case is more like field collapsing and does change what documents match (basically, only the first documents in each group, up to limit, \"match\").\n\nI'm not sure it's that simple, ie that we can so cleanly model collapsing as reducing the docs to consider and then running faceting on that reduced set.\n\nI think that's what was actually implemented in SOLR-236 IIRC, and what some people seem to be asking for.\n\nEG, the use case of getting correct facet counts for a field that has different values within the group, can't be handled by this approach?\n\nWell, correct is a matter of context  (for example, some have called the facet counts for the current grouping implementation \"incorrect\" because it didn't happen to match their use case).  Looking at the original description in LUCENE-3097, it seems you're talking about Martijn's 3rd method, while I was talking about the 2nd.  But maybe some people that were originally advocating for #2, really wanted #3?\n "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13044927",
            "date": "2011-06-06T16:03:01+0000",
            "content": "I think a good criteria for \"correct\" is if you were to click through on the facet (ie, take the current query and add a filter on facet field = facet value), would the hit count you see match the facet count you were just looking at?\n\nIe, drill down should be \"consistent\".\n\nBoth approaches will give the same facets counts if the field never varies within the group (ie, the field \"belongs\" to the \"parent\" docs); it's only \"child\" fields where you need faceting to be aware of the groups, so for apps that never display facets on child fields, only computing facets on the group heads will work.\n\nI suspect doc blocks will be the only practical way to implement faceting on child fields efficiently. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13044940",
            "date": "2011-06-06T16:23:22+0000",
            "content": "But: why not just use Double.MAX_VALUE?\nYes, I should have used that and I'll change that. I thought that the size was initially used to create the underline array. But it isn't! The array inside the caching collector initially starts with a length 128 and grows when needed.\n\nHow I've currently implemented LUCENE-3097 is that it will only get the most relevant document of each group. In terms of SOLR-236 that is the same as using collapse.threshold=1. I think what Yonik means is increasing the threshold so more documents and up in the docset, that eventually is used by the facet component. Increasing this threshold also means setting when to start to collapse. So when setting the collapse.threshold=3 this means that from the 4th document the collapsing starts. I think that the whole collaps.threshold feature doesn't scale very well. \n\nAnyway, I think when we go wire the 2nd method (LUCENE-3097) into Solr, we should first make it work for the most relevant group documents. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13045002",
            "date": "2011-06-06T18:25:28+0000",
            "content": "Browsing around this a bit more... the existing solr code selected the string based collectors for any ValueSource of StrFieldSource.  This patch resorts to exact getClass() checks against string and text fields which won't match in as many cases (either derived fields, or user custom fields that don't derive from either of the these field types) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13045027",
            "date": "2011-06-06T18:53:28+0000",
            "content": "I've been checking out the performance, and it generally seems fine.  But of course we normally short circuit based on comparators and often don't get beyond that... so to exercise & isolate the rest of the code, I tried a worst-case scenario where the short circuit wouldn't work (sort=docid desc) and solr trunk with this patch is ~16% slower than without it.  Any ideas what the problem might be?\n\n\nhttp://localhost:8983/solr/select?q=*:*&sort=_docid_ desc&group=true&group.cacheMB=0&group.field=single1000_i\n\n\n\nNote: the single1000_i field is a single valued int field with 1000 unique values "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13045079",
            "date": "2011-06-06T20:35:33+0000",
            "content": "Hmmm.  Was this with or without caching? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13045087",
            "date": "2011-06-06T20:46:55+0000",
            "content": "This was without caching to put them on an even footing (and given that the base query was all docs, caching would be slower anyway).  The URL above was the actual one used to test. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13045898",
            "date": "2011-06-08T11:02:06+0000",
            "content": "Attached a new patch that should select the term based collectors now more often:\n\n\tIn the case of grouping by function and the value source is StrFieldValueSource.\n\tAll field types that produce a StrFieldValueSource (in getValueSource) use now the term based collectors. So any custom field type can now also be supported. Both TextField and StrField produce a StrFieldValueSource. I'm not if this is the right approach, but it was the most easy way to implement.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13045904",
            "date": "2011-06-08T11:12:10+0000",
            "content": "\nI've been checking out the performance, and it generally seems fine. But of course we normally short circuit based on comparators and often don't get beyond that... so to exercise & isolate the rest of the code, I tried a worst-case scenario where the short circuit wouldn't work (sort=docid desc) and solr trunk with this patch is ~16% slower than without it. Any ideas what the problem might be?\n\nWhat might be the problem is that the trunk is using (Grouping.java 589):\n\nSearchGroup smallest = orderedGroups.pollLast();\n\n\n\nWhilst the AbstractFirstPassGroupingCollector (line 217) is using:\n\nfinal CollectedSearchGroup<GROUP_VALUE_TYPE> bottomGroup = orderedGroups.last();\norderedGroups.remove(bottomGroup);\n\n\nThe above also happen around line 271.\n\nI haven't checked this out, but I think it is the most likely explanation between those two implementations. Retrieving the bottom group will be done in almost all cases when the short circuit doesn't work  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13045920",
            "date": "2011-06-08T12:23:46+0000",
            "content": "Ah, good call Martijn - it must be that pollLast was replaced with two map operations.\nToo bad Lucene isn't on Java6 yet! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13045924",
            "date": "2011-06-08T12:27:07+0000",
            "content": "just send an email to the dev list... lots of people will +1, uwe will -1, but I dont see why this is any issue for 4.0 "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13046002",
            "date": "2011-06-08T15:17:59+0000",
            "content": "Ahh nice catch Martijn! "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13046072",
            "date": "2011-06-08T16:58:09+0000",
            "content": "I think we should decouple the \"should Lucene require Java 6\" (which I expect to be an..... involved, discussion) from making progress here?\n\nMy feeling is we should still commit this.  The 16% slowdown is on a very synthetic case (MatchAllDocsQuery, sorting by reversed docID, grouping by random int field)... unless we also see unacceptable slowdowns in more realistic cases?  Also, net/net the user should see a speedup, typically, since caching is enabled by default.\n\nWe should still open an issue to cutover this code back to the pollLast once we can use Java 6 code.\n\nAnother option is to allow the grouping module (separately from Lucene core) to use Java 6 code.... but even that could be \"involved\" \n\nYonik, how do you create the index used for this test?  Somehow you generate an int field w/ random 1000 unique values \u2013 do you have a client-side script you use to create random docs in Solr? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046091",
            "date": "2011-06-08T17:22:31+0000",
            "content": "Another option is to allow the grouping module (separately from Lucene core) to use Java 6 code\n\n+1\n\nYonik, how do you create the index used for this test? Somehow you generate an int field w/ random 1000 unique values \u2013 do you have a client-side script you use to create random docs in Solr?\n\nI have some CSV files laying around that I reuse for ad-hoc testing of a lot of stuff.  They were created with a simple python script.\nThen I simply index with\n\nURL=http://localhost:8983/solr\ncurl \"$URL/update/csv?stream.url=file:/tmp/test.csv&overwrite=false&commit=true\"\n\n\n\nIt was also my first reaction to think that this is a very synthetic case that people are unlikely to hit... until I thought about dates.  Indexing everything in date order is a pretty common thing to do, and so is sorting by date - which hits the exact same case.  Queries of : and simple filter queries on type, etc, also tend to be pretty common (i.e. full-text relevance/performance actually isn't an important feature for some users).\n\nHow complex must queries be for caching to generate a net benefit under load? I haven't tried to test this myself. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13046103",
            "date": "2011-06-08T17:42:03+0000",
            "content": "\nAnother option is to allow the grouping module (separately from Lucene core) to use Java 6 code.... but even that could be \"involved\" \n\nPersonally I am against parts of the code being java 6 and other parts being java 5. we already have this situation today (the solr part is java 6, everyhting else is java 5).\n\nCome on, its a major version, lets just cut over everything to java 6. Java 5 isn't even supported by oracle anymore, so why the hell do we support it? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13046105",
            "date": "2011-06-08T17:52:47+0000",
            "content": "Java 5 isn't even supported by oracle anymore, so why the hell do we support it?\n\n+1  (though this discussion should happen elsewhere)\n\nIf the 16% slowdown is worst case and under 'normal' use it would be equivolent/faster, i say lets move forward with that and push for java6 in a different issue.  Having a java6 version of lucene module (in solr?) seems like an mess.\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13046106",
            "date": "2011-06-08T17:54:24+0000",
            "content": "Just for grins, here's a Ruby script that'll do it (provided you have the solr-ruby gem installed):\n\n\nrequire 'solr'\nsolr = Solr::Connection.new\n1.upto(1000) {|i| solr.add(:id=>i, :single1000_i=>i)}\nsolr.commit\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046125",
            "date": "2011-06-08T18:34:41+0000",
            "content": "If the 16% slowdown is worst case\n\nActually, the worst case is twice as slow due to unneeded caching of a simple query.  Luckily this can be configured... but I still question the default, which can lead to surprisingly huge memory use (think up to a field cache entry or more allocated per-request).  One advantage to the dual-pass approach by default in the first place was avoiding surprisingly large memory usage by default (which can degrade less gracefully by causing OOM exceptions as people try to crank up the number of request threads). "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13046150",
            "date": "2011-06-08T19:25:52+0000",
            "content": "Actually, the worst case is twice as slow due to unneeded caching of a simple query.\n\nSorry, what do you mean here?\n\nbut I still question the default, which can lead to surprisingly huge memory use (think up to a field cache entry or more allocated per-request).\n\nI agree; -1 is a dangerous default.\n\nBut I think caching should still default to on, just limited as a pctg\nof the number of docs in the index.  Ie, by default we will cache the\nresult set if it's less than 20% (say) of total docs in your index.\nElse we fallback to 2-pass.\n\nI think this matches how Solr handles caching filters now?  Ie, filter\ncache evicts by total filter count and not net MB right, I think?  So\nthat if you have more docs in your index you'll spending more RAM on\nthe caching...\n\nCostly queries that return a smallish result set can see big gains\nfrom the caching. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046707",
            "date": "2011-06-09T17:56:22+0000",
            "content": "> Actually, the worst case is twice as slow due to unneeded caching of a simple query.\nSorry, what do you mean here?\n\nThe worst case with this patch as a whole (due to the caching by default).\nThis type of query is twice as slow:\n\nhttp://localhost:8983/solr/select?q=*:*&group=true&group.field=single1000_i\n\n\n\nWhich led to me wondering about how complex queries must be before the caching is a win.\n "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13046766",
            "date": "2011-06-09T19:29:19+0000",
            "content": "Ahh, I see.  Could we turn off caching if the query is instanceof AllDocsQuery? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13046845",
            "date": "2011-06-09T22:01:52+0000",
            "content": "\nBut I think caching should still default to on, just limited as a pctg\nof the number of docs in the index. Ie, by default we will cache the\nresult set if it's less than 20% (say) of total docs in your index.\nMaybe instead of specifying a maximum size for the second pass cache, we could specify it with a percentage (0 till 100) relative from maxdoc. In this case when the index grows in number of documents the cache is still used for a lot of queries (depending on the specified percentage). So if we go with this maybe group.cacheMB should be renamed to group.cache.percentage. The default can then be something like 20. Any thoughts about this? "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13046911",
            "date": "2011-06-09T23:14:11+0000",
            "content": "+1 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13047352",
            "date": "2011-06-10T18:23:51+0000",
            "content": "Marking this issue as a blocker for Solr 4.0 per McCandless comment in SOLR-2524...\n\n\nThat said, the plan is definitely to get Solr 4.0 cutover to the\ngrouping module; it's just a matter of time. I don't think we should\nship 4.0 until we've done so. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13047478",
            "date": "2011-06-10T21:36:16+0000",
            "content": "Hmm, I think this only needs to be a 4.0 blocker if we commit SOLR-2524 (3.x Solr grouping) first.\n\nBut at this point, since we are close on this issue, it looks like we should hold SOLR-2524 until we commit this, then backport & commit to 3.x. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13047917",
            "date": "2011-06-11T14:45:42+0000",
            "content": "Added updated patch.\n\n\tThe caching behaviour is now controlled by group.cache.percent=[0-100]. This percentage is relative to maxdoc. A value of 0 will disable group caching and the default is 20.\n\tAdded hits to\u00a0log as requested in SOLR-2337. This will add hits also when group.main is not true.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13047918",
            "date": "2011-06-11T14:48:46+0000",
            "content": "it looks like we should hold SOLR-2524 until we commit this, then backport & commit to 3.x.\nI agree with that. Also most of the discussion takes place in this issue. \n\nI think we're getting close to commit this issue. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13048307",
            "date": "2011-06-12T12:09:52+0000",
            "content": "Patch looks great Martijn... only thing pending (I think?) is to perhaps disable caching collector when the query is MatchAllDocsQuery instance, since we know cache only hurts perf in this case.\n\nElse, +1 to commit!  I don't think we should hold this pending the Java 1.6 discussion. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13048358",
            "date": "2011-06-12T15:40:38+0000",
            "content": "After some more testing I think it is better to disable the group cache by default.\nHere are my small test results on a index of 30M documents. Test were produced on a low end machine.\n\n\n\nQuery type\nQuery\nNr of matches\nSearch time with caching\nSearch time without caching\n\n\nTerm query\ncountry:es\n24270711\n2564\n2261\n\n\nBoolean query\ncountry:es OR country:tr\n25951654\n4014 \n4170\n\n\nFuzzy query\nregion:riviera~0.001\n977043\n146\n189\n\n\nWildcard query\nregion:r*\n1608781\n177\n190\n\n\nComplexer boolean query\nregion:r* OR region:riveria~0.0001 OR country:es OR country:tr\n27582783\n5328\n7289\n\n\n\n\nSo I think that we can conclude from here is that the group cache work well for boolean, wildcard and fuzzy queries. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13048360",
            "date": "2011-06-12T15:47:06+0000",
            "content": "Attached an update version of the patch.\n\n\tThe group.cache.percent defaults now to 0, which disables the grouping cache.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13050677",
            "date": "2011-06-16T19:44:40+0000",
            "content": "I also did some performance tests with the following query on random data in the example schema:\n\nhttp://localhost:8983/solr/select?q=*:*&sort=_docid_ desc&group=true&group.cacheMB=0&group.field=single1000_i\n\nThe field single1000_i had 1000 distinct values and the index has in total 100000 documents.\n\nI ran this query on the following Solr setups:\n\n\tLast nights nightly build.\n\tSolr build with this patch as it is.\n\tSolr build with this patch and the necessary changes in AbstractFirstPassGroupingCollector so that pollLast was used in all cases.\nDuring my tests I noticed that differences between the first and the second setups was neglectable smal, but the the last Solr setup was on average 32% faster than the two other setups. So moving to the Java6's pollLast() method has definitely a positive impact on performance!\n\n\n\nI also think that this patch is ready to be committed and that the pollLast should be added when Lucene or the grouping module is java 6. (I prefer the first option) I'll commit it in the coming day or so. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13051335",
            "date": "2011-06-17T21:31:50+0000",
            "content": "Committed in revision 1137037 "
        },
        {
            "author": "Matteo Melli",
            "id": "comment-13052492",
            "date": "2011-06-21T11:38:54+0000",
            "content": "Hi there,\n\nI'm testing this functionality into my project and found what I think it's a bug. The revision I'm working on is 1137889.\n\nI could reproduce the bug with a really simple index (the column is of type solr.String):\n\n\n\n\n Col1 \n\n\n 1 \n\n\n 2 \n\n\n 3 \n\n\n\n\n\nThe bug appear when I try to do a query with grouping mixing parameters start (with a value greather than 0) and group.main=true:\n\nhttp://localhost:8983/solr/test/select/?q=*:*&start=1&group=true&group.field=Col1&group.main=true\n\nThe error trace is:\n\nJun 21, 2011 1:32:10 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.ArrayIndexOutOfBoundsException: 3\n\tat org.apache.solr.search.DocSlice$1.nextDoc(DocSlice.java:119)\n\tat org.apache.solr.response.TextResponseWriter.writeDocuments(TextResponseWriter.java:247)\n\tat org.apache.solr.response.TextResponseWriter.writeVal(TextResponseWriter.java:153)\n\tat org.apache.solr.response.XMLWriter.writeResponse(XMLWriter.java:111)\n\tat org.apache.solr.response.XMLResponseWriter.write(XMLResponseWriter.java:37)\n\tat org.apache.solr.servlet.SolrDispatchFilter.writeResponse(SolrDispatchFilter.java:340)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:261)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:242)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\n\nThe problem does not appear without group.main=true so this may be a related bug to that option.\n\nPS: I was not sure if there where to open a bug since the version affected is still in development. Anyway sorry for any inconvenient. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13052850",
            "date": "2011-06-21T21:26:25+0000",
            "content": "Here's a simple patch that fixes the performance we lost when Solr cut over to the modules implementation (due to the fact that modules are on Java5 and Solr is on Java6). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13052856",
            "date": "2011-06-21T21:33:24+0000",
            "content": "+1 thats a really good solution, fixes the issue and we can still table the java5/java6 stuff for later. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13052857",
            "date": "2011-06-21T21:33:32+0000",
            "content": "That fix looks great Yonik!  Let the subclass use Java 6 only code... "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13063231",
            "date": "2011-07-11T09:08:12+0000",
            "content": "Since Lucene is now also Java 6 we can just change the code in AbstractFirstPassGroupingCollector and the TermFirstPassGroupingCollectorJava6 in grouping.java is no longer needed, right? "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13063244",
            "date": "2011-07-11T09:34:32+0000",
            "content": "Since Lucene is now also Java 6 we can just change the code in AbstractFirstPassGroupingCollector and the TermFirstPassGroupingCollectorJava6 in grouping.java is no longer needed, right?\nyes thats right "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13063272",
            "date": "2011-07-11T10:32:56+0000",
            "content": "Hi Matteo, I can also confirm the bug and only happens when group.main=true. I also think that this error occurs on 3x code base. I'll provide a fix for this issue soon. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-13063342",
            "date": "2011-07-11T13:48:49+0000",
            "content": "I've fixed the issue and added a test that triggered the exception.\nFixed in trunk in revision 1145173\nFixed in 3x branch in revision 1145176 "
        }
    ]
}