{
    "id": "SOLR-11741",
    "title": "Offline training mode for schema guessing",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Our data driven schema guessing doesn't work under many situations. For example, if the first document has a field with value \"0\", it is guessed as Long and subsequent fields with \"0.0\" are rejected. Similarly, if the same field had alphanumeric contents for a latter document, those documents are rejected. Also, single vs. multi valued field guessing is not ideal.\n\nProposing an offline training mode where Solr accepts bunch of documents and returns a guessed schema (without indexing). This schema can then be used for actual indexing. I think the original idea is from Hoss.\n\nI think initial implementation can be based on an UpdateRequestProcessor. We can hash out the API soon, as we go along.",
    "attachments": {
        "screenshot-3.png": "https://issues.apache.org/jira/secure/attachment/12904994/screenshot-3.png",
        "RuleForMostAccomodatingField.png": "https://issues.apache.org/jira/secure/attachment/12905000/RuleForMostAccomodatingField.png",
        "SOLR-11741-temp.patch": "https://issues.apache.org/jira/secure/attachment/12904998/SOLR-11741-temp.patch",
        "screenshot-1.png": "https://issues.apache.org/jira/secure/attachment/12904992/screenshot-1.png",
        "SOLR-11741.patch": "https://issues.apache.org/jira/secure/attachment/12920156/SOLR-11741.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-12-09T10:38:41+0000",
            "content": "\nI am working on the same.  ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16284694"
        },
        {
            "date": "2017-12-11T17:06:09+0000",
            "content": "This'll be great! ",
            "author": "David Smiley",
            "id": "comment-16286265"
        },
        {
            "date": "2018-01-06T00:02:45+0000",
            "content": "Proposing an offline training mode where Solr accepts bunch of documents and returns a guessed schema (without indexing). This schema can then be used for actual indexing. I think the original idea is from Hoss.\n\nI think initial implementation can be based on an UpdateRequestProcessor. We can hash out the API soon, as we go along.\n\nFWIW...\n\nWhat i suggested at one point (I don't remember where ... it may already be in a jira somewhere?) was an UpdateRequestProcessorFactory that could be configured instead of RunUpdateProcessorFactory in a chain (that does not already include AddSchemaFieldsUpdateProcessorFactory) after all of the various ParseFooFieldUpdateProcessorFactories.\n\n\n\tFor ADD commands, this processor (factory) would iterate over the SolrInputDocuments, and then iterate over the field names in those documents, and record in memory wether any docs had more then one value for that field name, as well as the \"Least Common Denominator\" of the java type of the values found \u2013 ie:\n\t\n\t\tif docA=1(int), but docB=1.1(float), docC=5.5(float) then we remember \"Float\"\n\t\tif docA=1(int), docB=1.1(float), docC=1.0000000001(double) then we remember \"Double\".\n\t\tIf docA=2017-12-12Z(date) and docB=42(int) then we remember \"String\"\n\t\n\t\n\tFor COMMIT commands, this processor (factory) would take all of the info it had accumulated from the ADDs recieved up to that point, and use them to exec Schema Field additions \u2013 using the same sort of \"java object class -> fieldType name\" mapping that AddSchemaFieldsUpdateProcessorFactory\n\n\n\nThe idea being that instead of having full \"schemaless\" mode enabled, there could be an /update/train-schema RequestHanlder configured to use this update.chain  Users could post a sampling of their docs to /update/train-schema then once they were don training send a /update/train-schema?commit=true command and the processor (factory) would add all the needed fields.\n\n\nBy no means should that idea be considered an end all be all solution / design.\n\nIt doesn't play very nicely with distributed updates (you'd either have to ensure all training data was sent to the same node where you send the \"commit\" or add special custom logic to ensure it all got forwarded to a special node) and there are probably a lot more sophisticated / smarter ways to do it ... it was just something i brainstormed one day as something that should be fairly easy to implement as a solr plugin leveraging most of the existing \"schemaless\" features of Solr \u2013 where \"Parse if possible\" update processors already do most of the heavy lifting.  \n\nPerhaps it can inspire a more robust solution? ",
            "author": "Hoss Man",
            "id": "comment-16314169"
        },
        {
            "date": "2018-01-06T07:26:30+0000",
            "content": "What i was thinking was something similar to the above implementation, just that instead of recording every value that ever appeared for a field, I would record all the distinct fieldTypes of the values that appeared for a each field. This will be the mapping of field -> supported types. This will need very small storage.  \n\nAnd instead of recording in memory, this data can be stored externally, (say zookeeper, or some temporary index inside solr.). I think it will get rid of the following problem.\n\nIt doesn't play very nicely with distributed updates (you'd either have to ensure all training data was sent to the same node where you send the \"commit\" or add special custom logic to ensure it all got forwarded to a special node) and there are probably a lot more sophisticated / smarter ways to do it\n ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16314428"
        },
        {
            "date": "2018-01-07T20:25:15+0000",
            "content": "This will be the mapping of field -> supported types. \n\nAt every point in time, every field will be mapped to only one possible (most granular) field type, isn't it?  ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16315462"
        },
        {
            "date": "2018-01-07T20:39:36+0000",
            "content": "The above approach can be optimised by replacing the Supported FieldTypes by  BitSets , \nAs shown in the following table:-\n\n\nWe can map every FieldType to a BitSet. For eg. String will be 10000 , Long will be 00100 and so on..  \n\n1. Now For every product, get the BitSet of the fieldType supported by each field \n2.  For every field, Find the BITWISE OR of the current BitSet with the BitSet value already recorded, and replace it.\n\nUse the following rule to decide the final FieldType that the field should have. \n\n\nSay if a field called price has values as following values: \nIn Product1 -> 12321  (Long, i.e. 00100)\nIn Product2 -> 77261.66  (Double, i.e. 01000) \nThe supported BitSet for price will have a final value of [ 00100 OR 01000 = 01100 ] , i.e. It should be assigned a Double. \n\nThe above rule can be extended to any number of types, just the number of bits will increase accordingly. \n\nUsing BitSets like above will decrease the storage space to 1 byte per field, will make the computation easier and faster, and will also remove the overhead of computing the trained schema separately, as they will be updated in-place with every Product.\n\nEvery api call to ask for Trained Schema,  will get the schema calculated till that point using the above rule.  ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16315465"
        },
        {
            "date": "2018-01-07T20:50:40+0000",
            "content": "Uploading patch SOLR-11741-temp.patch with above implementation within AddSchemaFieldsUpdateProcessorFactory itself.\nHave added a config param called mode.\n\nThe above URP will just Train The Schema when mode=train . By default mode=update, i.e. update the schema as usual.\n\nThis patch is temporary because it still needs test cases. Also, currently the state is being stored in-memory, in a map.\nHave to move that to the zookeeper. Will update that design in my next comments. ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16315469"
        },
        {
            "date": "2018-01-07T20:55:36+0000",
            "content": "@ Ishan Chattopadhyaya \nAt every point in time, every field will be mapped to only one possible (most granular) field type, isn't it?\n\nYes. ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16315472"
        },
        {
            "date": "2018-01-08T20:00:12+0000",
            "content": "What i suggested at one point (I don't remember where ... it may already be in a jira somewhere?) was an UpdateRequestProcessorFactory that could be configured instead of RunUpdateProcessorFactory in a chain...\n\nThe issue where Hoss mentioned this idea before was SOLR-6939. I linked it here for reference. ",
            "author": "Cassandra Targett",
            "id": "comment-16316913"
        },
        {
            "date": "2018-05-05T16:31:03+0000",
            "content": "Uploading the\u00a0updated patch, with following features:-\n\n\u00a0A new URP LearnSchemaUpdateRequestProcessorFactory: It simply learns from the incoming data to check what the current\u00a0 data type looks like. Based on, it updates the metadata about each field.\u00a0\n\n\u00a0\n\n\u00a0 ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16464829"
        },
        {
            "date": "2018-05-05T16:31:57+0000",
            "content": "SOLR-11741.patch ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16464830"
        },
        {
            "date": "2018-05-05T16:33:24+0000",
            "content": "SOLR-11741.patch\n\n\u00a0\n\nAdding updated patch.\u00a0 ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16464831"
        },
        {
            "date": "2018-05-05T17:16:14+0000",
            "content": "In order to use LearnSchemaUpdateRequestProcessorFactory, add\u00a0just it\u00a0to the URP chain.\u00a0\u00a0\n\nThe new API\u00a0details are :-\n\n\tGet A Training Id:**\n\n\n\nGET\n\n/<corename>/schema/train/start\n\nResponse:\n\n\u00a0\n\n{\"schemaTrainingId\" : \"<new schema training id>\"}\u00a0\n\n\n\u00a0\n\n2. Start Training:\n\nThis api is just like another update api, with\u00a0documents to be trained with.\u00a0 \u00a0\n\nPOST\n\n/<corename>/update?schemaTrainingId=<trainingId>\u00a0\n\n\u00a0\n\nBody: (Same as update request)\n[{}]\u00a0\n\n\n\u00a0\n\n3. Get the schema trained so far:-\n\nGET\n\n/schema/train/yield?schemaTrainingId=<currentTrainingId>\n\nResponse:\n\n\u00a0\n\n{\n\"schema\":{\n          \"add-field-type\": [\n             { \"name\":<fieldname1>, \"type\":<type>, \"multivalued\":<true/false>},\n             { \"name\":<fieldname2>, \"type\":<type>, \"multivalued\":<true/false>},\n...\n           ]\n}\n}\n\n\n**\n\n\u00a0\n\n\u00a0\n\n\u00a0 ",
            "author": "Abhishek Kumar Singh",
            "id": "comment-16464841"
        }
    ]
}