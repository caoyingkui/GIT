{
    "id": "LUCENE-8181",
    "title": "WordDelimiterTokenFilter does not generate all tokens appropriately",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "7.2.1",
        "status": "Open",
        "type": "Bug",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": []
    },
    "description": "When using word delimiter token filter some expected tokens are not generated.\n\nWhen I try to analyze the text \"ElasticSearch.TestProject\"\n\nI expect the tokens elastic, search, test, project, elasticsearch, testproject, elasticsearch.testproject to be generated since I have split_on_case_change, split_on_numerics on and using a whitespace tokenizer and have preserve original true.\n\nBut Actually I only see following tokens -\nelasticsearch.testproject, elastic, search, test, project",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16394729",
            "date": "2018-03-12T01:38:02+0000",
            "content": "I think this is the intended behavior of the filter at the moment. Having said that, it would be really useful for analyzing source code to have an option to generate those additional tokens.\n\nAnother interesting example to consider:\n\nFooBar.baz_qux\n\nIn this case, being able to produce the following tokens would be really useful:\n\nfoo, bar, baz, qux, foobar, baz_qux, foobar.baz_qux ",
            "author": "Robin Stocker"
        }
    ]
}