{
    "id": "LUCENE-2455",
    "title": "Some house cleaning in addIndexes*",
    "details": {
        "labels": "",
        "priority": "Trivial",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Today, the use of addIndexes and addIndexesNoOptimize is confusing - \nespecially on when to invoke each. Also, addIndexes calls optimize() in \nthe beginning, but only on the target index. It also includes the \nfollowing jdoc statement, which from how I understand the code, is \nwrong: After this completes, the index is optimized. \u2013 optimize() is \ncalled in the beginning and not in the end. \n\nOn the other hand, addIndexesNoOptimize does not call optimize(), and \nrelies on the MergeScheduler and MergePolicy to handle the merges. \n\nAfter a short discussion about that on the list (Thanks Mike for the \nclarifications!) I understand that there are really two core differences \nbetween the two: \n\n\taddIndexes supports IndexReader extensions\n\taddIndexesNoOptimize performs better\n\n\n\nThis issue proposes the following:\n\n\tClear up the documentation of each, spelling out the pros/cons of\n  calling them clearly in the javadocs.\n\tRename addIndexesNoOptimize to addIndexes\n\tRemove optimize() call from addIndexes(IndexReader...)\n\tDocument that clearly in both, w/ a recommendation to call optimize()\n  before on any of the Directories/Indexes if it's a concern. \n\n\n\nThat way, we maintain all the flexibility in the API - \naddIndexes(IndexReader...) allows for using IR extensions, \naddIndexes(Directory...) is considered more efficient, by allowing the \nmerges to happen concurrently (depending on MS) and also factors in the \nMP. So unless you have an IR extension, addDirectories is really the one \nyou should be using. And you have the freedom to call optimize() before \neach if you care about it, or don't if you don't care. Either way, \nincurring the cost of optimize() is entirely in the user's hands. \n\nBTW, addIndexes(IndexReader...) does not use neither the MergeScheduler \nnor MergePolicy, but rather call SegmentMerger directly. This might be \nanother place for improvement. I'll look into it, and if it's not too \ncomplicated, I may cover it by this issue as well. If you have any hints \nthat can give me a good head start on that, please don't be shy .",
    "attachments": {
        "LUCENE-2455_trunk.patch": "https://issues.apache.org/jira/secure/attachment/12445628/LUCENE-2455_trunk.patch",
        "LUCENE-2455_3x.patch": "https://issues.apache.org/jira/secure/attachment/12444493/LUCENE-2455_3x.patch",
        "index.31.nocfs.zip": "https://issues.apache.org/jira/secure/attachment/12445633/index.31.nocfs.zip",
        "index.31.cfs.zip": "https://issues.apache.org/jira/secure/attachment/12445632/index.31.cfs.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-05-11T10:04:54+0000",
            "content": "Remove optimize() call from addIndexes(IndexReader...)\n\nThis still makes me nervous.  Yeah it's bad that this method does optimize() now.  But if we remove it, it's bad that this method can attempt to do a ridiculously immense merge, since it [naively] just stuffs everything and and does one merge.  Ie, both at are bad.\n\nMaybe... we could do this: only merge the the incoming IndexReaders, appending a new segment to the end of the index?  Ie do no merging whatsoever of the current segments in the index.\n\nYes, this can result in \"unbalanced\" segments (ie, a huge segment appears after the long tail of level 0 segments), but, the merge policy can handle this \u2013 it'll work out whatever merges are then necessary to get this segment onto the level that roughly matches its size. \n\nSo unless you have an IR extension, addDirectories is really the one  you should be using.\n\nYou mean addIndexes(Directory..)?\n\n\nBTW, addIndexes(IndexReader...) does not use neither the MergeScheduler \nnor MergePolicy, but rather call SegmentMerger directly. This might be \nanother place for improvement. I'll look into it, and if it's not too \ncomplicated, I may cover it by this issue as well. If you have any hints \nthat can give me a good head start on that, please don't be shy .\n\nThis would be best of all   But it's tricky, because our MP/MS assume they are working w/ a SegmentInfo.  But, maybe it could somehow be made to work \u2013 eg IR does give us maxDoc, numDocs (so we can know del doc count).  But eg LogByteSizeMergePolicy goes and computes total byte size of the segment (via SegmentInfo) which we cannot do from an IR.\n ",
            "author": "Michael McCandless",
            "id": "comment-12866118"
        },
        {
            "date": "2010-05-11T18:18:05+0000",
            "content": "You mean addIndexes(Directory..)?\n\nYes, copy-paste error.\n\nMaybe... we could do this: only merge the the incoming IndexReaders, appending a new segment to the end of the index?\n\nI like it. IMO, that's what the method should do anyway, for better performance and service to the users. If I'm adding indexes, that doesn't mean I want a whole merge process to kick off. If I want that, I can call maybeMerge or optimize afterwards.\n\nBasically, what I would like to add (and I'm not sure it belongs to this issue) is a \"super fast\" addIndexes method, something like registerIndexes, which doesn't even traverses the posting lists, removes deleted docs etc. - simply registering the new segments in the Directory. If needed - do a bulk copy of the files and update segments*. Simple as that. Maybe it does fit in that issue, as part of the general \"house cleaning\"?\n\nI will look more closely into supporting MP + MS w/ addIndexes(readers). Can't promise anything as I learn the code as I go . ",
            "author": "Shai Erera",
            "id": "comment-12866245"
        },
        {
            "date": "2010-05-11T20:01:58+0000",
            "content": "I agree, addIndexes should be minimal in the work it does...\n\nBut bulk copy of the files isn't really possible for addIndexes(IR...) in general, since the readers can be arbitrary (eg FilterIndexReader). ",
            "author": "Michael McCandless",
            "id": "comment-12866302"
        },
        {
            "date": "2010-05-12T03:35:11+0000",
            "content": "Ok. But since addIndexes(IR) is for IR extensions only, I think the number of people tha will be limited by it is very low.\n\nBut, why wouldn't they be able to use the Directory... version of the method? Since it's a bulk copy, we don't need IR methods. Maybe just call dir.copyTo or something of that sort? The method will only be asked to copy files (in case they exist elsewhere). I was thinking of introducing just a Directoy version of such method.\n\nBasically, if you use NoMP and call addIndexesNoOptimize today, you get half of what I want, as only resolveExternals will be called. What I want is for the resolveExternals to be even faster, plain and shallow \"resolution\". ",
            "author": "Shai Erera",
            "id": "comment-12866444"
        },
        {
            "date": "2010-05-12T09:31:12+0000",
            "content": "But, why wouldn't they be able to use the Directory... version of the method?\n\nAdding indexes using FilterIndexReader is useful \u2013 eg look @ how the multi-pass index splitter tool works.\n\nWhat I want is for the resolveExternals to be even faster, plain and shallow \"resolution\".\n\nFor addIndexes(Directory), assuming the codecs are identical (the \"write\" codec equals the codec used to write the external segment), and assuming the doc stores of the external segment are private to it, I think we should be able to do a straight file-level copy, but renaming the segment in the process? ",
            "author": "Michael McCandless",
            "id": "comment-12866524"
        },
        {
            "date": "2010-05-12T10:57:19+0000",
            "content": "Adding indexes using FilterIndexReader is useful \n\nI'm not against that Mike. addIndexes should allow for both IndexReader and Directory. It's the registerIndexes (or whatever name we come up with) which should work with Directory only, and then, even if the app calls addIndexes with its own custom IR, it can still call registerIndexes w/ the Directory only, to do that fast copy/registration. Since no IR method will be involved in the process.\n\nSo let's not confuse the two - addIndexes will exist and work as they are today. registerIndexes will be a new one.\n\nassuming the codecs are identical (the \"write\" codec equals the codec used to write the external segment), and assuming the doc stores of the external segment are private to it\n\nRight. Thanks for pointing that out, as it will become an important NOTE in the documentation. This method (registerIndexes) is definitely for advanced users, that have to know exactly what's in the foreign indexes. For example, I need this because I'm building several indexes on several nodes and then I want to add them to a central/master one. I know they don't have deletions, and each is already optimized. Therefore traversing the posting lists (as fast as it would be) is completely unnecessary.\n\nbut renaming the segment in the process?\n\nSure! I think we should really 'register' them in the Directory, as if they are the newly flushed segments. I'm sure you have a general idea on how this can be done? Assuming through SegmentInfos or something? ",
            "author": "Shai Erera",
            "id": "comment-12866539"
        },
        {
            "date": "2010-05-14T13:22:04+0000",
            "content": "While changing addIndexes(reader), I've noticed it first obtains read lock and then calls startTransaction(true). In between it calls flush + optimize, which I've removed (as we no longer want to do that). When I ran the tests, TestIndexWriter.testAddIndexesWithThreads failed on the assert in startTransaction about numDocsInRam != 0. That's expected as I no longer call flush. The failure does not occur always.\n\nIn addIndexes(Dir) flush is called before startTransaction. But it makes sense to do it there, as the local segments are also merged. In the new addIndexes(reader) they won't and so I wonder if:\n\n\tI shouldn't call startTransaction at all, or\n\tI should, but also call flush before?\n\n ",
            "author": "Shai Erera",
            "id": "comment-12867498"
        },
        {
            "date": "2010-05-14T13:43:22+0000",
            "content": "Patch handles the changes for 3x:\n\n\taddIndexesNoOptimize deprecated - new addIndexes(Directory...) instead\n\tCHANGES updates\n\taddIndexes does not do optimize before\n\tTestAddIndexesNoOptimize renamed to TestAddIndexes\n\tChanged calls to addIndexesNoOpt to use addIndexes(Dir...) (except for backwards tests)\n\tChanged textual references to addIndexesNoOpt.\n\n\n\nYou should \"svn mv lucene/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java lucene/src/test/org/apache/lucene/index/TestAddIndexes.java\" before you apply the patch.\n\nThe patch is much smaller than it looks - it's the rename of TestAddIndexesNoOpt that takes a lot of space.\n\nAs I've mentioned before, I'm not sure about addIndexes calling startTransaction w/o flushing. Even though the tests pass, this seems wrong. So I'd appreciate a review. ",
            "author": "Shai Erera",
            "id": "comment-12867500"
        },
        {
            "date": "2010-05-14T15:10:38+0000",
            "content": "I think you should still call flush, and still start/commitTransaction \u2013 addIndexes is \"all or nothing\", which is why we have those transaction methods.  Ie, on exception, the rollbackTransaction puts the original segments back. ",
            "author": "Michael McCandless",
            "id": "comment-12867519"
        },
        {
            "date": "2010-05-14T15:13:47+0000",
            "content": "Patch looks good Shai!  Only a small typo in CHANGES (unles -> unless). ",
            "author": "Michael McCandless",
            "id": "comment-12867523"
        },
        {
            "date": "2010-05-14T16:39:22+0000",
            "content": "I see. I understand why it's called in addIndexes(Dir), because the local segments are also touched. But now in the Reader version, they aren't. So it looked odd to me that we flush whatever is in RAM. I think you said once that addIndexes should have done the merge outside, adding the new segment when it's done?\n\nBut if you think that flushing the RAM, even though its content is touched, is ok then I'll change it. ",
            "author": "Shai Erera",
            "id": "comment-12867554"
        },
        {
            "date": "2010-05-14T17:07:08+0000",
            "content": "I understand why it's called in addIndexes(Dir), because the local segments are also touched. But now in the Reader version, they aren't. So it looked odd to me that we flush whatever is in RAM. \n\nYeah maybe we should no longer flush (but still call start/commitTransaction).  I think there may've been a reason to flush first (besides that we were also merging local segments)... but I can't remember it.  If you comment out that assert (and the corresponding assert for deletions) do any tests fail?\n\nI think you said once that addIndexes should have done the merge outside, adding the new segment when it's done?\n\nYes, I would love to fix this \u2013 it'd mean we would not need the start/commit/rollbackTransaction code.\n\nIe, we play a dangerous game now, where addIndexes is allowed to muck with the in-memory SegmentInfos before it's complete.  It'd be better if all merging happened outside of its SegmentInfos, and only when addIndexes finished, it'd atomically commit to SegmentInfos.\n\nThis would then allow commit() to run immediately, not having to wait for any running addIndexes to finish first.  And we would not need to block add/updateDocument nor deleteDocuments while addIndexes is running.\n\nSo, actually, I think in addIndexes(IR...) you should not use the transaction logic at all?  Just do the merge externally & commit in the end?  (And try not flushing as well.). ",
            "author": "Michael McCandless",
            "id": "comment-12867567"
        },
        {
            "date": "2010-05-14T17:25:15+0000",
            "content": "Ha , I knew this will get more complicated and interesting ... So basically, it feels to me that if we'd have registerIndexes, we could in addIndexes merge outside IW and then call register?\n\nSo far, tests pass w/ startTransaction. But that test is multi-threaded so it may be a concurrency issue. I'll try to do the addIndexes outside IW and then commit the new segment. If that will be straightforward, then I think I'll understand better how to develop registerIndexes. ",
            "author": "Shai Erera",
            "id": "comment-12867572"
        },
        {
            "date": "2010-05-14T17:33:58+0000",
            "content": "Ha , I knew this will get more complicated and interesting ..\n\nIt always does!\n\nSo basically, it feels to me that if we'd have registerIndexes, we could in addIndexes merge outside IW and then call register?\n\nHmm but register will be an external API for copying over segments in a foreign directory, right?  (And segment must be renamed).\n\nVs these segments which will be in our directory already, with the right segment name, and just need to be committed to the segmentInfos?\n\nSo far, tests pass w/ startTransaction.\n\nYou mean w/o the flush? ",
            "author": "Michael McCandless",
            "id": "comment-12867579"
        },
        {
            "date": "2010-05-15T03:52:10+0000",
            "content": "You mean w/o the flush?\n\nYes. The start/commit transaction looks like that:\n\nstartTransaction(false);\n\ntry {\n  mergedName = newSegmentName();\n  merger = new SegmentMerger(this, mergedName, null);\n\n  for (IndexReader reader : readers)      // add new indexes\n    merger.add(reader);\n        \n  int docCount = merger.merge();                // merge 'em\n        \n  synchronized(this) {\n    info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                  -1, null, false, merger.hasProx());\n    setDiagnostics(info, \"addIndexes(IndexReader...)\");\n    segmentInfos.add(info);\n  }\n        \n  // Notify DocumentsWriter that the flushed count just increased\n  docWriter.updateFlushedDocCount(docCount);\n        \n  success = true;\n} finally {\n  if (!success) {\n    rollbackTransaction();\n  } else {\n    commitTransaction();\n  }\n}\n\n\n\n\n\tA new segment name is generated\n\tAll readers but the current one are merged\n\tThe new SI is added to the writer's SIs\n\tDocWriter's updateFlushedDocCount is updated\n\tThe transaction is committed or rolled back if there was an error.\n\n\n\nSo this looks like it already does the merge \"on the side\" and when it's done the new segment is registered? ",
            "author": "Shai Erera",
            "id": "comment-12867791"
        },
        {
            "date": "2010-05-15T04:12:51+0000",
            "content": "In fact, I've create newAddIndexes (just for the review) which works like that:\n\n  public void newAddIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n            -1, null, false, merger.hasProx());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n      }\n      \n      // Notify DocumentsWriter that the flushed count just increased\n      docWriter.updateFlushedDocCount(docCount);\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n\n\nQuestion: if we've just added the new SI to segmentInfos, why do we sync on this and check if it exists (when we create the compound file)? Is it because there could be a running merge which will merge it into a new segment before we reach that point?\n\nWhat do you think? Is that what you had in mind about merging on the side and committing in the end? ",
            "author": "Shai Erera",
            "id": "comment-12867793"
        },
        {
            "date": "2010-05-15T12:51:30+0000",
            "content": "Question: if we've just added the new SI to segmentInfos, why do we sync on this and check if it exists (when we create the compound file)? Is it because there could be a running merge which will merge it into a new segment before we reach that point?\n\nYes, exactly.\n\nWhat do you think? Is that what you had in mind about merging on the side and committing in the end?\n\nYup!  This looks great.... though I think you should move the docWriter.updateFlushedDocCount into the sync above it?  We didn't have to do this before because we blocked all add/updateDocument calls.\n\nAlso, you shouldn't call docWriter.resumeAllThreads (you didn't pause them).\n\nSo this change is a great step forward in concurrency of addIndexes(IndexReader...)! ",
            "author": "Michael McCandless",
            "id": "comment-12867870"
        },
        {
            "date": "2010-05-15T13:07:54+0000",
            "content": "Also, you shouldn't call docWriter.resumeAllThreads (you didn't pause them).\n\nOops, missed that . Thanks !\n\nI'll replace addIndexes w/ this code and run tests to check how it flies. ",
            "author": "Shai Erera",
            "id": "comment-12867871"
        },
        {
            "date": "2010-05-16T04:23:45+0000",
            "content": "I've looked into implementing registerIndexes, and that's the approach I'd like to take:\n\n\tFor each incoming Directory, read its SegmentInfos\n\tFor each SegmentInfo:\n\t\n\t\tGenerate a new segment name\n\t\tList its files\n\t\tCopy them from incoming Dir to local Dir, w/ the new segment name\n\t\tAdd such SI to the local IW segmentInfos\n\t\tUpdate DW docCount, like it's done in the addIndexes* methods.\n\t\n\t\n\n\n\nFew things:\n\n\tDoes that sound reasonable? Am I missing something?\n\tDirectory exposes a copyTo(Dir, Collection) which I thought to use. But the files are copied to the target Dir w/ their current name - while I need to copy them over w/ their new name.\n\t\n\t\tAdding rename to Dir feels wrong and dangerous to me\n\t\tAdding copyFile(Dir, String old, String new) seems ok\n\t\tAdding a variant of copyTo which accepts a Collection of the new names - the src and new should align. This also seems ok to me.\n\t\n\t\n\n\n\nI'd like to use Directory for the copy, since impls of Dir may do the copy very efficiently (i.e. FSDir vs. RAMDir) and I don't want to use IndexInput/Output for that.\n\nDo you know of another way I can achieve that? I only want to copy the actual segment files, w/o .gen and segments_N, so calling SI.files() seems ok?\n\nAnother question that popped into my head was about consistency of the incoming Dirs vs. the local one, w.r.t. to CFS files - should I worry about that? I think not because today one can create an index w/ CFS and then turn it off and some segments will be compound and others not? ",
            "author": "Shai Erera",
            "id": "comment-12867951"
        },
        {
            "date": "2010-05-16T18:03:52+0000",
            "content": "I've looked into implementing registerIndexes, and that's the approach I'd like to take:\n\nThis looks good.\n\nThough if the src segments share docStores, you can't do a simple\ncopy (I think you have to fallback to the resolveExternalSegments\napproach for such segments).\n\nDoes that sound reasonable? Am I missing something?\n\nI think this should work!\n\nIf the src segments are an older index rev, I think you are still OK.\nThey will just remain \"old\" on copy, and merge will eventually migrate\nthem forward.\n\nFor trunk... you should note in the jdocs that no codec conversion\ntakes place.  So the CodecProvider used in IW (and later used to read\nthis index) must know how to provide the codec used by the src\nsegments.\n\n\nDirectory exposes a copyTo(Dir, Collection) which I thought to use. But the files are copied to the target Dir w/ their current name - while I need to copy them over w/ their new name.\nAdding rename to Dir feels wrong and dangerous to me\nAdding copyFile(Dir, String old, String new) seems ok\nAdding a variant of copyTo which accepts a Collection of the new names - the src and new should align. This also seems ok to me.\nI'd like to use Directory for the copy, since impls of Dir may do the copy very efficiently (i.e. FSDir vs. RAMDir) and I don't want to use IndexInput/Output for that.\n\nDo you know of another way I can achieve that? I only want to copy the actual segment files, w/o .gen and segments_N, so calling SI.files() seems ok?\n\nSI.files() should be fine.\n\nI think falling back to copyFile is best?  Then copyTo could use it.\n\n\nAnother question that popped into my head was about consistency of the\nincoming Dirs vs. the local one, w.r.t. to CFS files - should I worry\nabout that? I think not because today one can create an index w/ CFS\nand then turn it off and some segments will be compound and others\nnot?\nI think that's fine, but we should advertise in the jdocs. ",
            "author": "Michael McCandless",
            "id": "comment-12868018"
        },
        {
            "date": "2010-05-16T19:55:28+0000",
            "content": "So it sounds like addIndexes should really be that registerIndexes. Specifically it should do a quick and dirty copy of all segments that don't share doc stores, and then resolveExternals those that do? Maybe we can get rid of those transactions and not block add/update/delete/commit/addIndexes attempts anymore?\n\nUsually, I expect this to be a win-win. In cases where you add Directories w/ plenty of segments that share doc stores it will be slower, because we won't utilize MP and MS. But this can be improved in the future as well by e.g. just taking care of the shared doc stores, and don't remove deleted document entries etc.\n\nBut .. it will prevent (in some cases) the use of PayloadProcessorProvider ... hmm. So it seems we do need a separate registerIndexes for the really quick & dirty addIndexes operation.\n\nBTW, Directory.copyTo* should be replaced w/ Directory.copy(Dir, File, File), for a couple of reasons:\n\n\tThere's no reason to believe the dest file should be named the same as the source file\n\tThe method is not entirely safe - only jdocs protect the user from doing really stupid thing such as overwriting the segments* files.\n\tI don't see a proper usecase for that method, other than copying a Directory into an empty one. At least, other use cases are very dangerous.\n\n\n\nInstead, the user should do this logic outside - call dir.listAll(), w/ and w/o FilenameFilter and copy the files of interest, and be allowed to rename them in the process. ",
            "author": "Shai Erera",
            "id": "comment-12868032"
        },
        {
            "date": "2010-05-17T06:51:28+0000",
            "content": "So ... after I slept over it, I don't think I can easily let go of the so-near victory \u2013 having addIndexes not blocking anything, done on the side, be as efficient as possible and give up a chance to do some serious code cleanup . So I'd like to propose the following, we can discuss them one by one, but they all aim at the same thing:\n\n\taddIndexes(Directory ...) will be a quick & dirty copy of segments. No deleted docs removal in the process, no merges.\n\t\n\t\tIt's clear how this can be done for segments that don't share doc stores.\n\t\tWhat isn't clear to me is why can't this work for segments that do share doc stores - can't we copy all of them at once, and add to segmentInfos when the segments + their doc store were copied? So, if I have 5 segments, w/ the following doc stores: 1-2, 3 and 4-5 (3 stores for 5 segments), can't I copy 1+2+store, 3, 4+5+store? Wouldn't that work? I'll give it a shot.\n\t\n\t\n\tPayloadProcessorProvider won't be used in the process. If you want, you can set it on the source writer, and call optimize(). Performance-wise it will be nearly identical - instead of processing the postings + payloads during addIndexes, you'll do that during optimize() (all segments will be processed anyway) and addIndexes will do a bulk IO copy, which on most modern machines is really cheap.\n\t\n\t\tFurther more, you will end up w/ just one segment, which means it can be copied at once for sure.\n\t\tIt will also simplify PPP \u2013 no need for DirPP anymore. PPP would get a Term and return a PP for that term, as it will always work on just one Directory.\n\t\tPeople can still use it with the target IW if they want, but not through addIndexes.\n\t\n\t\n\tApps can call maybeMerge or optimize() following addIndexes, if they want to.\n\n\n\nWhat remains is addIndexes(IndexReader...) and I'm not sure why this cannot be removed. In the back of my head I remember a discussion about it once, so I guess there is a good reason. But at least from what I see now, and armed w/ my use cases only, it seems like even if you use an extension of IndexReader you should still be able to do a bulk copy? Hmm ... unless if your extension assumes different postings structure or something like that, which the regular SegmentReader won't know about \u2013 then during addIndexes those postings are converted.\n\nBut, how common is this? And wouldn't it be better if such indexes are migrated beforehand? I mean, anyway after addIndexes those postings won't retain the custom IndexReader-assuming format? Or is there another reason?\n\nIf we go with that, then SegmentMerger can be simplified as well, assuming only SegmentReader?\n\nWhat do you think? ",
            "author": "Shai Erera",
            "id": "comment-12868103"
        },
        {
            "date": "2010-05-17T10:05:53+0000",
            "content": "What isn't clear to me is why can't this work for segments that do share doc stores \n\nYou are right!\n\nIf we copy over the doc stores, also renaming them, and fixup the incoming SegmentInto to reference the newly named one, this should work fine!\n\nPayloadProcessorProvider won't be used in the process.\n\nThis (and not needing DirPP anymore) is a great simplification.\n\nWhat remains is addIndexes(IndexReader...) and I'm not sure why this cannot be removed.\n\nI think we still need it... look at how multi-pass index splitter (contrib/misc) works. ",
            "author": "Michael McCandless",
            "id": "comment-12868158"
        },
        {
            "date": "2010-05-17T11:06:14+0000",
            "content": "look at how multi-pass index splitter (contrib/misc) works.\n\nI see ...\n\nI think this can be achieved by also deleting docs that fall outside the split range and calling optimize() / expungeDeletes()? So, you copy the index once (using one of the copying addIndexes methods), delete the docs that you don't care about and optimize/expunge. Then copy the index again and repeat the process, w/ a different range of ids. In fact, that's more or less what the method does - only it calls addIndexes, to copy into an existing/empty Directory.\n\nSo I think it can be changed to not call addIndexes? Only problem is that now the method adds the docs into the target Directory directly, while in the other solution it will need to create a Directory on the side w/ the range of requested IDs and then copy that one into the target Dir?\n\nBut I'm not sure that's worth it to have addIndexes(IndexReader...) and the relevant code in SM which handles the non-SegmentReader readers? Of course, this is just one scenario, but if that's our justifying case, then I'm not sure about how justifying it is. ",
            "author": "Shai Erera",
            "id": "comment-12868166"
        },
        {
            "date": "2010-05-17T11:33:53+0000",
            "content": "FYI, I'm working on a different version of IndexSplitter that uses the logic in SegmentMerger directly, without going through IW.addIndexes(FilterIndexReader).\n\nHowever, there are other applications for which this API is crucial, e.g. LUCENE-1812 or IndexSorter (in Nutch) - in short, any client apps that want to merge-in index data that does not correspond 1:1 to a Directory. For this reason I think the pair of IndexWriter.addIndexes(IndexReader...) and FilterIndexReader abstraction is extremely useful and that IndexWriter.addIndexes(Directory...) is not a sufficient replacement.\n\n(edit: unless there is a better user-level API based on the flex producers/consumers...) ",
            "author": "Andrzej Bialecki",
            "id": "comment-12868177"
        },
        {
            "date": "2010-05-17T12:11:05+0000",
            "content": "any client apps that want to merge-in index data that does not correspond 1:1 to a Directory\n\nI understand that. But when you call addIndexes w/ such IndexReaders, all they do is read the postings. Those are written down using the logic of the target IndexWriter. So I wonder how important is it for addIndexes to be in place, rather than say rewriting those indexes before they are added? I mean, all that addIndexes will do is call SegmentMerger and iterate on the readers and segments, merging the posting lists ...\n\nI don't object to that API. But, SM is used extensively, and is more of a main-path code, while addIndexes(IndexReader) is something only few out there use. Yet it affects everyone else who reads SM code, as well as those of us who are confused about which method to call (Reader or Directory) ... It almost feels like such operation - the relevant code from SM which handles non-SegmentReaders, should be extracted to a utility or something. But if I'm the only one that's bothered by it, then so be it. I can take care of the rest now, and resolve that one later. ",
            "author": "Shai Erera",
            "id": "comment-12868191"
        },
        {
            "date": "2010-05-17T12:26:07+0000",
            "content": "I understand - see the edited section in my comment: I think that extracting this non-SR code would be great. I would be in fact glad if there was an easier to control API that allows us to directly stream-process postings / stored / tvf-s / etc. in a way that results in a functioning index. Take for example LUCENE-1812 - the only reason it uses addIndexes(IndexReader) is that there was no easy way to modify postings in a way that would still result in a valid index, and there was no other API to add artificially created postings (i.e. not coming from a Directory) to a target index. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12868198"
        },
        {
            "date": "2010-05-17T13:23:13+0000",
            "content": "So can't the PrunningReader run on the side, converting the postings to whatever they're supposed to look like in the index they are about to be added to, and then call addIndexes w/ the Directory to do the bulk copy? I mean, instead of looking for a standalone tool, perhaps this can be solved on a case by case basis? Of course, if this can be made generic enough, then we can add it as a core utility, or IW method. ",
            "author": "Shai Erera",
            "id": "comment-12868215"
        },
        {
            "date": "2010-05-17T13:45:09+0000",
            "content": "So can't the PrunningReader run on the side, converting the postings to whatever they're supposed to look like \n\nErhm ... Currently the only way in the user API to write out existing postings (no matter how created) is to use IndexWriter.addIndexes(IndexReader). We can read postings just fine, using various *Enum classes that we can obtain from IndexReader, but there are no comparable high-level output methods  - Codecs and other flex classes are IMHO too low-level.\n\nAlso, with large indexes the amount of IO/CPU for writing out a Directory and reopening it is non-trivial - it's much more efficient to do this via streaming from the original, already open index.\n\nAlso, if we remove this method, then FilterIndexReader may as well go too, because it loses its utility. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12868221"
        },
        {
            "date": "2010-05-17T17:01:55+0000",
            "content": "Ok let's keep addIndexes(IndexReader) around. This means though that we cannot simplify the PPP API. We'll still need DirPP. ",
            "author": "Shai Erera",
            "id": "comment-12868277"
        },
        {
            "date": "2010-05-20T11:07:59+0000",
            "content": "I've started to implement addIndexes(Directory...) as agreed - copy files from the incoming ones into the local directory, while renaming them on the fly. This works really well with non-CFS segments: a new segment name is generated, the incoming files are renamed and this all flies smoothly (didn't test w/ deletions yet) - even shared doc stores work great.\n\nBut with CFS it doesn't work well because CFS writes the file names in the CFS file itself, and so even if the segment is renamed to _5 (for example), the names that are written in the file are _2.* (for example), and openInput fails to locate them. To overcome this, I propose we do the following:\n\n\n\tIntroduce on IndexFileNames a stripName method (3x and trunk) - will return the file name w/o the _x part.\n\tCFR ctor - strip names of read file names by calling IFN.stripName --> 3x only\n\tCFR.openInput - strip name by calling IFN.stripName --> 3x and trunk\n\tDocument that files should be created through IFN only --> 3x (for clarity) and trunk (otherwise may not be supported).\n\tNot save the name in CFS --> trunk only. Will remove the need to strip it off when it's read.\n\n\n\nThat will ensure that files are named following a certain convention which we can rely on in CFR. I don't think it's too hard to ask for. CFS itself already knows the name - it's named like it. So there's no value in storing the names of the files it holds.\n\nFor 3x it should work well b/c we don't allow for custom index files. For trunk we'll ask to go through IFN to name files - so one can create mycustom.file through IFN which will be called _x_mycustom.file.\n\nWhat do you think? ",
            "author": "Shai Erera",
            "id": "comment-12869563"
        },
        {
            "date": "2010-05-20T12:15:04+0000",
            "content": "Ahh sneaky that CFS still embeds the old segment's name (you're right).  The only other option would be to rewrite the CFS header, but then that's not easy to do a bulk copy on.  So I like you're approach!\n\nWe should  document in Codec.java that this (you must gen your filename via IFN's APIs) is a requirement of any custom files your codec wants to store in the index. ",
            "author": "Michael McCandless",
            "id": "comment-12869581"
        },
        {
            "date": "2010-05-22T06:14:23+0000",
            "content": "Patch includes the following:\n\n\taddIndexesNoOpt renamed to addIndexes2 for now, until we resolve the failing test (see below). I'll remove it and fix jdocs accordingly afterwards.\n\taddIndexes(Dir...) implements the simple file copy strategy.\n\tTests updated accordingly.\n\tSome minor changes to CompoundFileReader and IndexFileNames, as discussed before.\n\n\n\nAll tests pass except for TestIndexWriter.testAddIndexesWithThreads. I've debugged it, but cannot find the reason. addIndexes copies all segments, before it adds them to the writer's segmentInfos. Maybe I need to use start/commit transaction on that part only, to lock all ops? I don't see why, but maybe?\n\nAlso, TestAddIndexes.testWithPendingDeletes2() (and some others) fail before I added a call to flush to addIndexes. It seems that w/o it, existing buffered deleted docs are ignored after addIndexes returns (even when no multi-threading is involved). Can someone please confirm that?\n\nAlso, I cannot simplify PPP (to remove DirPP) because we kept addIndexes(Reader...). It's an annoyance if you don't call this method (need to return a DirPP for the target Dir always - if you want to use it), but maybe not so bad ... ",
            "author": "Shai Erera",
            "id": "comment-12870276"
        },
        {
            "date": "2010-05-23T11:16:57+0000",
            "content": "Attached patch includes:\n\n\tFixes a bug that caused some tests to fail.\n\tCFS is now versioned:\n\t\n\t\tCFW writes a version header, and CFR reads it\n\t\tCFW strips the segment name from the filename before writing it\n\t\tCFR back-supports pre-3.1 indexes depending on the existence/absence of the version header.\n\t\n\t\n\tTestBackwardsCompatibility now covers 3.0 indexes as well, and addIndexes* ops.\n\n\n\nThe beauty of all this is that IndexWriter no longer needs those transactions, and is now 500 lines of code + jdoc down !\n\nAfter we've iterated through this patch, I'll do the same changes on trunk. Backwards support should be much easier there, because we will provide an index migration tool anyway, and so CFW/CFR can always assume they're reading the latest version (at least in 4.0). CFW should probably use CodecUtils in trunk - it cannot be used in 3x because of how CFW works today - writing a VInt first, while CodecUtils assumes an Int. And I don't think it's healthy to do so much changes on 3x. ",
            "author": "Shai Erera",
            "id": "comment-12870391"
        },
        {
            "date": "2010-05-24T09:56:42+0000",
            "content": "Patch looks great!  So awesome seeing all the -'s in IW.java!!  Keep it up \n\nAnd it's great that you added 3.0 back compat case to\nTestBackwardsCompatibility...\n\nSome feedback:\n\n\n\tCan you change the code to read to a \"int firstInt\" instead of\n    version?  And make an explicit version (say \"PRE_VERSION\"), and\n    then check if version is PRE_VERSION in the code.  Ie, any tests\n    against version (eg version > 0) should be against constants\n    (version == PRE_VEFRSION) not against 0.\n\n\n\n\n\tCFW's comment should be \"make it 1 lower\" than the current one\n    right?  Ie, -2 is the next version?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12870548"
        },
        {
            "date": "2010-05-24T09:59:46+0000",
            "content": "Backwards support should be much easier there, because we will provide an index migration tool anyway, and so CFW/CFR can always assume they're reading the latest version (at least in 4.0).\n\nHmm I think we should do live migration for this (ie don't require a\nmigration tool to fix your index)?  This is trivial to do on the fly\nright (ie as you've done in 3.x).\n\nCFW should probably use CodecUtils in trunk - it cannot be used in 3x because of how CFW works today - writing a VInt first, while CodecUtils assumes an Int. And I don't think it's healthy to do so much changes on 3x.\n\nHmm yeah because of the live migration I think CodecUtils is not\nactually a fit here (trunk or 3x). ",
            "author": "Michael McCandless",
            "id": "comment-12870549"
        },
        {
            "date": "2010-05-24T16:31:47+0000",
            "content": "I'm not sure about the live migration, Mike. First because all the problems I've mentioned about CodecUtils in 3x will apply to live migration of 3.x indexes in 4.0 code. Second, if everyone who upgrades to 4.0 will need to run the migration tool, then why do any work in supporting online migration? What's the benefit? Do u think of a case where someone upgrades to 4.0 w/o migrating his indexes (unless he reindexes of course, in which case there is no problem)?\n\nI just think it's weird that we support online migration together w/ a migration tool. If we migrate the indexes w/ the tool to include the new format of CFS, then the online migration code won't ever run, right? And not doing this in the tool seems just a waste? I mean the user already migrates his indexes, so why incur the cost of an additional online migration? ",
            "author": "Shai Erera",
            "id": "comment-12870688"
        },
        {
            "date": "2010-05-24T17:27:54+0000",
            "content": "Sorry \u2013 for each major release, I think it'll be either live\nmigration or offline migration, but not both.\n\nSo far for 4.0 we haven't had a major enough structural change to the\nindex format, that'd make live migration too hard/risky, so, so far I\nthink we can offer live migration for 4.0.\n\nThe biggest change was flex, but it has the preflex codec to read (not\nwrite) the pre-4.0 format... so, so far I think we can still offer\nlive migration for 4.0? ",
            "author": "Michael McCandless",
            "id": "comment-12870724"
        },
        {
            "date": "2010-05-24T17:44:15+0000",
            "content": "Ahh, I knew we must be talking past each other . I assumed that the flex changes will go under the migration tool. If we have live migration for it, then I agree we should do live migration here.\n\nWith that behind us, did someone start an API migration guide? Since I remove addIndexesnoOptimize in favor of the new addIndexes, I wanted to document it somewhere. It's a tiny change, so perhaps it can go other the API Changes in CHANGES? ",
            "author": "Shai Erera",
            "id": "comment-12870735"
        },
        {
            "date": "2010-05-24T17:54:18+0000",
            "content": "With that behind us, did someone start an API migration guide?\n\nNot yet, I think?  Go for it! ",
            "author": "Michael McCandless",
            "id": "comment-12870743"
        },
        {
            "date": "2010-05-24T18:24:36+0000",
            "content": "I will document it in CHANGES under API section. I think the migration guide format will need its own discussion, and I don't want to block that issue. When we've agreed on the format (people have made few suggestions), I don't mind helping w/ porting everything relevant from changes to that guide. ",
            "author": "Shai Erera",
            "id": "comment-12870761"
        },
        {
            "date": "2010-05-25T06:41:01+0000",
            "content": "CFW's comment should be \"make it 1 lower\"\n\nRight ! I copied it from FieldsWriter where the versions are kept as positive ints. Will post a patch shortly. ",
            "author": "Shai Erera",
            "id": "comment-12871023"
        },
        {
            "date": "2010-05-25T06:43:07+0000",
            "content": "Patch applies Mike's comments. I think this is ready to go in. I'd like to commit to 3x before trunk, because there are lots of changes here. ",
            "author": "Shai Erera",
            "id": "comment-12871024"
        },
        {
            "date": "2010-05-25T09:52:36+0000",
            "content": "Could you fix \"firstInt' to have a very short life?\n\nMeaning, you read firstInt, and very quickly use that to assign to version & count, and no longer use it again.  Ie, all subsequent checks when loading should be against version, not firstInt...\n\nAlso, can you maybe rename CFW.PRE_VERSION -> CFW.FORMAT_PRE_VERSION?  (to match the other FORMAT_X).\n\nOtherwise looks great! ",
            "author": "Michael McCandless",
            "id": "comment-12871075"
        },
        {
            "date": "2010-05-25T12:17:03+0000",
            "content": "The only place I see firstInt is used perhaps unnecessarily is in the for-loop. So I've changed the code to look like this:\n\n\nint count, version;\nif (firstInt < CompoundFileWriter.FORMAT_PRE_VERSION) {\n  count = stream.readVInt();\n  version = firstInt;\n} else {\n  count = firstInt;\n  version = CompoundFileWriter.FORMAT_PRE_VERSION;\n}\n\n\n\nAnd then I query for version == CompoundFileWriter.FORMAT_PRE_VERSION inside the for-loop. Is that what you meant?\n\nThere is a check before all that ensuring that read firstInt does not indicate an index corruption \u2013 that should remain as-is, right? ",
            "author": "Shai Erera",
            "id": "comment-12871109"
        },
        {
            "date": "2010-05-25T16:14:07+0000",
            "content": "Update w/ comments. I plan to commit this either later today or tomorrow (and then port it to trunk). So if you haven't done so and want a last chance review - that's your chance. ",
            "author": "Shai Erera",
            "id": "comment-12871225"
        },
        {
            "date": "2010-05-25T16:30:28+0000",
            "content": "Patch looks good Shai!  Thanks. ",
            "author": "Michael McCandless",
            "id": "comment-12871233"
        },
        {
            "date": "2010-05-26T11:11:15+0000",
            "content": "Committed revision 948394 (3x).\n\nWill now port everything to trunk ",
            "author": "Shai Erera",
            "id": "comment-12871622"
        },
        {
            "date": "2010-05-26T11:30:53+0000",
            "content": "Hi Shai,\n\nI have seen this only lately. You added a 3.0 Index ZIP to the tests. This conflicts a little bit with trunk, where a 3.0 Index ZIP is already available. I would prefer to keep the \"older version\" ZIPs equal against each release, so it would be fine, if the trunk-added numerics backwards test could also be in 3.x branch. Would this be possible? You have to just merge the code.\n\nAlso it looks strange that the 3.0 backwards tests now contain also 3.0 index ZIPs, but there is no code for that??? Why have you added this to backwards? The 3.0 backwards tests should only modify this one addindexes test, but not add the zips. Maybe simple delete, they are not used.\n\nBy the way the 3.0 index zip file generation code is in the 3.0 branch, have you edited it there? You should commit the code there so one is able to regenerate the 3.0 ZIPs from the stable 3.0.x branch. ",
            "author": "Uwe Schindler",
            "id": "comment-12871630"
        },
        {
            "date": "2010-05-26T11:35:56+0000",
            "content": "I looked at the code, it simply tests trhat old indexes can be added. Maybe you just copy the trunk ZIPs for 3.0 to the 3x branch to keep them consistent. The files dont seem to be equal. ",
            "author": "Uwe Schindler",
            "id": "comment-12871631"
        },
        {
            "date": "2010-05-26T12:10:12+0000",
            "content": "Ok I added the indexes from trunk (didn't know they were there). I've changed CFS to write a version header in the file, so that's why I've added a 3.0 index - to make sure it can be read properly by 3.1. What I've added to TestBackwardsCompatibility are tests to ensure that addIndexes work on old indexes (which was good, because after the changes they weren't !).\n\nMaybe simple delete, they are not used.\n\nThe testAddIndexes were just added, and the 30 indexes are used. So I cannot delete them (see my comment above)\n\nBy the way the 3.0 index zip file generation code is in the 3.0 branch, have you edited it there?\n\nNope, it exists in TestBackwardsCompatibility as commented out, w/ instructions to uncomment. I've used that code. ",
            "author": "Shai Erera",
            "id": "comment-12871639"
        },
        {
            "date": "2010-05-26T12:11:37+0000",
            "content": "While porting the code to trunk, I've noticed that acquireRead/Write, releaseRead/Write, upgradeReadToWrite are either not called anymore, or called in relation to addIndexes. So I think these can be safely removed as well (from 3x and trunk)? ",
            "author": "Shai Erera",
            "id": "comment-12871641"
        },
        {
            "date": "2010-05-26T12:34:04+0000",
            "content": "So I think these can be safely removed as well (from 3x and trunk)?\n\nI think so! ",
            "author": "Michael McCandless",
            "id": "comment-12871718"
        },
        {
            "date": "2010-05-26T12:54:14+0000",
            "content": "Committed revision 948415 (copied the 3.0 indexes from trunk) and removed more unnecessary code from IndexWriter. ",
            "author": "Shai Erera",
            "id": "comment-12871728"
        },
        {
            "date": "2010-05-27T07:45:39+0000",
            "content": "Like the 3x patch, only this one changes IndexFileNames.segmentFileName to take another parameter for custom names, as well as update some jdocs to match flex (Codecs). I think this is ready to go in. ",
            "author": "Shai Erera",
            "id": "comment-12872127"
        },
        {
            "date": "2010-05-27T08:32:02+0000",
            "content": "Should we not add a 3.1 index (created with HEAD 3.x branch) to the TestBackwardsCompatibility? So we can verify that preflex indexes with new CFS header also work? ",
            "author": "Uwe Schindler",
            "id": "comment-12872140"
        },
        {
            "date": "2010-05-27T08:55:55+0000",
            "content": "Yes! I'll add them and update the tests. Will post a patch after I get more comments ",
            "author": "Shai Erera",
            "id": "comment-12872149"
        },
        {
            "date": "2010-05-27T09:13:28+0000",
            "content": "Hmm ... I've created the indexes using the 3x branch, copied them to trunk and updated TestBackwardsCompatibility to refer to them. All tests pass except for testNumericFields. It fails on both CFS and non-CFS indexes, and so I'm not sure it's related to this issue at all. The failure is this:\n\n\njunit.framework.AssertionFailedError: wrong number of hits expected:<1> but was:<0>\n\tat org.apache.lucene.index.TestBackwardsCompatibility.testNumericFields(TestBackwardsCompatibility.java:773)\n\n\n\nCan you try to run it on your checkout? ",
            "author": "Shai Erera",
            "id": "comment-12872153"
        },
        {
            "date": "2010-05-27T09:43:58+0000",
            "content": "For me it passes.\n\nAre you sure that you used the latest checkout of 3x. I added the index generation code yesterday after your last 3x commit. This code was not merged to 3x from trunk, as it was postflex added. This is done sice yesterday:\n\nAuthor: uschindler\nDate: Wed May 26 13:13:10 2010\nNew Revision: 948420\n\nURL: http://svn.apache.org/viewvc?rev=948420&view=rev\nLog:\nMerge the 3.0 index backwards tests from trunk (numeric field support). This makes it consistent across all branches.\n\nModified:\n    lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/index/   (props changed)\n    lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java   (contents, props changed)\n\n\n\nI attached the generated ZIP files from my 3x checkout. ",
            "author": "Uwe Schindler",
            "id": "comment-12872156"
        },
        {
            "date": "2010-05-27T11:23:07+0000",
            "content": "Yes - after I updated my checkout and re-create the indexes, the test passes. So I will include them with this patch as well. ",
            "author": "Shai Erera",
            "id": "comment-12872182"
        },
        {
            "date": "2010-05-27T15:37:06+0000",
            "content": "Committed revision 948861 (trunk). ",
            "author": "Shai Erera",
            "id": "comment-12872258"
        },
        {
            "date": "2011-03-30T15:50:31+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013510"
        }
    ]
}