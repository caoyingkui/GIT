{
    "id": "LUCENE-1161",
    "title": "Punctuation handling in StandardTokenizer (and WikipediaTokenizer)",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "It would be useful, in the StandardTokenizer, to be able to have more control over in-word punctuation is handled.  For instance, it is not always desirable to split on dashes or other punctuation.  In other cases, one may want to output the split tokens plus a collapsed version of the token that removes the punctuation.\n\nFor example, Solr's WordDelimiterFilter provides some nice capabilities here, but it can't do it's job when using the StandardTokenizer because the StandardTokenizer already makes the decision on how to handle it without giving the user any choice.\n\nI think, in JFlex, we can have a back-compatible way of letting users make decisions about punctuation that occurs inside of a token.  Such as e-bay or i-pod, thus allowing for matches on iPod and eBay.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2008-03-13T08:06:29+0000",
            "content": "I think WhitespaceTokenizer + WordDelimiterFilter + StandardFilter might work... ",
            "author": "Hiroaki Kawai",
            "id": "comment-12578179"
        },
        {
            "date": "2011-01-26T12:09:53+0000",
            "content": "The old StandardTokenizer behaviour was deprecated in Lucene 3.1 and replaced by a new one doing Unicode Standard Annex #29 segmentation. The deprecated code will not get any fixes anymore. ",
            "author": "Uwe Schindler",
            "id": "comment-12986975"
        }
    ]
}