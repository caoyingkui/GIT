{
    "id": "SOLR-4191",
    "title": "Exceptions thrown when /admin/mbeans or /admin/ping is accessed during update/commit",
    "details": {
        "affect_versions": "3.5,                                            4.1",
        "status": "Closed",
        "fix_versions": [
            "4.2",
            "6.0"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "I am getting the following exceptions in quick succession in the solr log when /admin/mbeans is accessed at the moment that an update/commit is happening:\n\nERROR - 2012-12-13 18:17:01.930; org.apache.solr.common.SolrException; null:org.eclipse.jetty.io.EofException\n\nERROR - 2012-12-13 18:17:01.982; org.apache.solr.common.SolrException; null:org.eclipse.jetty.io.EofException\n\nWARN  - 2012-12-13 18:17:01.984; org.eclipse.jetty.server.Response; Committed before 500 {msg=Broken pipe,trace=org.eclipse.jetty.io.EofException\n\nWARN  - 2012-12-13 18:17:01.984; org.eclipse.jetty.servlet.ServletHandler; /solr/s0live/admin/mbeans\njava.lang.IllegalStateException: Committed\n\nI will attach the full solr log.  Before SOLR-4135 was fixed, I got a lot of those exceptions, but these were far less common.  Now these appear to be the only thing I am getting in my logs, which log4j is logging at WARN.",
    "attachments": {
        "solr-2012-12-14[1].log": "https://issues.apache.org/jira/secure/attachment/12561021/solr-2012-12-14%5B1%5D.log"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shawn Heisey",
            "id": "comment-13532637",
            "date": "2012-12-14T20:25:30+0000",
            "content": "Solr log at WARN.\n\nThis is not instantly reproducible.  My uneducated guess is that maybe it only happens when a merge occurs and some of the index files disappear.  I used to get the exception from SOLR-4135 quite frequently, but the files it was complaining about on that issue are not related to merges.\n\nI fear that I may have to log at INFO and possibly turn on infostream, which is going to create an incredible amount of information.  The distributed index gets updated once a minute, and I also have a SolrJ servlet that accesses /admin/mbeans once a minute, often from multiple browsers.  Please let me know if that's the logical next step. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13532642",
            "date": "2012-12-14T20:34:02+0000",
            "content": "A closer look at the log has ruled out the idea that it's caused by merges.  The first two events are at 18:17 and 18:20.  An optimize would have happened at 18:00 on the only shard that gets new documents.  With my mergePolicy and updates happening once a minute, no merge could have happened at 18:17, and even if I'm wrong about that, one definitely didn't happen at 18:20.\n\n\n  <mergePolicy class=\"org.apache.lucene.index.TieredMergePolicy\">\n    <int name=\"maxMergeAtOnce\">35</int>\n    <int name=\"segmentsPerTier\">35</int>\n    <int name=\"maxMergeAtOnceExplicit\">105</int>\n  </mergePolicy>\n\n\n\nNext guess: it's purely a matter of timing. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13533097",
            "date": "2012-12-15T18:13:20+0000",
            "content": "I bumped logging to INFO and turned on infostream.  That's been running overnight, and not a single exception has been logged.  I'm guessing that the extra logging is throwing off the timing.  I had thought it might make the error MORE prevalent, but best guess is that now the updates take long enough that the stats requests are all done before the critical code section.  I'll try changing the servlet javascript code to wait until a couple of seconds past the minute, to try and make the collision more likely while logging is turned up. "
        },
        {
            "author": "Mark Smith",
            "id": "comment-13539538",
            "date": "2012-12-26T13:55:08+0000",
            "content": "This happening for me as well on Solr 4.0.  Interestingly enough, I'm not doing any writes.  I've got my solr app running for the past few weeks, and have not done any changes (no updates, no inserts, only selects).  Every few days I see this exception, so I simply kill my server and restart, and everything is fine.  Please let me know if anything I can do to help. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543119",
            "date": "2013-01-03T17:46:30+0000",
            "content": "I seem to be having much the same problem on my production Solr 3.5 servers.\n\nhttp://pastie.org/private/o2ekh0drs4syqb6t8re4w \n\nSome discussion on solr-user suggests that perhaps this exception happens when the client closes the connection before the response is delivered.  If that's the case, I think that Solr should log an INFO message saying that's what happened, and include the params of the request like it does when a request is successful, so that the problem can be investigated.\n\nAs for why requests are taking long enough to encounter client disconnects, I have theorized that it might be GC pauses.  I'll have to turn on GC logging to verify. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543146",
            "date": "2013-01-03T18:16:29+0000",
            "content": "Just got this in my Solr 3.5 log after turning on GC logging.  It does not correspond to anything in the GC log.  The jetty exception was one REALLY long line - I have broken it up for easier reading.\n\n\n2013-01-03 11:03:10.710:WARN::Committed before 500 null||org.mortbay.jetty.EofException|?at\n org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:791)|?at\n org.mortbay.jetty.AbstractGenerator$Output.flush(AbstractGenerator.java:569)|?at\n org.mortbay.jetty.HttpConnection$Output.flush(HttpConnection.java:1012)|?at\n sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:278)|?at\n sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)|?at\n java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)|?at\n org.apache.solr.common.util.FastWriter.flush(FastWriter.java:115)|?at\n org.apache.solr.servlet.SolrDispatchFilter.writeResponse(SolrDispatchFilter.java:344)|?at\n org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:265)|?at\n org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)|?at\n org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)|?at\n org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)|?at\n org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)|?at\n org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)|?at\n org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)|?at\n org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)|?at\n org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)|?at\n org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)|?at\n org.mortbay.jetty.Server.handle(Server.java:326)|?at\n org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)|?at\n org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)|?at\n org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)|?at\n org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)|?at\n org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)|?at\n org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)|?at\n org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)|\nCaused by: java.net.SocketException: Broken pipe|?at java.net.SocketOutputStream.socketWrite0(Native Method)|?at\n java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)|?at\n java.net.SocketOutputStream.write(SocketOutputStream.java:136)|?at\n org.mortbay.io.ByteArrayBuffer.writeTo(ByteArrayBuffer.java:368)|?at\n org.mortbay.io.bio.StreamEndPoint.flush(StreamEndPoint.java:129)|?at\n org.mortbay.io.bio.StreamEndPoint.flush(StreamEndPoint.java:161)|?at\n org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:714)|?\n... 25 more|\n2013-01-03 11:03:10.711:WARN::/solr/ncmain/admin/ping\njava.lang.IllegalStateException: Committed\n        at org.mortbay.jetty.Response.resetBuffer(Response.java:1023)\n        at org.mortbay.jetty.Response.sendError(Response.java:240)\n        at org.apache.solr.servlet.SolrDispatchFilter.sendError(SolrDispatchFilter.java:380)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:283)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\n\n\n\n2013-01-03T11:01:03.049-0700: 337.497: [GC 337.497: [ParNew: 1829516K->209664K(1887488K), 0.2861230 secs] 2884363K->1427860K(3984640K), 0.2862970 secs] [Times: user=1.37 sys=0.20, real=0.28 secs]\n2013-01-03T11:03:00.548-0700: 454.996: [GC 454.996: [ParNew: 1887488K->144139K(1887488K), 0.0926470 secs] 3105684K->1362378K(3984640K), 0.0931350 secs] [Times: user=0.48 sys=0.00, real=0.09 secs]\n2013-01-03T11:04:00.930-0700: 515.377: [GC 515.377: [ParNew: 1821908K->144450K(1887488K), 0.2553860 secs] 3040147K->1460243K(3984640K), 0.2555500 secs] [Times: user=0.84 sys=0.14, real=0.26 secs]\n2013-01-03T11:06:05.328-0700: 639.775: [GC 639.775: [ParNew: 1822274K->194859K(1887488K), 0.2209320 secs] 3138067K->1591694K(3984640K), 0.2210910 secs] [Times: user=0.79 sys=0.14, real=0.22 secs]\n2013-01-03T11:08:00.966-0700: 755.413: [GC 755.413: [ParNew: 1872683K->172132K(1887488K), 0.3693480 secs] 3269518K->1696739K(3984640K), 0.3695080 secs] [Times: user=1.13 sys=0.18, real=0.37 secs]\n2013-01-03T11:11:07.410-0700: 941.858: [GC 941.858: [ParNew: 1848944K->150685K(1887488K), 0.3002600 secs] 3373552K->1768856K(3984640K), 0.3004340 secs]\n\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543149",
            "date": "2013-01-03T18:19:04+0000",
            "content": "There was a DOWN event in haproxy for the 3.5 server that logged the above exceptions, corresponding to the same time:\n\n\nJan  3 11:03:09 localhost haproxy[12443]: Server idx/idxa1 is DOWN, reason: Layer7 timeout, check duration: 5001ms. 0\n active and 3 backup servers left. Running on backup. 8 sessions active, 0 requeued, 0 remaining in queue.\n\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543156",
            "date": "2013-01-03T18:26:01+0000",
            "content": "The exceptions above were from stderr - logged by jetty.  This is from solr's log:\n\n\nJan 3, 2013 11:03:10 AM org.apache.solr.common.SolrException log\nSEVERE: org.mortbay.jetty.EofException\n        at org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:791)\n        at org.mortbay.jetty.AbstractGenerator$Output.flush(AbstractGenerator.java:569)\n        at org.mortbay.jetty.HttpConnection$Output.flush(HttpConnection.java:1012)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:278)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)\n        at org.apache.solr.common.util.FastWriter.flush(FastWriter.java:115)\n        at org.apache.solr.servlet.SolrDispatchFilter.writeResponse(SolrDispatchFilter.java:344)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:265)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)\n        at org.mortbay.io.ByteArrayBuffer.writeTo(ByteArrayBuffer.java:368)\n        at org.mortbay.io.bio.StreamEndPoint.flush(StreamEndPoint.java:129)\n        at org.mortbay.io.bio.StreamEndPoint.flush(StreamEndPoint.java:161)\n        at org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:714)\n        ... 25 more\n\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543167",
            "date": "2013-01-03T18:33:28+0000",
            "content": "Here's the log from my SolrJ updating program for the same timeframe.  This runs on the same server that made the logs above.  As you can see, there was a commit going on during the ping request.  The time values reported are in milliseconds.  The numbers for delete and insert refer to the number of DB rows and Solr documents:\n\nINFO  - 2013-01-03 11:03:00.011; /---- chain.a: begin\nINFO  - 2013-01-03 11:03:00.011; /---- chain.b: begin\nINFO  - 2013-01-03 11:03:00.021; chain.b: new cycle, useBuild=false\nINFO  - 2013-01-03 11:03:00.021; chain.a: new cycle, useBuild=false\nINFO  - 2013-01-03 11:03:00.022; chain.b: oldmax=333052417 newmax=333052449\nINFO  - 2013-01-03 11:03:00.023; chain.a: oldmax=333052417 newmax=333052449\nINFO  - 2013-01-03 11:03:00.039; chain.b: Delete done, 6/6, time = 17\nINFO  - 2013-01-03 11:03:00.049; chain.a: Delete done, 6/6, time = 26\nINFO  - 2013-01-03 11:03:00.083; chain.b: Insert done, 26, time = 43\nINFO  - 2013-01-03 11:03:00.098; chain.a: Insert done, 26, time = 49\nINFO  - 2013-01-03 11:03:02.978; chain.b: Commit done, time = 2895\nINFO  - 2013-01-03 11:03:02.988; chain.b: Done, total time = 2967\nINFO  - 2013-01-03 11:03:02.988; ---- chain.b: end\nINFO  - 2013-01-03 11:03:18.235; chain.a: Commit done, time = 18137\nINFO  - 2013-01-03 11:03:18.243; chain.a: Done, total time = 18222\nINFO  - 2013-01-03 11:03:18.243; ---- chain.a: end "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13543187",
            "date": "2013-01-03T18:51:50+0000",
            "content": "I have been able to determine that all six of the deletes mentioned in the update log happened on my large \"cold\" shards, which is why the commit took so long - warming the filterCache is fairly slow on the large shards.\n\nI actually can't rule out GC yet - this is a distributed query going to seven shards on two servers.  I've now turned up the logging on the second server to INFO and have enabled GC logging there.  I'll now wait for the problem to happen again and then I can cross reference everything. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13544522",
            "date": "2013-01-05T02:15:22+0000",
            "content": "Can anyone tell me whether this indicates it's a GC problem?  I am interpreting the haproxy log to mean the ping query started at 16:37:34.  One thing I do not know - does the GC log timestamp indicate when the GC started or ended?  If it's the start time, then this is definitely a GC problem - specifically, the GC happening on the other shard server.\n\n\nhaproxy log:\nJan  4 16:37:39 localhost haproxy[12443]: Server idx/idxa1 is DOWN, reason: Layer7 timeout,\ncheck duration: 5001ms. 0 active and 3 backup servers left. Running on backup. 9 sessions\nactive, 0 requeued, 0 remaining in queue.\n\nJetty exception:\n2013-01-04 16:37:41.850:WARN::/solr/ncmain/admin/ping\njava.lang.IllegalStateException: Committed\n        at org.mortbay.jetty.Response.resetBuffer(Response.java:1023)\n        at org.mortbay.jetty.Response.sendError(Response.java:240)\n        at org.apache.solr.servlet.SolrDispatchFilter.sendError(SolrDispatchFilter.java:380)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:283)\n\nSolr exception:\nJan 4, 2013 4:37:41 PM org.apache.solr.common.SolrException log\nSEVERE: org.mortbay.jetty.EofException\n        at org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:791)\n        at org.mortbay.jetty.AbstractGenerator$Output.flush(AbstractGenerator.ja\n\nGC entry from server with exceptions:\n2013-01-04T16:36:48.930-0700: 101766.645: [GC 101766.645: [ParNew: 3774912K->350545K(3774912K),\n22.0665860 secs] 5837727K->2522917K(7969216K), 22.0667700 secs] [Times: user=0.00 sys=154.06,\nreal=22.07 secs]\n\nGC entry from other shard server:\n2013-01-04T16:37:32.694-0700: 101832.244: [GC 101832.244: [ParNew: 3722124K->419392K(3774912K),\n9.1200100 secs] 5800224K->2591046K(7969216K), 9.1201970 secs] [Times: user=10.46 sys=45.66,\nreal=9.12 secs]\n\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13544537",
            "date": "2013-01-05T02:46:23+0000",
            "content": "According to this (admitttedly outdated) document, GC timestamps are at the start of the collection, so this is apparently a problem caused by GC pauses.\n\nhttp://www.oracle.com/technetwork/java/gc-tuning-5-138395.html\n\nIf I add CMS incremental mode, would that help?\n\nRemaining problem for this issue is whether or not it would be possible to change the loud exception into a log indicating that the client connection has been closed - which might require cooperation from the servlet container. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13544756",
            "date": "2013-01-05T16:52:00+0000",
            "content": "Added -XX:+CMSIncrementalMode.  It didn't help.  I suppose that's not really surprising, since it's a ParNew collection.  The correlation between these DOWN events and a very long GC pause is too close to think it's anything else.  \n\n\nhaproxy log:\nJan  5 08:20:10 localhost haproxy[12443]: Server idx/idxa1 is DOWN, reason:\nLayer7 timeout, check duration: 5001ms. 0 active and 3 backup servers left.\nRunning on backup. 3 sessions active, 0 requeued, 0 remaining in queue.\n\njetty exception:\n2013-01-05 08:20:14.181:WARN::/solr/ncmain/admin/ping\n\ngc log:\n2013-01-05T08:20:01.418-0700: 28482.701: [GC 28482.701: [ParNew:\n1887482K->111621K(1887488K), 12.6964980 secs] 5062679K->3492727K(5430212K)\nicms_dc=15 , 12.6966660 secs] [Times: user=0.00 sys=18.41, real=12.69 secs]\n\n\n\nI'm going to try increasing NewRatio to 3.  My max heap is 8GB, NewRatio is currently 1, but watching the heap graph with jconsole suggests I can get by with a smaller young generation, which would reduce the cost of ParNew collections.  It'll result in more CMS collections, but perhaps with incremental mode on, that won't be a huge problem.\n\nOn my dev server with java7 and Solr 4.1, I have turned on the G1 collector's max pause target at 250 milliseconds. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13545174",
            "date": "2013-01-05T21:14:07+0000",
            "content": "I changed the NewRatio to 8 while I was doing a full-import on all shards.  I did have one DOWN event on the load balancer during that import, but it was not caused by a GC pause.  It appears that the ping request took 7.7 seconds, which is longer than the 5 second timeout on the load balancer check.  Best guess is that the I/O involved in indexing and merging made the ping take too long.\n\nAfter the full-import finished, I removed NewRatio and instead went with MaxNewSize.  Here is my memory config now:\n\n-Xms4096M -Xmx8192M -XX:MaxNewSize=256M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+CMSIncrementalMode -XX:MaxTenuringThreshold=3\n\nI will do another full-import later and then let it run uninterrupted for a few days to see what happens. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13545183",
            "date": "2013-01-05T21:39:13+0000",
            "content": "I will close this issue as not a problem.  I went into so much detail because others might run into something similar, this may help them fix it. "
        }
    ]
}