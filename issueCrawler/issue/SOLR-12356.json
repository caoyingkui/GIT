{
    "id": "SOLR-12356",
    "title": "Always auto-create \".system\" collection when in SolrCloud mode",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "The .system collection is currently used for blobs, and in SolrCloud mode it's also used for autoscaling history and as a metrics history store (SOLR-11779). It should be automatically created on Overseer start if it's missing.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-05-15T14:40:28+0000",
            "content": "The only issues I can see related to doing this automatically is how replicationFactor is decided, and how to prevent multiple replicas from ending up on the same node.  When somebody decides to run multiple nodes per host, ensuring proper replica placement is particularly important.\n\nThe first time an overseer starts in a cloud, there's probably only going to be one Solr node, so it won't be possible to create the collection with a replicationFactor higher than 1.  How do we handle that?  When nodes are added, how do we decide whether to automatically add a replica? My preference would be to do the add, but users may disagree, especially if they add a node in a location with limited bandwidth. ",
            "author": "Shawn Heisey",
            "id": "comment-16475926"
        },
        {
            "date": "2018-05-15T22:48:37+0000",
            "content": "Even if this is the default, can there be an opt out? (i.e. a solr.xml config option)? I imagine people upgrading from older versions of Solr that have built their own  metrics/scaling may not want this collection around. Features requiring .system collection then should have graceful failures in case of the collection not being there.  ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16476575"
        },
        {
            "date": "2018-05-15T23:21:15+0000",
            "content": "I would say we should create it only during first-use. if a POST request is made to the .system collection,  the collection is created, if not present.  Similarly, even if a GET request is performed on .system collection, we can auto create it  ",
            "author": "Noble Paul",
            "id": "comment-16476603"
        },
        {
            "date": "2018-05-16T11:01:40+0000",
            "content": "Does not the auto scaling framework allow us to create the collection with desired replicationFactor of 2, and some fancy rules will make sure that the 2nd replica is created whenever a new node is added? Can we similarly create a rule preventing that replica to land on a node sharing the same IP address as the leader, then all is good \u00a0I don't see any problems with Overseer making sure this collection exists, even if it is empty? ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16477233"
        },
        {
            "date": "2018-06-04T13:15:34+0000",
            "content": "If you use SolrJ to read/write from .system collection it fails because SolrJ does a check before even sending the request. So, auto-create can fail ",
            "author": "Noble Paul",
            "id": "comment-16500193"
        },
        {
            "date": "2018-06-19T08:00:41+0000",
            "content": "This collection is used not only for metrics, but also for autoscaling history and for blobs. IMHO the benefits of always having this collection outweigh the costs:\n\n\tI agree that components should fail gracefully if the collection is not present - this is simple to implement and it's a good practice to do this anyway.\n\tinitially when only 1 node is present the replicationFactor is 1 - however, we can use already existing mechanisms, such as AutoAddReplicasPlanAction to automatically increase RF to a \"safe\" default eg. 3, or a percentage of live nodes.\n\tautoscaling framework can automatically handle placing these replicas on different physical machines, eg using IP-based rules.\n\twe already have an API to manage cluster properties, including collection defaults - we can allow users to opt out of having this collection\n\n ",
            "author": "Andrzej Bialecki",
            "id": "comment-16516767"
        },
        {
            "date": "2018-06-19T08:53:57+0000",
            "content": "In other words, these are the main concerns and the answers:\n\n\twhen do we create the collection?\n\t\n\t\tit's too late to do this on first use because first updates will always fail\n\t\tCollectionsHandler can automatically create this collection when the first collection is created.\n\t\tusers can opt out of having this collection (by setting a property using cluster properties API), and components should handle the absence of this collection gracefully. In any case the cost of this collection should be negligible.\n\t\n\t\n\thow many replicas to create and what will be their placement?\n\t\n\t\tuse a configurable default RF eg. 3, unless there are fewer nodes in the cluster (then the limit is the number of nodes)\n\t\tuse AutoAddReplicasPlanAction to automatically increase the number of replicas to the desired RF as more nodes are added\n\t\tuse autoscaling preferences to automatically place replicas on different physical nodes\n\t\n\t\n\thow to control the size of the collection as new documents are being added?\n\t\n\t\twe could use the .scheduled_maintenance trigger with an action that periodically prunes the collection based either on index size or time-to-live criteria.\n\t\n\t\n\n ",
            "author": "Andrzej Bialecki",
            "id": "comment-16516817"
        },
        {
            "date": "2018-06-19T12:14:50+0000",
            "content": "use a configurable default RF eg. 3, unless there are fewer nodes in the cluster (then the limit is the number of nodes)\nThe default RF should be very high. By default, for a small cluster , there should be a replica of .system collection in every node. For a user of a small cluster , it's too much of a learning curve if his .system collection is not available \n\nuse AutoAddReplicasPlanAction to automatically increase the number of replicas to the desired RF as more nodes are added\n\nI don't think a user should know anything about the autoscaling thing to use .system collection. We should define the default behavior and it should all happen behind the scenes. If replicas are to be created, they should be created automatically.\n\nwe could use the .scheduled_maintenance trigger with an action that periodically prunes the collection based either on index size or time-to-live criteria.\n\nWe should have a sensible default for the maximum index size of the .system collection. Something like 100mb sounds OK. Once it crosses this threshold, the system should automaticlly do the pruning.The user should not be exposed to the autoscaling framework at all for this pruning to happen\n\nWhen I say we should not expose the user to the autoscaling framework, it's desirable to not even have an entry in the autoscaling.json. These can be implicit triggers which are automatically registered/unregistered based on presence/absence of certain cluster properties ",
            "author": "Noble Paul",
            "id": "comment-16516999"
        },
        {
            "date": "2018-06-20T06:06:10+0000",
            "content": "The default RF should be very high.\nWhy? we don't care about search performance of this collection that much, we only care about fault tolerance. Having a replica on every node seems an overkill - if your cluster is likely to lose N-1 nodes you're in a deep trouble anyway \n\nI don't think a user should know anything about the autoscaling thing to use .system collection.\nThey wouldn't have to - the config API can set up / re-config triggers as necessary.\n\nIf replicas are to be created, they should be created automatically.\nYes - and the existing mechanism of autoAddReplicas is already implemented and it works.\n\nWhen I say we should not expose the user to the autoscaling framework, it's desirable to not even have an entry in the autoscaling.json\nI disagree - actively hiding this from the users complicates the code and prevents them from understanding how it works. We can still provide all necessary simple setup and config options via the config API so that users are not exposed to the autoscaling API if they don't want to. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16517827"
        },
        {
            "date": "2018-06-23T02:11:38+0000",
            "content": "Why? we don't care about search performance of this collection that much, we only care about fault tolerance. Having a replica on every node seems an overkill - if your cluster is likely to lose N-1 nodes you're in a deep trouble anyway\u00a0\nImagine you have . an RF of 3 and you have 20 nodes. It's not uncommon to lose 3 nodes out of 20.\u00a0\nI disagree - actively hiding this from the users complicates the code and prevents them from understanding how it works.\u00a0\nThe\u00a0problem with system generated config coexisting with user created config is that it leads to\n\n\tconfig bloat which leads to poor readability.\u00a0\n\tLegacy configuration living in the cluster the user doesn't know how to upgrade when something changes in the framework\n\n\n\nOTOH, if we are maintaining that configuration hidden from users , we eliminate this problem altogether. Another place where we apply these principles is the implicitly registered responseWriters, requesthandlers, functions etc . We could have left them in the solrconfig.xml\u00a0and it would have caused the same problems as I mentioned above. In short, I'm not very happy to see the autoAddReplicas creating a huge blob of config in autoscaling.json which the user is left to manage  ",
            "author": "Noble Paul",
            "id": "comment-16520946"
        },
        {
            "date": "2018-07-04T20:40:17+0000",
            "content": "I don't like the idea of things being hidden (in general). And I think it should be possible to easily replace whatever .system exists with one configured to the user's needs (probably\u00a0when first building or when upgrading the cluster) also WRT sizing the use case that motivated\u00a0SOLR-8349\u00a0which and relies on the blob store too\u00a0was a postal code geo lookup table for UK+US that was around 1GB (and at the time the problem that was solved was that it was getting replicated in memory 40 times for 40 cores). ",
            "author": "Gus Heck",
            "id": "comment-16533050"
        }
    ]
}