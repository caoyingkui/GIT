{
    "id": "SOLR-4509",
    "title": "Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client.",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "7.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "By disabling the Apache HTTP Client stale check I've witnessed a 2-4x increase in throughput and reduction of over 100ms.  This patch was made in the context of a project I'm leading, called Yokozuna, which relies on distributed search.\n\nHere's the patch on Yokozuna: https://github.com/rzezeski/yokozuna/pull/26\n\nHere's a write-up I did on my findings: http://www.zinascii.com/2013/solr-distributed-search-and-the-stale-check.html\n\nI'm happy to answer any questions or make changes to the patch to make it acceptable.\n\nReviewBoard: https://reviews.apache.org/r/28393/",
    "attachments": {
        "baremetal-stale-nostale-med-latency.dat": "https://issues.apache.org/jira/secure/attachment/12571689/baremetal-stale-nostale-med-latency.dat",
        "0001-SOLR-4509-Move-to-non-deprecated-HttpClient-impl-cla.patch": "https://issues.apache.org/jira/secure/attachment/12795045/0001-SOLR-4509-Move-to-non-deprecated-HttpClient-impl-cla.patch",
        "baremetal-stale-nostale-throughput.dat": "https://issues.apache.org/jira/secure/attachment/12571691/baremetal-stale-nostale-throughput.dat",
        "IsStaleTime.java": "https://issues.apache.org/jira/secure/attachment/12571418/IsStaleTime.java",
        "baremetal-stale-nostale-med-latency.svg": "https://issues.apache.org/jira/secure/attachment/12571690/baremetal-stale-nostale-med-latency.svg",
        "baremetal-stale-nostale-throughput.svg": "https://issues.apache.org/jira/secure/attachment/12571692/baremetal-stale-nostale-throughput.svg",
        "SOLR-4509.patch": "https://issues.apache.org/jira/secure/attachment/12571231/SOLR-4509.patch",
        "SOLR-4509-4_4_0.patch": "https://issues.apache.org/jira/secure/attachment/12606817/SOLR-4509-4_4_0.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan Zezeski",
            "id": "comment-13588561",
            "date": "2013-02-27T17:56:44+0000",
            "content": "I generated the patch from my clone of the git mirror.  If this patch is unacceptable in anyway I'm willing to redo.  Just thought I'd take the easy route first. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13588597",
            "date": "2013-02-27T18:34:01+0000",
            "content": "Interested user here, not anyone linked to Solr or Lucene.\n\nWould the 100ms latency you have described here show up in QTime values or be purely client-side?  I ask because the median (not average) QTime value I see in disributed searches (seven shards on two servers) on Solr 3.5.0 is about 10 milliseconds, and even faster on 4.2-SNAPSHOT ... and that involves the stale check both on the client side and the seven shards.  The client is SolrJ 4.1, with max retries (1) and connection timeout (5000ms) being the only low-level parameters that are set.  SolrCloud is not involved here.\n\nThe idea of a sweeper thread makes me nervous, given the project's general reaction to the background threads created by an intermediate SOLR-1972 patch.  Also, with 5000 milliseconds between executions of the sweeper thread and an assumption of a server timeout of 50 milliseconds, will it be able to effectively avoid problems in all environments?  If these are not actual worries, then I can be quiet. "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13588657",
            "date": "2013-02-27T19:20:35+0000",
            "content": "> Would the 100ms latency you have described here show up in QTime values or be purely client-side?\n\nSo my latency measurements include end-to-end from client->yokozuna->solr coordinator->shards->solr coordinator->yokozuna->client.  I didn't track the QTimes, but in benchmarks I ran months ago I saw the same results whether I went through Yokozuna or hit Solr directly with the distributed search.  I could probably re-run to report QTime changes.\n\n> I ask because the median (not average)...\n\nI'm also reporting median latency, not mean.  In my first link you can also find graphs with 95th, 99th, 99.9 percentiles.\n\n> ...an assumption of a server timeout of 50 milliseconds\n\nThe assumption is 50s, but yes, it is an assumption.  A better patch would make the idle timeout configurable as well as the period.\n\n> If these are not actual worries, then I can be quiet.\n\nNo, these are great points.  This patch, as it sits now, is for a specific context.  I did what I needed to do to improve my specific situation.  Honestly, I would like to see another person run this patch and verify at least some amount of speed-up.  Otherwise, it might best stay downstream. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13588743",
            "date": "2013-02-27T20:58:47+0000",
            "content": "Ryan, this is very interesting indeed! I've read your patch and it looks good, i'll be testing this in the next few days.\n\nI'm not very sure enabling TCP_NODELAY (e.g. disable Nagle's algorithm) will help a lot. In theory it could but we already did performance tests a few months ago on two clusters where one had it enabled and the other didn't. I could not measure a significant difference, my tests may have been bad of course \n "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13588787",
            "date": "2013-02-27T21:30:16+0000",
            "content": "> ...i'll be testing this in the next few days.\n\nGreat, I would love to see these results reproduced by someone else.\n\n> I'm not very sure enabling TCP_NODELAY (e.g. disable Nagle's algorithm) will help a lot\n\nI agree, it probably doesn't matter but I disabled it anyways given that nagle is really meant for things like telnet and such.  I figured it couldn't hurt to disable.\n "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13589557",
            "date": "2013-02-28T14:27:14+0000",
            "content": "Hi Ryan,\n\nUntil now i've not seen real significant latency improvements when performing concurrent stress tests. The response time still hovers between 70ms and 100ms with and without the patch using today's trunk. I did see some disturbing exceptions, it looks like sometimes connections are reused that are already dead, returning a connection refused exception. Sometimes shards are also reported dead yielding the dreaded no servers hosting shard error.\n\nHere are some exceptions:\n\n\nCaused by: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[...hosts here...]\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:333)\n        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:171)\n        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:135)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        ... 3 more\nCaused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://host/solr/shard_d\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:416)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:264)\n        ... 10 more\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\n        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:95)\n        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:62)\n        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:254)\n        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:289)\n        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:252)\n        at org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:191)\n        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:300)\n        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:127)\n        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:717)\n        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:522)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:353)\n        ... 12 more\n\n\n\n\n2013-02-28 13:00:15,666 WARN [solr.cloud.RecoveryStrategy] - [main-EventThread] - : Stopping recovery for zkNodeName=178.21.118.192:8080_solr_openindex_dcore=openindex_d\n2013-02-28 13:00:17,199 WARN [solr.update.PeerSync] - [main-EventThread] - : PeerSync: core=shard_d url=http://host/solr  couldn't connect to http://host/shard/, counting as success\n2013-02-28 13:00:17,201 ERROR [solr.cloud.SyncStrategy] - [main-EventThread] - : Sync request error: org.apache.solr.client.solrj.SolrServerException: Server refused connection at: http://host/shard\n2013-02-28 13:00:17,205 ERROR [solr.cloud.SyncStrategy] - [recoveryCmdExecutor-17-thread-1] - : http://host/shard/: Could not tell a replica to recover:org.apache.solr.\nclient.solrj.SolrServerException: Server refused connection at: http://host\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:409)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)\n        at org.apache.solr.cloud.SyncStrategy$1.run(SyncStrategy.java:298)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: org.apache.http.conn.HttpHostConnectException: Connection to http://host refused\n        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:190)\n        at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:294)\n        at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:645)\n        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:480)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:353)\n        ... 5 more\nCaused by: java.net.ConnectException: Connection refused\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\n        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)\n        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)\n        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)\n        at java.net.Socket.connect(Socket.java:529)\n        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:127)\n        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:180)\n        ... 12 more\n\n\n\nI'm not sure yet why these occur, maybe it's my Tomcat settings that do not match Jetty's defaults. "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13589601",
            "date": "2013-02-28T15:21:41+0000",
            "content": "Markus,\n\n> Until now i've not seen real significant latency improvements when performing concurrent stress tests. The response time still hovers between 70ms and 100ms with and without the patch using today's trunk.\n\nFair enough, it could be that my particular setup had something to do with it.  I should verify on a 2nd set of hardware.\n\nWhat is this \"concurrent stress test\" you are running?  Is it something public I could run as well?  How many shards is each query hitting and is each shard on its own physical machine?\n\nAlso, I'm going to attach a BTrace script.  If you could run this script it would be really helpful as it would tell us how long the stale check is taking in your environment.  I included instructions on running the script in the comments of the source.\n\n> I'm not sure yet why these occur, maybe it's my Tomcat settings that do not match Jetty's defaults.\n\nMy patch currently assumes a 50s idle timeout (or larger) on the server.  Any smaller and you might see socket reset errors.\n\nI'm not sure why you are seeing the errors you mentioned.  Your first trace doesn't match up with the 4.1.3 HTTP Client code.  Has the client version been updated for the latest?\n\n "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13589602",
            "date": "2013-02-28T15:22:32+0000",
            "content": "BTrace script to dynamically time the stale check "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13591128",
            "date": "2013-03-02T00:05:01+0000",
            "content": "I have some new results from a different cluster.  The short story is\nthat I still see improvement from removing the stale check, just not\nas dramatic as on my SmartOS cluster.  Throughput improved by 108-120%\nand there was a 0-5ms delta in latency.\n\nWhat I take from this is that the benefits of removing the stale check\nwill vary depending on # of nodes, hardware, query and load.  In\ntheory removing the stale check should never hurt as removing blocking\nsyscalls should only help.  But I totally understand if about being\ncautious with a change like this.  Personally I'd like to see at least\none other person confirm a non-negligible difference before I bother to\nmake this patch more acceptable.  Best to let this ticket stir a while\nI suppose.\n\n\n\t\n\t\n\t\tCluster Specs\n\t\n\t\n\n\n\nAdd nodes are running on baremetalcloud so this time they are truly\ndifferent physical machines with no virtualization involved.\n\n\n\t8 nodes/shards\n\t1 x 2.66GHz Woodcrest E5150 (2 cores)\n\t2GB DDR2-667\n\t73GB SAS 10k RPM\n\tUbuntu 12.04\n\tOracle JDK: Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)\n\t512MB max heap\n\tExample schema\n\n\n\n\n\t\n\t\n\t\tBench Runner\n\t\n\t\n\n\n\n\n\t1 node\n\t2 x 2.66GHz Woodcrest E5150\n\t8GB DDR2-667\n\tUsing Basho Bench as load gen\n\n\n\n\n\t\n\t\n\t\tQueries\n\t\n\t\n\n\n\n\nAll queries hit all shards.  All queries were single term queries\nexcept for alpha which is conjunction.  The numbers listed are the\nnumber of documents matching each term query.\n\n\n\talpha: 100K, 100K, 0\n\tlima: 1\n\tmike: 10\n\tnovember: 100\n\toscar: 1K\n\tpapa: 10K\n\tquebec: 100K\n\n\n\nAttached is the aggregate data (.dat) and corresponding plots (.svg)\nof that data.  The data was aggregated from raw data collected by\nBasho Bench (and this raw data is actually the aggregate of all events\nat 10s intervals).  E.g. the median latency is actually the mean of\nthe median latencies calculated against all events in a given 10s\nperiod.  What I'm saying is, it's rollup or a rollup so while there\nare 2 decimals of precision those numbers are not actually that\nprecise.  But this should be good for ballpark figures (if you're a\nstats geek please let me know if I'm committing a sin here).\n\nThere is a big delta in latency for the mike benchmark but I'm\nchalking that up to an anomaly for the time being. "
        },
        {
            "author": "Raintung Li",
            "id": "comment-13591942",
            "date": "2013-03-04T02:41:54+0000",
            "content": "Markus,\nThe tomcat default work threads are only 150, connection refuse is cased by Socket listener queue full, you can expand the work threads in the tomcat configuration. "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13786290",
            "date": "2013-10-04T16:24:36+0000",
            "content": "I recently updated Yokozuna (1) to use Solr 4.4.0.  After running a\nquery benchmark I noticed that throughput had dropped to 44% of the\nbaseline.  After some head scratching I realized that my distributed\nsearch patch had not applied successfully.  Sure enough, after I\nupdated the patch for 4.4.0 throughput returned to 100%+ of baseline.\nBelow is a table showing results of query benchmark for 4.3.0, 4.4.0\nand 4.4.0 without this patch.  The throughput drops to less than half\nof Solr 4.4.0 with the patch and the latency more than doubles.\n\n\n\n\nMeasurement     \nSolr 4.3.0       \nSolr 4.4.0       \nSolr 4.4.0 w/o Patch  \n\n\n----------------\n-----------------\n-----------------\n----------------------\n\n\nMean Throughput \n1512 ops/s       \n1525 ops/s       \n670 ops/s (44%)       \n\n\nMedian Latency  \n22.0ms           \n21.6ms           \n46.2ms (2.1x)         \n\n\n95th Latency    \n29.8ms           \n29.4ms           \n76.8ms (2.6x)         \n\n\n99th Latency    \n35.3ms           \n34.6ms           \n86.2ms (2.5x)         \n\n\n\n\n\nThese results are against a 4-node cluster all hosted on 1 physical\nmachine.  Manual distributed search is used for querying, there is no\nuse of SolrCloud.  There are only 1 million small text documents\nstored.  The query matches only 1 of these documents.  The query\nresults and filter caches are enabled and should have a high hit\nratio.  The point is to make the queries inexpensive as possible to\nsee what other overhead might occur.  There may very well be scenarios\nwhere this patch makes little to no difference.  But in this case it\nseems to make a big one.\n\nThis update is not to prove that my patch makes a significant\ndifference in all cases.  Rather, I accidentally ran this benchmark\nand was surprised at the difference I saw.  I wanted to ping this\nissue in hopes that others might try the patch to see if it helps.\n\nHere is the corresponding ticket on the Yokozuna repo: https://github.com/basho/yokozuna/pull/197\n\n1: Yokozuna is a project with integrates Solr with the Riak database.  https://github.com/basho/yokozuna "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-13786294",
            "date": "2013-10-04T16:26:00+0000",
            "content": "Version of the patch that applies to 4.4.0. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14221753",
            "date": "2014-11-22T02:15:53+0000",
            "content": "My biggest hesitation on this patch is:\n\n\n+    // NOTE: The sweeper task is assuming hard-coded Jetty max-idle of 50s.\n+    final Runnable sweeper = new Runnable() {\n+            public void run() {\n+                mgr.closeIdleConnections(40, TimeUnit.SECONDS);\n+            }\n+        };\n+    final ScheduledExecutorService stp = Executors.newScheduledThreadPool(1);\n+    stp.scheduleWithFixedDelay(sweeper, 5, 5, TimeUnit.SECONDS);\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14222276",
            "date": "2014-11-23T01:38:23+0000",
            "content": "Great write up Ryan! I looked this over once or twice when you first filed it, but I just never remembered it when I had the time to dig into it. Sorry this didn't get more attention sooner, but sometimes a good issue just can't find a committer for some time for one reason or another. Hrishikesh Gadre has also been looking into connection reset errors during distributed updates (an occasionally reported issue) and tracked it down to this stale check being imperfect. It really seems wise to remove it at this point.\n\nI'll put up a draft patch for comment in a bit that builds on this one. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14222392",
            "date": "2014-11-23T15:34:43+0000",
            "content": "FYI: Some info around the stale check mentioning that it's not 100% reliable and suggestion of this workaround http://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html : 2.5. Connection eviction policy "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14222395",
            "date": "2014-11-23T15:41:03+0000",
            "content": "Ryan Zezeski, what drove your decision to make the sweeper thread interval 5 seconds (considering what the default should be)? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14222486",
            "date": "2014-11-23T20:13:25+0000",
            "content": "This patch removes some nocommits around config and cleans a few things up a bit. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14222495",
            "date": "2014-11-23T20:54:52+0000",
            "content": "I barely remember making my earlier comment on this issue.  A year and a half ago I was still running 3.5, apparently!  I'm pretty sure that my distributed search still doesn't have 100 milliseconds of latency to lose, but hopefully there will be some improvement when this change is made. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14222515",
            "date": "2014-11-23T22:00:13+0000",
            "content": "My main concern is the flakey stale check in httpclient. I'll happily accept any perf improvements. HttpClient docs also suggest a 10-30ms per req cost if I remember right. The. It seems the blocking call can prob make it even worse under concurrency.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14223056",
            "date": "2014-11-24T15:38:20+0000",
            "content": "New patch. Some more cleanup. Also added closeExpiredConnections to the sweeper thread as the documentation recommends. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14223064",
            "date": "2014-11-24T15:43:53+0000",
            "content": "I've added the latest patch to review board: https://reviews.apache.org/r/28393/ "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14223855",
            "date": "2014-11-25T00:47:10+0000",
            "content": "Hrishikesh Gadre has also been looking into connection reset errors during distributed updates (an occasionally reported issue) and tracked it down to this stale check being imperfect. It really seems wise to remove it at this point.\n\nCan you or Hrishikesh Gadre explain this more?  From my reading of it, there's nothing you can do to be 100% correct, i.e. the server closes the connection for whatever reason, you make a request on the client before anything is returned, and you get back a RST.  I could be wrong about that \u2013 the few tcp state machine diagrams I looked up don't cover that case.  I guess I'm trying to understand the purpose of this patch \u2013 is it to reduce latency?  to reduce the number of connection resets?  both? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14224444",
            "date": "2014-11-25T11:56:56+0000",
            "content": "It reduces latency and removes the stale check that can easily race with the server. Did you read the linked original write up and http client docs around stale check? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14224494",
            "date": "2014-11-25T12:56:13+0000",
            "content": "To sum up the very high level: currently, each connection thread does a stale check, and of course, even it if it passes the server could close the connection a millisecond later. It's better to remove this stale check that every thread has to deal with and have a dedicated thread that handles stale connections itself, and in a way that they are closed on the client before they are closed on the server. We want the client to handle connection lifecycle. Nothing promises you that you will never get a connection reset - but rather, it should go from fairly easy to randomly happen for apparently no good reason to almost never happening (and perhaps it is a better reason than bad luck timing if it does - a reason you might be able to address). "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-14224815",
            "date": "2014-11-25T16:57:01+0000",
            "content": "Would be good to remove that stale check also in solrj. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14225048",
            "date": "2014-11-25T19:15:40+0000",
            "content": "Did you read the linked original write up and http client docs around stale check?\n\nYes, I was responding specifically to the \"tracked it down to this stale check being imperfect\" \u2013 given that any check is imperfect, I wanted to make sure I understood the motivation.  Your latest comment addresses that. "
        },
        {
            "author": "Ryan Zezeski",
            "id": "comment-14225183",
            "date": "2014-11-25T20:51:14+0000",
            "content": "> Ryan Zezeski, what drove your decision to make the sweeper thread interval 5 seconds (considering what the default should be)?\n\nIt had to be shorter than the server connection timeout; 5 seconds was\nless than the 60 second server timeout I had.\n\nIt seems you are running with this patch nicely. Feel free to modify\nit however you see fit.\n\nAs for the latency differences that Shawn mentioned, keep in mind I\nwas running microbenchmarks. It was a tight loop of small queries\nrunning at max throughput and everything was a cache hit. IIRC, faster\nCPUs showed less of a gain. Part of this is explained because it\nremoves at least one syscall for every request. Faster, more modern,\nCPUs should context switch more efficiently. If I was doing this\nbenchmark over again today I would try other variations (low-rate to\navoid any CPU run-queue buildup and focus on latency only) and examine\nmore performance metrics.\n\nThat aside, this patch should avoid the case where a stale conn is\nchosen and a new one has to be created as part of the request which\nwill result in latency outliers. A win in my book. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14226842",
            "date": "2014-11-26T21:38:40+0000",
            "content": "I've got a patch coming that tries to extend this to all our http client usage - that ended up being a fairly long thread to pull on. Ill post a progress patch soon but there are still some things to address.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14226858",
            "date": "2014-11-26T21:56:50+0000",
            "content": "given that any check is imperfect\n\nYeah, I was more referring to the fact that the whole thing kind of seems like a bad bug to me - except that in pure http, a retry is often fine and I guess keeps it from being a flat out buggy situation. If you turn off retries, it does seem like a buggy implementation, though a tough one to solve generally for HttpClient I guess. Even with Solr, I'm not super happy you have to line up the server and client idle setting reasonably, but it appears the best we can do.  \n\nI guess in my mind, a non imperfect implementation would not have this built in 'random race condition fail'. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14227089",
            "date": "2014-11-27T01:16:28+0000",
            "content": "Here is the latest in progress patch. It moves configuration to system properties and away from solr.xml so that we can try and use the new stale connection approach with more of our HttpClient usage.\n\nReviewBoard doesn't seem to like a move that I did of IOUtils in the patch, so moving back to the JIRA issue. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14227105",
            "date": "2014-11-27T01:30:38+0000",
            "content": "I thought I had killed stale connection checks long ago.  Maybe it was long enough ago that it was back in the CNET days or something...\n\nCan we tell when this happens (and that we are certain that the server did not receive the request) so that we can do a retry?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14227140",
            "date": "2014-11-27T02:15:44+0000",
            "content": "Can we tell when this happens (and that we are certain that the server did not receive the request) so that we can do a retry?\n\nFrom what I can tell, the default retry handler will retry in cases we don't want. We may be able to implement one that works, but it seems a little tricky to be 100% sure of that work, and it still kind of sucks that every thread has to deal with this stale connection check (docs claim 10-30ms hit, Ryan ran into worse troubles under concurrency) rather than pulling out that work to a background thread. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14227191",
            "date": "2014-11-27T02:58:45+0000",
            "content": "rather than pulling out that work to a background thread.\n\nYeah, I was talking about even after this patch is applied. This should drop the errors by an order of magnitude or so... but it would be nice to drop to zero. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14227202",
            "date": "2014-11-27T03:08:12+0000",
            "content": "Ah, okay - probably worth following up with another issue. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14228570",
            "date": "2014-11-29T00:36:05+0000",
            "content": "I was about to try upgrading httpclient to 4.3.6 on branch_5x to see if anything breaks, and went looking at release notes to see if there's any compelling reason to commit the upgrade.\n\nAfter checking out the latest 4.3 release notes, I looked at the 4.4 beta release notes.  Looks like stale connections are getting attention there too.\n\nhttp://www.apache.org/dist/httpcomponents/httpclient/RELEASE_NOTES-4.4.x.txt\n\nI don't know what the timeframe for the new HC release is, but perhaps we can let it be solved upstream? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14228837",
            "date": "2014-11-29T16:50:07+0000",
            "content": "Thanks for the info Shawn - certainly worth investigating. On a high level reading, my worry is that this works to minimize the performance penalty but I don't know that it tries to solve the connection reset issue that I'm more concerned about. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14229159",
            "date": "2014-11-30T16:53:53+0000",
            "content": "On a high level reading, my worry is that this works to minimize the performance penalty but I don't know that it tries to solve the connection reset issue that I'm more concerned about.\n\nThis seems to be the case. It uses the same stale check, it just doesn't do it on every request anymore. https://issues.apache.org/jira/browse/HTTPCLIENT-411 "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14230564",
            "date": "2014-12-01T22:20:51+0000",
            "content": "One other thing to consider, and I'm not sure if this applies to that release note, is that there are major API changes that in HttpClient 4.4 that you need to use to get the new features.  In general everything is done via builders, so you can't change many configuration settings after creating the httpclient.  There are APIs in HttpSolrServer and elsewhere that let you change the configuration that would have to be reworked. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14230619",
            "date": "2014-12-01T22:52:09+0000",
            "content": "We have SOLR-5604 to address the deprecated HttpClient methods that we currently use.  I already attempted it once ... it's not going to be a trivial change, and my knowledge of HttpClient is too limited to be useful.  Thankfully those methods will stick around until HC 5.0 comes out, so there's not currently a pressing need.\n\nIf the method of dealing with stale checks that is being developed here is superior to HC 4.4, then we might want to ask that Oleg Kalnichevski consider it for HC itself.  I haven't looked at either solution, and I doubt that I would understand it even if I did look. "
        },
        {
            "author": "Oleg Kalnichevski",
            "id": "comment-14231190",
            "date": "2014-12-02T08:55:21+0000",
            "content": "The idea behind connection check optimizations in HC 4.4 is rather straight-forward: given the 'stale' check cannot be 100% reliable (the connection can turn 'stale' immediately after passing the check but before a request is issued) there is no point checking the connection indiscriminately before each and every request. It generally should be sufficient to check those connections that are more likely to get 'stale'. Usually those are the connections that have been sitting idle in the connection pool for a considerable period of time.\n\nHC 4.4 is most likely going to get delayed by a month or two. So, there should enough time to contribute an alternative solution if it turns out to be better than what we currently have.\n\nOleg "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14248342",
            "date": "2014-12-16T15:05:16+0000",
            "content": "I've been pushing this along off and on for a while now. This had a rather large impact on a variety of tests - especially with SSL. I think I'm pretty close though - one more flaky test to investigate I believe.\n\nThere is a rather interesting affect from this change - the 'close idle' thread will trump the socket timeout. So we are looking at lower socket timeouts or longer idle timeouts or some compromise in between. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14250753",
            "date": "2014-12-17T23:02:28+0000",
            "content": "My latest work attached. I think this is almost ready.\n\nWe do need to do a retry, but it's only once, and it's much more limited than the default retry impl. Without that, simple things like stopping and starting a server can lead to client side connection reset errors. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14250786",
            "date": "2014-12-17T23:19:30+0000",
            "content": "stopping and starting a\n\nWell, depending on how graceful. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14251087",
            "date": "2014-12-18T03:12:18+0000",
            "content": "After this patch, tests have gone from about 16-17 minutes to run for me to 10-11 minutes. I've tweaked Jetty shutdown to be faster, but most of this is probably related to SSL tests and SOLR-5776 and SOLR-6293.\n\nBecause this change effectively lowered client idle timeouts, didn't have any retries for a while, and perhaps my system random entropy has been low, I really had to clamp down on how much SSL is done in a test run to be able to consistently get clean runs with timeouts from SSL lack of random entropy blocking. As I'm getting speed much closer to what I used to get pre SOLR-6293, this looks like a prime suspect for that major loss. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14251815",
            "date": "2014-12-18T15:40:02+0000",
            "content": "There is a rather interesting affect from this change - the 'close idle' thread will trump the socket timeout.\n\nI gathered this because when I run the close idle connection thread at a lower interval than the socket timeout, slow SSL tests where hitting a socket error (no response from server) rather than the socket timeout they would hit if I raised the close idle connection thread interval.\n\nLooking at the code though, I think this idle connection close should only apply to unused connections in the pool. I have to look closer at what is happening. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14254036",
            "date": "2014-12-19T21:22:50+0000",
            "content": "Okay, this will not affect socket timeout settings. I was seeing something else with the difference in exceptions from SSL slowness it seems.\n\nI have something pretty solid now. I still have not figured out why the SolrCloud tests that use the proxy stuff to simulate network partitions don't work yet, but other than that things are coming together nicely I think. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14254938",
            "date": "2014-12-20T19:47:27+0000",
            "content": "So one advantage that the stale check had was that it could handle (at least most of the time) a NoHttpResponseException. We can't currently retry on that if we have sent the whole request, but stale check can detect cases of it before sending. We can retry on pertinent connection reset and broken pipe socket exceptions though. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14254972",
            "date": "2014-12-20T22:22:25+0000",
            "content": "I skimmed through the patch. Looks good, thank you for taking this up! "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-14254978",
            "date": "2014-12-20T22:58:29+0000",
            "content": ">>So one advantage that the stale check had was that it could handle (at least most of the time) a NoHttpResponseException\n\nCould you please elaborate? I don't quite get it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14255031",
            "date": "2014-12-21T03:42:41+0000",
            "content": "Here is a dump of my latest state. More to come. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14255034",
            "date": "2014-12-21T03:49:53+0000",
            "content": "One exception that pops up in some places it did not before is NoHttpResponseException - after we have sent the full request and so we cannot retry. It would appear that the stale check could usually detect this broken connection before even sending the request and then get a new valid connection before making the request. I've seen this exception even with the stale check, so I think there is still a race, just like connection reset, but often, it seems it can handle this. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14264170",
            "date": "2015-01-05T04:20:19+0000",
            "content": "I brought this up to date with trunk and spent some more time hardening and investigating test issues. Getting close to a resolution. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14268037",
            "date": "2015-01-07T19:11:33+0000",
            "content": "The final 1% of this patch has gotten a bit complicated. This patch has also grown to the point where it is difficult to maintain.\n\nI'm going to spin off and commit a few sub patches. First, adding a limited retry and second, closing all httpclient instances properly. That and a few other fixes / changes I've made along the way will significantly reduce the size and overhead of this patch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14269731",
            "date": "2015-01-08T17:42:17+0000",
            "content": "SOLR-6931 We should do a limited retry when using HttpClient. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14269751",
            "date": "2015-01-08T17:52:56+0000",
            "content": "SOLR-6932 All HttpClient ConnectionManagers and SolrJ clients should always be shutdown in tests and regular code. "
        },
        {
            "author": "Alan Woodward",
            "id": "comment-14298543",
            "date": "2015-01-30T12:12:46+0000",
            "content": "I'm seeing lots of test fails on the mailing list due to NoHttpResponseExceptions, which this StackOverflow answer suggests is due to stale connections: http://stackoverflow.com/questions/10558791/apache-httpclient-interim-error-nohttpresponseexception.\n\nFollowing the links to http://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html, the section on connection evictions policy says that you should have a separate thread that periodically closes idle or dead connections.  Is this something we should look into for HttpSolrClient? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14298759",
            "date": "2015-01-30T15:27:02+0000",
            "content": "You are probably seeing SOLR-6944. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14298802",
            "date": "2015-01-30T16:22:17+0000",
            "content": "Here is a first pass at shrinking this patch to trunk now that all the issues I spun off are committed.\n\nIt is not complete anymore because of the change to Jetty 9. "
        },
        {
            "author": "bryan hunt",
            "id": "comment-14532901",
            "date": "2015-05-07T15:54:03+0000",
            "content": "For the record (if anyone is interested) the patch on Yokozuna has moved since Ryan originally commented on this ticket. The repository moved, and the original patch can now be found at https://github.com/basho/yokozuna/pull/26 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15198174",
            "date": "2016-03-16T21:10:38+0000",
            "content": "I'd like to come back to this. Previously, it was causing spurious connection resets in tests. I'm hoping we have addressed that with other fixes. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15198221",
            "date": "2016-03-16T21:32:36+0000",
            "content": "SOLR-6625 seems to have given users the ability to use any httpclient impl they want - that kind of sucks in this case. To do advanced stuff like this, we pretty much need to own the impl.\n\nI see nothing that calls setHttpClientImpl and there are no tests or documentation though. I think we should find an alternative to whatever was done here (no one even doc'd it?). We don't want the user to be able to set HttpClientImpls via a static method...not documented, not tested, not a thread safe way to configure.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200780",
            "date": "2016-03-18T01:33:27+0000",
            "content": "I've just about got a new patch ready to show here.\n\nI still don't have a decent way to hook into httpclient close anymore, so that's an issue that needs to be worked around outside of just tests (as I've done).\n\nSome things to consider:\n\nSupposedly the stale connection check is not as bad a performance killer as it used to be as it's not done every request any longer?\nWithout the stale check, when a server drops, even if it comes back up, the client might try to use a bad connection - in the past the stale connection check could catch that.\nHowever, the stale connection check is still not 100% reliable and I assume the perf optimization that did has a similar issue as above in the right circumstance.\nIt still would be nice to try and control connection lifecycle from the client as much as possible. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200810",
            "date": "2016-03-18T01:56:24+0000",
            "content": "Actually, the work around for the stale check they have done is the answer to the bad connection after quick restart issue.\n\nWe need to get off deprecated classes though. The work in SOLR-6625 interferes with that, but we have always needed to consider this stuff internal. We cant expose such low level impl details as supported user surface area. Bug fixes and advances are not going into these deprecated classes like the new ones, and they will be gone eventually. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200893",
            "date": "2016-03-18T02:52:16+0000",
            "content": "Nice, if we move to the new API's we also get most of the work I've been doing here built in, including the idle connection sweeper. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200978",
            "date": "2016-03-18T04:12:50+0000",
            "content": "I've got this mostly working - new httpclient impls are much better. Security and SSL have really nestled into this late config binding that the httpclientconfigurer stuff allowed and does not anymore (builder pattern) though, so I'll probably bail on it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200982",
            "date": "2016-03-18T04:16:20+0000",
            "content": "I hate that we leaked httpclient apis into our user apis   I think it was just the quick and easy way to deal with SSL and then security glommed on later as well. Too late for 6 now though, and even 7 would need a bunch of work to make that all work with preconfiguration and no impl level to user leakage. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15200999",
            "date": "2016-03-18T04:31:47+0000",
            "content": "Actually this snuck in even before SSL, it was added in 4.0.  It was quietly added, but internal features did not count on it or use it. SSL only used it for test purposes later on. Security has bear hugged it though - it's how you configure security now and is part of a user plugin api. Bummer given the old deprecated HttpClient classes involved and the extra pain to move to preconfiguration. We should try and minimize exposing internal client API's as part of our API's to users. It really locks us in. "
        },
        {
            "author": "Oleg Kalnichevski",
            "id": "comment-15201673",
            "date": "2016-03-18T15:49:44+0000",
            "content": "One can override SSL configuration on a per request basis by setting a custom connection socket registry in the HttpContext of the request. \n\nhttp://hc.apache.org/httpcomponents-client-4.5.x/httpclient/xref/org/apache/http/impl/conn/DefaultHttpClientConnectionOperator.html#66\n\nThis however can potentially lead to SSL connections being re-used by another thread with a different user identity / security context, that is why we do not really advertise this feature.\n\nOleg "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15208803",
            "date": "2016-03-23T17:18:36+0000",
            "content": "6.0 isn't out the door yet... can we somehow change the status of the current stuff that relies/exposes those deprecated APIs to allow for change/improvement (and upgrading HttpClient) in 6.x? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15208981",
            "date": "2016-03-23T19:19:11+0000",
            "content": "Linking to SOLR-5604, where this was explored. Given the non-trivial nature of such a refactoring (as it appears to me from the linked issue, but I could be wrong as I haven't understood all aspects of that refactoring yet), I think we should target this for later (possibly 7.0). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15208988",
            "date": "2016-03-23T19:21:47+0000",
            "content": "Still a bunch of work to do to finish beating this into shape, but it compiles and handles most of the API changes and builder patterns we need to move to. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15208991",
            "date": "2016-03-23T19:22:56+0000",
            "content": "I think we should target this for later (possibly 7.0).\n\nLuckily, the plugin API itself is marked as experimental even, so I see no need to wait to fix our connection management for a full major release. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15209623",
            "date": "2016-03-24T02:47:34+0000",
            "content": "Might be able to fix those APIs now, but we will see. This issue is a b$@& as is.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15211154",
            "date": "2016-03-24T23:53:23+0000",
            "content": "Here is a much more functionally complete patch. Tests should be passing, but a few are ignored. Much closer to done, but still some things to do. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15211155",
            "date": "2016-03-24T23:54:31+0000",
            "content": "Might be able to fix those APIs now, but we will see.\n\nActually, it's probably not very useful given we cannot put the new HttpClient APIs in the plugin API, and we basically forced to expose that. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15212408",
            "date": "2016-03-25T21:06:10+0000",
            "content": "Had not cleared the solrj tests. Still a mess, but some light at the end of the tunnel. Now all tests, including solrj, should be passing. A few remain ignored. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15215504",
            "date": "2016-03-29T06:09:23+0000",
            "content": "New patch. Getting pretty damn close. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15215565",
            "date": "2016-03-29T06:53:29+0000",
            "content": "Okay, removed most of those nocommits. One final nocommit left around using the SolrPortAwareCookieSpecFactory for kerberos. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15216810",
            "date": "2016-03-29T20:49:25+0000",
            "content": "Okay, here is pretty reasonable patch I think. I will start reviewing all these changes. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15217366",
            "date": "2016-03-30T04:38:58+0000",
            "content": "Patch after first pass of review and cleanup. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15220332",
            "date": "2016-03-31T17:59:42+0000",
            "content": "I think we should make these interfaces instead of abstract classes. Before Java 8 we could not add methods to interfaces w/o breaking compatibility. So, we always created abstract base classes . Now that we have moved to java 8 we can add methods at will \n\npublic static abstract class HttpRequestInterceptorProvider {\n    public abstract HttpRequestInterceptor getHttpRequestInterceptor();\n  }\n  \n  public static abstract class CredentialsProviderProvider {\n    public abstract CredentialsProvider getCredentialsProvider();\n  }\n  \n  public static abstract class AuthSchemeRegistryProvider {\n    public abstract Lookup<AuthSchemeProvider> getAuthSchemeRegistry();\n  }\n  \n  public static abstract class CookieSpecRegistryProvider {\n    public abstract Lookup<CookieSpecProvider> getCookieSpecRegistry();\n  }\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15221917",
            "date": "2016-04-01T16:21:28+0000",
            "content": "Sure, we can make the interfaces.\n\nI'll commit this to master shortly so we can see how Jenkins takes to it and iterate from there. Rather large set of changes to keep up to date. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15221950",
            "date": "2016-04-01T16:46:05+0000",
            "content": "Commit ce172acb8fec6c3bbb18837a4d640da6c5aad649 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ce172ac ]\n\nSOLR-4509: Move to non deprecated HttpClient impl classes to remove stale connection check on every request and move connection lifecycle management towards the client. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15222989",
            "date": "2016-04-02T18:14:02+0000",
            "content": "Commit d0156b1126f094e4e469172d55842ed77cb82943 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d0156b1 ]\n\nSOLR-4509: Fix test failures with Java 9 module system by doing a correct cleanup "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-15222992",
            "date": "2016-04-02T18:16:59+0000",
            "content": "Hi Mark Miller,\nI committed a fix for 3 tests that now leaked SolrClients in static fields, which made the test leak detector angry on Java 9's module system (it tried to measure private, internal classes). If this is backported, please also backport my fix. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15257323",
            "date": "2016-04-26T00:21:55+0000",
            "content": "\nGIT commit ce172acb appears to have broken SolrCLI so that bin/solr client actions (ie: bin/solr status bin/solr create ..., etc..) no longer work when solr.in.sh is configured to use SSL.\n\nTracking this in SOLR-9040 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15261101",
            "date": "2016-04-27T22:41:15+0000",
            "content": "Commit 9ab76a1e41d7019fd07b16a79a587653cf6d76a4 in lucene-solr's branch refs/heads/master from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9ab76a1 ]\n\nSOLR-9040 / SOLR-4509: Fix default SchemaRegistryProvider so javax.net.ssl.* system properties are respected by default "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15264391",
            "date": "2016-04-29T17:32:05+0000",
            "content": "FWIW: It doesn't seem likely that this issue (SOLR-4509) is going to be backported to 6x since it has some incompatible solrj level changes (Configurer->Builder+SchemaProvider) but if I'm wrong and someone does decide to try and backport it, please note that SOLR-9028 has already been backported from master->6x and quite a few conflicts due to SOLR-4509 changes were resolved there that might cause new conflicts here.\n\nIf SOLR-4509 is backported, it might be easiest to:\n\n\trevert the branch_6x changes related to SOLR-9028\n\tbackport SOLR-4509\n\trebackport the master changes for SOLR-9028\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15277308",
            "date": "2016-05-09T23:26:16+0000",
            "content": "\nManually correcting fixVersion per Step #S6 of LUCENE-7271 "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15355032",
            "date": "2016-06-29T11:09:21+0000",
            "content": "This has broken support for specifying connection and read timeout values for UpdateShardHandler's http client. I opened SOLR-9262 to fix. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15358541",
            "date": "2016-07-01T07:17:18+0000",
            "content": "Commit 2b4420c4738bb3aed3ae759fd93b6cbbdbc1eefd in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2b4420c ]\n\nSOLR-9262: Connection and read timeouts are being ignored by UpdateShardHandler after SOLR-4509 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15358588",
            "date": "2016-07-01T07:46:58+0000",
            "content": "Commit 51fde1cbf954b6f67283ad945525e8c6b5197fb9 in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=51fde1c ]\n\nSOLR-9262: Connection and read timeouts are being ignored by UpdateShardHandler after SOLR-4509 "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-15571798",
            "date": "2016-10-13T12:39:37+0000",
            "content": "Discovered that while SolrCLI changed to \n\nString builderClassName = System.getProperty(\"solr.authentication.httpclient.builder\");\n\n\nthe property set in bin/solr is still the old solr.authentication.httpclient.configurer\n\nif [ \"$SOLR_AUTHENTICATION_CLIENT_CONFIGURER\" != \"\" ]; then\n  AUTHC_CLIENT_CONFIGURER_ARG=\"-Dsolr.authentication.httpclient.configurer=$SOLR_AUTHENTICATION_CLIENT_CONFIGURER\"\nfi\n\n\nand likewise in bin/solr.in.sh\n\nAlso looks like the Windows scripts lack support for these settings. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-15581898",
            "date": "2016-10-17T10:59:28+0000",
            "content": "Covered in SOLR-9255 "
        }
    ]
}