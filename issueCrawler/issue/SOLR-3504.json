{
    "id": "SOLR-3504",
    "title": "Clearly document the limit for the maximum number of documents in a single index",
    "details": {
        "affect_versions": "3.6",
        "status": "Closed",
        "fix_versions": [
            "7.2",
            "master (8.0)"
        ],
        "components": [
            "documentation",
            "update"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Although the actual limit to the number of documents supported by a Solr implementation depends on the number of shards, unless the user is intimately familiar with the implementation of Lucene, they may not realize that a single Solr index (single shard, single core) is limited to approximately 2.14 billion documents regardless of their processing power or memory. This limit should be clearly documented for the Solr user.\n\nGranted, users should be strongly discouraged from attempting to create a single, unsharded index of that size, but they certainly should have to find out about the Lucene limit by accident.\n\nA subsequent issue will recommend that Solr detect and appropriately report to the user when and if this limit is hit.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Jack Krupansky",
            "id": "comment-13288266",
            "date": "2012-06-03T21:53:32+0000",
            "content": "Solr has a number of interfaces for adding documents, such as Update XML, Update CSV, SolrCell, Data Import Handler, SolrCloud, etc. Generally, each has a wiki page, to which the following section should be added:\n\n\"Limitations\n\nAlthough a Solr implementation can scale into the billions of documents by using a number of shards, each individual shard or Solr core is limited by the Lucene limit for an index which is approximately 2.14 billion documents (2,147,483,647 to be exact) in the current implementation of Lucene. In practice, it is unlikely that such a large number of documents would fit and perform well in a single index. In extreme cases it may be possible, but in no case can the number of documents in a single index exceed that number.\"\n\nThis limitation could also be added to the Solr tutorial page.\n\nThere are probably a few other locations in the Solr docs when this limitation should be noted. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16246431",
            "date": "2017-11-09T20:12:05+0000",
            "content": "Commit 0546a64acfaed28da1cd1af8de9a069ed292a2c1 in lucene-solr's branch refs/heads/master from Cassandra Targett\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0546a64 ]\n\nSOLR-3504: add note about hard num docs limit in Lucene to planning installation section "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16246433",
            "date": "2017-11-09T20:13:08+0000",
            "content": "Commit 514be8c4e5a76848ad96f3cbbe319c0c715a23b1 in lucene-solr's branch refs/heads/branch_7x from Cassandra Targett\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=514be8c ]\n\nSOLR-3504: add note about hard num docs limit in Lucene to planning installation section "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-16246444",
            "date": "2017-11-09T20:18:15+0000",
            "content": "I haven't looked closely but if we have over 2B docs spread across multiple shards in the single collection and we do a match all query , does Solr deal with it correctly?  "
        }
    ]
}