{
    "id": "SOLR-9591",
    "title": "Shards and replicas go down when indexing large number of files",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.5.2",
        "status": "Closed",
        "resolution": "Not A Problem",
        "priority": "Major"
    },
    "description": "Solr shards and replicas go down when indexing a large number of text files using the default extracting request handler.\n\ncurl 'http://localhost:8983/solr/myCollection/update/extract?literal.id=someId' -F \"myfile=/data/file1.txt\"\n\n\nand committing after indexing 5,000 files using:\n\ncurl 'http://localhost:8983/solr/myCollection/update?commit=true&wt=json'\n\n\n\nThis was on Solr (SolrCloud) version 5.5.2 with an external zookeeper cluster \nof five nodes. I also tried this on a single node SolrCloud with the embedded ZooKeeper but the collection went down as well. In both cases the error message is always \"ERROR null DistributedUpdateProcessor ClusterState says we are the leader,\u200b but locally we don't think so\"\n\nI managed to come up with a work around that helped me index over 400K files without getting replicas down with that error message. The work around is to index 5K files, restart Solr, wait for shards and replicas to get active, then index the next 5K files, and repeat the previous steps.\n\nIf this is not enough to investigate this issue, I will be happy to provide more details regarding this issue.",
    "attachments": {
        "solr_log_20161002_1504": "https://issues.apache.org/jira/secure/attachment/12831404/solr_log_20161002_1504"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-10-03T20:07:14+0000",
            "author": "Kevin Risden",
            "content": "Can you provide the full stack trace? Redact anything sensitive. If the whole log isn't too big that could be helpful too.\n\nFew other questions:\n\n\tAre these plain text files or something else?\n\tDoes it happen with any set of text files you tried?\n\tAre these text files publically available?\n\tSince you trying single node with embedded, does this same error occur with the latest Solr release?\n\n ",
            "id": "comment-15543274"
        },
        {
            "date": "2016-10-03T21:04:58+0000",
            "author": "Khalid Alharbi",
            "content": "Thank you Kevin for looking at this.\n\nAre these plain text files or something else?\nYes, plain text files that contain the Android framework API calls used by apps. Each text file belongs to an Android app and has all the Android API methods that the app uses.\n\n\nDoes it happen with any set of text files you tried?\nNo. In fact, I thought that was the case but after indexing files and restarting Solr, Solr indexed the ones that failed at without any problem. If I do not restart Solr, it will fail with the aforementioned error message.\n\n\nAre these text files publicly available?\nYes, but only 60 text files. GitHub Link\n\n\nSince you trying single node with embedded, does this same error occur with the latest Solr release?\nI tried this on each Solr 5.x releases between 5.1.0 to 5.5.2.\n\n\nCan you provide the full stack trace?\n\nSure, I have attached one of the log files and below is the error message that keeps occurring over and over when I index those files.\n\n\n2016-10-02 14:13:29.194 ERROR (qtp263094610-15) [c:my_collection s:shard1 r:core_node2 x:my_collection_shard1_replica1] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: ClusterState says we are the leader (http://XX.XX.XX.XX:8983/solr/my_collection_shard1_replica1), but locally we don't think so. Request came from null\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doDefensiveChecks(DistributedUpdateProcessor.java:622)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.setupRequest(DistributedUpdateProcessor.java:385)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.setupRequest(DistributedUpdateProcessor.java:315)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:671)\n\tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:48)\n\tat org.apache.solr.update.processor.AbstractDefaultValueUpdateProcessorFactory$DefaultValueUpdateProcessor.processAdd(AbstractDefaultValueUpdateProcessorFactory.java:93)\n\tat org.apache.solr.handler.extraction.ExtractingDocumentLoader.doAdd(ExtractingDocumentLoader.java:126)\n\tat org.apache.solr.handler.extraction.ExtractingDocumentLoader.addDoc(ExtractingDocumentLoader.java:131)\n\tat org.apache.solr.handler.extraction.ExtractingDocumentLoader.load(ExtractingDocumentLoader.java:237)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:69)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:155)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2102)\n\tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:654)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:460)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:257)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:208)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:499)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n\tat org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n ",
            "id": "comment-15543419"
        },
        {
            "date": "2016-10-04T09:00:42+0000",
            "author": "Kevin Risden",
            "content": "Does the issue happen if you commit after a smaller number of documents? Say 50 or 100 or 1000?\n\nWhat is your heap size set to? ",
            "id": "comment-15544824"
        },
        {
            "date": "2016-10-04T14:11:43+0000",
            "author": "Khalid Alharbi",
            "content": "\nDoes the issue happen if you commit after a smaller number of documents? Say 50 or 100 or 1000?\nYes, I did try committing after indexing 500 documents in a loop but the shards went down in the fifth iteration (after indexing and committing around 2500 docs). Interestingly, when the collection is empty, it can index up to 20,000 docs without an issue. This issue surfaces when I have over 20K docs in a collection.\n\n\nWhat is your heap size set to?\nThe heap size is 5g. \nRunning ./bin/solr status shows this: \"memory\":\"2.1 GB (%44) of 4.8 GB\" ",
            "id": "comment-15545446"
        },
        {
            "date": "2016-10-06T16:44:00+0000",
            "author": "Kevin Risden",
            "content": "It looks like you have an autocommit enabled every 15000ms. If you disable the autocommit does that let you index more documents? Are there any errors in the ZooKeeper logs?  ",
            "id": "comment-15552442"
        },
        {
            "date": "2016-10-06T16:54:27+0000",
            "author": "Kevin Risden",
            "content": "Another thought: I looked at some of the files and they are say 15MB in size. That would mean that 500*15MB is 7.5 GB of heap just to load them into memory. I would try the following and see if the error goes away:\n\n\n\tdisable the autocommit for now\n\tindex 50 files at once\n\tcommit without opening a new searcher\n\trepeat index/commit without opening a new search\n\tafter done indexing, commit with a new searcher so that results are visible.\n\n\n\nMy guess is you are hitting JVM garbage collection since it looks like Solr is disconnecting from ZK. There are also errors in there about ZK fsyncing log:\n\nfsync-ing the write ahead log in SyncThread:0 took 3445ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide ",
            "id": "comment-15552489"
        },
        {
            "date": "2016-10-06T18:42:38+0000",
            "author": "Pushkar Raste",
            "content": "Have you looked into GC logs to see if there are any long GC pauses. ",
            "id": "comment-15552791"
        },
        {
            "date": "2016-10-06T19:39:56+0000",
            "author": "Khalid Alharbi",
            "content": "Thank you Kevin and Pushkar,\nYes, I did notice that I'm hitting a long GC STW pauses.\nLike this one in one of the solr_gc_log files:\n\n2016-09-05T16:41:33.245+0000: 74811.405: Total time for which application threads were stopped: 15.1094490 seconds, Stopping threads to ok: 15.1092010 seconds\n\n\n\nI will try indexing a smaller number of files followed by a soft commit as per Kevin's suggestions and report back here. ",
            "id": "comment-15552966"
        },
        {
            "date": "2016-10-06T21:15:53+0000",
            "author": "Pushkar Raste",
            "content": "Are you using MMapDirectory? Using MMApDirectory keep index off heap and reduces pressure on the garbage collector.\n\nIn my experience G1GC with ParallelRefProcEnabled helps a lot to have short GC pauses.  ",
            "id": "comment-15553195"
        },
        {
            "date": "2016-10-09T20:47:34+0000",
            "author": "Khalid Alharbi",
            "content": "Thank you Kevin and Pushkar.\nI've tried steps similar to what Kevin suggested, and it worked great without an issue. I believe Kevin is right that the reason behind the problem was the long GC pauses that caused Solr to disconnect from Zookeeper. I'm currently using the default NRTCachingDirectoryFactory but will look at other implementation options for storing index files in the file system.\nI will close this issue for now but I'd love to see this documented in the FAQ or any appropriate place since I could not get an answer on the mailing list.\nThanks again Kevin for your time and help! ",
            "id": "comment-15560598"
        }
    ]
}