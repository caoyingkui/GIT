{
    "id": "SOLR-2462",
    "title": "Using spellcheck.collate can result in extremely high memory usage",
    "details": {
        "affect_versions": "3.1",
        "status": "Closed",
        "fix_versions": [
            "3.3",
            "4.0-ALPHA"
        ],
        "components": [
            "spellchecker"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "When using \"spellcheck.collate\", class SpellPossibilityIterator creates a ranked list of every possible correction combination.  But if returning several corrections per term, and if several words are misspelled, the existing algorithm uses a huge amount of memory.\n\nThis bug was introduced with SOLR-2010.  However, it is triggered anytime \"spellcheck.collate\" is used.  It is not necessary to use any features that were added with SOLR-2010.\n\nWe were in Production with Solr for 1 1/2 days and this bug started taking our Solr servers down with \"infinite\" GC loops.  It was pretty easy for this to happen as occasionally a user will accidently paste the URL into the Search box on our app.  This URL results in a search with ~12 misspelled words.  We have \"spellcheck.count\" set to 15.",
    "attachments": {
        "SOLR-2462_3_1.patch": "https://issues.apache.org/jira/secure/attachment/12478183/SOLR-2462_3_1.patch",
        "SOLR-2462.patch": "https://issues.apache.org/jira/secure/attachment/12475820/SOLR-2462.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "James Dyer",
            "id": "comment-13017493",
            "date": "2011-04-08T15:41:11+0000",
            "content": "This sets the maximum limit to 1000 possibilities.  When this limit is reached, the list is sorted by rank then reduced to the top 100.  From then on, only collations with a rank equal or better than the 100th are added.  This process repeats until finished or until it has taken 50ms, at which time it quits.\n\nI also added a \"maxTimeAllowed\" setting of 50ms to the collation test queries as an additional performance safeguard. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13028832",
            "date": "2011-05-04T17:16:10+0000",
            "content": "The original patch would not apply cleanly for me against 3.1 without fuzz and whitespace options, and when those are used, it applies incorrectly.  Here's a new patch specific to 3.1.  Before creating this, I checked 3.1 out from SVN and then applied the patch for SOLR-2469, which should not interfere in any way.\n\nHopefully the patch is suitable.  I am only putting it up here for convenience, in case anyone else runs into this. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042675",
            "date": "2011-06-02T10:07:03+0000",
            "content": "Hi James, this sounds like an important issue to fix!\n\nI'm sorry you have all these open improvements to the spellchecker, lets see if we can try to get some of them resolved.\n\n\nThis sets the maximum limit to 1000 possibilities. When this limit is reached, the list is sorted by rank then reduced to the top 100. From then on, only collations with a rank equal or better than the 100th are added. This process repeats until finished or until it has taken 50ms, at which time it quits.\n\nMaybe we want to use a priority queue instead? It sorta seems like this is what you are doing. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13042872",
            "date": "2011-06-02T16:53:18+0000",
            "content": "This patch uses a PriorityQueue instead of a sorted List to store the RankedSpellPossibility objects.  I also went with far simpler logic in safeguarding the performance:  this version simply quits at 10,000 elements.  I did this because:\n\n1. With a PriorityQueue, there is no simple way to get the 100th element and find its rank to determine whether or not to add subsequent elements.\n2. With the simpler logic, there is no need to keep calling \"currentTimeMillis()\" as a final fallback (in itself a performance hog).\n3. It is highly unlikely a competitive spellcheck collation will ever be found past the 10,000 combination.\n\nIn all, this is a more elegant solution than the prior one. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042885",
            "date": "2011-06-02T17:06:44+0000",
            "content": "1. With a PriorityQueue, there is no simple way to get the 100th element and find its rank to determine whether or not to add subsequent elements.\n\nHi James, you might consider using peek() here to check\n\nas an example, you can take a look at the main loop of DirectSpellChecker: suggestSimilar(Term, int, IndexReader, int, int, float, CharsRef)\nhttp://svn.apache.org/repos/asf/lucene/dev/trunk/modules/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java "
        },
        {
            "author": "James Dyer",
            "id": "comment-13042931",
            "date": "2011-06-02T17:49:18+0000",
            "content": "Right...because the elements are sorted already I don't have to go back to the 100th element to compare.  I can just look at the last element using peek() as you suggest.\n\nThis version uses the more sophisticated methods of the original patch but accomplishes it with less code.  Also, we're using nanoTime() instead of currentTimeMillis() to reduce any overhead, and are checking the clock only once every 10000 iterations.\n\nFrom code comments:\n\nThree performance & memory-usage safeguards:\n  1. Quit if the RankedPossibilities queue grows larger than 10000.\n  2. If the RankedPossibilities queue is bigger than 1000, only add competitive possibilities.\n  3. Check the clock periodically to be sure we haven't taken more than 50ms.  If so, quit immediately.\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042940",
            "date": "2011-06-02T17:55:16+0000",
            "content": "Now that we have a PQ (thanks for iterating here!), maybe we should just bound the size by some value (say 20, or whatever the top-N the user requested is).\n\nThen, we only add competitive possibilities always once the size reaches 20.\n(this is how the spellchecker link i provided works, see the line \"// possibly drop entries from queue\")\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13042969",
            "date": "2011-06-02T18:36:48+0000",
            "content": "This is along the lines of what I initially intended on doing but didn't have time back when I first submitted this.\n\nI felt particularly guilty in gathering all these RankedSpellPossibility objects in cases where the user isn't even using new functionality from SOLR-2010 (upgrade from 1.4 then collate becomes more expensive!).  \n\nThank you for another opportunity to absolve my guilt.\n\nI ran these tests and they all pass:  SpellPossibilityIteratorTest, SpellCheckCollatorTest, SpellCheckComponentTest & DistributedSpellCheckComponentTest "
        },
        {
            "author": "James Dyer",
            "id": "comment-13042972",
            "date": "2011-06-02T18:40:28+0000",
            "content": "fixes a silly error in the last patch. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042988",
            "date": "2011-06-02T19:02:55+0000",
            "content": "OK, this is looking better, but we can now keep the PQ at a fixed size, pretend for this example that maximumRequiredSuggestions = 20.\n\nthe idea is after your offer(), you check if the pq's size is > maximumRequiredSuggestions (say its 21), and then if so, you poll() to remove the lowest element (its now 20 again)\n\nthis way, the size never grows past 20... and i then think you can then remove the rankedPossibilities < 10000 check because it will never get larger than the user's requested amount. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13043018",
            "date": "2011-06-02T19:59:20+0000",
            "content": "Robert,\n\nI'm not sure we can do this last suggestion.  If as the queue grows we poll(), we will in fact be discarding all of the best (lowest-ranked) suggestions.  What we would need to do is insert the 21st element, letting it fall into its place in order, and then we'd need an operation to remove the worst suggestion from the tail of the queue.  Looking at PriorityQueue's API docs I'm not sure there is a method exposed that does that.\n\nDid I miss something here? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043058",
            "date": "2011-06-02T21:22:02+0000",
            "content": "this the general workflow\n\n1. check if competitive\n2. offer\n3. if > size, poll.\n\nthere are more examples than mine in directspellchecker, we use this in lots of places. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043100",
            "date": "2011-06-02T22:17:30+0000",
            "content": "James sorry for my short-barely-comprehensible response... in addition what i should have said is that if poll() returns the best suggestion, you need to reverse the comparator for this to work  "
        },
        {
            "author": "James Dyer",
            "id": "comment-13043898",
            "date": "2011-06-03T17:34:49+0000",
            "content": "Here's another patch.  This time PossibilityIterator is guaranteed not to save/return more than the # of collations the user requested with \"maxCollationTries\".\n\nChanging this also invalidated some of the tests in SpellCheckCollatorTest.java .  My research indicates this is because many of the possibilities end up with the same score so this is not indicative of a new bug.  I changed the test to be less brittle in this regard.\n\nWhile I generally like both of these last two patches, I am still unsure of the wisdom of this last change.  It is true this last change ensures we never will store more Collations than the app might possibly use.  On the other hand, the Collations ought to enter the PQ somewhat sorted already.  Having it churn in/out all of the low-ranking ones introduces a lot of extra add/remove operations for the common cases in return for saving a bit of memory in the more rare cases. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043913",
            "date": "2011-06-03T17:57:12+0000",
            "content": "Hi, this is looking much better!\n\nTo eliminate your last concerns, I think what is needed is a tie-breaker in RankedSpellPossibility's comparator, ideally something like a sequential identifier.\n\nThis way, when the PQ fills, and the lowest score is 20, then subsequent items that also have a score of 20 will not be competitive, and rejected by the competitive check... remember peek() is constant time \n\nThis is the way Lucene collectors work (tiebreaker is lucene docid), and the way e.g. FuzzyQuery/FuzzyLikeThisQuery works (tiebreaker is term text) "
        },
        {
            "author": "James Dyer",
            "id": "comment-13043976",
            "date": "2011-06-03T19:02:18+0000",
            "content": "This version reduces churn in the PriorityQueue.  Rather than add a tie-breaker variable, I changed the rank comparison from > to >= ... This made SpellCheckCollatorTest.testCollateWithFilter() do 11 Adds and 1 Remove instead of 13 Adds and 3 Removes.  \n\nThe 4 spell-check-related tests I mentioned before still pass.   "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043981",
            "date": "2011-06-03T19:08:25+0000",
            "content": "great! So what about the 50ms time? can we eliminate this now?\n\nit seems unrelated to this issue (memory usage), as the memory usage is now constant... \n\nI think I would prefer if we want to have a time-based limit that we do this independently (and in a way where it can be configured)\n\nOther than this, I think its ready to commit! "
        },
        {
            "author": "James Dyer",
            "id": "comment-13043991",
            "date": "2011-06-03T19:32:44+0000",
            "content": "Yeah, the I agree the time limit is a bit of a hack.  On the other hand, the list of possibilities it needs to evaluate can get really long really fast.  If you're returning 15 or 20 suggestions per word and the user misspells 10 or so words, you get a pretty big list of combinations (in our case users were pasting the URL in the search box generating a query with 12 \"misspelled\" words...)  Then again, this latest version is much faster than what I had put out there originally...\n\nMaybe we can just put a hard limit on the number of possibilities it will evaluate?  It could be really high like a million or something.  We could make it a configurable parameter, something like \"spellcheck.maxCollationPossibilitiesToEval\" , but then again that seems silly.  Who would really change it if a million was the default ?\n\nAt the end of the day, I'd feel better where I am at if Solr had some kind of secondary fallback here.  One thing that really made me nervous about our previous search engine is it wasn't terribly hard to send a query over to it that would crash the thing or make it churn a long time just to return nothing.  So far my experience is that Solr is less prone to this kind of failure and I'd really like to keep it that way... "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043992",
            "date": "2011-06-03T19:35:47+0000",
            "content": "\nMaybe we can just put a hard limit on the number of possibilities it will evaluate? It could be really high like a million or something. We could make it a configurable parameter, something like \"spellcheck.maxCollationPossibilitiesToEval\" , but then again that seems silly. Who would really change it if a million was the default ?\n\nWell, I think this sounds much better than being time-based? And you know, use your best judgement as a default, definitely I'm ok with it as long as its configurable and has good defaults. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13044006",
            "date": "2011-06-03T20:03:54+0000",
            "content": "I named the new parameter \"spellcheck.maxCollationEvaluations\" and gave it a default of 10,000.  Its very unlikely that a competitive combination will occur past there and as you requested, it is user-configurable should a different limit be desired.  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13044125",
            "date": "2011-06-03T23:27:53+0000",
            "content": "Hi James, when applying the latest patch, I noticed a test fail:\n\n    [junit] Testsuite: org.apache.solr.client.solrj.response.TestSpellCheckResponse\n    [junit] Testcase: testSpellCheckCollationResponse(org.apache.solr.client.solrj.response.TestSpellCheckResponse):\tFAILED\n    [junit] \n    [junit] junit.framework.AssertionFailedError: \n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)\n    [junit] \tat org.apache.solr.client.solrj.response.TestSpellCheckResponse.testSpellCheckCollationResponse(TestSpellCheckResponse.java:153)\n\n\n\nThis seemed odd... maybe a comparator is off somewhere? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13044901",
            "date": "2011-06-06T15:10:07+0000",
            "content": "I guess I should have run that one myself too.  This test is very similar to the ones in SpellCheckCollatorTest.  I guess while the ones in SCCT test whether or not it can collate properly, TSCR checks that the response it sends back is proper.\n\nIn any case, this is just another one of my brittle tests!  Because we're using a different comparator, results with tied scores don't come back exactly the same as before.  So now this test needs more than 5 tries to find the 2nd valid collation.  I up'ed it from 5 to 10 and now it passes. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13045028",
            "date": "2011-06-06T18:54:14+0000",
            "content": "Thanks for the explanation and updated patch James... I'll test this out shortly! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13045039",
            "date": "2011-06-06T19:19:44+0000",
            "content": "Committed revision 1132729 (trunk), 1132730 (branch_3x)\n\nThanks James! "
        },
        {
            "author": "James Dyer",
            "id": "comment-13045082",
            "date": "2011-06-06T20:39:51+0000",
            "content": "I added \"spellcheck.maxCollationEvaluations\" to the wiki.  Thanks, Robert for taking time helping get this fixed! "
        },
        {
            "author": "Peter Wolanin",
            "id": "comment-13052601",
            "date": "2011-06-21T14:52:17+0000",
            "content": "I generated a patch for 3.2 looking at the commit on branch_3x.  It looks somewhat different from the last patch by James.\n\nI also just compared the trunk commit to the last patch and it doesn't match https://issues.apache.org/jira/secure/attachment/12481574/SOLR-2462.patch  \n\nDid the wrong patch get committed, or was the final patch just never get posted to this issue before commit? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13052623",
            "date": "2011-06-21T15:40:19+0000",
            "content": "Peter,\n\nI reviewed Robert's commits (r1132730 to branch_3x ; r1132729 to trunk), and they appear to match the 06/Jun/11 15:10 version of the patch.  I looked mostly at the change in TestSpellCheckResponse.java, which is the last tweak that was made.  Keep in mind there are a few things that were committed that aren't in the patch (changes.txt, etc).  Did you have other specific discrepancies in mind? "
        },
        {
            "author": "Mitsu Hadeishi",
            "id": "comment-13057483",
            "date": "2011-06-29T21:36:53+0000",
            "content": "We just ran into this bug when we upgraded to 3.2, and suddenly SOLR was blowing up as soon as we built the spellcheck dictionary. I attempted to apply the patch to the 3.2 source code tgz file downloadable from http://www.apache.org/dyn/closer.cgi/lucene/solr, but it didn't apply cleanly. I manually applied the patch, as we're using the released version of 3.2. "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13057486",
            "date": "2011-06-29T21:41:05+0000",
            "content": "We just ran into this bug when we upgraded to 3.2\n3.3 should be released in the next two days which has a fix for this. So maybe you just check the mailinglist for the release mail tomorrow or the day after!\n\nsimon "
        },
        {
            "author": "Mitsu Hadeishi",
            "id": "comment-13058836",
            "date": "2011-07-01T22:15:17+0000",
            "content": "Oh now you tell us.  Well, we already built the patched 3.2 so we're going with that for now  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13058966",
            "date": "2011-07-02T02:43:15+0000",
            "content": "Bulk close for 3.3 "
        }
    ]
}