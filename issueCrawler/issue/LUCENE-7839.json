{
    "id": "LUCENE-7839",
    "title": "Optimize the default NormsFormat for the case that all norms are in 0..16",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Task",
        "components": [],
        "fix_versions": []
    },
    "description": "Given how we now store the length of the field in norms, we could optimize the default norms format for the case that all norms are in 0..16 and store it on 4 bits. This would be picked up for short fields that have less than 16 terms (eg. title fields) and reduce disk utilization by 2.",
    "attachments": {
        "LUCENE-7839.patch": "https://issues.apache.org/jira/secure/attachment/12869678/LUCENE-7839.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16016628",
            "date": "2017-05-18T23:07:36+0000",
            "content": "Should this really be necessary given an iterator API? It seems too highly specialized and fragile (just like what random access APIs were doing), versus using a e.g. block-level compression like the posting lists. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16023153",
            "date": "2017-05-24T16:14:30+0000",
            "content": "I tried to leverage the iterator API similarly to what numeric doc values do, but luceneutil seems to notice a performance hit:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n                HighTerm      569.71     (11.5%)      490.35      (9.0%)  -13.9% ( -30% -    7%)\n              OrHighHigh      138.08     (11.6%)      123.27      (7.1%)  -10.7% ( -26% -    9%)\n               OrHighMed      295.37     (11.2%)      269.99      (8.1%)   -8.6% ( -25% -   12%)\n               OrHighLow      379.17      (9.1%)      351.63      (6.4%)   -7.3% ( -20% -    9%)\n                 MedTerm     1518.29     (11.9%)     1421.77      (6.8%)   -6.4% ( -22% -   14%)\n             AndHighHigh      386.22      (9.3%)      367.76      (9.0%)   -4.8% ( -21% -   14%)\n                 LowTerm     3236.73      (8.3%)     3118.34      (8.3%)   -3.7% ( -18% -   14%)\n         MedSloppyPhrase      555.94      (9.6%)      537.02      (6.3%)   -3.4% ( -17% -   13%)\n   HighTermDayOfYearSort      330.62     (12.2%)      320.20      (9.8%)   -3.2% ( -22% -   21%)\n               MedPhrase      635.77      (9.6%)      616.12      (8.1%)   -3.1% ( -18% -   16%)\n        HighSloppyPhrase      147.02      (8.6%)      142.77      (7.9%)   -2.9% ( -17% -   14%)\n                  IntNRQ      117.56      (9.8%)      114.43     (10.2%)   -2.7% ( -20% -   19%)\n            HighSpanNear       57.73      (7.9%)       56.21      (7.4%)   -2.6% ( -16% -   13%)\n         LowSloppyPhrase      385.52      (8.9%)      375.39      (6.5%)   -2.6% ( -16% -   13%)\n               LowPhrase      653.67      (9.7%)      637.17      (7.4%)   -2.5% ( -17% -   16%)\n                 Prefix3      287.63     (12.3%)      281.78     (10.3%)   -2.0% ( -21% -   23%)\n                 Respell      144.41      (7.8%)      141.67      (6.7%)   -1.9% ( -15% -   13%)\n              AndHighMed      676.46      (8.3%)      665.05      (9.8%)   -1.7% ( -18% -   17%)\n                Wildcard      214.90      (8.5%)      211.57      (7.0%)   -1.5% ( -15% -   15%)\n              HighPhrase       20.11      (9.7%)       20.03      (8.5%)   -0.4% ( -17% -   19%)\n             MedSpanNear      476.40      (8.7%)      476.48      (7.7%)    0.0% ( -15% -   18%)\n              AndHighLow      964.81      (9.8%)      965.18      (8.0%)    0.0% ( -16% -   19%)\n       HighTermMonthSort     1190.72      (9.6%)     1194.44     (11.4%)    0.3% ( -18% -   23%)\n             LowSpanNear      421.27      (7.8%)      423.97      (9.9%)    0.6% ( -15% -   19%)\n                  Fuzzy2       49.17     (16.2%)       50.09     (19.1%)    1.9% ( -28% -   44%)\n                  Fuzzy1      129.89     (12.6%)      132.32     (11.9%)    1.9% ( -20% -   30%)\n\n\n\nYou can find the patch that I played with attached. It keeps the current levels of compression, but just splits values into blocks of 2^14 values and decides on the number of bits on a per-block basis. Maybe there is a better way to do this... ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16023248",
            "date": "2017-05-24T17:09:22+0000",
            "content": "OK, i will try to look at the patch. I'm not sure its necessary to do delta compression (max-min) in this case, in case it makes things simpler.  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16023643",
            "date": "2017-05-24T20:37:34+0000",
            "content": "Agreed. At the moment the patch does not do delta compression, it just reads plain byte/short/int/long values like master. The only difference with master is that it splits values into blocks of 16384 and encodes each block independently. It does not even specialize the case that norms are in 0..15 since I first wanted to get an idea of the performance impact of leveraging the iterator API so that a single outlier does not raise the number of bits per value for document. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16023718",
            "date": "2017-05-24T21:22:32+0000",
            "content": "Right, but this is still old-style in the sense that it could still be done with the random access API with shift+mask ... and this block compression has been done before and always gave overhead.\n\nI was suggesting much more like postings to eliminate the readByte()/readByte()/readByte() stuff for the worst case. ",
            "author": "Robert Muir"
        }
    ]
}