{
    "id": "LUCENE-2666",
    "title": "ArrayIndexOutOfBoundsException when iterating over TermDocs",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "3.0.2",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "A user got this very strange exception, and I managed to get the index that it happens on. Basically, iterating over the TermDocs causes an AAOIB exception. I easily reproduced it using the FieldCache which does exactly that (the field in question is indexed as numeric). Here is the exception:\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 114\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:104)\n\tat org.apache.lucene.index.SegmentTermDocs.next(SegmentTermDocs.java:127)\n\tat org.apache.lucene.search.FieldCacheImpl$LongCache.createValue(FieldCacheImpl.java:501)\n\tat org.apache.lucene.search.FieldCacheImpl$Cache.get(FieldCacheImpl.java:183)\n\tat org.apache.lucene.search.FieldCacheImpl.getLongs(FieldCacheImpl.java:470)\n\tat TestMe.main(TestMe.java:56)\n\nIt happens on the following segment: _26t docCount: 914 delCount: 1 delFileName: _26t_1.del\n\nAnd as you can see, it smells like a corner case (it fails for document number 912, the AIOOB happens from the deleted docs). The code to recreate it is simple:\n\n        FSDirectory dir = FSDirectory.open(new File(\"index\"));\n        IndexReader reader = IndexReader.open(dir, true);\n\n        IndexReader[] subReaders = reader.getSequentialSubReaders();\n        for (IndexReader subReader : subReaders) {\n            Field field = subReader.getClass().getSuperclass().getDeclaredField(\"si\");\n            field.setAccessible(true);\n            SegmentInfo si = (SegmentInfo) field.get(subReader);\n            System.out.println(\"--> \" + si);\n            if (si.getDocStoreSegment().contains(\"_26t\")) \n{\n                // this is the probleatic one...\n                System.out.println(\"problematic one...\");\n                FieldCache.DEFAULT.getLongs(subReader, \"__documentdate\", FieldCache.NUMERIC_UTILS_LONG_PARSER);\n            }\n        }\n\nHere is the result of a check index on that segment:\n\n  8 of 10: name=_26t docCount=914\n    compound=true\n    hasProx=true\n    numFiles=2\n    size (MB)=1.641\n    diagnostics = \n{optimize=false, mergeFactor=10, os.version=2.6.18-194.11.1.el5.centos.plus, os=Linux, mergeDocStores=true, lucene.version=3.0.2 953716 - 2010-06-11 17:13:53, source=merge, os.arch=amd64, java.version=1.6.0, java.vendor=Sun Microsystems Inc.}\n    has deletions [delFileName=_26t_1.del]\n    test: open reader.........OK [1 deleted docs]\n    test: fields..............OK [32 fields]\n    test: field norms.........OK [32 fields]\n    test: terms, freq, prox...ERROR [114]\njava.lang.ArrayIndexOutOfBoundsException: 114\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:104)\n\tat org.apache.lucene.index.SegmentTermDocs.next(SegmentTermDocs.java:127)\n\tat org.apache.lucene.index.SegmentTermPositions.next(SegmentTermPositions.java:102)\n\tat org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:616)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:509)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:299)\n\tat TestMe.main(TestMe.java:47)\n    test: stored fields.......ERROR [114]\njava.lang.ArrayIndexOutOfBoundsException: 114\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:104)\n\tat org.apache.lucene.index.ReadOnlySegmentReader.isDeleted(ReadOnlySegmentReader.java:34)\n\tat org.apache.lucene.index.CheckIndex.testStoredFields(CheckIndex.java:684)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:512)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:299)\n\tat TestMe.main(TestMe.java:47)\n    test: term vectors........ERROR [114]\njava.lang.ArrayIndexOutOfBoundsException: 114\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:104)\n\tat org.apache.lucene.index.ReadOnlySegmentReader.isDeleted(ReadOnlySegmentReader.java:34)\n\tat org.apache.lucene.index.CheckIndex.testTermVectors(CheckIndex.java:721)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:515)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:299)\n\tat TestMe.main(TestMe.java:47)\n\n\n\nThe creation of the index does not do something fancy (all defaults), though there is usage of the near real time aspect (IndexWriter#getReader) which does complicate deleted docs handling. Seems like the deleted docs got written without matching the number of docs?. Sadly, I don't have something that recreates it from scratch, but I do have the index if someone want to have a look at it (mail me directly and I will provide a download link).\n\nI will continue to investigate why this might happen, just wondering if someone stumbled on this exception before. Lucene 3.0.2 is used.",
    "attachments": {
        "checkindex-out.txt": "https://issues.apache.org/jira/secure/attachment/12469235/checkindex-out.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-09-24T10:23:22+0000",
            "content": "This looks like index corruption \u2013 somehow the deleted docs bit vector is too small for that segment.  We have to get to the root cause of how the corruption happened.\n\nEG if you can enable IndexWriter's infoStream, then get the corruption to happen, and post the resulting log...\n\nAlso, try enabling assertions... it may catch the corruption sooner.\n\nCan you describe how you use Lucene?  Do you do any direct file IO in the index dir?  (eg, for backup/restore or something).\n\nAre you certain only one writer is open on the index?  (Do you disable Lucene's locking?)\n\nWhich OS, filesystem, java impl are you using? ",
            "author": "Michael McCandless",
            "id": "comment-12914401"
        },
        {
            "date": "2011-01-14T07:15:05+0000",
            "content": "Hi, \n\nI am getting this issue as well?\nWe are doing quite a lot of update updates during indexing. Could this be causing the problem ?\n\nThis seems to only have happened when we deployed to our linux test server - it didn't appear to occur on MAC OS X during development - with the same data set.\n\nDoes this only affect Lucene 3.0.2 ? Would a rollback be a good work around ? \n\n\n\nThe exact stack strace:\n\njava.lang.ArrayIndexOutOfBoundsException: 5475\n        at org.apache.lucene.util.BitVector.get(BitVector.java:104)\n        at org.apache.lucene.index.SegmentTermDocs.next(SegmentTermDocs.java:127)\n        at org.apache.lucene.index.SegmentTermPositions.next(SegmentTermPositions.java:102)\n        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:207)\n        at org.apache.lucene.search.PhrasePositions.skipTo(PhrasePositions.java:52)\n        at org.apache.lucene.search.PhraseScorer.advance(PhraseScorer.java:120)\n        at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:249)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:218)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:199)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:177)\n        at org.apache.lucene.search.MultiSearcher$MultiSearcherCallableWithSort.call(MultiSearcher.java:410)\n        at org.apache.lucene.search.MultiSearcher.search(MultiSearcher.java:230)\n        at org.apache.lucene.search.Searcher.search(Searcher.java:49)\n\n\n ",
            "author": "Nick Pellow",
            "id": "comment-12981649"
        },
        {
            "date": "2011-01-14T07:16:24+0000",
            "content": "I've also noticed this occurring since I started using a numeric field and accessing the its field cache for boosting. ",
            "author": "Nick Pellow",
            "id": "comment-12981650"
        },
        {
            "date": "2011-01-14T17:58:01+0000",
            "content": "Can you run CheckIndex on this index and post the result?  And, enable assertions.\n\nAnd if possible turn on IndexWriter's infoStream and capture/post the output leading up to the corruption.\n\nMany updates during indexing is just fine... and I know whether rolling back to older Lucene releases will help (until we've isolated the issue).  But: maybe try rolling forward to 3.0.3?  It's possible you're hitting a big fixed in 3.0.3 (though this doesn't ring a bell for me). ",
            "author": "Michael McCandless",
            "id": "comment-12981843"
        },
        {
            "date": "2011-01-17T04:01:21+0000",
            "content": "Hi MIchael, \n\nThanks for the update. I have added an infoStream to the writer and triggered a re-index. Unfortunately, I didn't see the corruption occur this time.\nI am about to deploy to a different environment so will let you know.\n\nWe are already upgraded to Lucene 3.0.3, unfortunately.\n\nHopefully we will see the problem re-occur and be able to capture the necessary output to track down the problem.\n\nI've also added a call to writer.prepareCommit(). Previously, only writer.commit() was being called. Could that have an effect ?\n\nCheers,\nNick ",
            "author": "Nick Pellow",
            "id": "comment-12982468"
        },
        {
            "date": "2011-01-17T11:25:30+0000",
            "content": "OK thanks.  Hopefully we can catch this under infoStream's watch.\n\nNot calling prepareCommit is harmless \u2013 IW simply calls it for you under the hood when commit() is called, if you hadn't already called prepareCommit().\n\nThe two APIs are separate in case you want to involve Lucene in a 2 phased commit w/ other resources. ",
            "author": "Michael McCandless",
            "id": "comment-12982597"
        },
        {
            "date": "2011-01-24T00:56:34+0000",
            "content": "Hi Michael, \n\nWe managed to catch this happening again. I've created a bug for our project over at: http://jira.atlassian.com/browse/CRUC-5486 ( Since I can't seem to upload the log to this JIRA instance?).\n\nMy hunch is that this occurs if a search is performed at the same time as a re-index - and a lucene cache is potentially not being closed/cleared correctly.\nIt appears that a re-start of the application causes this problem to go away.\n\nCheers,\nNick.\n ",
            "author": "Nick Pellow",
            "id": "comment-12985474"
        },
        {
            "date": "2011-01-24T17:30:55+0000",
            "content": "Thanks Nick; I'll look at the log.\n\nAside: you should be able to attach files here... not sure why you saw otherwise... ",
            "author": "Michael McCandless",
            "id": "comment-12985833"
        },
        {
            "date": "2011-01-24T18:25:11+0000",
            "content": "Nick, the infoStream output looks healthy \u2013 I don't see any exceptions.  Can you post the output from CheckIndex against the index that corresponds to this infoStream? ",
            "author": "Michael McCandless",
            "id": "comment-12985873"
        },
        {
            "date": "2011-01-25T04:14:13+0000",
            "content": "Hi Michael, \n\nThanks for looking at that log.\nI ran CheckIndex on the corrupt index - and have attached the output here. It doesn't appear to have detected any problems.\n\nDo you think this problem could be caused by a cache not being flushed correctly ?\n\nCheers,\nNick ",
            "author": "Nick Pellow",
            "id": "comment-12986209"
        },
        {
            "date": "2011-01-25T18:11:03+0000",
            "content": "Hmmm \u2014 given that exception, I would expect CheckIndex to have also seen this issue.\n\nSearching at the same time as indexing shouldn't cause this.  Lucene doesn't cache postings, but does cache metadata for the term, though I can't see how that could lead to this exception.\n\nThis could also be a hardware issue?  Do you see the problem on more than one machine? ",
            "author": "Michael McCandless",
            "id": "comment-12986566"
        },
        {
            "date": "2011-01-27T01:17:20+0000",
            "content": "Hi MIchael, \n\nWe have now seen this issue on more than 1 machine. I don't think it is a hardware issue.\nWe are using the ConcurrentMergeScheduler on the writer - so not sure if that has known issues?\nA restart definitely 'fixes' this problem though.\n\nThe stack-trace is:\n\n\njava.lang.ArrayIndexOutOfBoundsException: 3740\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:104)\n\tat org.apache.lucene.index.SegmentTermDocs.next(SegmentTermDocs.java:127)\n\tat org.apache.lucene.index.SegmentTermPositions.next(SegmentTermPositions.java:102)\n\tat org.apache.lucene.search.PhrasePositions.next(PhrasePositions.java:41)\n\tat org.apache.lucene.search.PhraseScorer.init(PhraseScorer.java:147)\n\tat org.apache.lucene.search.PhraseScorer.nextDoc(PhraseScorer.java:78)\n\tat org.apache.lucene.search.DisjunctionSumScorer.initScorerDocQueue(DisjunctionSumScorer.java:101)\n\tat org.apache.lucene.search.DisjunctionSumScorer.<init>(DisjunctionSumScorer.java:85)\n\tat org.apache.lucene.search.BooleanScorer2$1.<init>(BooleanScorer2.java:154)\n\tat org.apache.lucene.search.BooleanScorer2.countingDisjunctionSumScorer(BooleanScorer2.java:149)\n\tat org.apache.lucene.search.BooleanScorer2.makeCountingSumScorerNoReq(BooleanScorer2.java:218)\n\tat org.apache.lucene.search.BooleanScorer2.makeCountingSumScorer(BooleanScorer2.java:208)\n\tat org.apache.lucene.search.BooleanScorer2.<init>(BooleanScorer2.java:101)\n\tat org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:336)\n\tat org.apache.lucene.search.function.CustomScoreQuery$CustomWeight.scorer(CustomScoreQuery.java:359)\n\tat org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:306)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:210)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:170)\n\tat org.apache.lucene.search.MultiSearcher$MultiSearcherCallableNoSort.call(MultiSearcher.java:363)\n\tat org.apache.lucene.search.MultiSearcher.search(MultiSearcher.java:208)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:98)\n\n\n\nI am going to spend some time trying to reproduce this locally today, with a debugger attached.\n\nCheers,\nNick ",
            "author": "Nick Pellow",
            "id": "comment-12987328"
        },
        {
            "date": "2011-01-27T07:19:35+0000",
            "content": "Hi Michael, \n\nWe've done some analysis on how we are using Lucene and discovered the following:\n\n\tthe only time we construct a new reader IndexReader.open(directory, true) is when we search the index for the first time since the server start.\n\tevery other time, we are using reader.reopen() each time we detect that a write has occurred to the index.\n\n                            final IndexReader newReader = oldReader.reopen();\n                            if (newReader != oldReader) {\n                                oldReader.decRef();\n                                reader = newReader;\n                            }\n\n\n\tthe bug definitely goes away when the system is restarted and a new Reader is instantiated.\n\tonce we see the AIOOBE, it happens on every search until we restart\n\trunning CheckIndex never reports any errors\n\n\n\nTherefore we believe that reader.reopen() is most likely causing certain data structures to be shared and creates inconsistency which leads to this exception.\n\nThe latest stack trace we are getting is in the comment above.\n\nGiven this information would you have any more clues for us?\n\nThank you very much for your help so far,\ngreatly appreciated.\nNick\n\n ",
            "author": "Nick Pellow",
            "id": "comment-12987415"
        },
        {
            "date": "2011-01-27T23:31:02+0000",
            "content": "Hi Michael, \n\nWe have a memory dump of the instance that is affected by this. Would you know the best place to start looking for the possibly outdated BitVector?\nWe could make this available to you if you wish - all 1.8GB of it though  \n\nCheers,\nNick\n ",
            "author": "Nick Pellow",
            "id": "comment-12987829"
        },
        {
            "date": "2011-01-28T14:01:31+0000",
            "content": "Nick, are you running Lucene w/ asserts enabled?  Are you able to take a src patch and run it through your test?  If so, I can add some verbosity/asserts and we can try to narrow this down.\n\nIt does  sound like somehow the wrong delete BitVector is getting associated w/ a SegmentReader.\n\nIt looks like you don't use NRT readers right?  Ie, you always .commit() from IW and then do IR.reopen? ",
            "author": "Michael McCandless",
            "id": "comment-12988086"
        },
        {
            "date": "2011-02-07T09:48:00+0000",
            "content": "Hi Michael, \n\nThis issue was entirely a problem with our code, and I doubt Lucene could have done a better job.\n\nThe problem was that on upgrade of the index (done when fields have changed etc), we recreate the index in the same location using \nIndexWriter.create(directory, analyzer, true, MAX_FIELD_LENGTH).\n\nSome code was added just before this however, that deleted every single file in the directory. This meant that some other thread performing a search could have seen a corrupt index, thus causing the AIOOBE. The developer was paranoid that IndexWriter.create was leaving old files lying around.\n\nI'm glad we got to the bottom of this, and very much so that it was not a bug in Lucene!\n\nThanks again for helping us track this down.\n\nBest Regards,\nNick Pellow ",
            "author": "Nick Pellow",
            "id": "comment-12991315"
        },
        {
            "date": "2011-02-07T19:46:51+0000",
            "content": "Ahh, thanks for bringing closure Nick!  Although, I'm a little confused how removing files from the index while readers are using it, could lead to those exceptions...\n\nNote that it's perfectly fine to pass create=true to IW, over an existing index, even while readers are using it; IW will gracefully remove the old files itself, even if open IRs are still using them. IW just makes a new commit point that drops all references to prior segments... ",
            "author": "Michael McCandless",
            "id": "comment-12991545"
        }
    ]
}