{
    "id": "SOLR-7090",
    "title": "Cross collection join",
    "details": {
        "components": [],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [
            "5.2",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Although SOLR-4905 supports joins across collections in Cloud mode, there are limitations,  the secondary collection must be replicated at each node where the primary collection has a replica, (ii) the secondary collection must be singly sharded.\n\nThis issue explores ideas/possibilities of cross collection joins, even across nodes. This will be helpful for users who wish to maintain boosts or signals in a secondary, more frequently updated collection, and perform query time join of these boosts/signals with results from the primary collection.",
    "attachments": {
        "SOLR-7090-fulljoin.patch": "https://issues.apache.org/jira/secure/attachment/12765898/SOLR-7090-fulljoin.patch",
        "SOLR-7090.patch": "https://issues.apache.org/jira/secure/attachment/12697457/SOLR-7090.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-09T11:08:19+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Here's an implementation for this using a value source, backed by a per core cache.\n\nHere's how to use:\n\nAdd this to solrconfig.xml's <query> section,\n\n    <cache name=\"join\"\n                class=\"solr.LRUCache\"\n                size=\"4096\"\n                initialSize=\"1024\"\n                autowarmCount=\"1024\"\n               regenerator=\"org.apache.solr.util.SolrPluginUtils$IdentityRegenerator\"\n                />\n\nAt query time, the \"coljoin\" function can be used:\ncoljoin(fromCollection,fromKey,fromVal,toKey)\n\nfromCollection: the name of the secondary/\"from\" collection to be joined from\nfromKey: the field name of the foreign key in the \"from\" collection to be joined against\nfromVal: the field name of the value to be returned from \"from\" collection\ntoKey: the field name of the key in primary collection to be joined against \n\nImplementation details:\nAll values from the secondary collection are fetched at the primary collection's cores and cached into an LRU \"join\" cache. An executor thread runs continuously in the background to update the cache (by fetching values again from secondary collection) at specified intervals (in this patch this is 2000ms). ",
            "id": "comment-14312108"
        },
        {
            "date": "2015-02-23T19:47:45+0000",
            "author": "Noble Paul",
            "content": "All this stuff goes into memory? I'm just wondering if there is a way to limit the amount of memory used , instead of throwing an OOM ",
            "id": "comment-14333717"
        },
        {
            "date": "2015-02-23T20:29:56+0000",
            "author": "Anshum Gupta",
            "content": "I am yet to really look deeper into this but here are a few things:\n1. The idea of auto-refreshing every 2 seconds looks scary to me.\n2. As Noble mentioned, in case of a cache, the size should be configurable (I see a TODO there, would be good to have that).\n3. Instead of using FieldCacheSource, make it mandatory for the field to have DocValues enabled and then just go with that? ",
            "id": "comment-14333765"
        },
        {
            "date": "2015-04-30T20:24:50+0000",
            "author": "David Smiley",
            "content": "FYI I noticed we've got two cache regenerators with identical implementations: SolrPluginUtils.IdentityRegenerator (currently showing as unused) and NoOpRegenerator.  I propose we remove the former both because it's currently not used and because referencing from solrconfig.xml is weird due to the inner class reference. ",
            "id": "comment-14522189"
        },
        {
            "date": "2015-09-30T00:34:04+0000",
            "author": "Scott Blum",
            "content": "I checked out the patch, but I found it a bit puzzling.  I couldn't figure out how to use the ValueSourceParser formulation as a drop-in replacement for a QParserPlugin.  Where would you put the query to actually select the original set of docs you want to join from?  I also saw some assumptions about keys and values being integers, which maybe I don't understand. ",
            "id": "comment-14936152"
        },
        {
            "date": "2015-09-30T20:19:56+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "I couldn't figure out how to use the ValueSourceParser formulation as a drop-in replacement for a QParserPlugin. \nIt is not supposed to be a replacement. This is a way in which some field from a secondary collection can be augmented to the results (most usually useful for use in scoring of docs), joined on a key.\n\nWhere would you put the query to actually select the original set of docs you want to join from?\nThe main query, as you see in the test, is q=::\n\n    String joinFunction = \"coljoin(collection2,id,boost_i,key_i)\";\n    \n    ModifiableSolrParams query =  new ModifiableSolrParams();\n    query.set(\"q\", \"*:*\");\n    query.set(\"sort\", \"key_i asc\");\n    query.set(\"fl\", \"id,$joinedVal\");\n    query.set(\"joinedVal\", joinFunction);\n\n\nYou can put in your main query there (q, or an fq), and add the colljoin() function in a fl or score parameter.\n\nI also saw some assumptions about keys and values being integers, which maybe I don't understand.\nOff the top of my head, I remember that that was true only for this PoC patch, and not a limitation of the approach per se. The patch can deal with integer keys only, but this can be extended to support all kinds of keys to join from/to. ",
            "id": "comment-14938789"
        },
        {
            "date": "2015-10-05T21:37:09+0000",
            "author": "Scott Blum",
            "content": "I have this basically working as a QParser.  Under the hood, it uses a distributed Facet query to collect the appropriate term list, which it then applies to the local core.\n\nThe main test works, but the random tests I'm having trouble with, and I'm not sure what I'm doing wrong.  I'm getting a different set of failures on trunk than I was getting on a similar patch against ~5.2.1.\n\nOn trunk, the final result set tends to have too few documents in it, (e.g. 10 != 7), even though the fulljoin is actually recording that it found 10 docs.  I've been digging on this but haven't figured it out yet.\n\nOn ~5.2.1, I was getting a different failure related to caching.  On index clear + commit, a fulljoin query result would get cached, and subsequent commits would not invalidate the result, so by the time a query would be performed, it would miss all but the first few docs.\n\nAny help would be much appreciated! ",
            "id": "comment-14944094"
        },
        {
            "date": "2015-10-06T21:16:51+0000",
            "author": "Scott Blum",
            "content": "I was able to figure out why my random test wasn't passing.  After committing some updates, during SolrIndexSearcher.warm(), the cache will attempt to autowarm recent queries.  In my case, it's trying to auto-warm the fulljoin.  The problem is, if I try to synchronously run my facet query during the warming process, I get the OLD facet results, which are no longer valid.  As a result I end up warming the cache with incorrect data.\n\nHow should I fix this?\n\n1) I can throw an exception from the scorer() if I detect that it's a warming=true query.  But I hate spamming errors into the logs.\n\n2) I tried running my facet query with cache=false for warming queries, but that didn't work, it still got the old results.\n\nHelp! \nScott ",
            "id": "comment-14945823"
        },
        {
            "date": "2015-10-06T21:46:55+0000",
            "author": "Scott Blum",
            "content": "Tests passing.  I'm doing something kind of hacky to avoid the auto-warm. ",
            "id": "comment-14945890"
        },
        {
            "date": "2015-10-09T20:22:22+0000",
            "author": "Scott Blum",
            "content": "All tests passing I think. ",
            "id": "comment-14951133"
        },
        {
            "date": "2016-03-14T20:20:48+0000",
            "author": "Erick Erickson",
            "content": "These two JIRAs are at least in the same ballpark if you think of an external collection as a specialized external data source in SOLR-7341.\n\nMy point in linking these is to make sure both are considered if it turns out that these should be folded together. ",
            "id": "comment-15194054"
        },
        {
            "date": "2016-03-22T04:01:01+0000",
            "author": "Erick Erickson",
            "content": "Disclaimer: I only skimmed this patch and the patch for SOLR-7341, so take this with a grain of salt.\n\nBoth of these seem, from my limited review to form a query against the \"from\" collection, return some kind of representation of the matched docs then apply those to the \"to\" query. What I'm wondering is if this is really the right way to go for these kinds of operations or whether the Streaming Aggregation process is better.\n\nMy concern is mostly that there's a fair bit of complexity here, and I'm very suspicious of the performance across large Solr collections, especially for the \"from\" collections.\n\nI'd be reluctant to see this functionality go into Solr without some performance numbers. Since we're now regularly seeing Solr used with very large corpi I have to ask whether this is complexity we want to add (and then support). I'd at least like to see what kinds of use-cases are solved by this functionality that aren't handled by Streaming Aggregation and/or whether we could implement this functionality with Streaming Aggregation instead.\n\nThe discussion changes if there are use-cases this functionality supports that we can't implement with a Streaming Aggregation solution, I'd just like to see them enumerated before we jump in with both feet.\n ",
            "id": "comment-15205740"
        },
        {
            "date": "2016-03-22T19:17:42+0000",
            "author": "Scott Blum",
            "content": "Makes sense to me.  BTW, the patch posted is not meant to be \"This is what I think we should do.\"  It was more a provision exploration of the possibilities. ",
            "id": "comment-15207079"
        },
        {
            "date": "2016-11-18T23:41:44+0000",
            "author": "Phil Phillips",
            "content": "The \"fulljoin\" patch seems to do the trick for me.  One thing I noticed is that in the facet query, I had to add facet.limit=-1 to get it to return all the results (this was for a larger index). So far, it's performing reasonably well. I can report back once I add in more documents.\n\nAs Erick suggested, I did try streaming as well. I was able to get what I wanted with a stream expression (with sort and innerJoin).  However, I could only get it to work the way I wanted if I stored the fields that I was joining/sorting on. That wouldn't be ideal in my scenario. ",
            "id": "comment-15678131"
        },
        {
            "date": "2016-11-19T00:48:29+0000",
            "author": "Erick Erickson",
            "content": "Phil:\n\nStreaming should definitely not require that the fields be stored. If you use qt=/export it should throw an error if they're not docValues. They must be docValues=true fields though.\n\nAnd you'd have to be sure and index from scratch with the fields set with docValues. IIUC if this is really a requirement, then it's a bug in streaming so could you double check the field definitions? If stored=true is required, we'll want to know b/c it sounds at this remove like a bug. ",
            "id": "comment-15678251"
        },
        {
            "date": "2016-11-19T08:09:57+0000",
            "author": "Phil Phillips",
            "content": "Aha, I didn't have the fields set with docValues. It's working a lot better with that. ",
            "id": "comment-15678833"
        },
        {
            "date": "2016-11-19T16:01:03+0000",
            "author": "Erick Erickson",
            "content": "Whew! Thanks for bringing closure. NOTE: if you only have stored=true set, then performance over large data sets would be horrible.\n\nJoel BernsteinKevin RisdenDennis Gove I'm surprised that Phil's SA example worked with DV=false, stored=true, is this expected? Seems trappy, someone gets it working on their 1,000 document set then it blows up. ",
            "id": "comment-15679470"
        },
        {
            "date": "2016-11-21T17:17:25+0000",
            "author": "Kevin Risden",
            "content": "Not 100% sure on this, but my guess is that Phil Phillips was using just a regular /select search in the streaming expression. There is nothing stopping that from a streaming expressions point of view. As you pointed out, using qt=/export and then docValues will help a lot. The simple /select search may just do regular paging (not sure it uses cursor marks?).\n\nSome items to check:\n\n\twas search() being used against normal /select without qt=/export\n\tsearch() - does it use cursormark instead of regular paging?\n\n ",
            "id": "comment-15684139"
        },
        {
            "date": "2016-11-30T16:53:55+0000",
            "author": "Dorian",
            "content": "Just for reference, distributed join is implemented in this elasticsearch plugin: https://github.com/sirensolutions/siren-join ",
            "id": "comment-15709093"
        }
    ]
}