{
    "id": "LUCENE-5797",
    "title": "improve speed of norms merging",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.10"
        ]
    },
    "description": "Today we use the following procedure:\n\n\ttrack HashSet<Long> uniqueValues, until it exceeds 256 unique values.\n\tconvert to array, sort and assign ordinals to each one\n\tcreate encoder map (HashMap<Long,Integer>) to encode each value.\n\n\n\nThis results in each value being hashed twice... but the vast majority of the time people will just be using single-byte norms and a simple array is enough for that range.",
    "attachments": {
        "LUCENE-5797.patch": "https://issues.apache.org/jira/secure/attachment/12653202/LUCENE-5797.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14047957",
            "author": "Robert Muir",
            "content": "Attached is a patch that speeds it up, but I'm not happy with the complexity.\n\nI benchmarked by indexing geonames with every field as indexed TextField with norms (160 segments), then timed merging all of these:\n\n\nSM 0 [Mon Jun 30 14:19:47 EDT 2014; Lucene Merge Thread #1]: 1533 msec to merge norms [1340190 docs]\nSM 0 [Mon Jun 30 14:19:59 EDT 2014; Lucene Merge Thread #1]: 1603 msec to merge norms [1509620 docs]\nSM 0 [Mon Jun 30 14:20:11 EDT 2014; Lucene Merge Thread #0]: 2432 msec to merge norms [1380799 docs]\nSM 0 [Mon Jun 30 14:20:13 EDT 2014; Lucene Merge Thread #1]: 3043 msec to merge norms [1601868 docs]\nSM 0 [Mon Jun 30 14:20:25 EDT 2014; Lucene Merge Thread #0]: 1785 msec to merge norms [1819675 docs]\nSM 0 [Mon Jun 30 14:21:19 EDT 2014; Lucene Merge Thread #0]: 8900 msec to merge norms [8330469 docs]\n\n\n\n\nSM 0 [Mon Jun 30 14:22:15 EDT 2014; Lucene Merge Thread #1]: 1119 msec to merge norms [1340190 docs]\nSM 0 [Mon Jun 30 14:22:26 EDT 2014; Lucene Merge Thread #1]: 1214 msec to merge norms [1509620 docs]\nSM 0 [Mon Jun 30 14:22:37 EDT 2014; Lucene Merge Thread #0]: 1110 msec to merge norms [1380799 docs]\nSM 0 [Mon Jun 30 14:22:38 EDT 2014; Lucene Merge Thread #1]: 1284 msec to merge norms [1601868 docs]\nSM 0 [Mon Jun 30 14:22:49 EDT 2014; Lucene Merge Thread #0]: 1335 msec to merge norms [1819675 docs]\nSM 0 [Mon Jun 30 14:23:41 EDT 2014; Lucene Merge Thread #0]: 6834 msec to merge norms [8330469 docs]\n\n\n\nComparing the other values (e.g. time to merge postings/stored fields) between the two runs there wasn't much noise, so I think removing all the hashing helps. ",
            "date": "2014-06-30T18:40:06+0000"
        },
        {
            "id": "comment-14048819",
            "author": "Adrien Grand",
            "content": "The patch looks good to me. I think the complexity is ok, I was just a bit confused why the size was stored as a short when looking at NormsMap out of context, maybe we could just have a comment about this limitation? ",
            "date": "2014-07-01T12:44:16+0000"
        },
        {
            "id": "comment-14048829",
            "author": "Robert Muir",
            "content": "I'll try to add an assert as well. ",
            "date": "2014-07-01T12:51:53+0000"
        },
        {
            "id": "comment-14048878",
            "author": "ASF subversion and git services",
            "content": "Commit 1607074 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1607074 ]\n\nLUCENE-5797: Optimize norms merging ",
            "date": "2014-07-01T13:37:16+0000"
        },
        {
            "id": "comment-14048891",
            "author": "ASF subversion and git services",
            "content": "Commit 1607080 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1607080 ]\n\nLUCENE-5797: Optimize norms merging ",
            "date": "2014-07-01T13:54:42+0000"
        }
    ]
}