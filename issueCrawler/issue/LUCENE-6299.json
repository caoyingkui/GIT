{
    "id": "LUCENE-6299",
    "title": "IndexWriter's enforcement of 2.1B doc limits is buggy",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "4.10.4",
            "5.0",
            "5.1",
            "6.0"
        ],
        "priority": "Blocker",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "E.g. if you pass an already > 2.1B docs to either addIndexes, it can fail to enforce properly.\n\nIW's private reserveDocs should refuse to accept negative values.\n\nIW.deleteAll fails to set the pendingNumDocs to 0.",
    "attachments": {
        "LUCENE-6299-410x.patch": "https://issues.apache.org/jira/secure/attachment/12701087/LUCENE-6299-410x.patch",
        "LUCENE-6299_addIndexes.patch": "https://issues.apache.org/jira/secure/attachment/12700974/LUCENE-6299_addIndexes.patch",
        "LUCENE-6299.patch": "https://issues.apache.org/jira/secure/attachment/12701015/LUCENE-6299.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14337485",
            "author": "Robert Muir",
            "date": "2015-02-25T23:42:13+0000",
            "content": "I wish the exception was different when it strikes at runtime too. For a DirectoryReader, IMO it should be a CorruptIndexException, since IndexWriter prevents it, and thats how it behaves to users. On the other hand for a MultiReader, the current IllegalArgument is good (you multi'd too much stuff). "
        },
        {
            "id": "comment-14337923",
            "author": "Robert Muir",
            "date": "2015-02-26T05:44:08+0000",
            "content": "Here is a patch for addIndexes.\nwe return an error before we do any copying, and don't allow any overflows.\n\nThe test is kinda evil in the way it cheats, but its fast and does the job.\n\ndisk full testing in general was broken for addIndexes in case of FSDir, because MockDir would delegate optional copyBytes method directly, and not count those bytes. Instead this mess of optional methods is cleaned up, so that MockDir checks everything, but when we run tests with the \"raw\" dir, we delegate everything directly to it (except close, for checkindex).\n "
        },
        {
            "id": "comment-14338191",
            "author": "Michael McCandless",
            "date": "2015-02-26T10:19:25+0000",
            "content": "Thanks Rob, here's a new patch merged with yours.\n\nI added tests + fixes for the other issues, and discovered a\nridiculous (index corruption) bug: on init IndexWriter does not set\nits pendingNumDocs to the number of docs already in the index.\n\nSo this check is completely broken and it's trivial to make a corrupt\nindex today!  Grrr... I added a test and fixed that.\n\nI also added a test for addIndexes(CodecReader[]) too, fixed\nBaseCompositeReader to throw CorruptIndexException when it's a\nDirectoryReader, changed the exception to IllegalArgumentException,\nchanged reserveDocs to just take a long and simplified the checking in\naddIndexes (just calls reserveDocs).\n\nGiven that this is an index corruption issue I think we should\nback-port for 4.10.4, but on back-port I would only do the minimal bug\nfixes here. "
        },
        {
            "id": "comment-14338308",
            "author": "Robert Muir",
            "date": "2015-02-26T12:14:20+0000",
            "content": "We should rethink how the patch adds CorruptIndexException to DR etc signatures. I don't like this. I would prefer IOException.\n\nmaking it corruptindexexception means every single user has to think about this crazy ass corner case, then they are left thinking 'ok what should my catch block do if the index is corrupt'. It makes things too hard to use. "
        },
        {
            "id": "comment-14338312",
            "author": "Robert Muir",
            "date": "2015-02-26T12:18:54+0000",
            "content": "Also on 4.x, we should not change the signatures of DR etc to 'throws IOException' either, I think its too heavy of a break?\n\nLets just sneaky-throw the CorruptIndexException or something for 4.10.x? "
        },
        {
            "id": "comment-14338409",
            "author": "Michael McCandless",
            "date": "2015-02-26T14:03:56+0000",
            "content": "I would prefer IOException.\n\nIOExc is a good idea, I'll switch to that.\n\nAlso on 4.x, we should not change the signatures of DR etc to 'throws IOException' either, I think its too heavy of a break?\n\nI think we shouldn't backport that change to 4.x?  I was just going to fix the IW bugs and add the test cases (except maybe the addIndexes test cases since they added RawDirectoryWrapper). "
        },
        {
            "id": "comment-14338459",
            "author": "Robert Muir",
            "date": "2015-02-26T14:30:50+0000",
            "content": "sounds great. "
        },
        {
            "id": "comment-14338506",
            "author": "Michael McCandless",
            "date": "2015-02-26T15:06:47+0000",
            "content": "Here's what I'd like to commit for 4.10.x.  It fixes IW.deleteAll to set pending doc count to 0, IW ctor to set pending doc count to maxDoc, and both addIndexes to not overflow int when incoming indices are already too many docs.  It also switches to IllegalArgumentExc. "
        },
        {
            "id": "comment-14338542",
            "author": "Michael McCandless",
            "date": "2015-02-26T15:22:44+0000",
            "content": "New patch for 5.x / trunk, cutting over to IOException.  I think it's ready... "
        },
        {
            "id": "comment-14338569",
            "author": "Robert Muir",
            "date": "2015-02-26T15:38:54+0000",
            "content": "Can we fix javadocs on SIS.totalDocCount() [its being used more often here]. Its extremely confusing as I try to think about deletions, since it says it does not include deletions, but in fact it does. "
        },
        {
            "id": "comment-14338574",
            "author": "Robert Muir",
            "date": "2015-02-26T15:42:16+0000",
            "content": "I also think we shoudl open a followup issues to rename all these to be intuitive, e.g. totalNumDocs() -> totalMaxDoc(), getDocCount() -> maxDoc() and so on. this will make life easier on everyone and prevent bugs. "
        },
        {
            "id": "comment-14338588",
            "author": "Robert Muir",
            "date": "2015-02-26T15:48:18+0000",
            "content": "I think i figured out the problem i found with MDW. if you accdientally do MDW(MDW) i was seeing corruption, i think the problem is that MDW sometimes doesnt delegate fsync calls (to speed up tests). if you look at the code you will see it does a crazy unwrap-instanceof NRTCaching because this behavior is already problematic there.\n\nNow we have DisableFSyncFS inserted most of the time into tests, so i think this should just be removed. This means the fsync calls are no-ops but its happening at a lower level and Directories are unaware. "
        },
        {
            "id": "comment-14338606",
            "author": "Michael McCandless",
            "date": "2015-02-26T15:59:32+0000",
            "content": "Can we fix javadocs on SIS.totalDocCount()\n\nGood point, I'll change to:\n\n\n  /** Returns sum of all segment's maxDoc. */\n\n\n\nI'll open a separate issue to rename SIS.totalDocCount -> getTotalMaxDoc and SI.docCount SI.maxDoc.\n\nI also think we shoudl open a followup issues to rename all these to be intuitive,\n\n+1, I'll open followup.\n\nNow we have DisableFSyncFS inserted most of the time into tests, so i think this should just be removed.\n\n+1, sneaky! "
        },
        {
            "id": "comment-14338621",
            "author": "Robert Muir",
            "date": "2015-02-26T16:10:58+0000",
            "content": "I updated patch with the MDW fixes, and added a test. "
        },
        {
            "id": "comment-14338638",
            "author": "Michael McCandless",
            "date": "2015-02-26T16:24:16+0000",
            "content": "Thanks for fixing MDW Rob! "
        },
        {
            "id": "comment-14338675",
            "author": "Robert Muir",
            "date": "2015-02-26T16:43:58+0000",
            "content": "I don't like how deleteAll unconditionally sets pendingNumDocs. We are synced on lots of things, but everywhere else except startup is using atomic addAndGet with deltas so i don't worry. Can we do the same here?\n\nI also don't like how addindexes now does the reserveDocs before a bunch of i/o operations. stuff could go wrong during copySegmentAsIs/etc and this is actually fine, it wont corrupt the writer, so why should it leave it with a bloated pendingNumDocs? Previously this happened right before checkpoint. "
        },
        {
            "id": "comment-14338689",
            "author": "Robert Muir",
            "date": "2015-02-26T16:56:55+0000",
            "content": "Also the new ctor check at startup, we don't check that this value is within bounds. Given we know this stuff didnt work right/exist before, I think it would be nice to check and throw corruptindexexception on startup. "
        },
        {
            "id": "comment-14338726",
            "author": "Robert Muir",
            "date": "2015-02-26T17:25:32+0000",
            "content": "Updated patch. I added a test for opening a writer on a too-big index.\n\nSIS.readCommit() checks and throws CorruptIndexException. This also means tools like CheckIndex detect it too always. I also added an assertion inside its totalNumDocs method. "
        },
        {
            "id": "comment-14338729",
            "author": "Michael McCandless",
            "date": "2015-02-26T17:26:03+0000",
            "content": "I don't like how deleteAll unconditionally sets pendingNumDocs. We are synced on lots of things, but everywhere else except startup is using atomic addAndGet with deltas so i don't worry. Can we do the same here?\n\nOK, you mean something like pendingNumDocs.addAndGet(-pendingNumDocs.get())?\n\nI also don't like how addindexes now does the reserveDocs before a bunch of i/o operations.\n\nThat's a good point ... I'll put it back where it was before, and put the \"best effort\" check back in both addIndexes. "
        },
        {
            "id": "comment-14338751",
            "author": "Michael McCandless",
            "date": "2015-02-26T17:34:20+0000",
            "content": "I made a branch here: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene6299 "
        },
        {
            "id": "comment-14338754",
            "author": "ASF subversion and git services",
            "date": "2015-02-26T17:35:32+0000",
            "content": "Commit 1662503 from Michael McCandless in branch 'dev/branches/lucene6299'\n[ https://svn.apache.org/r1662503 ]\n\nLUCENE-6299: make branch "
        },
        {
            "id": "comment-14338761",
            "author": "ASF subversion and git services",
            "date": "2015-02-26T17:38:04+0000",
            "content": "Commit 1662504 from Michael McCandless in branch 'dev/branches/lucene6299'\n[ https://svn.apache.org/r1662504 ]\n\nLUCENE-6299: commit rob's last patch "
        },
        {
            "id": "comment-14338764",
            "author": "Robert Muir",
            "date": "2015-02-26T17:40:58+0000",
            "content": "\nOK, you mean something like pendingNumDocs.addAndGet(-pendingNumDocs.get())?\n\nNo, i mean why can't we compute it from the segments we drop and just subtract that? "
        },
        {
            "id": "comment-14338985",
            "author": "Michael McCandless",
            "date": "2015-02-26T19:37:27+0000",
            "content": "No, i mean why can't we compute it from the segments we drop and just subtract that?\n\nOK I committed a change on the branch to do this ... and put addIndexes back to \"best-effort up front and true reserveDocs just before changing SIS\". "
        },
        {
            "id": "comment-14338992",
            "author": "ASF subversion and git services",
            "date": "2015-02-26T19:39:17+0000",
            "content": "Commit 1662544 from Michael McCandless in branch 'dev/branches/lucene6299'\n[ https://svn.apache.org/r1662544 ]\n\nLUCENE-6299: fix deleteAll to deduct from pendingNumDocs more carefully; fix addIndexes to do best-effort check up front and only reserve for real just before changing SIS "
        },
        {
            "id": "comment-14339038",
            "author": "Robert Muir",
            "date": "2015-02-26T19:53:01+0000",
            "content": "+1 "
        },
        {
            "id": "comment-14339188",
            "author": "ASF subversion and git services",
            "date": "2015-02-26T21:26:25+0000",
            "content": "Commit 1662571 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1662571 ]\n\nLUCENE-6299: IndexWriter was failing to enforce the 2.1 billion doc limit in one index "
        },
        {
            "id": "comment-14339251",
            "author": "ASF subversion and git services",
            "date": "2015-02-26T21:56:15+0000",
            "content": "Commit 1662576 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1662576 ]\n\nLUCENE-6299: IndexWriter was failing to enforce the 2.1 billion doc limit "
        },
        {
            "id": "comment-14339973",
            "author": "ASF subversion and git services",
            "date": "2015-02-27T10:22:56+0000",
            "content": "Commit 1662654 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1662654 ]\n\nLUCENE-6299: IndexWriter was failing to enforce the 2.1 billion doc limit "
        },
        {
            "id": "comment-14344830",
            "author": "Michael McCandless",
            "date": "2015-03-03T09:35:43+0000",
            "content": "Argh: the CHANGES.txt entry for this issue went into the wrong place in 4.10.x, even though the fix was in fact committed for 4.10.4.... "
        },
        {
            "id": "comment-14348908",
            "author": "Michael McCandless",
            "date": "2015-03-05T15:36:27+0000",
            "content": "Bulk close for 4.10.4 release "
        }
    ]
}