{
    "id": "SOLR-2880",
    "title": "Investigate adding an overseer that can assign shards, later do re-balancing, etc",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "",
    "attachments": {
        "SOLR-2880.patch": "https://issues.apache.org/jira/secure/attachment/12505009/SOLR-2880.patch",
        "SOLR-2880-merge-elections.patch": "https://issues.apache.org/jira/secure/attachment/12505476/SOLR-2880-merge-elections.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Sami Siren",
            "id": "comment-13156692",
            "date": "2011-11-24T13:19:31+0000",
            "content": "This patch implements a simple overseer (cluster leader). Currently it does not do more than assign shard ids to cores so there is no added functionality. I wanted to post what I currently have here to get some feedback and improvement ideas.\n\nTwo new top level zkNodes are created:\n\n/node_assignments\n/node_states\n\nBasically the integration point is the ZKController:\n\nWhen a core is registered its state is registered locally and nodes current state is published under /node_states\n\nWhen overseer assigns a shard id the state is stored locally (to overseer) and the assignments for the node are published under /node_assignments.\n\nOverseer gets the number of shards required for a collection from the collections properties in ZK. So if you wanted to add a new collection you'd create a new collection node and record the required number of shards (among other things like the used configuration) into its properties.\n\nEven if a node hosts multiple cores it only creates single node (/node_states/<id>) and reads a single node (/node_assignments/<id>). It might be simpler to have multiple subnodes (one for each core) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13157272",
            "date": "2011-11-25T18:04:46+0000",
            "content": "Nice, thanks Sami! I'd like to get this committed to the branch quickly so that we don't have to juggle patch updates much. \n\nDo all tests pass yet for you? I see the full distrb test failing - before I look into it, just wanted to make sure it's expected - if so, I'm happy to dig in.\n\nAlso, quick comment on the node assignments zookeeper node - do we really need it? Can't we figure out the assignments from the clusterstate info? Does it add anything?\n\nMore comments to come. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13158327",
            "date": "2011-11-28T10:06:57+0000",
            "content": "Do all tests pass yet for you\n\nI think there might be atleast something timing related: I do not see failures every time. I also see some failures that seems unrelated, like:\nin SoftAutoCommitTest, CoreAdminHandlerTest#testCoreAdminHandler, TestCoreContainer#testPersist\n\nb1. Also, quick comment on the node assignments zookeeper node - do we really need it? Can't we figure out the assignments from the clusterstate info? Does it add anything?\n\nThe motivation for adding it was that it might reduce the amount of data that needs to be transferred but currently it does not do even that because ZKController still uses the state -> perhaps the assignments node should be dropped. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13158398",
            "date": "2011-11-28T12:57:42+0000",
            "content": "I also see some failures that seems unrelated, like: in SoftAutoCommitTest, CoreAdminHandlerTest#testCoreAdminHandler, TestCoreContainer#testPersist\n\nThose went away with svn up. However I now see TestRecovery fail constantly when running solr tests:\n\n\n<testcase classname=\"org.apache.solr.search.TestRecovery\" name=\"testLogReplay\" time=\"60.774\">\n    <failure type=\"junit.framework.AssertionFailedError\">junit.framework.AssertionFailedError: \n        at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)\n        at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)\n        at org.apache.solr.search.TestRecovery.testLogReplay(TestRecovery.java:82)\n        at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:521)\n</failure>\n  </testcase>\n\n\n\nIf I only run that single test it passes. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13158403",
            "date": "2011-11-28T13:02:58+0000",
            "content": "However I now see TestRecovery fail constantly when running solr tests:\n\nProbably my fault - I removed the previous version update processor and switched the tests to use the new distributed update processor.  I'll try a fresh checkout (too many unrelated changes in my current copy). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13158460",
            "date": "2011-11-28T14:03:08+0000",
            "content": "Thanks Sami - all tests are passing for me at the moment.\n\nIs there much that is special about the overseer election code vs the leader election code? Seems there is a lot of code dupe there, both in the classes and the tests. Can we merge that up? "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13159228",
            "date": "2011-11-29T12:50:09+0000",
            "content": "The attached patch merges the election code. The OverseerElectionTest should be removed because it now has nothing special to test. SliceLeaderElector should perhaps be renamed to LeaderElector or something like that. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13159248",
            "date": "2011-11-29T13:17:52+0000",
            "content": "Committed - thanks Sami! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13160500",
            "date": "2011-12-01T00:25:36+0000",
            "content": "Would it be more convenient to store whats on the nodes in node_states and node_assigments as human readable? Like the ZkNodeProps?\n\nI actually just noticed we use the same ZKNodeProps for the current leader zk node as we use for the node in the cluster state - not a problem for info that doesn't change, but now that we store the \"active,replicating\" state there, as well as other properties that could be dynamic, it ends up with stale data as those properties change. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13160946",
            "date": "2011-12-01T15:26:58+0000",
            "content": "Would it be more convenient to store whats on the nodes in node_states and node_assigments as human readable\n\nI agree, I'll change that.\n\nI actually just noticed we use the same ZKNodeProps for the current leader zk node as we use for the node in the cluster state\n\n\tnot a problem for info that doesn't change, but now that we store the \"active,replicating\" state there, as well as other properties that could be dynamic, it ends up with stale data as those properties change.\n\n\n\nWho uses the information from the leader node (what is actually needed there?) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13161833",
            "date": "2011-12-02T20:23:25+0000",
            "content": "Who uses the information from the leader node (what is actually needed there?)\njust the url right now I think. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13161836",
            "date": "2011-12-02T20:24:22+0000",
            "content": "Why does the overseer class have it's own cloud state and watches on live nodes and stuff? Isn't this all just redundant? The ZkControllers ZkStateReader is already tracking all this stuff and should be the owner of the cloud state, shouldn't it? "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13162811",
            "date": "2011-12-05T14:42:28+0000",
            "content": "Why does the overseer class have it's own cloud state and watches on live nodes and stuff?\n\nThe watch for live nodes is also used for adding watches for node states: when a new node pops up a watch is generated for /node_states/<node-name>\n\nThe ZkControllers ZkStateReader is already tracking all this stuff and should be the owner of the cloud state, shouldn't it?\n\nYeah, makes sense. I'll see how that would work. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13163792",
            "date": "2011-12-06T20:13:39+0000",
            "content": "For those following along, here's what the ZK layout looks like after starting 4 nodes (with the default 3 shards).\n\n\n...../NODE_ASSIGNMENTS (v=0 children=4)\n          ROGUE:7574_SOLR (v=4) \"[{ \"_collection\":\"collection1\", \"_core\":\"Rogue:7574_solr_\", \"shard_name\":\"shard2\"}]\"\n          ROGUE:2222_SOLR (v=2) \"[{ \"_collection\":\"collection1\", \"_core\":\"Rogue:2222_solr_\", \"shard_name\":\"shard1\"}]\"\n          ROGUE:1111_SOLR (v=2) \"[{ \"_collection\":\"collection1\", \"_core\":\"Rogue:1111_solr_\", \"shard_name\":\"shard3\"}]\"\n          ROGUE:8983_SOLR (v=2) \"[{ \"_collection\":\"collection1\", \"_core\":\"Rogue:8983_solr_\", \"shard_name\":\"shard1\"}]\"\n     /NODE_STATES (v=0 children=4)\n          ROGUE:7574_SOLR (v=4) \"[{ \"shard_id\":\"shard2\", \"_collection\":\"collection1\", \"roles\":\"\", \"_core\":\"Rogue:7574_solr_\", \"s...\"\n          ROGUE:2222_SOLR (v=2) \"[{ \"shard_id\":\"shard1\", \"_collection\":\"collection1\", \"roles\":\"\", \"_core\":\"Rogue:2222_solr_\", \"s...\"\n          ROGUE:1111_SOLR (v=2) \"[{ \"shard_id\":\"shard3\", \"_collection\":\"collection1\", \"roles\":\"\", \"_core\":\"Rogue:1111_solr_\", \"s...\"\n          ROGUE:8983_SOLR (v=2) \"[{ \"shard_id\":\"shard1\", \"_collection\":\"collection1\", \"roles\":\"\", \"_core\":\"Rogue:8983_solr_\", \"s...\"\n     /ZOOKEEPER (v=0 children=1) \"\"\n          QUOTA (v=0) \"\"\n     /CLUSTERSTATE.JSON (v=10) \"{\"collection1\":{ \"shard1\":{ \"Rogue:2222_solr_\":{ \"shard_id\":\"shard1\", \"_collection\":\"collection1...\"\n     /LIVE_NODES (v=4 children=3)\n          ROGUE:2222_SOLR (ephemeral v=0)\n          ROGUE:1111_SOLR (ephemeral v=0)\n          ROGUE:8983_SOLR (ephemeral v=0)\n     /COLLECTIONS (v=1 children=1)\n          COLLECTION1 (v=0 children=2) \"{ \"configName\":\"myconf\", \"num_shards\":\"3\"}\"\n               SHARDS_LOCK (v=0)\n               LEADER_ELECT (v=0 children=3)\n                    SHARD1 (v=0 children=2)\n                         ELECTION (v=0 children=2)\n                              N_0000000001 (ephemeral v=0)\n                              N_0000000000 (ephemeral v=0)\n                         LEADER (v=0 children=1)\n                              ROGUE:8983_SOLR_ (ephemeral v=0) \"{ \"shard_id\":\"shard1\", \"roles\":\"\", \"state\":\"recovering\", \"node_name\":\"Rogue:8983_solr\", \"url\":\"...\"\n                    SHARD2 (v=0 children=2)\n                         ELECTION (v=0 children=1)\n                              N_0000000000 (ephemeral v=0)\n                         LEADER (v=0 children=1)\n                              ROGUE:7574_SOLR_ (ephemeral v=0) \"{ \"shard_id\":\"shard2\", \"roles\":\"\", \"state\":\"recovering\", \"node_name\":\"Rogue:7574_solr\", \"url\":\"...\"\n                    SHARD3 (v=0 children=2)\n                         ELECTION (v=0 children=1)\n                              N_0000000000 (ephemeral v=0)\n                         LEADER (v=0 children=1)\n                              ROGUE:1111_SOLR_ (ephemeral v=0) \"{ \"shard_id\":\"shard3\", \"roles\":\"\", \"state\":\"recovering\", \"node_name\":\"Rogue:1111_solr\", \"url\":\"...\"\n     /OVERSEER_ELECT (v=0 children=2)\n          ELECTION (v=0 children=4)\n               N_0000000004 (ephemeral v=0)\n               N_0000000003 (ephemeral v=0)\n               N_0000000001 (ephemeral v=0)\n               N_0000000000 (ephemeral v=0)\n          LEADER (v=0)\n\n\n\nNotes: Starting up the first node by bootstrapping fails the first time you try.  Run it again and it works.\njava -Dbootstrap_confdir=./solr/conf -Dcollection.configName=myconf -DzkRun -jar start.jar "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13163809",
            "date": "2011-12-06T20:31:10+0000",
            "content": "Here's the data on some of the bigger nodes:\n\n\n/node_assignments/Rogue:7574_solr\n[{\n    \"_collection\":\"collection1\",\n    \"_core\":\"Rogue:7574_solr_\",\n    \"shard_name\":\"shard2\"}]\n\n\n/node_states/Rogue:7574_solr\n[{\n    \"shard_id\":\"shard2\",\n    \"_collection\":\"collection1\",\n    \"roles\":\"\",\n    \"_core\":\"Rogue:7574_solr_\",\n    \"state\":\"active\",\n    \"node_name\":\"Rogue:7574_solr\",\n    \"url\":\"http://Rogue:7574/solr/\"}]\n\n\n/collections/collection1/leader_elect/shard2/leader/Rogue:7574_solr_\n{\n  \"shard_id\":\"shard2\",\n  \"roles\":\"\",\n  \"state\":\"recovering\",\n  \"node_name\":\"Rogue:7574_solr\",\n  \"url\":\"http://Rogue:7574/solr/\"}\n\n\n/clusterstate.json\n{\"collection1\":{\n    \"shard1\":{\n      \"Rogue:2222_solr_\":{\n        \"shard_id\":\"shard1\",\n        \"_collection\":\"collection1\",\n        \"roles\":\"\",\n        \"_core\":\"Rogue:2222_solr_\",\n        \"state\":\"active\",\n        \"node_name\":\"Rogue:2222_solr\",\n        \"url\":\"http://Rogue:2222/solr/\"},\n      \"Rogue:8983_solr_\":{\n        \"shard_id\":\"shard1\",\n        \"_collection\":\"collection1\",\n        \"roles\":\"\",\n        \"_core\":\"Rogue:8983_solr_\",\n        \"state\":\"active\",\n        \"node_name\":\"Rogue:8983_solr\",\n        \"url\":\"http://Rogue:8983/solr/\"}},\n    \"shard2\":{\"Rogue:7574_solr_\":{\n        \"shard_id\":\"shard2\",\n        \"_collection\":\"collection1\",\n        \"roles\":\"\",\n        \"_core\":\"Rogue:7574_solr_\",\n        \"state\":\"active\",\n        \"node_name\":\"Rogue:7574_solr\",\n        \"url\":\"http://Rogue:7574/solr/\"}},\n    \"shard3\":{\"Rogue:1111_solr_\":{\n        \"shard_id\":\"shard3\",\n        \"_collection\":\"collection1\",\n        \"roles\":\"\",\n        \"_core\":\"Rogue:1111_solr_\",\n        \"state\":\"active\",\n        \"node_name\":\"Rogue:1111_solr\",\n        \"url\":\"http://Rogue:1111/solr/\"}}}}\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13164457",
            "date": "2011-12-07T15:28:50+0000",
            "content": "Some random comments so far... Naming:\n\n\tnumShards vs num_shards... we should try to make system properties consistent with the names that actually appear in ZK\n\t_core, _collection? why the underscores?\n\n\n\nI'm not sure num_shards belongs as a configuration item anywhere (in solr.xml or as a collection property in ZK). The number of shards a collection has is always just the number you see in ZK under the collection. This will make it easier for people with custom sharding to just add another shard. Whoever is creating the initial layout should thus create all of the shards at once. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13165249",
            "date": "2011-12-08T15:30:28+0000",
            "content": "Here's a patch that fixes a bug that causes some state updates to be missed when overseer node crashes. It also removes the \"_\" prefixes from the mentioned property names and use numShards as shard count property name. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13165256",
            "date": "2011-12-08T15:36:35+0000",
            "content": "This will make it easier for people with custom sharding to just add another shard\n\nYeah, that's a good point. Does the \"initial layout creation\" code exist anywhere yet? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165280",
            "date": "2011-12-08T15:50:36+0000",
            "content": "Thanks Sami! Committed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165298",
            "date": "2011-12-08T16:21:35+0000",
            "content": " The number of shards a collection has is always just the number you see in ZK under the collection. This will make it easier for people with custom sharding to just add another shard. Whoever is creating the initial layout should thus create all of the shards at once. \n\nThis seems more complicated to me?\n\nCustom shards are easy at the moment - numShards is only for the auto sharder - other code that needs to know the number of shards (like around hashing) simply counts the number of shards. If you add another shard manually, it should be no problem.\n\nnumShards should just be called autoNumShards or something perhaps. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165302",
            "date": "2011-12-08T16:27:04+0000",
            "content": "Now that I got my initial thought down...\n\nAfter adding your custom shard, you would then have to change the autoShardNum for further replicas to be placed correctly with the current code...\n\nThe other option seems to be perhaps starting up all your nodes, each is a new shard, and then you make some call that says you are done adding shards. I didn't really like that compared to just passing a sys prop up front, but perhaps it is better in the end... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165337",
            "date": "2011-12-08T17:20:43+0000",
            "content": "Notes: Starting up the first node by bootstrapping fails the first time you try. Run it again and it works.\njava -Dbootstrap_confdir=./solr/conf -Dcollection.configName=myconf -DzkRun -jar start.jar \n\nWhats the failure? I have not seen this. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13165401",
            "date": "2011-12-08T18:18:51+0000",
            "content": "I didn't really like that compared to just passing a sys prop up front\n\nThat's what I'm proposing - that numShards not be stored anywhere in ZK at all.  Creation of a new collection requires someone creating some initial nodes (i.e. the collection node at a minimum).  Currently I believe this is just the bootstrapping code \n\nScenario 1: autosharding\nWhoever creates the new collection also creates a placeholder for each shard.  The current bootstrapping code can look at a numShards system property, or a create collection API could have a numShards parameter.\n\nScenario 2: custom sharding\nCreate the new collection as normal, but just don't create any placeholders for each shard.  Adding a new shard is just \n\nWe also shouldn't rely on the number of shards to split up the hash range (except initially when creating a collection) - each shard should advertise what it's range is (in the case of autosharding or sharding by hash).   This is important for future splitting of shards (i.e. you could concurrently have shards that covers 0-9, 0-4, and 5-9)\n\nSo it seems like numShards is really just an input into new collection creation, not an intrinsic property of the collection. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165416",
            "date": "2011-12-08T18:36:32+0000",
            "content": "Okay, that's more in line with what I was originally thinking. Nothing actually uses the hsarNim in zk - just something that came in with Samis changes and is floating. In terms of shards advertising ranges, certainly it's something we eventually need to store in zk - it's not necessary until we support adding/removing shards though. It's easy to add when we need it and doesn't give us anything until we do. And you will still need to determine those ranges based on the number of shards.  "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13165429",
            "date": "2011-12-08T18:53:36+0000",
            "content": "> Nothing actually uses the hsarNim in zk\n\nIt's used in shard assignment, if the prop does not exist in collection node the system property is used. I thought it would be nice to have it in configuration so that you'd not need to boot the node that runs overseer with the system property on. But it seems that if there will be a createCollection method somewhere that creates the initial layout the property is not needed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13165500",
            "date": "2011-12-08T20:05:32+0000",
            "content": "Got you - my orig plan until we cane up with something better is that you'd always pass the syspeop and up the number when you want to reshard. Not ideal, but worked as a start. I put it in solr.xml because I was handling other props that way but didn't intend for it to stay there. Actually forgot it was there. Currently though, always passing a sys prop seems easier than changing it in zk - of course it doesnt address multiple collections served by the same instance. Long term, none of this seems ideal.  "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13166077",
            "date": "2011-12-09T11:15:22+0000",
            "content": "The attached patch removes the numShards property from the collection node. It also simplifies Overseer by converting it to use ZkStateReader instead of maintaining the read side of cloudstate internally and by removing bunch of not useful code. I also removed the unneeded IOException from method signatures that I added when I converted the serialization to json (they become obsolete when Yonik improved the ser/deser code). "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13166086",
            "date": "2011-12-09T11:16:37+0000",
            "content": "The correct patch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13166192",
            "date": "2011-12-09T13:59:23+0000",
            "content": "Committed. Thanks Sami! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13166645",
            "date": "2011-12-09T23:00:25+0000",
            "content": "just saw this random fail\n\n    [junit] Testcase: testOverseerFailure(org.apache.solr.cloud.OverseerTest):\tCaused an ERROR\n    [junit] (null)\n    [junit] java.lang.NullPointerException\n    [junit] \tat org.apache.solr.cloud.OverseerTest.waitForSliceCount(OverseerTest.java:142)\n    [junit] \tat org.apache.solr.cloud.OverseerTest.testOverseerFailure(OverseerTest.java:319)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTe "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13166679",
            "date": "2011-12-09T23:47:53+0000",
            "content": "just got another:\n    [junit] Testsuite: org.apache.solr.cloud.OverseerTest\n    [junit] Testcase: testOverseerFailure(org.apache.solr.cloud.OverseerTest):\tCaused an ERROR\n    [junit] KeeperErrorCode = NoNode for /node_states/node1\n    [junit] org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /node_states/node1\n    [junit] \tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n    [junit] \tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n    [junit] \tat org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1228)\n    [junit] \tat org.apache.solr.common.cloud.SolrZkClient.setData(SolrZkClient.java:242)\n    [junit] \tat org.apache.solr.common.cloud.SolrZkClient.setData(SolrZkClient.java:411)\n    [junit] \tat org.apache.solr.cloud.OverseerTest.testOverseerFailure(OverseerTest.java:316)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] Tests run: 3, Failures: 0, Errors: 1, Time elapsed: 5.118 sec  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13166710",
            "date": "2011-12-10T00:30:12+0000",
            "content": "I raised the max iterations timeout for the first fail. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13171046",
            "date": "2011-12-16T16:21:00+0000",
            "content": "This patch \n-fixes a bug in overseer where when two cores were registered close to each other the edits for the latter would have gone to a stale cloudState.\n\n-the zk nodes the overseer requires are now done in single method call before the live node is created\n\n-some old cruft is also removed "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13171050",
            "date": "2011-12-16T16:26:58+0000",
            "content": "the last patch missed some edits, here's a new one. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13171077",
            "date": "2011-12-16T17:08:23+0000",
            "content": "Thanks Sami - it's in. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13175411",
            "date": "2011-12-23T12:57:17+0000",
            "content": "Attached patch allows overseer to track shard leaders and update cloud state accordingly. Shard leader is marked with prop leader=\"true\" in its cloudstate props.\n\nAlso some of the tests are improved so that they are less likely to fail due to timing issues. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13175442",
            "date": "2011-12-23T14:27:11+0000",
            "content": "Patch applied - thanks again. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13177132",
            "date": "2011-12-29T12:22:45+0000",
            "content": "another patch: got rid of CoreAssignment - ZkController now receives shardids through cloudstate, pumped up some counters to get rid of problems with timings, small improvements in OverseerTest "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13182754",
            "date": "2012-01-09T20:02:30+0000",
            "content": "Doh - think i missed this last patch and now it doest apply at all - can you update this to the branch Sami? "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13183645",
            "date": "2012-01-10T22:34:24+0000",
            "content": "think i missed this last patch\n\nIt seems like it was applied in r1225550. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13183651",
            "date": "2012-01-10T22:38:12+0000",
            "content": "Ah, good. I was going to check that - usually I comment after I apply, so I thought I must have missed it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13184183",
            "date": "2012-01-11T16:42:45+0000",
            "content": "I wonder if it makes sense for the overseer to change the state of a node in the cluster.json file when a node goes down? It's kind of odd to have it listed as active even when we know a node is out of commission. Should we add another state and start marking nodes as down there? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13184305",
            "date": "2012-01-11T19:24:36+0000",
            "content": "I'm seeing OverseerTest fail quite often now:\n\n\n    [junit] Testcase: testShardAssignmentBigger(org.apache.solr.cloud.OverseerTest):\tFAILED\n    [junit] could not find counter for shard:null\n    [junit] junit.framework.AssertionFailedError: could not find counter for shard:null\n    [junit] \tat org.apache.solr.cloud.OverseerTest.testShardAssignmentBigger(OverseerTest.java:248)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13184559",
            "date": "2012-01-11T23:56:57+0000",
            "content": "I'm seeing OverseerTest fail quite often now:\n\nI think I have taken care of this - we now wait longer for a leader to show up when registering compared to the short wait we do for updates. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13196256",
            "date": "2012-01-30T18:01:08+0000",
            "content": "Thanks Sami! "
        }
    ]
}