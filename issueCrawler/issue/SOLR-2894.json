{
    "id": "SOLR-2894",
    "title": "Implement distributed pivot faceting",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.10",
            "6.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Following up on SOLR-792, pivot faceting currently only supports undistributed mode.  Distributed pivot faceting needs to be implemented.",
    "attachments": {
        "SOLR-2894-mincount-minification.patch": "https://issues.apache.org/jira/secure/attachment/12645331/SOLR-2894-mincount-minification.patch",
        "48.pivotfails.log.bz2": "https://issues.apache.org/jira/secure/attachment/12661710/48.pivotfails.log.bz2",
        "pivotfail.log": "https://issues.apache.org/jira/secure/attachment/12661573/pivotfail.log",
        "SOLR-2894-reworked.patch": "https://issues.apache.org/jira/secure/attachment/12538443/SOLR-2894-reworked.patch",
        "dateToObject.patch": "https://issues.apache.org/jira/secure/attachment/12633586/dateToObject.patch",
        "pivot_mincount_problem.sh": "https://issues.apache.org/jira/secure/attachment/12644208/pivot_mincount_problem.sh",
        "SOLR-2894.patch": "https://issues.apache.org/jira/secure/attachment/12517099/SOLR-2894.patch",
        "SOLR-2894_cloud_test.patch": "https://issues.apache.org/jira/secure/attachment/12644671/SOLR-2894_cloud_test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ben Roubicek",
            "id": "comment-13154368",
            "date": "2011-11-21T19:00:01+0000",
            "content": "Based on SOLR-792, it looked like there was some traction in getting distributed pivoting in the trunk codebase beyond the functional prototype.  This feature has a lot of value within my company where we perform 50 separate queries where one would suffice if we had distributed pivot support.     "
        },
        {
            "author": "Antoine Le Floc'h",
            "id": "comment-13190915",
            "date": "2012-01-23T08:02:35+0000",
            "content": "Do you think that this will be available for Solr 4.0 ? I would think that this is very similar to distributing regular facets ? "
        },
        {
            "author": "Dan Cooper",
            "id": "comment-13222450",
            "date": "2012-03-05T17:00:26+0000",
            "content": "Added a patch to provide distributed pivot faceting.  We've been running this code for a while now and it seems to work OK, also created a unit test to test distributed pivot faceting on a small set of data.\n\nThe patch was created against Solr trunk revision 1297102.\n\nIt should perform in much the same way as single shard pivot faceting.  It only sorts by count if you specify that option otherwise it returns results in the order they were generated (may be useful is performance is important but ordering is not).  Most will want to specify facet.sort=count.  This patch also supports limiting results using facet.limit.\n\nTo do the merge I'm converting the NamedList objects that get returned by each shard in a giant map (should be more efficient for merging the results) and then converting back into a NamedList when the merge is complete.  This merge should support N depth pivots but I've only properly tested a depth of 2.\n\nI've added some new parameters to support the features we require from pivot faceting and thought they may as well go in the patch in case others find them useful.\n\n\n\tfacet.pivot.limit.method\n\t\n\t\tset to 'combined' if you want only the N number of top results to be returned across all pivots, where N is set by facet.limit. e.g. if you pivoted by country,manufacturer and limited by 5, obviously the top 5 countries would be returned, but only the top 5 manufacturers by combined total would be returned too. e.g. Each country would return the same 5 manufacturers (or less if no results).\n\t\n\t\n\n\n\n\n\tfacet.pivot.limit.ignore\n\t\n\t\tIgnores the specified field from the limiting operations. e.g. if you pivoted by country,manufacturer and limited by 5 and set facet.pivot.limit.ignore=country then you would get all available countries returned (not limited) but only 5 manufacturers for each country.\n\t\n\t\n\n\n\nCan someone test the patch and give some feedback? "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13259947",
            "date": "2012-04-23T20:57:41+0000",
            "content": "Hi Dan.\nI have been working with your patch, 2894, to Solr and I am having some issues with unit testing.\n\nFirst of all the patch doesn't seem to apply cleanly:\ncrussell@WAT-CRUSSELL /cygdrive/d/matrixdev/solr_1297102/CBSolr/SolrLucene\n$ patch -p0 -i SOLR-2894.patch\npatching file solr/core/src/java/org/apache/solr/handler/component/EntryCountComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java\nHunk #10 FAILED at 797.\n1 out of 16 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java.rej\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\nHunk #2 FAILED at 106.\n1 out of 2 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java.rej\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotNamedListCountComparator.java\npatching file solr/core/src/java/org/apache/solr/util/NamedListHelper.java\npatching file solr/core/src/java/org/apache/solr/util/PivotListEntry.java\npatching file solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotTest.java\npatching file solr/solrj/src/java/org/apache/solr/common/params/FacetParams.java\n$ patch --version\npatch 2.5.8\nCopyright (C) 1988 Larry Wall\nCopyright (C) 2002 Free Software Foundation, Inc.\n\nA lot of the contents of your original patch seemed to be formatting changes like where line breaks should go and how spacing should be handled.\nI was able to examine the patch and manually incorporate the changes from the failed chunks.\nOne question, on this line (1119) of your patch, why did you choose not to initialize the map as the ones above it are? Couldn't that cause an NRE?\n+    public SimpleOrderedMap<List<NamedList<Object>>> pivotFacets;\n\nWhile looking at DistributedFacetPivotTest.java I noticed an error on line 42.  Your \"q\" should probably be \":\" instead of \"*\". Edit: asterisk colon asterisk instead of just asterisk.\n\nI've attached the patch file I came up with.  I added the initialization and test correction I mentioned.\n\nWhen I ran the solr/lucene unit tests after I patched, there are some unit test failures like this one:\n    [junit] Testsuite: org.apache.solr.TestDistributedGrouping\n    [junit] Testcase: testDistribSearch(org.apache.solr.TestDistributedGrouping):       FAILED\n    [junit] .facet_counts.size()==5,4skipped=0,0\n    [junit] junit.framework.AssertionFailedError: .facet_counts.size()==5,4skipped=0,0\n    [junit]     at org.apache.solr.BaseDistributedSearchTestCase.compareResponses(BaseDistributedSearchTestCase.java:656)\n    [junit]     at org.apache.solr.BaseDistributedSearchTestCase.query(BaseDistributedSearchTestCase.java:383)\n    [junit]     at org.apache.solr.TestDistributedGrouping.doTest(TestDistributedGrouping.java:51)\n    [junit]     at org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:671)\n    [junit]     at org.apache.lucene.util.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:20)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:736)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:632)\n    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:531)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:593)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit]     at org.apache.lucene.util.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:20)\n    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit]\n    [junit]\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 2.685 sec\n    [junit]\n    [junit] ------------- Standard Error -----------------\n    [junit] 2520 T1 oas.BaseDistributedSearchTestCase.compareResponses SEVERE Mismatched responses:\n    [junit]     {responseHeader=\n{status=0,QTime=19}\n,grouped={a_si={matches=0,groups=[]}},facet_counts={facet_queries={},facet_fields={a_t={}},facet_dates={},facet_ranges={},facet_pivot={}}}\n    [junit]     {responseHeader=\n{status=0,QTime=18}\n,grouped={a_si={matches=0,groups=[]}},facet_counts={facet_queries={},facet_fields={a_t={}},facet_dates={},facet_ranges={}}}\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDistributedGrouping -Dtestmethod=testDistribSearch -Dtests.seed=-c7cfa73dbca93c9:-4751af558bf6f59:3e523b50870b3b1b -Dargs=\"-Dfile.encoding=Cp1252\"\n\nIt looks like the facet_pivot is being included in the results of one query, and not the other.  I'm trying to figure out why this is occurring.\nAny insight would be appreciated. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13259949",
            "date": "2012-04-23T20:59:46+0000",
            "content": "Some modifications to SOLR-2894.patch that I made while trying to get it to patch on rev 1297102. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13260994",
            "date": "2012-04-24T20:41:00+0000",
            "content": "I figured out the unit test.  It's because facet_pivot is different from the other facet_blah in that it only comes back when you request pivots, whereas the others always come back.  In Dan's patch he had facet_pivot coming back even when it was empty or not requested, and this did not match the behavior of pivots in a non-distributed setting.  I am working on an update to my patch. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13261300",
            "date": "2012-04-25T05:08:07+0000",
            "content": "facet_pivot will not show up in distrib search if no contents, reversed behavior of sorting to comply with solr standard for facet.sort "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13294737",
            "date": "2012-06-13T23:55:55+0000",
            "content": "Erik: Can you triage this for 4.0? commit if you think it's ready, otherwise remove the fix version? "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13294795",
            "date": "2012-06-14T03:50:06+0000",
            "content": "For what it's worth, we're actively using the April 25th version of this patch in production at CareerBuilder (with an older version of trunk) with no issues. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13295026",
            "date": "2012-06-14T13:37:51+0000",
            "content": "Trey - thanks for the positive feedback.  I'll apply the patch, run the tests, review the code, and so on.   Might be a couple of weeks, unless I can get to this today. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13295257",
            "date": "2012-06-14T19:35:06+0000",
            "content": "Patch updated to 4x branch.\n\nSimon, just for you, I removed NamedListHelper as well   (folded its one method into PivotFacetHelper)\n\nTests pass. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13295261",
            "date": "2012-06-14T19:40:18+0000",
            "content": "Trey - would you be in a position to test out the latest patch?   I built my latest one by starting with the March 5, 2012 SOLR-2894.patch file. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13295389",
            "date": "2012-06-14T23:49:45+0000",
            "content": "Erik, what revision of solr did you apply the patch to? Did you not encounter the issues I encountered? "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13295426",
            "date": "2012-06-15T02:21:16+0000",
            "content": "Erik, I can't get your patch to apply cleanly to solr 1350445\n\n$ patch -p0 -i SOLR-2894.patch\npatching file solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotTest.java\npatching file solr/core/src/java/org/apache/solr/handler/component/EntryCountComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotNamedListCountComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\nHunk #2 FAILED at 103.\n1 out of 2 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java.rej\npatching file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java\nHunk #11 FAILED at 799.\n1 out of 17 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java.rej\npatching file solr/core/src/java/org/apache/solr/util/PivotListEntry.java\npatching file solr/solrj/src/java/org/apache/solr/common/params/FacetParams.java\npatching file solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13295608",
            "date": "2012-06-15T11:58:40+0000",
            "content": "Chris - I generated the patch from an r1350348 checkout (which is not branch_4x as I mentioned above).  It might be a couple of weeks before I can get back to this and sort it out though, unfortunately.  Sorry. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13403341",
            "date": "2012-06-28T18:33:08+0000",
            "content": "I have posted an enhancement to this patch as SOLR-3583.  It is based on the Apr 25th version. "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13407459",
            "date": "2012-07-05T20:31:53+0000",
            "content": "Hi Erik,\n\nSorry, I missed your original message asking me if I could test out the latest patch - I'd be happy to help.  I just tried both your patch and the April 25th patch against the Solr 4.0 Alpha revision and neither applied immediately.  I'll see if I can find some time on Sunday to try to get a revision sorted out which will work with the current version.\n\nI think there are some changes in the April 24th patch which may need to be re-applied if your changes were based upon the earlier patch.  I'll know more once I've had a chance to dig in later this weekend.\n\nThanks,\n\n-Trey "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13412175",
            "date": "2012-07-11T22:26:14+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13421409",
            "date": "2012-07-24T13:43:58+0000",
            "content": "No way I'm getting to this any time soon, sorry.  Trey - feel free to prod this one forward with more testing/feedback. "
        },
        {
            "author": "Dzmitry Zhemchuhou",
            "id": "comment-13425338",
            "date": "2012-07-30T22:46:01+0000",
            "content": "I have reapplied the SOLR-2894 patch from Jun 14th to the trunk while removing most of code formatting changes that were in it. On top of that I changed the FacetComponent.refineFacets() method to add facet_pivot key-value only when there are values in the pivotFacets map, which fixes distributed search unit tests.  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13429829",
            "date": "2012-08-07T03:43:26+0000",
            "content": "rmuir20120906-bulk-40-change "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13452196",
            "date": "2012-09-10T17:41:57+0000",
            "content": "moving all 4.0 issues not touched in a month to 4.1 "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13455182",
            "date": "2012-09-13T19:59:13+0000",
            "content": "Regarding facet.pivot.limit.method and facet.limit, it looks like these are not checked on a per-field basis?\nSo, if a user sets different limits for different fields and wants 'combined' limiting, that is not possible?\nFor example a user might set:\n\nf.field1.facet.limit=10\nf.field1.facet.pivot.limit.method=combined\nf.field2.facet.limit=20 \n\nAnd the combined method will not be used...\nIf the user sets facet.pivot.limit.method=combined it looks like the same limit will be used for all fields?  Whatever the global facet.limit is set to? "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13482734",
            "date": "2012-10-23T21:50:56+0000",
            "content": "In my experience using this patch, it seems that it does not over-request when enforcing a limit?\nThis is problematic because, for example, in a situation where you have many slaves and you are pivoting on a fairly evenly distributed field and setting your facet limit to X, the Xth distinct value for that field by document count on each slave is likely to be different.  The result is that some facet values close to your limit boundary will not get reported for aggregation, which will make your ultimate results somewhat inaccurate.\n\nIt was my impression that other facet-based features of solr over-request when there is a limit to combat this situation?  For example if you specify limit 10, the distributed query might have limit 100 or 1000, and then during aggregation it would be limited to the top 10.\n\nI am working on similar functionality for this patch. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13492619",
            "date": "2012-11-07T19:29:52+0000",
            "content": "In regards to my above comment, I have determined that it is because if you specify a limit for a field that you are not requesting facet counts for, solr will not automatically over-request on that field.  \ni.e.\nfacet.pivot=somefield\nf.somefield.facet.limit=10\n\nThis will make your pivots weird because the limit of 10 will not be over requested unless you add this line:\nfacet.field=somefield\n\nSince solr does not do distributed pivoting yet, this has not been an issue yet.\nI am working on an update to the patch that will correct this issue. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13495296",
            "date": "2012-11-12T14:39:48+0000",
            "content": "Updated to apply to trunk 1404975. (Based on Dzmitry's update)\n\nAdded ability to limit on individual fields. (f.fieldname.facet.limit)\nAdded pivot fields to fields being over-requested during distributed queries.\nMade it so you can pivot on a single field. "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13527907",
            "date": "2012-12-10T12:32:25+0000",
            "content": "Just thought I'd add some feedback on this valuable patch.\n\nI run some tests with the latest patch (Nov. 12) and the limit-per-field feature seems to be working alright. (Nice job Chris!)\n\nI did, however, encounter 2 other issues (which I guess are related):\n(1) There's no default sorting method. i.e. if no facet.sort is specified then results are not sorted. (this is a deviation from the current non-distributed pivot faceting behavior)\n(2) Sorting per-field does not work. (i.e. f.<field>.facet.sort=<method> does not work) "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13534219",
            "date": "2012-12-17T19:43:38+0000",
            "content": "Thanks Shahar.\n\nIn regard to #1, there is (count) and there are tests that cover that in DistributedFacetPivotTest.java.  Are you sure the patch applied correctly to your version of solr?\n\nIn regard to #2, that is correct.  Does per field sorting work with non-distributed pivots?  I guess it was never implemented by the original author. "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13535829",
            "date": "2012-12-19T10:00:38+0000",
            "content": "Hi Chris,\n\n#1 Yes, I believe the patch applied correctly. Once more, default sorting method is not defined and I didn't see where DistributedFacetPivotTest is testing the default sort. (I did however see where facet.sort is tested after facet.sort=count was explicitly specified).\n#2 Yes, Per field sorting works with non-distributed pivots. We were working in a non-distributed configuration up to some point and, until then, per field sorting worked properly (that was on Solr 4.0)\n\nWe also encountered another issue when sorting by count:\nThere might be a case where 2 of the returned values have the same count. In such a case, the PivotNamedListCountComparator attempts to sort by value. The problem is that when facet.missing=true is specified then one of the 2 values which have the same count may be null (missing) and in such cases the get(\"value\").toString() operation will fail (NullPointerException). "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13537333",
            "date": "2012-12-20T20:08:10+0000",
            "content": "Implemented default pivot facet sort.\nImplemented per-field pivot facet sorting.\nFixed NRE with sorting when facet.missing is on. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13537334",
            "date": "2012-12-20T20:08:20+0000",
            "content": "Shahar, I have corrected the issues that you mentioned, I believe.\nDefault sorting is now count if a facet limit exists, otherwise index.\nI fixed the facet.missing stuff, and I went with the sorting convention that the rest of solr seems to have which is that null values always go to the bottom of the list.\n\nTry the new patch. "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13539395",
            "date": "2012-12-25T12:32:47+0000",
            "content": "Hi Chris,\n\nI appreciate your efforts on this.\nI tried the new patch and run into numerous NPEs and didn't get to verify the sorting.\n\nHere's what I'm getting:\n\nSEVERE: null:java.lang.NullPointerException\n        at java.util.TreeMap.compare(TreeMap.java:1188)\n        at java.util.TreeMap.put(TreeMap.java:531)\n        at org.apache.solr.handler.component.PivotFacetHelper.convertPivotsToMaps(PivotFacetHelper.java:317)\n        at org.apache.solr.handler.component.FacetComponent.countFacets(FacetComponent.java:542)\n        at org.apache.solr.handler.component.FacetComponent.handleResponses(FacetComponent.java:336)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:309)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:144)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1830)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:455)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:276)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337) \n        ....\n\n\n\nAnd also:\n\nSEVERE: null:java.lang.NullPointerException\n        at java.util.TreeMap.getEntry(TreeMap.java:342)\n        at java.util.TreeMap.get(TreeMap.java:273)\n        at org.apache.solr.handler.component.FacetComponent.mergePivotFacet(FacetComponent.java:692)\n        at org.apache.solr.handler.component.FacetComponent.countFacets(FacetComponent.java:552)\n        at org.apache.solr.handler.component.FacetComponent.handleResponses(FacetComponent.java:336)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:309)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:144)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1830)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:455)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:276)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337)\n        ....\n\n\n\nSince I couldn't get to test the sorting fix, I peaked at the PivotNamedListCountComparator fix and I believe it will sort the \"null\" values on top (due to the return values of handleSortWhenOneValueIsNull) and regardless of the count number (due to the fact that the null values are checked before comparing the counts) - correct me if I'm wrong.\n\nAppreciate your help with this,\n\nShahar. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13540023",
            "date": "2012-12-27T16:33:41+0000",
            "content": "That is odd, it worked when I tested it on my box.  I will take another look. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13543002",
            "date": "2013-01-03T15:44:46+0000",
            "content": "Fixed NRE when using facet.missing.\nAdded test for over-requesting / refinement.\nFixed issue that broke over-requesting when local params were present in facet.field.\n\nIn correcting the issue with over-requesting I noticed that no refinement is being done for distributed pivot facets.\nThis is an issue because it means that your pivot facet counts may not be correct.\nI am working on an enhancement to add refinement for pivot facets. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13543004",
            "date": "2013-01-03T15:47:05+0000",
            "content": "Shahar, I think I've corrected the issues you reported.  Take a look.\nAnd yes, the sorting will always put the nulls on one side, although I believe it is the bottom of the list based on my testing.  This matches current single-core behavior from solr, as far as I can tell. "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13552166",
            "date": "2013-01-13T10:55:28+0000",
            "content": "Hi Chris,\n\nThanks for the updated patch.\n\nI started testing this latest patch and encountered a few problems when NULL values are present:\n(1) If, for example, FIELD_A contains null values and FIELD_B does not, then \"facet.pivot=FIELD_A,FIELD_B\" will return more than 1 entry of NULLs for FIELD_A. To be exact, it return an NULL entry per shard. (In non-distributed search, there is a single NULL entry for FIELD_A)\n(2) If facet.pivot=FIELD_B,FIELD_A then one may see that for a given of field FIELD_B there is more than 1 entry of FIELD_A with NULL value. (In non-distributed search, there's only 1 entry of a NULL value under a given FIELD_B)\n\nIt seems as if there's a problem when merging null values from cross-shard pivots.\nDoes the relevant test-case include some sort of check on data with null values?\n\nAs far as sorting is concerned, results seem to be sorted properly (per field as well).\n\nShahar. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13553060",
            "date": "2013-01-14T20:05:49+0000",
            "content": "Corrected null aggregation issues when docs contain null values for fields pivoting on. Added logic to remove local params from pivot QS vars when determining over-request. "
        },
        {
            "author": "Ken Ip",
            "id": "comment-13555149",
            "date": "2013-01-16T15:57:04+0000",
            "content": "Hi Chris,\n\nThanks for the patch. Any chance this can be applied to 4_0 or 4_1 branch? We have no problem applying it to truck but it can't be applied to 4_0 nor 4_1. Appreciated.\n\n\u279c  lucene_solr_4_1  patch -p0 -i SOLR-2894.patch --dry-run\npatching file solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotTest.java\npatching file solr/core/src/test/org/apache/solr/SingleDocShardFeeder.java\npatching file solr/core/src/test/org/apache/solr/TestRefinementAndOverrequestingForFieldFacetCounts.java\npatching file solr/core/src/java/org/apache/solr/handler/component/EntryCountComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotNamedListCountComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java\nHunk #1 FAILED at 16.\nHunk #2 FAILED at 35.\nHunk #5 succeeded at 287 with fuzz 2.\n2 out of 5 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java.rej\npatching file solr/core/src/java/org/apache/solr/handler/component/NullGoesLastComparator.java\npatching file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java\nHunk #1 FAILED at 17.\nHunk #2 FAILED at 43.\n2 out of 11 hunks FAILED \u2013 saving rejects to file solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java.rej\npatching file solr/core/src/java/org/apache/solr/util/PivotListEntry.java\npatching file solr/solrj/src/java/org/apache/solr/common/params/FacetParams.java "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13563902",
            "date": "2013-01-27T19:40:59+0000",
            "content": "Hi Chris,\n\nOnce again, your efforts are much appreciated!!\n\nI tested the latest patch (dated Jan. 14th) and the null aggregation issues were indeed resolved - thanks!\n\nAny chance this could be integrated into the upcoming release? (4.2?)\n\nThanks,\n\nShahar. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13563920",
            "date": "2013-01-27T20:26:08+0000",
            "content": "Ken and Shahar, thank you both for your interest in getting this patch usable with the current and upcoming releases.\nAt this point in time, since refinement is not present in distributed pivoting, I would be hesitant to integrate this with release versions of solr.  Indeed, where I work we have put features related to this functionality on hold until refinement can be implemented.\nI started working on implementing refinement for distributed pivots a couple of weeks ago and have handed it off to a teammate who will be seeing it through to completion.\nCurrently we expect this will take another two to four weeks.\nAt that time we will update what is here and look in to backporting / getting this committed.\nThanks again. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13579505",
            "date": "2013-02-15T21:14:37+0000",
            "content": "As Chris said above me I've been working on distributed pivot faceting. The model had to be reworked to support this change. The new workflow is each shards result goes into a shardRequest map, those maps are combined and converted into a list what is what is is looked at. The combined results are compared to each shard's and refinement requests are made. Once all the refinement requests are made another round of refinement is queued up, this time going one level deeper than the previous refinement requests. Because of the nature of the system distributedprocess(rb) needed to be called from the refinepivotfacets to actually take the enqueued refinement requests and make them into fully fledged refinement queries. This caused some issue where other refinment types would get recalled as their refinement identifier was never cleared, Field facets are slightly modified to have a boolean for \"needsRefinements\" and one for \"wasRefined\" to help distinguish. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13583280",
            "date": "2013-02-21T15:32:41+0000",
            "content": "An update to my previous patch. After more consideration, I've pulled the pivot facet logic out of DistributedProcess and call those specific parts when doing the iterative refinement process to avoid any side effects of other people putting code into DistributedProcess. I've also added better support for null values and added tests to ensure that you can send refinement requests for null values and not blow up the stack. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13594772",
            "date": "2013-03-06T15:16:08+0000",
            "content": "Andrew, someone on IRC is trying to apply your patch, but we can't find a revision that it will apply to successfully, and there's no revision information in the patch.  Also, whatever diff utility you used has put backslashes into the paths and that has to be manually fixed before it'll find the files on Linux.  Can you update your source tree to the current version and use 'svn diff' (or 'git diff' if you've checked out with git) to create the patch and re-upload? "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13595170",
            "date": "2013-03-06T22:04:36+0000",
            "content": "New patch file for trunk, its a git patch from trunk solr pulled today, let me know if there are any issues applying. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13595174",
            "date": "2013-03-06T22:07:54+0000",
            "content": "Thanks Andrew - this isn't my area of expertise, but hopefully someone will get to this eventually. Lot's of votes and watchers  "
        },
        {
            "author": "Monica Skidmore",
            "id": "comment-13595202",
            "date": "2013-03-06T22:29:06+0000",
            "content": "Thanks, Andrew!  We're upgrading our version of Solr, and this will make our users very happy.  I'm hoping it will be committed now... "
        },
        {
            "author": "William Harris",
            "id": "comment-13595885",
            "date": "2013-03-07T13:53:27+0000",
            "content": "Hey, Andrew. I'm that someone from IRC. \nI managed to apply your patch ( even though a single hunk failed ), but I get a NullPointerException when I attempt a query with 3 pivots.\n\nSEVERE: null:java.lang.NullPointerException\n        at org.apache.solr.handler.component.PivotFacetHelper.getCountFromPath(PivotFacetHelper.java:373)\n        at org.apache.solr.handler.component.FacetComponent.processTopElement(FacetComponent.java:779)\n        ...\n\n  "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13596046",
            "date": "2013-03-07T16:49:12+0000",
            "content": "This patch applies cleanly to trunk for me, apologies as the last one upon review was flawed. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13596079",
            "date": "2013-03-07T17:27:04+0000",
            "content": "I have created a unit test that causes an NPE on a 3 pivot request, I am taking a look now. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13599398",
            "date": "2013-03-11T22:17:32+0000",
            "content": "Fixes the NPEs in 3pivot along with solving a hidden issue with null values when .missing was false and dealt with an issue where the facet.mincount is different from the facet.pivot.mincount (which is true for the default). The mincount issue only showed itself during index sorting. "
        },
        {
            "author": "William Harris",
            "id": "comment-13600972",
            "date": "2013-03-13T09:20:27+0000",
            "content": "I get the following error with the latest patch.\n\norg.apache.solr.search.SyntaxError: Expected identifier at pos 28 str='{!terms=xxxx xxxx 2}field1,field2,field3'\n\n\n\nAs far as I can tell it seems to occur whenever it encounters values in field1 that end with an integer, or contain non-alphanumeric characters. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13603758",
            "date": "2013-03-15T19:48:41+0000",
            "content": "The issue lies in how the refinement requests were formatted and how they were parsed on the shard side, I've made changes that should alleviate this issue and I'll push out a patch soon "
        },
        {
            "author": "Shahar Davidson",
            "id": "comment-13605053",
            "date": "2013-03-18T12:17:24+0000",
            "content": "Hi Andrew (and Chris), I just wanted to report a problem that we found in the patch from Jan 14th.\n\nIn short, the problem seems to be related to facet.limit and the symptom is that a distributed pivot returns less terms than expected.\n\nHere's a simple scenario:\nIf I run a (non-distributed) pivot such as:\n\nhttp://myHost:8999/solr/core-A/select?q=*:*&wt=xml&facet=true&facet.pivot=field_A,field_B&rows=0&facet.limit=-1&facet.sort=count\n\nthen I would get N terms for field_A. (where, in my case, N is in the thousands)\n\nBUT, if I run a distributed pivot such as:\n\nhttp://myHost:8999/solr/core-B/select?shards=myHost:8999/solr/core-A&q=*:*&wt=xml&facet=true&facet.pivot=field_A,field_B&rows=0&facet.limit=-1&facet.sort=count\n\nthen I would get at most 160 terms for field_A.\n(Why exactly 160?? I have no idea)\n\nOn the other hand, if I use f.<field_name>.facet.limit=-1 then things work as expected. For example:\n\nhttp://myHost:8999/solr/core-B/select?shards=myHost:8999/solr/core-A&q=*:*&wt=xml&facet=true&facet.pivot=field_A,field_B&rows=0&f.field_A.facet.limit=-1&f.field_B.facet.limit=-1&facet.sort=count\n\nThis will return exactly N terms for field_A as expected.\n\nI'll appreciate your help with this.\n\nShahar. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13605228",
            "date": "2013-03-18T15:46:44+0000",
            "content": "The Jan 14th patch is totally missing the refinement step, so its hard for me to say what problem is causing your issue. Please download the newest patch and let me know if that problem continues. "
        },
        {
            "author": "Vishal Deshmukh",
            "id": "comment-13608312",
            "date": "2013-03-20T22:10:21+0000",
            "content": "Hi, we are looking for this feature desperately. can anyone give ETA on this? Thanks in advance. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13608968",
            "date": "2013-03-21T14:22:57+0000",
            "content": "This was built for 4_1_0 but git thinks it'll apply to trunk no problem.\n\nThis solves a myriad of issues surrounding the formatting of the refinement requests, should support all field types and deals with jagged pivot facet result sets due to nulls or empty data on pivoted fields. "
        },
        {
            "author": "Stein J. Gran",
            "id": "comment-13626417",
            "date": "2013-04-09T09:22:35+0000",
            "content": "Andrew, which version does the latest patch apply to? I've tried applying it to trunk, branch_4x and 4.2.1 without any luck so far. I'm planning on testing this patch in a SolrCloud environment with lots of pivot facet queries.\n\nFor trunk I get this:\npatching file `solr/core/src/java/org/apache/solr/request/SimpleFacets.java'\nHunk #1 succeeded at 323 with fuzz 2 (offset 51 lines).\nHunk #2 FAILED at 374.\n1 out of 2 hunks FAILED \u2013 saving rejects to solr/core/src/java/org/apache/solr/\nrequest/SimpleFacets.java.rej\n\nThe rej file seems similar for trunk and the 4.2.1 tag "
        },
        {
            "author": "Sviatoslav Lisenkin",
            "id": "comment-13626441",
            "date": "2013-04-09T10:07:39+0000",
            "content": "Hello, everyone. \nI had applied the latest patch two weeks ago (rev.1465879), faced the issues with merging in SimpleFacets class near 'incomingMinCount' variable, fixed them manually (just renaming). Simple pivot faceting via web UI and sample Solr installation with two nodes worked fine. I really appreciate if someone have a chance to test it under load etc.\nHope, this patch (and feature) will be included in the upcoming release. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13626656",
            "date": "2013-04-09T14:33:01+0000",
            "content": "It was built for 4.1. There are a few application failures with 4_2 but nothing major from what I've seen. "
        },
        {
            "author": "Stein J. Gran",
            "id": "comment-13627641",
            "date": "2013-04-10T09:43:30+0000",
            "content": "Sviatoslav and Andrew: Thank you, only small changes to the SimpleFacets.java file was necessary to get the patch in for the 4.2.1 tag.\n\nI have now been testing the patch in a small SolrCloud environment with two shards (-DnumShards=2), and I have found the following:\n1. Distributed pivot facets work great on string fields \n2. No values are returned if one of the facet.pivot fields is a date field\n\nFor scenario 2:\na) There are no error messages in the Solr log file\n\nb) The URL I use is http://localhost:8983/solr/coxitocollection/select?facet=true&facet.sort=true&q=*:*&facet.limit=1000&facet.pivot=dateday_datetime,firstreplytime\n\nc) If I add \"&distrib=false\" to the URL, I get values back\n\nd) The fields used are defined like this in schema.xml:\n<field name=\"dateday_datetime\" type=\"date\" indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n<field name=\"firstreplytime\" type=\"int\" stored=\"true\" multiValued=\"false\" />\n\ne) I tried using the tdate field instead of date, but this had no effect\n\nf) The date and tdate fields are defined like this in schema.xml:\n<fieldType name=\"date\" class=\"solr.TrieDateField\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n<fieldType name=\"tdate\" class=\"solr.TrieDateField\" precisionStep=\"6\" positionIncrementGap=\"0\"/>\n\ng) If I run with -DnumShards=1 this scenario works great, both with and without \"distrib=false\"\n\nh) This was tested with 4.2.1 with the patch from March 21st with the following change: The non-existing variable \"mincount\" was replaced with \"incomingMinCount\" in SimpleFacets.java "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13661866",
            "date": "2013-05-20T09:27:43+0000",
            "content": "Hi,\n\nI want to report a problem that we found in the patch of March 21st.\nIt seems that the problem Shahar reported is now solved, but there is another similar problem.\nIn short, the problem seems to be related to facet.limit per field definition and the symptom is that a distributed pivot returns less terms than expected.\nHere's a simple scenario:\n\nif I run a distributed pivot such as:\nhttp://myHost:8999/solr/core-B/select?shards=myHost:8999/solr/core-A&q=*:*&wt=xml&facet=true&facet.pivot=field_A&rows=0&facet.limit=-1&facet.sort=index\n\nit will return exactly number of terms for field_A as expected.\n\nOn the other hand, if I use f.<field_name>.facet.limit=-1:\nhttp://myHost:8999/solr/core-B/select?shards=myHost:8999/solr/core-A&q=*:*&wt=xml&facet=true&facet.pivot=field_A&rows=0&f.field_A.facet.limit=-1&facet.sort=index\n\nthen it will return at most 100 terms for field_A.\n\nI'll appreciate your help with this.\n\nThanks. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13670363",
            "date": "2013-05-30T14:13:57+0000",
            "content": "I will take a look. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13697420",
            "date": "2013-07-02T02:12:45+0000",
            "content": "Tons of votes and watchers on this issue!\nChris Russell, Andrew Muldowney, and Trey Grainger - any luck with this by any chance?  It would be great to get this in 4.4! "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13708757",
            "date": "2013-07-15T18:11:09+0000",
            "content": "Im working on this patch again, looking into the limit issue and the fact that exclusion tags aren't being respected. They both boil down to improperly formatted refinement requests, so I'm going through and cleaning those up to look more and more like the distributed field facet code. Should also have time to get to the datetime problem, where you cannot refine on datetimes because the datetime format returned by the shards is not queryable when refining. "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13709360",
            "date": "2013-07-16T02:04:16+0000",
            "content": "@Otis Gospodnetic, we have this patch live in production for several use cases (as a pre-requisite for SOLR-3583, which we've also worked on @CareerBuilder), but the currently known issues which would prevent this from being committed include:\n1) Tags and Excludes are not being respected beyond the first level\n2) The facet.limit=-1 issue (not returning all values)\n3) The lack of support for datetimes\n\nWe need #1 and Andrew is working on a project currently to fix this.  He's also looking to fix #3 and find a reasonably scalable solution to #2.  I'm not sure when the Solr 4.4 vote is going to be, but it'll probably be a few more weeks until this patch is all wrapped up.\n\nMeanwhile, if anyone else finds any issues with the patch, please let us know so they can be looked into.  Thanks! "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13715674",
            "date": "2013-07-22T21:15:17+0000",
            "content": "Built on 4_4\nThis version fixes the following:\n\n1) Indecisive faceting not being respected on refinement queries\n2) Key not being respected\n3) Facet.offset not being respected\n4) datetimes breaking when trying to refine\n\n\nOne point of contention is this:\nThe SolrExampleTests.java (for the SolrJ stuff) had a check that required pivot facet boolean results as strict Boolean.TRUE as opposed to the string \"true\".\n\nThis came about from the change that was required to fix datetime. \n\nI can't find anywhere else where we require a boolean field's value to equal Boolean.True so I think this test was just an artifact of how the original pivot facetting code was written.\n\nAs it stands now the SolrExampleTests.doPivotFacetTest:1151 has been changed to \"true\" instead of Boolean.TRUE "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13716169",
            "date": "2013-07-23T06:52:21+0000",
            "content": "Andrew, Thank you very much for the fix!\n\nDoes this version fix the issue of f.field.facet.limit not being respected?\n\nThanks. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13716428",
            "date": "2013-07-23T14:38:35+0000",
            "content": "Yes, it should "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13718122",
            "date": "2013-07-24T08:39:49+0000",
            "content": "Hi Andrew, \n\nI have tried applying latest patch to 4.2.1 and there were a few problems.\nWhich version does it apply to?\n\nThanks.   "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13718429",
            "date": "2013-07-24T14:45:22+0000",
            "content": "Elran Dvir - didn't try applying it yet, but 99.9% sure it is/was the trunk. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13718527",
            "date": "2013-07-24T16:15:55+0000",
            "content": "I built it for 4_4.\n\nBut I didn't have trouble patching it to 4_2_1. Did you pull from git or svn? "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13719417",
            "date": "2013-07-25T09:12:50+0000",
            "content": "I have downloaded the source code from Solr's website.\nThen opened it with my IDE: Intellij.\nwhen I tried applying the patch, Intellij reported there were problems with some files.\n\nThanks.   "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13719885",
            "date": "2013-07-25T18:29:05+0000",
            "content": "Fixed an issue where commas in string fields would cause infinite refinement loops. "
        },
        {
            "author": "Stein J. Gran",
            "id": "comment-13726461",
            "date": "2013-08-01T14:10:27+0000",
            "content": "I have now re-tested the scenarios I used on April 10th (see my comment above from that date), and all of those issues I found then are now resolved  I applied the July 25th patch to the lucene_solr_4_4 branch (Github) and performed the tests on this version.\n\nWell done Andrew   Thumbs up from me. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13731408",
            "date": "2013-08-06T22:52:44+0000",
            "content": "I've found a small error which causes largely sharded (30+) data to spiral out of control on refinement requests.\n\nI've fixed the error on a previous version of solr and I'll be forward porting it to my 4_4 build by tomorrow.\n\nIf you are having issues with complex string fields this should help. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13731433",
            "date": "2013-08-06T23:20:43+0000",
            "content": "Andrew Muldowney - I didn't have time to link issues, but Joel Bernstein is working on at least one issue that is, if I recall correctly, an alternative implementation of this.... "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13732818",
            "date": "2013-08-07T22:06:11+0000",
            "content": "Fixed the run-away-but-eventually-coalesing refinement query issue\n\nAt this point all known issues have been resolved. "
        },
        {
            "author": "William Harris",
            "id": "comment-13743793",
            "date": "2013-08-19T13:13:11+0000",
            "content": "Hey, Andrew. Really appreciate the effort here!\nI am seeing this error with the latest patch.\n\n\"error\": {\n    \"msg\": \"java.lang.RuntimeException: Invalid version (expected 2, but 60) or the data in not in 'javabin' format\",\n    \"trace\": \"org.apache.solr.common.SolrException: java.lang.RuntimeException: Invalid version (expected 2, but 60) or the data in not in 'javabin' format\\n\\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:302)\\n\\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\\n\\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1850)\\n\\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:703)\\n\\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:406)\\n\\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:195)\\n\\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\\n\\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\\n\\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\\n\\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\\n\\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\\n\\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\\n\\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\\n\\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298)\\n\\tat org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:857)\\n\\tat org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588)\\n\\tat org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\\n\\tat java.lang.Thread.run(Thread.java:724)\\nCaused by: java.lang.RuntimeException: Invalid version (expected 2, but 60) or the data in not in 'javabin' format\\n\\tat org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:110)\\n\\tat org.apache.solr.client.solrj.impl.BinaryResponseParser.processResponse(BinaryResponseParser.java:41)\\n\\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:407)\\n\\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:180)\\n\\tat org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:155)\\n\\tat org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:118)\\n\\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\\n\\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\\n\\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n\\t... 1 more\\n\",\n    \"code\": 500\n  }\n\n\nTrying it with 2 pivots across 8 shards with a total of ~3 million docs.\nNot sure what's causing it, but let me know if I can do anything to help! "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13743842",
            "date": "2013-08-19T14:23:51+0000",
            "content": "What are your exact field types? What is your query?\nFrom what I'm seeing online that error is because a shard failed for some reason. Knowing why a shard failed will yield a much better error message. "
        },
        {
            "author": "William Harris",
            "id": "comment-13744859",
            "date": "2013-08-20T11:05:21+0000",
            "content": "its a pretty simple query: q=<star>:<star>&facet=on&facet.pivot=fieldA,fieldB , both regular single valued solr.TextFields with solr.LowerCaseFilterFactory filters. All shards work well individually.\nI'm looking at the logs but unfortunately I'm not seeing any other error messages there.\n\nIt works as long as I use less than 6 shards. With 6 or more it fails with that error, regardless of which shards I use. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13745106",
            "date": "2013-08-20T16:36:32+0000",
            "content": "It shouldn't be a shard amount issue, were running this patch on a 50 shard cluster over several servers with solid results.\nTry the shards.tolerant=true parameter on your distributed search? It supposedly includes error information if available.  "
        },
        {
            "author": "William Harris",
            "id": "comment-13745938",
            "date": "2013-08-21T10:58:24+0000",
            "content": "shards.tolerant=true did indeed yield a more descriptive error:\n\nERROR - 2013-08-21 12:54:17.392; org.apache.solr.common.SolrException; null:java.lang.NullPointerException\n        at org.apache.solr.handler.component.FacetComponent.refinePivotFacets(FacetComponent.java:882)\n        at org.apache.solr.handler.component.FacetComponent.handleResponses(FacetComponent.java:411)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:311)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1850)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:703)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:406)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:195)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:857)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:724\n\n\n\nI also reindexed everything replacing the values of all string fields with their corresponding hashes in order to see if the error could be caused by some odd strings, but the same error occurs.\nI am also seeing this error after i switched to MD5 hashes for document IDs:\n\nERROR - 2013-08-22 14:28:25.248; org.apache.solr.common.SolrException; null:java.lang.NullPointerException\n        at org.apache.solr.handler.component.QueryComponent.mergeIds(QueryComponent.java:903)\n        at org.apache.solr.handler.component.QueryComponent.handleRegularResponses(QueryComponent.java:649)\n        at org.apache.solr.handler.component.QueryComponent.handleResponses(QueryComponent.java:628)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:311)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1850)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:703)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:406)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:195)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:857)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:724\n\n "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13747675",
            "date": "2013-08-22T17:21:03+0000",
            "content": "The first error occurs on a line where the refinement response is being mined for its information. The line asks for the value and it gets an NPE. Does your data contain nulls? I have code in to deal with that situation but its possible I'm missing an edge case. Do you have any suggestions for a test case that would create this error?\n\nThe second error never gets to anything I've changed so I think MD5ing your docIDs is causing all sorts of other issues unrelated to this patch. "
        },
        {
            "author": "William Harris",
            "id": "comment-13748428",
            "date": "2013-08-23T09:44:47+0000",
            "content": "I thought the issue might be related to me not assigning those fields values in every document, but I tried reindexing giving them all values and the error still occurs.\nI tampered with the source a bit, and managed to trace the error to srsp.getSolrResponse().getResponse(), meaning getSolrResponse() is returning null. Hope that helps. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13749549",
            "date": "2013-08-25T06:38:10+0000",
            "content": "Hi Andrew,\n\nin class PivotFacetProcessor in function doPivots, I have noticed a change in the code.\nthe current implementation is:\n        SimpleOrderedMap<Object> pivot = new SimpleOrderedMap<Object>();\n        pivot.add( \"field\", field );\n        if (null == fieldValue) \n{\n          pivot.add( \"value\", null );\n        }\n else \n{\n          termval = new BytesRef();\n          ftype.readableToIndexed(fieldValue, termval);\n          pivot.add( \"value\", fieldValue );\n        }\n        pivot.add( \"count\", kv.getValue() );\n\nIt means we are getting the values as strings in pivots.\n\nIn the past the implementation was \"pivot.add( \"value\", ftype.toObject(sfield, termval))\"  - meaning we were getting the values as objects.\nI am using SolrJ and it's very important to me to get the values as objects.\n\nIs there a reason not returning the values as objects?\n\nThank you very much. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13750109",
            "date": "2013-08-26T14:34:12+0000",
            "content": "I wrote about that on 22/Jul/13 22:15.\n\nBy having it .ToObject it was taking an internal datetime reference and giving me the pretty format, which was nonconvertible when trying to do refinement on that value. I'm not sure where the .ToObject call should go now as the datetimes in facets never seem to come out in that pretty format. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13814729",
            "date": "2013-11-06T09:12:24+0000",
            "content": "Hi Andrew,\n\nSorry for the long delay.\nI am still seeing the issue I reported on 20/May/13 12:27 (f.field_A.facet.limit=-1 returns at most 100 terms for field_A).\n\nAlso, can you please dircet me to the line of code where datetimes are breaking when trying to refine (caused by \"pivot.add( \"value\", ftype.toObject(sfield, termval))\")\nI need pivot to return values as objects.\n\nThank you very much. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13814953",
            "date": "2013-11-06T15:17:49+0000",
            "content": "By returning an object you get a pretty date format that does not work when doing:\ngetListedTermCounts(field,firstFieldsValues); -PivotFacetProcessor::104\n\nWhen it attempts to convert the pretty date it has into the datatype of the field it will fail to do so. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13815993",
            "date": "2013-11-07T14:26:06+0000",
            "content": "I exained the code.\nIt seems that the refinement process occures before doPivots (where  the call \"ftype.toObject(sfield, termval))\" was).\nSo it seems toObject shouldn't affect the refinement process .\nWhat am I missing?\n\nThanks. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13816058",
            "date": "2013-11-07T15:41:07+0000",
            "content": "DoPivots is called on every shard once to get each shard's response. It has built into it support for refinement but this first run through has no refinement information yet.\n\nWhen the master box sends out its refinement requests, DoPivots is run again on the shards recieving requests, at this point it utilizes the refinement steps DoPivots has, and blows up on dates, because instead of getting something like \"1995-12-31T23:59:59Z\" it gets \"Tuesday December 12th, 1995 at 23:59\". The latter does not convert to the former, so the entire thing blows up. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13818422",
            "date": "2013-11-10T11:37:05+0000",
            "content": "I didn't manage to make ditributed pivot on date field to blow up with toObject.\nCan you please attach an example query that blows Solr up and I'll adjust it to my environment? \n\nThanks.  "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13894547",
            "date": "2014-02-07T14:03:02+0000",
            "content": "FYI, the last distributed pivot facet patch functionally works, but there are some sub-optimal data structures being used and some unnecessary duplicate processing of values.  As a result, we found that for certain worst-case scenarios (i.e. data is not randomly distributed across Solr cores and requires significant refinement) pivot facets with multiple levels could take over a minute to aggregate and process results. This was using a dataset of several hundred million documents and dozens of pivot facets across 120 Solr cores distributed over 20 servers, so it is a more extreme use-case than most will encounter.\n\nNevertheless, we've refactored the code and data structures and brought the processing time from over a minute down to less than a second using the above configuration. We plan to post the patch within the next week. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13894554",
            "date": "2014-02-07T14:11:02+0000",
            "content": "Sweet, nice work Trey! "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13895413",
            "date": "2014-02-08T03:38:36+0000",
            "content": "Thanks, Yonik. I worked on the architecture and design, but it's really been a team effort by several of us at CB. Chris worked with the initial patch, Andrew hardened it, and Brett (who will post the next version) focused on the soon-to-be-posted performance optimizations. We're deploying the new version to production right now to sanity check it before posting the patch, but I think the upcoming version will finally be ready for review for committing. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13904524",
            "date": "2014-02-18T20:16:02+0000",
            "content": "This is the updated version of our implementation of Pivot Facets, as mentioned by Trey.  We have significantly improved performance for cases which involve a large number of shards through changing the underlying data structure and the way that data from the shards is merged together. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13910397",
            "date": "2014-02-24T15:21:49+0000",
            "content": "Brett (and your team), thank you very much for your hard work. We've been having a lot of performance issues with using the previous version patch (which we love), and initial testing shows they are now resolved.\n\nWe did also notice a few issues:\n\n1) f.field.facet.limit=-1 is not being respected (as reported by me on 20/May/13 10:27)\n\n2) pivot queries are returning String instead of Object (as reported by me on 25/Aug/13 07:38) except for boolean fields. \nI know the reason is there was a problem with datetime fields. I changed it back to toObject and I can't reproduce any issues running unit test locally via maven.\nI'd be glad to help fix this problem, if anyone can create a simple test case that fails ?\n\n3) The following query throws an exception:\n\nq=:&rows=0&f.fieldA.facet.sort=index&f.fieldA.facet.limit=-1&f.fieldA.facet.missing=true&f.fieldA.facet.mincount=1&f.fieldB.facet.sort=index&f.fieldB.facet.limit=-1&f.fieldB.facet.missing=true&f.fieldB.facet.mincount=1&facet=true&facet.pivot=fieldA,fieldB&shards=127.0.0.1:8983/solr/shardA,127.0.0.1:8983/solr/shardB\n\njava.lang.IllegalArgumentException: fromIndex(0) > toIndex(-1) at java.util.ArrayList.subListRangeCheck(ArrayList.java:975) at java.util.ArrayList.subList(ArrayList.java:965) at org.apache.solr.handler.component.PivotFacetField.refineNextLevelOfFacets(PivotFacetField.java:276) at org.apache.solr.handler.component.PivotFacetField.queuePivotRefinementRequests(PivotFacetField.java:231) at org.apache.solr.handler.component.PivotFacet.queuePivotRefinementRequests(PivotFacet.java:86) at org.apache.solr.handler.component.FacetComponent.countFacets(FacetComponent.java:565) at org.apache.solr.handler.component.FacetComponent.handleResponses(FacetComponent.java:413) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:311) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1904) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:659) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:362) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:158) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1474) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) at org.eclipse.jetty.server.Server.handle(Server.java:370) at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:489) at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53) at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:949) at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1011) at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72) at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) at java.lang.Thread.run(Thread.java:804)\n\nThank you again for this great patch!\n\n\n "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13910562",
            "date": "2014-02-24T17:41:57+0000",
            "content": "We are at DevNexus today and tomorrow, but Brett, Andrew and I will look at it on Weds. Thanks for the feedback. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13911259",
            "date": "2014-02-25T05:25:28+0000",
            "content": "Elran \u2013 Thanks for the feedback.  I'm glad to hear the performance issues have been fixed.  The exception is being thrown because the facet limit is -1 which causes an attempt to allocate a negatively sized sub list.  If you need a work around for the short term, just avoid using that for the facet limit.  I will try to fix that this week. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13911620",
            "date": "2014-02-25T14:39:15+0000",
            "content": "This is an update to the previous patch I uploaded which excludes whitespace changes and eliminates dead code.  This does not yet include a fix for the -1 facet limit. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13912660",
            "date": "2014-02-26T09:03:22+0000",
            "content": "By the way, when I profiled our very slow distributed pivot, I noticed most of the time is wasted in val.get in trimExcessValuesBasedUponFacetLimitAndOffset in PivotFacetHelper.java.\nThe following change (in the first two lines of the function) has shown a significant improvement:\n List<NamedList<Object>> newVal = new LinkedList<NamedList<Object>>();\n if (val == null) return val;\nto:\nif (val == null) return val;\nList<NamedList<Object>> newVal = new ArrayList<NamedList<Object>>(val.size());\n\nThanks. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13916293",
            "date": "2014-02-28T20:06:25+0000",
            "content": "Hi Elran,\n\nRegarding the string/object issue:  We have not been able to revert back to toObject for all data types because doing so results in the following exception being thrown by the testDistribSearch case of DistributedFacetPivotTest.\norg.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: Invalid Date String:'Sat Sep 01 08:30:00 BOT 2012\n\nThis is part of the test case which demonstrates the issue:\n    //datetime\n    this.query( \"q\", \":\",\n                \"rows\", \"0\",\n                \"facet\",\"true\",\n                \"facet.pivot\",\"hiredate_dt,place_s,company_t\",\n                \"f.hiredate_dt.facet.limit\",\"2\",\n                \"f.hiredate_dt.facet.offset\",\"1\",\n            FacetParams.FACET_LIMIT, \"4\"); //test default sort (count)\n\nI am producing that error by running:\nant -Dtests.class=\"org.apache.solr.handler.component.DistributedFacetPivotTest\" clean test "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13920417",
            "date": "2014-03-05T02:49:10+0000",
            "content": "We have been trying to finish up the distributed pivot facet patch to submit it for committing, but we've run into one issue raised in the JIRA issue for the patch.\n\nDistributed field facets appear to convert all values to strings, and refine on those strings when needed.  The solrj test cases around the distributed field facets also expect all values to be represented as strings.  Please correct me if I'm wrong.\n\nThe most recent revision of the patch for SOLR-2894 converts all values to strings as well, except in the case of booleans.  If we don't have this exception, the solrj test case of distributed pivot facets will fail because there is an assert verifying that the value returned is Boolean.TRUE rather than \"true\".  If we do not convert object values to strings, we are unable to refine on date time's because the format will be incorrect and the date time class is unable to convert it back.\n\nOur feeling is that this assert is incorrect and that it should be looking for the string true.  Changing this test case would allow us to bring the behavior of distributed pivot facets into consistency with the behavior of distributed field facets.  We didn't want to just go ahead and make that change because it would change the behavior of solrj.\n\nIt appears our options are:\n1.  Convert all values to strings.  Modify the solrj test case to expect this.  We feel this makes things consistent and is the ideal option.\n2.  Convert all values exception booleans to strings.\n3.  Leave all values except datettimes as is.\n\nWe are just looking for some input from the community prior to adding this change to our patch. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13920946",
            "date": "2014-03-05T15:11:08+0000",
            "content": "I think we should preserve object values in disruibuted pivot, as in regular pivot.\nI want to help fix the toObject problem with datetime fields.\nI tried to apply the most recent patch to the latest Solr trunk revision. There were some problems applying it.\nCan you please create a new patch against latest Solr trunk revision, or indicate which revision the patch was created againt?\n\nThanks! "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13921047",
            "date": "2014-03-05T17:01:19+0000",
            "content": "This is an updated patch which should apply cleanly against trunk.  I've used this against revision 885cdea13918fee0c49d5ac0c5fa1fd286d5b466.\n\nThis should include a fix for the unlimited facet that Elran brought up.  It does not address the toObject issue being discussed.\n\nDoes anyone have additional input or thoughts as to which route to go with the toObject/string issue? "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13922092",
            "date": "2014-03-06T07:13:24+0000",
            "content": "Thanks for the patch.\nI will take a lool at the toObject problem with datetime fields.\nDoes the patch fix issues 1 and 3 I reported on February 24?\n\nThanks. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13922686",
            "date": "2014-03-06T16:11:20+0000",
            "content": "Elran - A facet limit of -1 in distributed pivot facets is not a use case we use in our environment, but we did go ahead and make the fixes in order to support the community.  I've tested the changes locally on a box with success and added unit tests around it, but we have not yet deployed those changes to a production cluster.  The exception you were seeing was directly related to the facet limit being negative, and that has been fixed in the patch I uploaded yesterday. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13925156",
            "date": "2014-03-09T09:08:05+0000",
            "content": "I think I solved the the toObject problem with datetime fields.\nPlease see the patch attached.\nAll tests pass now.\nLet me know what you think.\nThanks. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13925211",
            "date": "2014-03-09T15:00:17+0000",
            "content": "I have checked the latest patch.\nProblem 3 (field with negative limit threw exception) is now solved. Thanks!\nBut I still see problem 1 (f.field.facet.limit=-1 is not being respected).\n\nThank you very much.   "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13925768",
            "date": "2014-03-10T14:44:37+0000",
            "content": "Elran - Can you give me an example test case or query for which the -1 facet limit fails?  I'll be glad to take a look and fix it if I can reproduce an issue with it. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13925787",
            "date": "2014-03-10T15:22:13+0000",
            "content": "Hi,\n\nI don't know where I should exactly put the test in DistributedFacetPivotTest, but this the test:\n1)index more than 100 docs (you can index docs only with id)\n2)run  the following query:\nthis.query( \"q\", \":\",\n            \"rows\", \"0\",\n            \"facet\",\"true\",\n            \"facet.pivot\",\"id\",\n            \"f.id.facet.limit\",\"-1\");\n\nyou expect to get as many ids as you indexed, but you will get only 100.\n\nThanks. "
        },
        {
            "author": "Chris Russell",
            "id": "comment-13925995",
            "date": "2014-03-10T18:21:07+0000",
            "content": "Elran, interesting, does that happen if you use facet.limit=-1 instead of the f.fieldname.facet.limit syntax?  I am wondering if some code is checking the global limit but not the per-field limit. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13930026",
            "date": "2014-03-11T06:56:18+0000",
            "content": "No.\nIt doesn't happen when I use facet.limit=-1 instead of the f.fieldname.facet.limit syntax.\n\nThanks. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13930835",
            "date": "2014-03-11T19:35:41+0000",
            "content": "I've uploaded the newest version of the patch.  This includes a fix for the -1 facet limit when specified on a specific field and incorporates Elran's toObject fix. "
        },
        {
            "author": "AJ Lemke",
            "id": "comment-13932304",
            "date": "2014-03-12T20:26:32+0000",
            "content": "Thanks for the hard work on this.  \n\nAfter applying the patch (to trunk) I noticed that the results for the pivot facet have changed from the example given here:\nhttp://wiki.apache.org/solr/SimpleFacetParameters#Pivot_.28ie_Decision_Tree.29_Faceting-1\nMy results:  http://pastebin.com/zs9rWMC5\nIs this expected behavior?\n\nAlso I am getting \"java.lang.String cannot be cast to org.apache.lucene.util.BytesRef\" errors when sorting boolean fields.\nhttp://localhost:8983/solr/select?wt=json&q=*:*&fl=*, score&sort=inStock desc\nMy results: http://pastebin.com/x9QnGfZA\n\nMy environment: http://pastebin.com/AyMAwXD3 "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13935726",
            "date": "2014-03-14T22:04:08+0000",
            "content": "AJ - It appears that the pivot output is different in trunk because the example data has changed.  I manually checked it against the trunk example data and the counts you are seeing look to be correct.  The wiki is just based on older example files.\n\nI tried your second query on a 4.2 box and a trunk box and neither of them had an exception thrown.  Do you get that error with the same version of Solr without using this patch?  If so, that might be something worth raising on the solr users list. "
        },
        {
            "author": "AJ Lemke",
            "id": "comment-13945336",
            "date": "2014-03-24T16:50:45+0000",
            "content": "Brett,\n\nWhen looking at the the output of the pivot, the data type has changed from a struct to an array. \nIs this the expected behavior or is the data type for each of the pivot facets to remain as a struct and I am experiencing a bug? \n\nExamples:\n4.7 pre patch:\n{\n  field: \"cat\",\n  value: \"electronics\",\n  count: 12,\n  pivot: [\n    \n{field: \"popularity\",value: 7,count: 4}\n,\n    \n{field: \"popularity\",value: 6,count: 3}\n,\n    \n{field: \"popularity\",value: 1,count: 2}\n,\n    \n{field: \"popularity\",value: 0,count: 1}\n,\n    \n{field: \"popularity\",value: 5,count: 1}\n,\n    {field: \"popularity\",value: 10,count: 1}\n  ]\n},\n\nPost Patch:\n[\n  \"field\",\n  \"cat\",\n  \"value\",\n  \"electronics\",\n  \"count\",\n  12,\n  \"pivot\",\n  [\n    [\"field\",\"popularity\",\"value\",7,\"count\",4],\n    [\"field\",\"popularity\",\"value\",6,\"count\",3],\n    [\"field\",\"popularity\",\"value\",1,\"count\",2],\n    [\"field\",\"popularity\",\"value\",0,\"count\",1],\n    [\"field\",\"popularity\",\"value\",5,\"count\",1],\n    [\"field\",\"popularity\",\"value\",10,\"count\",1]\n  ]\n],\n\n\nEdit: formatting. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13959081",
            "date": "2014-04-03T18:51:07+0000",
            "content": "Hi AJ,\n\nI just pulled the latest code from the repo, and when I patched the tip of the 4.7 branch with the SOLR-2894 patch, I am not able to reproduce this issue.  (Revision 5981529c65d4ae671895948f43d8770daa58746b)  This is the output I get using a pivot facet of cat,popularity.  I used this url while running the example: http://localhost:8983/solr/select?wt=json&q=*:*&rows=0&facet=true&facet.pivot=cat,popularity\n\n[\n        {\n          \"field\": \"cat\",\n          \"value\": \"electronics\",\n          \"count\": 12,\n          \"pivot\": [\n            \n{ \"field\": \"popularity\", \"value\": 7, \"count\": 4 }\n,\n            \n{ \"field\": \"popularity\", \"value\": 6, \"count\": 3 }\n,\n            \n{ \"field\": \"popularity\", \"value\": 1, \"count\": 2 }\n,\n            \n{ \"field\": \"popularity\", \"value\": 0, \"count\": 1 }\n,\n            \n{ \"field\": \"popularity\", \"value\": 5, \"count\": 1 }\n,\n            { \"field\": \"popularity\", \"value\": 10, \"count\": 1 }\n          ]\n        }, \n\t... \n]\n\nCould you try with this same revision, applying only the SOLR-2894 patch and let me know if you see something different?\n\n-Brett "
        },
        {
            "author": "AJ Lemke",
            "id": "comment-13960125",
            "date": "2014-04-04T16:58:38+0000",
            "content": "Hi Brett,\n\nI grabbed revision \"5981529c65d4ae671895948f43d8770daa58746b\" from the git repository and ran my tests.  \nI am seeing still seeing the pivots as arrays rather than JSON objects.\n\nIf I change numShards to 1 I see the the pivots as an array of objects.\n(http://localhost:8983/solr/admin/collections?action=CREATE&name=collection1&numShards=1&replicationFactor=2&maxShardsPerNode=4)\n\n    field: \"cat\",\n    value: \"electronics\",\n    count: 12,\n    pivot: [\n        {field: \"popularity\",value: 7,count: 4},\n        {field: \"popularity\",value: 6,count: 3},\n        {field: \"popularity\",value: 1,count: 2},\n        {field: \"popularity\",value: 0,count: 1},\n        {field: \"popularity\",value: 5,count: 1},\n        {field: \"popularity\",value: 10,count: 1}\n    ]\n\n\n\nIf I change numShards to > 1 I see the the pivots as an array of arrays.\n(http://localhost:8983/solr/admin/collections?action=CREATE&name=collection1&numShards=2&replicationFactor=2&maxShardsPerNode=4)\n\n    \"field\",\n    \"cat\",\n    \"value\",\n    \"electronics\",\n    \"count\",\n    12,\n    \"pivot\",\n    [\n        [\"field\",\"popularity\",\"value\",7,\"count\",4],\n        [\"field\",\"popularity\",\"value\",6,\"count\",3],\n        [\"field\",\"popularity\",\"value\",1,\"count\",2],\n        [\"field\",\"popularity\",\"value\",0,\"count\",1],\n        [\"field\",\"popularity\",\"value\",5,\"count\",1],\n        [\"field\",\"popularity\",\"value\",10,\"count\",1]\n    ]\n\n\n\nI am using Windows 7 as my test environment and starting with this batch file:\n\nBatch File\necho Deleting Temp Data\nrmdir /s /q D:\\dev\\lucene-solr\\solr\\node1\nrmdir /s /q D:\\dev\\lucene-solr\\solr\\node2\n\necho Copying the example folder\nxcopy /s /e /y /i D:\\dev\\lucene-solr\\solr\\example D:\\dev\\lucene-solr\\solr\\node1\nxcopy /s /e /y /i D:\\dev\\lucene-solr\\solr\\example D:\\dev\\lucene-solr\\solr\\node2\n\necho removing the collection1 folder\nrmdir /s /q D:\\dev\\lucene-solr\\solr\\node1\\solr\\collection1\nrmdir /s /q D:\\dev\\lucene-solr\\solr\\node2\\solr\\collection1\n\necho Starting Solr Processes\ncd D:\\dev\\lucene-solr\\solr\\node1\nstart cmd /c java -DzkRun -jar start.jar\nsleep 10\n\ncd D:\\dev\\lucene-solr\\solr\\node2\nstart cmd /c java -Djetty.port=7574 -DzkRun -DzkHost=localhost:9983 -jar start.jar\n\necho All Solr Processes started\n\nsleep 10\n\ncd D:\\dev\\lucene-solr\\solr\\example\\scripts\\cloud-scripts\ncmd /c zkcli.bat -zkhost localhost:9983 -cmd upconfig -confdir D:\\dev\\lucene-solr\\solr\\example\\solr\\collection1\\conf -confname collection1\ncmd /c zkcli.bat -zkhost localhost:9983 -cmd linkconfig -collection collection1 -confname collection1\necho Zookeeper updated to contain collection1\n\ncmd /c curl \"http://localhost:8983/solr/admin/collections?action=CREATE&name=collection1&numShards=2&replicationFactor=2&maxShardsPerNode=4\"\ncmd /c curl \"http://localhost:8983/solr/admin/collections?action=RELOAD&name=collection1\"\n\nsleep 10\n\ncd D:\\dev\\lucene-solr\\solr\\example\\exampledocs\ncmd /c java -Dtype=text/xml -Doptimize=yes -Durl=http://localhost:8983/solr/collection1/update -jar post.jar *.xml\ncmd /c java -Dtype=text/csv -Doptimize=yes -Durl=http://localhost:8983/solr/collection1/update -jar post.jar *.csv\ncmd /c java -Dtype=application/json -Doptimize=yes -Durl=http://localhost:8983/solr/collection1/update -jar post.jar *.json\necho Collection1 has been bootstrapped and optimized\n\n\n\nCould you show me how you are starting yours?\n\nAJ "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13962111",
            "date": "2014-04-07T18:31:49+0000",
            "content": "Hi AJ,\n\nThanks for all that info.  It was super helpful and I've narrowed down the issue.  If you'd like to fix it on your setup, you only need to make two changes to PivotFacetValue.java.  At the top, add an import for org.apache.solr.common.util.SimpleOrderedMap, and in the convertToNamedList() function in that file, change this line:\n NamedList<Object> newList = new NamedList<Object>();\nto\n NamedList<Object> newList = new SimpleOrderedMap<Object>();\n\nI will be posting a new SOLR-2894 patch within the next day or two, but that patch will be for trunk and will likely not apply cleanly to the 4.7 branch.  If you need this working in 4.7, make the above changes.  If you are using trunk, then this fix will be incorporated into the upcoming patch.\n\n-Brett "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13962294",
            "date": "2014-04-07T21:41:47+0000",
            "content": "I've uploaded a patch to include changes needed to patch against trunk. (Revision caccba783be7c9f4d7b25c992ed4c49e5a2bddf7).  Additionally, this fixes the JSON output formatting issue discovered by AJ. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971291",
            "date": "2014-04-16T12:57:41+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13973094",
            "date": "2014-04-17T16:03:15+0000",
            "content": "After nearly 2 years of on-and-off development, I think this patch is finally ready to be committed. Brett's most recent patch includes significant performance improvements as well as fixes to all of the reported issues and edge cases mentioned by the others currently using this patch. We have just finished a large spike of work to get this ready for commit, so I'd love to get it pushed in soon unless there are any objections.\n\nErik Hatcher, do you have any time to review this for suitability to be committed (since you are the reporter)? If there is anything additional that needs to be changed, I'll happily sign us up (either myself or someone on my team at CareerBuilder) to do it it will help. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13978002",
            "date": "2014-04-23T09:03:12+0000",
            "content": "We have encountered a Java heap memory problem using distributed pivots \u2013 perhaps someone can shed some light on it.\n\nThe scenario is as follows:\nWe run Solr 4.4 (with patch for SOLR-2894) with 20 cores and the maximum java heap size is 1.5 GB.\nThe following query with distributed facet pivot generates an out of memory exception:\nrows=0&\nq=:&\nfacet=true&\nfacet.pivot=f1,f2,f3,f4,f5&\nf.f1.facet.sort=count&\nf.f1.facet.limit=10&\nf.f1.facet.missing=true&\nf.f1.facet.mincount=1&\nf.f2.facet.sort=index&\nf.f2.facet.limit=-1&\nf.f2.facet.missing=true&\nf.f2.facet.mincount=1&\nf.f3.facet.sort=index&\nf.f3.facet.limit=-1&\nf.f3.facet.missing=true&\nf.f3.facet.mincount=1&\nf.f4.facet.sort=index&\nf.f4.facet.limit=-1&\nf.f4.facet.missing=true&\nf.f4.facet.mincount=1&\nf.f5.facet.sort=index&\nf.f5.facet.limit=-1&\nf.f5.facet.missing=true&\nf.f5.facet.mincount=1&\nshards=127.0.0.1:8983/solr/shard1,127.0.0.1:8983/solr/shard2\n\nNumber of docs in each shard:\nshard1: 16,234\nshard2: 169,089\n\nThese are the fields terms' distribution:\nf1: shard1 - 16,046, shard2 - 38\nf2: all shards - 232\nf3: all shards - 53\nf4: all shards - 6\nf5: all shards - 10\n\nWhen we use a maximum java heap size of 8GB, the query finishes. It seems about of 6GB is used for pivoting.\nIt doesn\u2019t seem reasonable that the facet.pivot on 2 cores with 200,000 docs requires that much memory.\n\nWe tried looking into the code a little and it seems the sharded queries run with facet.pivot.mincount=-1 as part of the refinement process.\nWe also noticed that in this scenario, the parameter skipRefinementAtThisLevel in the method queuePivotRefinementRequests in the class PivotFacetField is false.\nWe think all of this is the cause of the memory consumption \u2013 but we couldn't pinpoint the underlying issue.\n\nIs there a way to alter the algorithm to consume less memory?\nIf anyone can explain offline the way refinement works here, we would be happy to try and help resolve this.\n\nThank you very much. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13978442",
            "date": "2014-04-23T16:52:43+0000",
            "content": "Hi Elran,\n\nHaving a mincount of -1 for the shards is correct.  The reason is that while a given shard may have a count lower than mincount for a given term, the aggregate total count for that value when combined with the other shards could exceed the mincount, so we do need to know about it.  For example, consider a mincount of 10.  If we have 3 shards with a count of 5 for a term of \"Boston\", we would still need to know about these because the total count would be 15, and would be higher than the mincount.\n\nI would expect the skipRefinementAtThisLevel to be false for the top level pivot facet, and true for each other level.  Are you seeing otherwise?\n\nIf you were to set a facet.limit of 10 for all levels of the pivot, what is the memory usage like?\n\n-Brett "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13979128",
            "date": "2014-04-24T00:22:10+0000",
            "content": "We should get this in to get more feedback. Wish I had some time to tackle it, but I won't in the near term. "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-13979420",
            "date": "2014-04-24T07:58:31+0000",
            "content": "Brett, thanks for your response.\n\n>>Having a mincount of -1 for the shards is correct. The reason is that while a given shard may have a count lower than mincount for a given term, the aggregate total count for that value >>when combined with the other shards could exceed the mincount, so we do need to know about it. For example, consider a mincount of 10. If we have 3 shards with a count of 5 for a term >>of \"Boston\", we would still need to know about these because the total count would be 15, and would be higher than the mincount.\nIf mincount of 1 is asked for a field, couldn't it be more efficient? Is mincount of -1 necessary in this case?\n>>I would expect the skipRefinementAtThisLevel to be false for the top level pivot facet, and true for each other level. Are you seeing otherwise? \nNo. You are right.\n>>If you were to set a facet.limit of 10 for all levels of the pivot, what is the memory usage like?\nThe memory usage in this case is about 200 MB.\n\nThanks again.  "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13979898",
            "date": "2014-04-24T16:19:09+0000",
            "content": "Andrew actually raised that question to me yesterday as well and I spent a little bit of time looking into it.  For the initial request to a shard, we only lower the mincount to 0 if the facet limit is set to something other than -1.  If the facet limit is -1, we lower the mincount to 1.  In your case, this would the limit would be 10 for the top level pivot, so we know we will (at most) get back 15 terms from each shard in this case.  Because we are only faceting on a limited number of terms, having a mincount of 0 here provides us the benefit of potentially avoiding refinement.  In refinement requests, we still need to know when a shard has responded to us with it's count for a term, so the mincount is -1 in that case because we are interested in the term even if the count is zero.  It allows us to mark the shard as having responded and continue on.  It's possible that we might be able to change this, but at the point of refinement, it's a rather targeted request so I don't expect there to be a significant benefit to doing so.  In your case, with the facet limit being -1 on f2-f5, no refinement would be performed anyway.\n\nWhen we designed this implementation, the most important factor for us was speed, and we were willing to get it at a cost of memory.  By making these changes, we reduced queries which previously took around 70 seconds for us down to around 600 milliseconds.  I suspect that the biggest factor in the poor memory utilization is the wide open nature of using a facet.limit of -1, especially on a pivot so deep.  Keep in mind that for each level of depth you add to a pivot, memory and time required will grow exponentially.\n\nDon't forget that if you are querying a node and all of the shards are located within the same Java VM, you are incurring the memory cost of both shards plus the node responding to the user query all within the same heap.\n\nI took a quick look at the code today while waiting for some other processes to finish, and I don't see any obvious low hanging fruit to free up memory.   "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13980597",
            "date": "2014-04-25T01:20:15+0000",
            "content": ">>Mark Miller said:\n>>We should get this in to get more feedback. Wish I had some time to tackle it, but I won't in the near term. \n\nIs there a committer who has interest in this issue and would be willing to look over it for (hopefully) getting it pushed into trunk?  It's the top voted for and the top watched issue in Solr right now, so there's clearly a lot of community interest. Thanks! "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13980624",
            "date": "2014-04-25T02:09:44+0000",
            "content": "If nobody volunteers, since this doesn't change existing behaviour (right?) I suggest just committing it and improving it from there once people start using it and suggesting improvements/bug fixes. "
        },
        {
            "author": "Trey Grainger",
            "id": "comment-13980662",
            "date": "2014-04-25T03:52:23+0000",
            "content": "Hi Otis, I appreciate your interest here. That's correct: no previously working behavior was changed, and there are two things added with this patch: 1) distributed support, and 2) support for a single-level pivot facets (this previously threw an exception but is now supported: facet.pivot=aSingleFieldName).\n\nFor context on #2, we found no good reason to disallow a single-level pivot facet (functions like a field facet but with the pivot facet output format), it made implementing distributed pivot faceting easier since a single level could be considered when refining, and there was work in some downstream issues like SOLR-3583 (adding percentiles and other stats to pivot facets) which was dependent upon being able to easily alternate between any number of facet levels for analytics purposes, so we just added the support for a single level. This also makes it easier to build analytics tools without having to arbitrarily alternate between field facets and pivot facets and their corresponding output formats based upon the number of levels.\n\nThe end result is that no previously working capabilities have been modified, but distributed support for any number of pivot levels has been added, which should make this safe to commit to trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13981308",
            "date": "2014-04-25T17:50:21+0000",
            "content": "I have not worked on faceting code in the past, so this is really not my area.\n\nHowever, here is a patch I just worked up to apply against 5x. I had to make some small changes - DateField is deprecated and there was an ndate field in the tests that could not be found. I removed it in this patch. I also fixed a few issues around licenses and formatting - this patch passes precommit except for a nocommit at the DateField change - someone should review if that change has any other ramifications. All tests pass.\n\nThis code does touch some existing faceting code in a way that demands a deeper review I think, but until I have a lot more time, I'm not the man for that job. Perhaps Chris Hostetter (Unused)? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13990122",
            "date": "2014-05-06T00:14:33+0000",
            "content": "I'm going to spend some time this week reviewing the state of things.\n\nFirst up, some minor tweaks to the latest patch...\n\n\n\tfixed a typo in TestDistributedSearch (\"facet.fiedl\" -> \"facet.field\")\n\t\n\t\tthis is my biggest anoyance about most of our existing distributed serach tests \u2013 they just assert that queries return the same thing as single node tests, but don't assert anything about the response, so mistakes in the input, or mistakes in indexing hte docs, resulting in a useless test aren't caught)\n\t\tthis also relates to marks comment about removing \"ndate\" since that field no longer exists in the test configs - using tdate_a & tdate_b here should be fine\n\t\n\t\n\tremoved the \"nocommit\" mark mentioned regarding DateField - that method moved to TrieDateField so his fix is correct.\n\n\n\n\nSome comments/questions based on what I've reviewed so far (note: many of these comments/questions come from a place of genuine ignorance since i've only reviewed about 30% of the patch so far)...\n\n\n\teven at a glance, it's obvious the SimpleFacets changes are a simple refactoring and totally fine.\n\tIn FacetComponent - Setting asside the core pivot facet changes...\n\t\n\t\tMost of the other changes in seem like straight forward (and much needed!) variable renaming (+1) to help eliminate ambiguity between the existing field faceting refinement and the new pivot faceting refinement\n\t\tthe new \"fieldsToOverRequestOn\" Map confuses me in a few ways...\n\t\t\n\t\t\tAs is, i don't understand why this is a Map and not a Set.\n\t\t\tSome odd conditional logic is used when iterating over this \"Set\" to determine the overrequest limit - i'm still trying to wrap my head arround this but in particular the comment // dff has the info, the params have been scrubbed confuses me \u2013 where are these params \"scrubbed\" ?\n\t\t\tI like these new explict overrequest count/ratio params, and i get that the end-game here is that they can be used to affect the amount of overrequest done for both fact.field and facet.pivot \u2013 but i'm not understanding the value of building up this \"fieldsToOverRequestOn\" set of names (for every shard request) and then iterating over it and consulting either the DFF or the params to decide which limit value to use on the shard requests, and then (conditionally?) removing the limit/offset/overrequest params from the shard requests.  Wouldn't it be simplier to have modifyRequest always remove the limit/offset/overrequest params from the shard params, and then have the individual code paths (for both facet.field & facet.pivot) take responsibility for adding back the new limit params based on the overrequest calculations using the original request params (ie: rb.req.getParams()).\n\t\t\tMy chief concern here being that (at first glance) this change seems like it adds a small amount of overhead to the overrequest limit calculations, and makes this bit of code more confusing, w/o any obvious (to me) advantage.\n\t\t\n\t\t\n\t\n\t\n\tI don't yet understand the need for the new \"PURPOSE_REFINE_PIVOT_FACETS\" stage of shard requests? ... can someone clarify why  pivot facets can't just be refined during the existing \"PURPOSE_REFINE_FACETS\" stage?\n\tI notice that the new DistributedFacetPivotTest directly extends BaseDistributedSearchTestCase and uses a fixed shard count, and indexes some docs directly to certain clients\n\t\n\t\tis there something about the functionality (or about the test) that requires certain data locality (ie: certain docs on same shard) to work?\n\t\tif not: is there any other reason we can't switch this over to a Cloud based test with a variable numbers of shards and compairons against the control collection?\n\t\n\t\n\n\n "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13990941",
            "date": "2014-05-06T18:23:31+0000",
            "content": "I spoke with Andrew regarding the tests.  The test does do some Query() testing to verify that the output of distributed and single node is the same, however further down it does to explicit testing.  He said it's important that we don't use the variable number of shards in this particular because it exercises a number of specific sharding situations to ensure that we get the correct answers.\n\nLooking at fieldsToOverRequestOn, I think you are correct.  It looks like this probably could be a set.  I suspect this is an artifact of multiple pass-throughs at the implementation of this feature.  I will spend some time on this on Thursday to see if I can clean this area up and make it a little more straight forward.\n\nRegarding PURPOSE_REFINE_PIVOT_FACETS:  If it was truly needed, we probably could use a single purpose at a very slight cost.  Keep in mind that the purposes are OR'd together, so they should happen in the same shard request.  Having two different purposes allows us to call refineFacets and/or refinePivotFacets only as needed to avoid looping through the shard responses an extra time.  If your comment is more to the effect that the loop in refineFacets() and refinePivotFacets() could be merged into a single loop \u2013 that's probably true and if you feel it's a better route to go, let me know and I can work on that change. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13991118",
            "date": "2014-05-06T20:55:54+0000",
            "content": "He said it's important that we don't use the variable number of shards in this particular because it exercises a number of specific sharding situations to ensure that we get the correct answers.\n\nthat makes complete sense \u2013 i just wanted to make sure it's a test requirement and not a feature requirement.  We should definitely comment the test to that effect, and add a more randomized cloud based test with some dynamic shard assignment as well to try and help catch strange edge cases.  I'll try to work on that to help me better understand the feature (in general, it's been a long time since i looved at pivot faceting)\n\nI will spend some time on this on Thursday to see if I can clean this area up and make it a little more straight forward.\n\nthat would be great, thanks.  The Map vs Set aspect would be a trivial improvement, but in general i'm more concerned about the odd flow \u2013 it seems like something that could easily bite us in the ass later when people try to maintain it.  It seems like it would be a lot more straight forward to:\n\n\thave modifyRequest unconditionally remove all limit/offset/overrequest params from the shard requests\n\thave a simple \"getOverrequestLimitAmount(String fieldName)\" method in the FacetInfo class (that consults the original request params)\n\thave each code path (facet.field & facet.pivot & whatever down the road...) that sets params on the shard request and cares about over requesting call sreq.params.set(pre + FACET_LIMIT, getOverrequestLimitAmount(fieldName))\n\n\n\nHaving two different purposes allows us to call refineFacets and/or refinePivotFacets only as needed to avoid looping through the shard responses an extra time. If your comment is more to the effect that the loop in refineFacets() and refinePivotFacets() could be merged into a single loop...\n\nI don't have an opinion, i just wanted to understand the purpose of the new \"PURPOSE\" (heh) since we don't have a special one for range facets or query facets \u2013 but in hindsight i realize that of course we don't: those don't need refinement.  "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13991142",
            "date": "2014-05-06T21:12:53+0000",
            "content": "\nI don't have an opinion, i just wanted to understand the purpose of the new \"PURPOSE\" (heh) since we don't have a special one for range facets or query facets \u2013 but in hindsight i realize that of course we don't: those don't need refinement.\nPrimarily it's just for performance.  Field facet refinement should only occur once, but pivot facet refinement may occur a number of times due to the tiering. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13993233",
            "date": "2014-05-08T22:53:30+0000",
            "content": "\nI still haven't had a chance to really dig into the implementation details of the patch, but i wanted to spend some time testing things out from a user perspective...\n\n\n\nOne of the first things i noticed, is that the refinement requests seem to be extra verbose.  For example, given this user request (using the example data, with a 2 shard cloud setup):\n\nhttp://localhost:8983/solr/select?q=*:*&sort=id+desc&rows=2&facet=true&facet.pivot=cat,inStock&facet.limit=3&facet.pivot=manu_id_s,inStock\n\nThis is what the refinement requests in the logs of each shard looked like...\n\n\n3434041 [qtp1282186295-19] INFO  org.apache.solr.core.SolrCore  \u2013 [collection1] webapp=/solr path=/select params={manu_id_s,inStock_8__terms=samsung&facet=true&sort=id+desc&facet.limit=3&manu_id_s,inStock_9__terms=viewsonic&distrib=false&cat,inStock_1__terms=search&wt=javabin&version=2&rows=0&manu_id_s,inStock_6__terms=maxtor&manu_id_s,inStock_7__terms=nor&NOW=1399584452682&shard.url=http://127.0.1.1:8983/solr/collection1/&df=text&cat,inStock_2__terms=software&q=*:*&manu_id_s,inStock_3__terms=canon&manu_id_s,inStock_4__terms=ati&facet.pivot.mincount=-1&isShard=true&cat,inStock_0__terms=hard+drive&facet.pivot={!terms%3D$cat,inStock_0__terms}cat,inStock&facet.pivot={!terms%3D$cat,inStock_1__terms}cat,inStock&facet.pivot={!terms%3D$cat,inStock_2__terms}cat,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_3__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_4__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_5__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_6__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_7__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_8__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_9__terms}manu_id_s,inStock&manu_id_s,inStock_5__terms=eu} hits=14 status=0 QTime=3 \n\n3424918 [qtp1282186295-16] INFO  org.apache.solr.core.SolrCore  \u2013 [collection1] webapp=/solr path=/select params={cat,inStock_10__terms=memory&manu_id_s,inStock_15__terms=dell&facet=true&manu_id_s,inStock_12__terms=apple&sort=id+desc&facet.limit=3&manu_id_s,inStock_13__terms=asus&manu_id_s,inStock_16__terms=uk&distrib=false&wt=javabin&manu_id_s,inStock_14__terms=boa&version=2&rows=0&NOW=1399584452682&shard.url=http://127.0.1.1:7574/solr/collection1/&df=text&manu_id_s,inStock_11__terms=corsair&q=*:*&facet.pivot.mincount=-1&isShard=true&facet.pivot={!terms%3D$cat,inStock_10__terms}cat,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_11__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_12__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_13__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_14__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_15__terms}manu_id_s,inStock&facet.pivot={!terms%3D$manu_id_s,inStock_16__terms}manu_id_s,inStock} hits=18 status=0 QTime=2 \n\n\n\nOr if we prune that down to just the interesting params (as far as pivot faceting goes)...\n\nshard1\nfacet.pivot.mincount=-1\ncat,inStock_0__terms=hard+drive\ncat,inStock_1__terms=search\ncat,inStock_2__terms=software\nmanu_id_s,inStock_3__terms=canon\nmanu_id_s,inStock_4__terms=ati\nmanu_id_s,inStock_5__terms=eu\nmanu_id_s,inStock_6__terms=maxtor\nmanu_id_s,inStock_7__terms=nor\nmanu_id_s,inStock_8__terms=samsung\nmanu_id_s,inStock_9__terms=viewsonic\nfacet.pivot={!terms=$cat,inStock_0__terms}cat,inStock\nfacet.pivot={!terms=$cat,inStock_1__terms}cat,inStock\nfacet.pivot={!terms=$cat,inStock_2__terms}cat,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_3__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_4__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_5__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_6__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_7__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_8__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_9__terms}manu_id_s,inStock\n\n\n\nshard2\nfacet.pivot.mincount=-1\ncat,inStock_10__terms=memory\nmanu_id_s,inStock_11__terms=corsair\nmanu_id_s,inStock_12__terms=apple\nmanu_id_s,inStock_13__terms=asus\nmanu_id_s,inStock_14__terms=boa\nmanu_id_s,inStock_15__terms=dell\nmanu_id_s,inStock_16__terms=uk\nfacet.pivot={!terms=$cat,inStock_10__terms}cat,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_11__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_12__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_13__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_14__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_15__terms}manu_id_s,inStock\nfacet.pivot={!terms=$manu_id_s,inStock_16__terms}manu_id_s,inStock\n\n\n\nI believe that what's going on here is basically:\n\n\n\ttop level params are being used for the individual terms that need refined (which is smart, helps eliminate risk of terms needing special escaping with local params)\n\tthe top level param names for these terms that need refined use a per-(user)request \"global\" counter to ensure that they are unique (+1)\n\tthe top level term param names also include the facet.pivot spec they are needed for \u2013 this seems redundant since the counter is clearly global (even across multiple \"facet.pivot\" specs)\n\tthese top leve term param names are then added only to the shard requests where refinement is actually needed for those terms (+1) and are referenced as variables in facet.pivot commands using the \"terms\" local param (which the shards evidently look for to know when this is a refinement request)\n\tbecause many terms may need refinement, that means each user specified facet.pivot=X,Y param results in many shard params of facet.pivot={!terms=$N}X,Y\n\n\n\nI realize that local params don't play nice with multi-valued params at all, let alone make it easy to use a single variable to refer to a multi-valued param \u2013 But wouldn't it be simpler (and less verbose over the wire) to just ignore Solr's built in param variable derefrencing and instead generate 1 unique param name to use for all the terms we care about (for each unique pivot spec), and then refer to that name once in a local param for a single facet.pivot param (which the pivot facet could would then go and explicitly fetch from the top level SolrParams as a multi-value) \n\nThe result being, that instead of the refinement requests shows above, the refinement requests for each shard could be something much simpler like...\n\nshard1_proposed\nfacet.pivot.mincount=-1\n_fpt_1=hard+drive\n_fpt_1=search\n_fpt_1=software\n_fpt_2=canon\n_fpt_2=ati\n_fpt_2=eu\n_fpt_2=maxtor\n_fpt_2=nor\n_fpt_2=samsung\n_fpt_2=viewsonic\nfacet.pivot={!fpt=1}cat,inStock\nfacet.pivot={!fpt=2}manu_id_s,inStock\n\n\n\nshard2_proposed\nfacet.pivot.mincount=-1\n_fpt_1=memory\n_fpt_2=corsair\n_fpt_2=apple\n_fpt_2=asus\n_fpt_2=boa\n_fpt_2=dell\n_fpt_2=uk\nfacet.pivot={!fpt=1}cat,inStock\nfacet.pivot={!fpt=2}manu_id_s,inStock\n\n\n\n(where _fpt_ is just a short prefix for \"facet pivot terms\" that i pulled out of my ass)\n\n\n\nAnother thing I noticed is that with my 2 shard exampledocs setup, the following URL seems to send the pivot faceting into an infinite loop of refinement requests (note the typo: there's a space embeded in a field name manu_id_s != manu_+id_s )...\n\nhttp://localhost:8983/solr/select?q=*:*&sort=id+desc&rows=2&facet=true&facet.pivot=cat,manu_+id_s,inStock&facet.limit=3\n\n...not clear what's going on there, but definitely something that needs fixed before committing (\"garbage in -> garbage out\" is one thing, \"garbage in -> crash your cluster\" is another)\n\n\n\nthe multi-level refinement is sooooooo sweet. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13994027",
            "date": "2014-05-09T23:04:53+0000",
            "content": "Something's wonky with the way mincount is handled - if you run the attached \"pivot_mincount_problem.sh\" script while a 2 node cluster is running with th example configs you can see the problem by comparing these 3 URLs...\n\n\n\n\tPivot1: http://localhost:8983/solr/select?rows=0&wt=json&indent=true&q=single_7_s:%284%205%206%29&facet=true&facet.pivot=multi_50_ss,single_100_s&facet.limit=10\n\tFilter: http://localhost:8983/solr/select?rows=0&wt=json&indent=true&q=single_7_s:%284%205%206%29&fq=multi_50_ss:35&fq=single_100_s:79\n\tPivot2: http://localhost:8983/solr/select?rows=0&wt=json&indent=true&q=single_7_s:%284%205%206%29&facet=true&facet.pivot=multi_50_ss,single_100_s&facet.limit=10&facet.pivot.mincount=10\n\n\n\n\nAccording to the \"Pivot1\" URL, there are 4244 total docs matching the query, of those 586 match multi_50_ss:35 and of those 13 match single_100_s:79\n\nThis all jives with what the \"Filter\" URL tells us (where we ignore the pivot facets and just apply those as filters)\n\nBut if we add facet.pivot.mincount=10 to the original pivot request to get the \"Pivot2\" URL, no values for single_100_s make the cut as sub-facets of the 586 multi_50_ss:35 docs.\n\nLooking at the logs of the shard queries, it appears that facet.pivot.mincount=-1 is set only on the refinement queries, but non in the initial sub-shard queries (where the limit over requesting happens to find the top terms).  So terms that don't match above the mincount on at least one single shard won't be considered at all for the cumulative total. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13995229",
            "date": "2014-05-12T16:44:00+0000",
            "content": "Hmn.  Yes, both of those sound concerning.  I will take a look this week and get those squared away.  I did a bit of work last week to clean up the dff parameters block, but I still need to work out a few kinks there.  Let me know if you encounter anything else and I will continue to address these issues. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13995457",
            "date": "2014-05-12T18:50:35+0000",
            "content": "Hoss -\n\nI am working to prioritize the changes you've brought up.  While the size of the shard parameters may not strictly be as efficient as possible, is it such that we can run with that for now and circle back to this at a later point, or are you uncomfortable with including the parameters as is in the initial commit?\n\n-Brett "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-13996656",
            "date": "2014-05-13T17:18:03+0000",
            "content": "Another thing I noticed is that with my 2 shard exampledocs setup, the following URL seems to send the pivot faceting into an infinite loop of refinement requests (note the typo: there's a space embeded in a field name manu_id_s != manu_+id_s )...\n\nhttp://localhost:8983/solr/select?q=*:*&sort=id+desc&rows=2&facet=true&facet.pivot=cat,manu_+id_s,inStock&facet.limit=3\n\n...not clear what's going on there, but definitely something that needs fixed before committing (\"garbage in -> garbage out\" is one thing, \"garbage in -> crash your cluster\" is another)\n\nI'm not able to reproduce this.  Could you tell me a little more about your setup?  I am trying to recreate using the example data split to the two shards.  (a-m example files on shard1, n-z on shard2).  I've run your script and added your data as well, and then gone to the URL you provided and added a &shards=localhost:8983/solr,localhost:7574/solr onto it.  It comes back each time without locking up.  I am using revision 566d6371c77fd07d11f2e1b3033a669e26692a58 with only the SOLR-2894 patch applied. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13996730",
            "date": "2014-05-13T18:21:04+0000",
            "content": "I'm not able to reproduce this. Could you tell me a little more about your setup?\n\ntrunk, with patch applied, build the example and then run the Simple Two-Shard Cluster ...\n\n\nhossman@frisbee:~/lucene/dev/solr$ cp -r example node1\nhossman@frisbee:~/lucene/dev/solr$ cp -r example node2\n\n# in term1...\nhossman@frisbee:~/lucene/dev/solr/node1$ java -DzkRun -DnumShards=2 -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -jar start.jar\n\n# wait for node1 startup, then in term2...\nhossman@frisbee:~/lucene/dev/solr/node2$ java -Djetty.port=7574 -DzkHost=localhost:9983 -jar start.jar\n\n# wait for node2 startup, then in term3...\nhossman@frisbee:~/lucene/dev/solr/example/exampledocs$ java -jar post.jar *.xml\nSimplePostTool version 1.5\nPosting files to base url http://localhost:8983/solr/update using content-type application/xml..\n...\n14 files indexed.\nCOMMITting Solr index changes to http://localhost:8983/solr/update..\nTime spent: 0:00:01.763\nhossman@frisbee:~/lucene/dev/solr/example/exampledocs$ curl 'http://localhost:8983/solr/select?q=*:*&sort=id+desc&rows=2&facet=true&facet.pivot=cat,manu_+id_s,inStock&facet.limit=3' > /dev/null\n\n# watch the logs in term1 and term2 go spinning like mad\n\n\n\n\n\nWhile the size of the shard parameters may not strictly be as efficient as possible, is it such that we can run with that for now and circle back to this at a later point, or are you uncomfortable with including the parameters as is in the initial commit?\n\nHmm... not sure how i feel about it w/o more testing - from what i was seeing, with non-trivial field names, term values, and facet.limit the refinements requests were getting HUGE so I suspect it's something we're going to want to tackle before releasing \u2013 but refactoring it to be smaller definitely seems like something that should be a lower priority to some of the correctness related issues we're finding, and adding more tests (so we can be confident the refactoring is correct)\n\n\n\nI'm attaching a \"SOLR-2894_cloud_test.patch\" that contains a new cloud based randomized test i've been working at off and on over the last few days (I created it as a standalone patch because i didn't want to conflict with anything Brett might be in the middle of, and it was easy to do - kept me focused on the test and not dabbling with the internals).  \n\nThe test builds up a bunch of random docs, then does a handfull of random pivot facet queries.  For each pivot query, it recursively walks the pivot response executing verification queries using \"fq\" params it builds up from the pivot constraints \u2013 so if pivot.facet=a,b,c says that \"a\" has a term \"x\" with 4 matching docs, it adds an \"fq=a:x\" to the original query and checks the count; then it looks a the pivot terms for field \"b\" under \"a:x\" and also executes a query for each of them with another fq added, etc...\n\nAs is, the patch currently passes, but that's only because of a few nocommits...\n\n\n\trandomization of mincount is disabled due to the refinement bug i mentioned before\n\tit's currently only doing pivots on 2 string fields (one multivalued and one single valued) ... any attempts at pivot faceting the numeric/date/boolean fields (already included in the docs) causes an NPE in the SolrJ QueryResponse class (i haven't investigated why yet)\n\n\n "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-13998067",
            "date": "2014-05-14T21:19:49+0000",
            "content": "Hey Hoss, I'm working with Brett to clear up these issues and get 2894 commited.\n\nI've done the work on the MinCount and simplified the logic and made it a fair bit easier to read and fixed the issue with pivot facets. The example you showed with the second query failing to return results has been rectified.\n\nLets talk about the refinement terms. We can easily shorten the name of the !terms we use because as you've correctly asserted that the number makes them unique the rest is just filler. But I don't understand how your example functions under the covers.\n\n\nshard2_proposed\nfacet.pivot.mincount=-1\n_fpt_1=memory\n_fpt_2=corsair\n_fpt_2=apple\n_fpt_2=asus\n_fpt_2=boa\n_fpt_2=dell\n_fpt_2=uk\nfacet.pivot={!fpt=1}cat,inStock\nfacet.pivot={!fpt=2}manu_id_s,inStock\nWhat under the covers is used to make _fpt_2 match up to fpt=2 ? The one-to-many relation alters my understanding of how this works. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13998875",
            "date": "2014-05-15T16:19:27+0000",
            "content": "I've done the work on the MinCount and simplified the logic and made it a fair bit easier to read and fixed the issue with pivot facets. The example you showed with the second query failing to return results has been rectified.\n\nawesome.\n\nWhat under the covers is used to make _fpt_2 match up to fpt=2 ? The one-to-many relation alters my understanding of how this works.\n\nI haven't been getting much sleep lately, so forgive me if i'm misunderstanding your question or if my answer is obvious giberish:  I think what i had in mind before was just a few lines of new code in the pivot-refinement logic that runs on the shards to construct a param name for hte top levle multi-valued param using the numeric id, that the pivot could would lookup directly instead of relying on local param variable dereferencing \u2013 which as mentioned, doesn't support any sort of 1-to-many variable refs.\n\n(i don't have the patch in front of me in an editor, so i'll make up variable names and do this mostly in psuedo-code)...\n\n\nSolrParams reqParams = req.getParams()\nString[] allPivots = reqParams.getParams(\"facet.pivot\")\nfor (String pivot : allPivots) {\n  SolrParams localParams = parseLocalParams(pivot)\n  String refine_id = localParams.get(\"fpt\")\n  if (refine_id == null) {\n    // TODO: not a refinement ... do full pivoting\n  } else {\n    String[] refinements = reqParams.getParams(\"_fpt_\" + refine_id)\n    for (String r : refinements) {\n      // TODO: compute the refinement count for \"r\" relative to the current \"pivot\"\n    }\n  }\n}\n\n\n\n...does that make sense? "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14000337",
            "date": "2014-05-16T21:27:07+0000",
            "content": "Thank you Hoss, that explanation gave me everything I needed. \n\nSo this patch breaks up the \"modifyRequest\" block into three parts. First is the global removal and then running of the \"modifyRequestForFieldFacets\" and \"modifyRequestForPivotFacets\" and includes the changed mincount for pivot facet fields.\n\nThis also changes the refinement queries from\n facet.pivot={!terms=$cat,inStock_10__terms}cat,inStock\nto\nfacet.pivot={!fpt=1}cat,inStock\n\nThis caused some problems since before each term had its own facet.pivot and thusly its own context and PivotFacetProcessor. \nNow that we only have one context for all the refinement requests we needed to manage our DocSet since it gets messed with. But those issues seem to be fixed. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14002327",
            "date": "2014-05-19T20:23:49+0000",
            "content": "The latest patch upload has included Brett's change, along with the changes I outlined earlier.\n\nHoss, I think we've addressed everything up to this point. I've got time to correct any other issues you find. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14003672",
            "date": "2014-05-20T17:25:16+0000",
            "content": "I haven't had a lot of time to review the updatd patch in depth, but I did spend some time trying to improve TestCloudPivotFacet to resolve some of the nocommits \u2013 but i'm still seeing failures...\n\n1) I realized the \"depth\" check i was trying to do was bogus and commented it out (still need to purge the code - didn't want to muck with that until the rest of the test was passing more reliably)\n\n\n2) the NPE I mentioned in QueryResponse.readPivots is still happening, but i realized that it has nothing to do with the datatype of the fields being pivoted on \u2013 it only seemed that way because of the poor randomization of values getting put in the single valued string fields vs the multivalued fields in the old version of the test.\n\nThe bug seems to pop up in some cases where a pivot constraint has no sub-pivots.  Normally this results in a NamedList with 3 keys (field,value,count) \u2013 the 4th \"pivot\" key is only included if there is a list of at least 1 sub-pivot.  But in some cases (I can't explain from looking at the code why) the server is responding back with a 4th entry using hte key \"pivot\" but the value is \"null\"\n\nWe need to get to the bottom of this \u2013 it's not clear if there is a bug preventing real sub-pivot constraints from being returned correctly, or if this is just a mistake in the code where it's putting \"null\" in the NamedList instead of not adding anything at all (in which case it might be tempting to make QueryResponse.readPivots smart enough to deal with it, but if we did that it would still be broken for older clients \u2013 best to stick with teh current API semantics)\n\n\nIn the attached patch update, this seed will fail showing the null sub-pivots problem...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=680E68425E7CA1BA -Dtests.slow=true -Dtests.locale=es_US -Dtests.timezone=Canada/Eastern -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 41.7s | TestCloudPivotFacet.testDistribSearch <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: Server sent back 'null' for sub pivots?\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([680E68425E7CA1BA:E9E8E65A2923C186]:0)\n   [junit4]    > \tat org.apache.solr.client.solrj.response.QueryResponse.readPivots(QueryResponse.java:383)\n   [junit4]    > \tat org.apache.solr.client.solrj.response.QueryResponse.extractFacetInfo(QueryResponse.java:363)\n   [junit4]    > \tat org.apache.solr.client.solrj.response.QueryResponse.setResponse(QueryResponse.java:148)\n   [junit4]    > \tat org.apache.solr.client.solrj.response.QueryResponse.<init>(QueryResponse.java:91)\n   [junit4]    > \tat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:91)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:301)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:161)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.doTest(TestCloudPivotFacet.java:145)\n\n\n\n\n\n\n3) Independent (i think) from the NPE issue, there is still something wonky with the refined counts when mincount is specified...\n\nHere for example is a seed that gets based the QueryResponse.readPivots, but then fails the numFound validation queries used to check the pivot counts...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=F08A107C384690FC -Dtests.slow=true -Dtests.locale=ar_LY -Dtests.timezone=Jamaica -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 27.0s | TestCloudPivotFacet.testDistribSearch <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: {main({main(facet.pivot.mincount=9),extra({main(facet.limit=12),extra({main(facet.pivot=pivot_y_s%2Cpivot_x_s1),extra(facet=true&facet.pivot=pivot_x_s1%2Cpivot_x_s)})})}),extra(rows=0&q=id%3A%5B*+TO+503%5D)} ==> pivot_y_s,pivot_x_s1: {params(rows=0),defaults({main(rows=0&q=id%3A%5B*+TO+503%5D),extra(fq=%7B%21term+f%3Dpivot_y_s%7D)})} expected:<9> but was:<14>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([F08A107C384690FC:716C9E644F19F0C0]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:190)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.doTest(TestCloudPivotFacet.java:145)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:863)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:744)\n   [junit4]    > Caused by: java.lang.AssertionError: pivot_y_s,pivot_x_s1: {params(rows=0),defaults({main(rows=0&q=id%3A%5B*+TO+503%5D),extra(fq=%7B%21term+f%3Dpivot_y_s%7D)})} expected:<9> but was:<14>\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertNumFound(TestCloudPivotFacet.java:403)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:208)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:176)\n   [junit4]    > \t... 42 more\n\n\n\n\nThis is saying that while doing a request with a pivot on the \"pivot_y_s,pivot_x_s1\" fields it looped over the (top level) pivot constraints in \"pivot_y_s\" - and for one of those term values (it just happens to be the empty string \"\") it got a pivot count of 9, but when it executed a query filtering the main results on that term (\"fq={!term f=pivot_y_s}\") the total number of results found were 14.\n\nIf you comment out the line of the test that sets the FACET_PIVOT_MINCOUNT param, this seed stats to pass, suggesting that it's almost certianly the mincount logic that's putting a kink in the correctness of the final refined counts. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14005984",
            "date": "2014-05-22T15:02:00+0000",
            "content": "Me and Brett discovered serveral bugs with our mincount and the changes I made to our refinement requests that resulted in the odd behavior you were seeing.  Not everything is super happy. I get what look like solrcloud errors when running certain seeds\nI forgot this patch also comments out the randomUsableUnicodeString to just be a simple string, BUT I've changed it back on my box and It seems to be fine.\n\n215 T12 oasc.SolrResourceLoader.locateSolrHome using system property solr.solr.home: ..\\..\\C:\\Users\\AMULDO~1\\AppData\\Local\\Temp\\solr.cloud.TestCloudPivotFacet-A515DED004CF1660-001\\tempDir-002\n5216 T12 oasc.SolrResourceLoader.<init> new SolrResourceLoader for directory: '..\\..\\C:\\Users\\AMULDO~1\\AppData\\Local\\Temp\\solr.cloud.TestCloudPivotFacet-A515DED004CF1660-001\\tempDir-002\\'\n5421 T12 oasc.ConfigSolr.fromFile Loading container configuration from D:\\hmm\\lucene-solr\\..\\..\\C:\\Users\\AMULDO~1\\AppData\\Local\\Temp\\solr.cloud.TestCloudPivotFacet-A515DED004CF1660-001\\tempDir-002\\solr.xml\n5422 T12 oass.SolrDispatchFilter.init ERROR Could not start Solr. Check solr/home property and the logs\n5483 T12 oasc.SolrException.log ERROR null:org.apache.solr.common.SolrException: Could not load SOLR configuration\n\t\tat org.apache.solr.core.ConfigSolr.fromFile(ConfigSolr.java:71)\n\t\tat org.apache.solr.core.ConfigSolr.fromSolrHome(ConfigSolr.java:96)\n\t\tat org.apache.solr.servlet.SolrDispatchFilter.loadConfigSolr(SolrDispatchFilter.java:157)\n\t\tat org.apache.solr.servlet.SolrDispatchFilter.createCoreContainer(SolrDispatchFilter.java:188)\n\t\tat org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:137)\n\t\tat org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:119)\n\t\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:719)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.updateMappings(ServletHandler.java:1309)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.setFilterMappings(ServletHandler.java:1345)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.addFilterMapping(ServletHandler.java:1085)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.addFilterWithMapping(ServletHandler.java:931)\n\t\tat org.eclipse.jetty.servlet.ServletHandler.addFilterWithMapping(ServletHandler.java:888)\n\t\tat org.eclipse.jetty.servlet.ServletContextHandler.addFilter(ServletContextHandler.java:340)\n\t\tat org.apache.solr.client.solrj.embedded.JettySolrRunner$1.lifeCycleStarted(JettySolrRunner.java:327)\n\t\tat org.eclipse.jetty.util.component.AbstractLifeCycle.setStarted(AbstractLifeCycle.java:174)\n\t\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:65)\n\t\tat org.apache.solr.client.solrj.embedded.JettySolrRunner.start(JettySolrRunner.java:432)\n\t\tat org.apache.solr.client.solrj.embedded.JettySolrRunner.start(JettySolrRunner.java:405)\n\t\tat org.apache.solr.cloud.AbstractFullDistribZkTestBase.createJetty(AbstractFullDistribZkTestBase.java:481)\n\t\tat org.apache.solr.BaseDistributedSearchTestCase.createJetty(BaseDistributedSearchTestCase.java:351)\n\t\tat org.apache.solr.cloud.AbstractFullDistribZkTestBase.createServers(AbstractFullDistribZkTestBase.java:282)\n\t\tat org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:863)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:606)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)\n\t\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:53)\n\t\tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\n\t\tat org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\n\t\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\t\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\n\t\tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\n\t\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\n\t\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)\n\t\tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:793)\n\t\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:453)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)\n\t\tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:53)\n\t\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\t\tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\n\t\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\n\t\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\t\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\n\t\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\t\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\n\t\tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\n\t\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\t\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)\n\t\tat java.lang.Thread.run(Thread.java:724)\n\tCaused by: org.apache.solr.common.SolrException: solr.xml does not exist in D:\\hmm\\lucene-solr\\..\\..\\C:\\Users\\AMULDO~1\\AppData\\Local\\Temp\\solr.cloud.TestCloudPivotFacet-A515DED004CF1660-001\\tempDir-002\\solr.xml cannot start Solr\n\t\tat org.apache.solr.core.ConfigSolr.fromFile(ConfigSolr.java:62)\n\n\n\nI've run your TestCloudPivotFacet test a bunch of times and I only get ZK errors, no value mismatch or null sub-pivots. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14026707",
            "date": "2014-06-10T17:22:10+0000",
            "content": "Started getting back into this yesterday (i should have several large blocks of time for this issue this week & next week)...\n\nMe and Brett discovered serveral bugs with our mincount and the changes I made to our refinement requests that resulted in the odd behavior you were seeing. \n\nAwesome! ... glad to see the test was useful.\n\nNot everything is super happy. I get what look like solrcloud errors when running certain seeds\n\nHmmm... that is a weird error.  People sometimes see errors in solr tests that use threads related to timing and/or assertions of things that haven't happened yet - but i don't remember ever seeing anything like this type of problem with initialization of the cores.\n\ndo these failures reproduce for you with the same seeds?  can you post the full reproduce line that you get with these failures?\n\nI forgot this patch also comments out the randomUsableUnicodeString to just be a simple string, BUT I've changed it back on my box and It seems to be fine.\n\nyep \u2013 it also still had one of my nocommits so that it was only pivoting on string fields, but even w/o that it's worked great for me on many iterations.\n\n\n\nRevised patch - mostly cleaning up the lingering issues in TestCloudPivotFacet but a few other minor fixes of stuff i noticed.\n\nDetailed changes compared to previous patch...\n\n\n\tremoved \"TestDistributedSearch.java.orig\" that seems to have been included in patch by mistake\n\tcleanup TestCloudPivotFacet\n\t\n\t\tfixed randomUsableUnicodeString()\n\t\tfix nocommit about testing pivot on non-string fields\n\t\tfixed the depth checking (we can assert the max depth, but that's it)\n\t\tremoved weird (unused) \"int ss = 2\" that got added to assertNumFound\n\t\t\n\t\t\twas also in some dead code in PivotFacetProcessor?\n\t\t\n\t\t\n\t\trefactored cut/pate methods from Cursor test into baseclass\n\t\n\t\n\tI removed the NullGoesLastComparator class and replaced it with a compareWithNullLast helper method in PivotFacetField (and added a unit test for it)\n\t\n\t\tthe Comparator contract is pretty explicit about null, and this class violated that\n\t\tit was only being used for simple method calls, not passed to anything that explicitly needed a Comparator, so there wasn't a strong need for a standalone class\n\t\n\t\n\n\n\n\n\nMy \"next step\" plans...\n\n\n\treview DistributedFacetPivotTest in depth more - add more strong assertions\n\t\n\t\tat first glance, it looks like a lot of the test is following the example of most existing distrib tests of relying on comparisons between the controlClient and the distrib client \u2013 in my opinion that's a bad pattern, and i'd like to add some explicit assertions on the results of all the this.query(...) calls\n\t\n\t\n\tre-review the new pivot code (and the changes to facet code) in general\n\t\n\t\tit's been a while since my last skim, and i know you've tweaked a bunch based on my previous comments\n\t\ti'll take a stab at adding more javadocs to some of the new methods as i make sense of them\n\t\twhere possible, i'm going to try to add unit tests for some of the new low level methods you've introduced \u2013 largely as a way to help ensure i understand what they do\n\t\n\t\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14027285",
            "date": "2014-06-11T00:57:05+0000",
            "content": "review DistributedFacetPivotTest in depth more - add more strong assertions\n\nAttaching updated patch with progress along this line: in addition so some new explicit assertions, it also includes some refactoring & simplification of setupDistributedPivotFacetDocuments\n\nOne thing that jumped out at me when reviewing this is even though the test does some queries with large overrequest params as well disabling overrequest, there doesn't seem to be any assertions about how the overrequesting affects the results \u2013 in fact, because of how the controlClient is compared with the distributed client, it seems that with this sample data disabling overrequest doesn't even change the results at all.\n\nI definitely want to add some test logic around that \u2013 if for no other reason then to prove that when the overrequesting is used, it can help with finding constraints in the long tail\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14028689",
            "date": "2014-06-12T01:24:22+0000",
            "content": "I definitely want to add some test logic around that \u2013 if for no other reason then to prove that when the overrequesting is used, it can help with finding constraints in the long tail\n\nUpdated patch...\n\n\tnew DistributedFacetPivotLongTailTest\n\t\n\t\tcrafts the shard distribution specifically to demonstrate that overrequesting is affecting things as expected\n\t\n\t\n\tsplit DistributedFacetPivotTest into DistributedFacetPivotSmallTest and DistributedFacetPivotLargeTest\n\t\n\t\tthis was already 2 very different sets of data with two very differnet styles of asserting expected results \u2013 so i went ahead and split it up\n\t\tNow there isn't the weird suprise that halfway through a test all the data is deleted and new data is added and more asertions are made.\n\t\n\t\n\n\n\nBrett & Andrew: would really appreciate if you guys could review my changes to your existing test as well as the new LongTail test and help sanity check that the assertions all look correct.\n\nAssuming you guys don't spot any problems with the tests: next up i'll move back into reviewing the code more in depth, and documenting/refactoring/unit-testing as needed to help myself understand all this awesomeness you guys have added. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14029319",
            "date": "2014-06-12T16:21:06+0000",
            "content": "I'm on it. \nI'm loving all these added asserts. I've added a few asserts that test the exact data that proves refinement worked,  I'll work through the whole changeset and post up what I've got.\n\nI want to bring up a separate issue that we've been dealing with in this patch.\n\nSolrJ and its response types. I'm fairly certain that field facets always return the value of any field as a string, so \"true\" instead of boolean.true and dates in the external format not the pretty printed format. This isn't wholly true in pivot facets right now and it causes some wierdness in the code. \nI'd personally like to just use the string data types like field facet does, and remove the hacky functionality that sends back objects for solrJ\nHoss, thoughts? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14029461",
            "date": "2014-06-12T17:27:59+0000",
            "content": "This isn't wholly true in pivot facets right now and it causes some wierdness in the code.\nI'd personally like to just use the string data types like field facet does, and remove the hacky functionality that sends back objects for solrJ\n\nCan you elaborate on what kind of weirdness/hacky functionality you are talking about?\n\npersonally i'm a big fan of the fact that the facet.pivot returns the correct data types for the corresponding FieldType \u2013 i think that's a big improvement over facet.field. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14029531",
            "date": "2014-06-12T18:09:38+0000",
            "content": "Can you elaborate on what kind of weirdness/hacky functionality you are talking about?\n\nIn the PivotFacetProcessor (shards) we .toObject each value. This is weird in the non-distributed mode because nothing clears up those into strings for the response -XML or JSON. This is a problem with dates, because \"2012-11-01T12:30:00Z\" becomes \"Nov 1 4:30 EST 2012\". I don't know what methods get run after process in the non-distrib mode that we could hook into to change these values back into what they should be.\n\nIn distributed this can be trouble because when trying to assign a refinement path we must get the .toexternal value of the datefield so that it can be properly looked up in the index. Elran has a fix for this, which works fine but we'll need to extend this to the PivotFacetValue's ConvertToNamedList for the output reponse to look right and we end up having conditionals for dates in a bunch of places.\n\nMost other datatypes are fine, but date is the worst of this set. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14029605",
            "date": "2014-06-12T18:53:15+0000",
            "content": "Hoss, I can't find any faults with your test additions\n\nI'd add \n\n    //Microsoft will come back wrong if refinement was not done correctly\n    PivotField microsoft = firstPlace.getPivot().get(1);\n    assertEquals(\"company_t\", microsoft.getField());\n    assertEquals(\"microsoft\",microsoft.getValue());\n    assertEquals(56,microsoft.getCount());\n\n\nto the \n// basic check w/ limit & default sort (count) \nin the distributedpivotfacetlargetest\n\nThe microsoft pivot is the key value that is wrong if refinement didn't work, so might as well sanity check it.\n\nOtherwise you've just included the CursorPagingTest which is probably from a different patch?\n\nThanks for your help making this patch better "
        },
        {
            "author": "Elran Dvir",
            "id": "comment-14031792",
            "date": "2014-06-15T06:16:05+0000",
            "content": "I think it's very important to keep pivot's response values as objects. \nWe should consider changing facet's  response values from string to object.\nIn objects, there is , ofcourse, more information than strings. \nFor example, there is no ability in Solr to sort by index desc. With objects it can be done in the client. \n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14045284",
            "date": "2014-06-26T22:39:58+0000",
            "content": "I think it's very important to keep pivot's response values as objects. \n\n+1\n\nIn the PivotFacetProcessor (shards) we .toObject each value. This is weird in the non-distributed mode because nothing clears up those into strings for the response -XML or JSON. This is a problem with dates, because \"2012-11-01T12:30:00Z\" becomes \"Nov 1 4:30 EST 2012\". I don't know what methods get run after process in the non-distrib mode that we could hook into to change these values back into what they should be.\n\nI don't think that's weird \u2013 i think the toObject() call you have is exactly what it should be \u2013 i'm not really following your point about the XML or JSON responses, the response writers already know how to handle the various Object types that (Like Dates, and Integers, etc...) that might be included.\n\nBased on your comment about PivotFacetValue's convertToNamedList, i think what you mean is that the main underlying problem with using the real Object representation of the values is that when you then want to build up the paths in PivotFacetValue's createFromNamedList for the purposes of the refinement queries, there is no corrollary to \"toObject\" that can be used.\n\nThis is very similar to the problem we encountered in SOLR-5354 \u2013 the solution there was a new FieldType methd specific to marshalling and unmarshalling sort values.  we can't simply re-use that new method as is because the Objects used as Sort values don't neccessarily have a 1-to-1 corrispondence with the Objects that matter here.  \n\nIdeally there should be a similar method on the FieldType for doing this, that let's you round trip the output of FieldType.toObject() for the purposes of building up a simple query string \u2013 but that doesn't exist at the moment.\n\nMy vote would be to leave the code the way it is right now (assuming it can toString() anything except a \"Date\" object) and open a new issue to improve on this for custom FieldTypes at a later date.  That way people who want to go ahead and use Distributed Pivot Faceting for out of the box field types like Strings/Dates/Numbers can, and have the benefits of well structured objects in the response \u2013 w/o waiting on a more robust solution that can work with arbitrary custom field types. (which can come later)\n\nOtherwise you've just included the CursorPagingTest which is probably from a different patch?\n\nCursorPagingTest is included in the patch because of methods refactored up into SolrTestCaseJ4 for use in this patch.\n\n\n\u2014 \n\nI've been making my way further through the code review slowly \u2013 Attaching a revised patch...\n\n\n\tupdated to trunk\n\tadded microsoft asserts to DistributedFacetPivotLargeTest (per Andrew)\n\tmake FacetParams.FACET_OVERREQUEST package-private since it's not a usable param (just a base)\n\tStrUtils\n\t\n\t\tmore javadocs\n\t\tnew escapeTextWithSeparator test -> TestUtils\n\t\trefactor duplicated code with existing \"join\" method into new private method\n\t\n\t\n\tPivotListEntry\n\t\n\t\tmore javadocs\n\t\tkill some dead code (multiple enums with same index?)\n\t\trefactored to leverage standard java Enum plumbing better\n\t\n\t\n\tPivotFacetValue\n\t\n\t\tadded a nocommit regarding custom fieldtypes to createFromNamedList that we either need a better solution to the Object->String problem, or we need to file a new issue prior to commiting and update the comment\n\t\tswitched if-else-if-else-if on PivotListEntry instances to be an enum switch\n\t\n\t\n\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14046533",
            "date": "2014-06-27T23:43:01+0000",
            "content": "I started reviewing again this afternoon and made a few more tweaks but then quickly encountered a troubling situation:\n\nThere seems to be some set of circumstances that can cause pivot refinement to go into an (infinite?) ridiculously long loop.\n\nHere's an example log snippet from a test run that i eventually had to explicitly kill after several minutes (normally it finishes in ~40 seconds on my laptop)..\n\n\n...\n   [junit4]   2> 365476 T48 C473 P35623 oasc.SolrCore.execute [collection1] webapp= path=/select params={facet.limit=14&facet.pivot={!fpt%3D3557}pivot_y_s,pivot_l1&isShard=true&distrib=false&facet=true&shard.url=https://127.0.0.1:35623/collection1/|https://127.0.0.1:35174/collection1/&version=2&q=*:*&NOW=1403905534861&facet.pivot.mincount=-1&rows=0&fpt3557=-8197981690463795098&fpt3557=-7333481702750443698&fpt3557=-5750361150833026124&fpt3557=-1254664925684537075&fpt3557=-790491513359287891&fpt3557=-259812169693239119&fpt3557=5005&fpt3557=5023&fpt3557=434325197357513755&fpt3557=1208379606676285112&fpt3557=2157244738088160377&fpt3557=4049867752092041147&wt=javabin} hits=384 status=0 QTime=3 \n   [junit4]   2> 365484 T53 C473 P35623 oasc.SolrCore.execute [collection1] webapp= path=/select params={facet.limit=14&facet.pivot={!fpt%3D3558}pivot_y_s,pivot_l1&isShard=true&distrib=false&facet=true&shard.url=https://127.0.0.1:35623/collection1/|https://127.0.0.1:35174/collection1/&version=2&q=*:*&NOW=1403905534861&facet.pivot.mincount=-1&rows=0&fpt3558=-8197981690463795098&fpt3558=-7333481702750443698&fpt3558=-5750361150833026124&fpt3558=-1254664925684537075&fpt3558=-790491513359287891&fpt3558=-259812169693239119&fpt3558=5005&fpt3558=5023&fpt3558=434325197357513755&fpt3558=1208379606676285112&fpt3558=2157244738088160377&fpt3558=4049867752092041147&wt=javabin} hits=384 status=0 QTime=3 \n   [junit4]   2> 365493 T50 C473 P35623 oasc.SolrCore.execute [collection1] webapp= path=/select params={facet.limit=14&facet.pivot={!fpt%3D3559}pivot_y_s,pivot_l1&isShard=true&distrib=false&facet=true&shard.url=https://127.0.0.1:35623/collection1/|https://127.0.0.1:35174/collection1/&version=2&q=*:*&NOW=1403905534861&facet.pivot.mincount=-1&rows=0&fpt3559=-8197981690463795098&fpt3559=-7333481702750443698&fpt3559=-5750361150833026124&fpt3559=-1254664925684537075&fpt3559=-790491513359287891&fpt3559=-259812169693239119&fpt3559=5005&fpt3559=5023&fpt3559=434325197357513755&fpt3559=1208379606676285112&fpt3559=2157244738088160377&fpt3559=4049867752092041147&wt=javabin} hits=384 status=0 QTime=5 \n...\n\n\n\nA few things to note about those above log lines:\n\n\twith the seed used in this run there was only 740 total docs in the index\n\tall three of those requests were made to the same shard/core (C473) on the same port (P35623)\n\tthe \"pivot_l1\" field being refined in these requests is a single valued long field - which means even if every random value generated for it were unique, in an index with 740 docs there can only be 740 possible long values here.\n\tthese requests are already upto fpt=3559 \u2013 way more refinements then should be neccessary for this field\n\tthe shard is being asked to refine the same pivot values over and over again (but with increasing \"fpt#####\" keys)\n\n\n\nUnfortunately while trying to get to the bottom of this, i realized the way the test was picking the random pivots it used wasn't reproducible with a consistent test seed.  I've fixed that, but now i need to hammer on this test some more to try and reproduce again with a reliable seed. \n\n\n\nSmall changes to the patch ...\n\n\n\tTestCloudPivotFacet\n\t\n\t\tadded explicit sort to String[] fieldNames so buildRandomPivot would reproduce with consistent seed\n\t\n\t\n\tSimpleFacets tweaks i made before encountering the test bug:\n\t\n\t\tmore javadocs on some subtly diff methods\n\t\tchange the new getTermCounts(String,Integer,DocSet) to private since it's only used as a helper for the other public methods\n\t\n\t\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14046565",
            "date": "2014-06-28T00:41:20+0000",
            "content": "Ok, with the last patch, here are a couple of seeds that seem to reliably reproduce some sort of infinite loop for me...\n\n\n\nant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=BE59C186858EBC0E -Dtests.slow=true -Dtests.locale=es_US -Dtests.timezone=Canada/Eastern -Dtests.file.encoding=UTF-8 \n\n...\n\n   [junit4]   2> 75419 T68 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&fpt14287=false,&distrib=false&facet.pivot={!fpt%3D14287}pivot_b,pivot_y_s&facet.limit=17&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&isShard=true} hits=112 status=0 QTime=2 \n   [junit4]   2> 75425 T67 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&distrib=false&facet.pivot={!fpt%3D14289}pivot_b,pivot_y_s&fpt14289=false,&facet.limit=17&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&isShard=true} hits=112 status=0 QTime=1 \n   [junit4]   2> 75430 T69 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&distrib=false&facet.pivot={!fpt%3D14291}pivot_b,pivot_y_s&facet.limit=17&fpt14291=false,&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&isShard=true} hits=112 status=0 QTime=2 \n   [junit4]   2> 75435 T70 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&fpt14293=false,&distrib=false&facet.pivot={!fpt%3D14293}pivot_b,pivot_y_s&facet.limit=17&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&isShard=true} hits=112 status=0 QTime=2 \n   [junit4]   2> 75440 T71 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&distrib=false&facet.pivot={!fpt%3D14295}pivot_b,pivot_y_s&facet.limit=17&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&fpt14295=false,&isShard=true} hits=112 status=0 QTime=1 \n   [junit4]   2> 75446 T68 C104 P58648 oasc.SolrCore.execute [collection1] webapp=/vv_ path=/select params={NOW=1403913254268&version=2&facet.pivot.mincount=-1&facet=true&distrib=false&facet.pivot={!fpt%3D14297}pivot_b,pivot_y_s&fpt14297=false,&facet.limit=17&fq=id:[*+TO+232]&shard.url=https://127.0.0.1:58648/vv_/collection1/|https://127.0.0.1:58190/vv_/collection1/&rows=0&q=*:*&wt=javabin&isShard=true} hits=112 status=0 QTime=1 \n\n\n\n\n\n\nant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=FFB687151132403E -Dtests.slow=true -Dtests.locale=es_US -Dtests.timezone=Canada/Eastern -Dtests.file.encoding=UTF-8\n\n...\n\n   [junit4]   2> 196659 T68 C356 P40661 oasc.SolrCore.execute [collection1] webapp=/vtut path=/select params={facet=true&fpt25929=-1.37306931E9&fpt25929=-1.1585728E9&fpt25929=-3.86510688E8&fpt25929=-3.42199296E8&fpt25929=-2.79124352E8&fpt25929=-2.6666448E8&fpt25929=-1.54946432E8&fpt25929=0.125&fpt25929=2956621.2&fpt25929=5.4770541E8&fpt25929=1.16071846E9&q=*:*&wt=javabin&rows=0&facet.limit=12&isShard=true&facet.pivot={!fpt%3D25929}pivot_x_s1,pivot_f&shard.url=http://127.0.0.1:40661/vtut/collection1/|http://127.0.0.1:35181/vtut/collection1/&facet.pivot.mincount=-1&version=2&distrib=false&NOW=1403914294688} hits=269 status=0 QTime=5 \n   [junit4]   2> 196668 T64 C356 P40661 oasc.SolrCore.execute [collection1] webapp=/vtut path=/select params={facet=true&q=*:*&wt=javabin&rows=0&facet.limit=12&isShard=true&fpt25931=-1.37306931E9&fpt25931=-1.1585728E9&fpt25931=-3.86510688E8&fpt25931=-3.42199296E8&fpt25931=-2.79124352E8&fpt25931=-2.6666448E8&fpt25931=-1.54946432E8&fpt25931=0.125&fpt25931=2956621.2&fpt25931=5.4770541E8&fpt25931=1.16071846E9&facet.pivot={!fpt%3D25931}pivot_x_s1,pivot_f&shard.url=http://127.0.0.1:40661/vtut/collection1/|http://127.0.0.1:35181/vtut/collection1/&facet.pivot.mincount=-1&version=2&distrib=false&NOW=1403914294688} hits=269 status=0 QTime=5 \n   [junit4]   2> 196678 T66 C356 P40661 oasc.SolrCore.execute [collection1] webapp=/vtut path=/select params={facet=true&q=*:*&wt=javabin&rows=0&facet.limit=12&isShard=true&facet.pivot={!fpt%3D25933}pivot_x_s1,pivot_f&shard.url=http://127.0.0.1:40661/vtut/collection1/|http://127.0.0.1:35181/vtut/collection1/&facet.pivot.mincount=-1&version=2&distrib=false&fpt25933=-1.37306931E9&fpt25933=-1.1585728E9&fpt25933=-3.86510688E8&fpt25933=-3.42199296E8&fpt25933=-2.79124352E8&fpt25933=-2.6666448E8&fpt25933=-1.54946432E8&fpt25933=0.125&fpt25933=2956621.2&fpt25933=5.4770541E8&fpt25933=1.16071846E9&NOW=1403914294688} hits=269 status=0 QTime=5 \n   [junit4]   2> 196687 T69 C356 P40661 oasc.SolrCore.execute [collection1] webapp=/vtut path=/select params={facet=true&fpt25935=-1.37306931E9&fpt25935=-1.1585728E9&fpt25935=-3.86510688E8&fpt25935=-3.42199296E8&fpt25935=-2.79124352E8&fpt25935=-2.6666448E8&fpt25935=-1.54946432E8&fpt25935=0.125&fpt25935=2956621.2&fpt25935=5.4770541E8&fpt25935=1.16071846E9&q=*:*&wt=javabin&rows=0&facet.limit=12&isShard=true&facet.pivot={!fpt%3D25935}pivot_x_s1,pivot_f&shard.url=http://127.0.0.1:40661/vtut/collection1/|http://127.0.0.1:35181/vtut/collection1/&facet.pivot.mincount=-1&version=2&distrib=false&NOW=1403914294688} hits=269 status=0 QTime=5 \n\n\n\n\n(NOTE: Since the whole problem is that these seeds seem to go into infinite loops, and i didn't feel like waiting for the test framework to time them out after an hour, i pulled the seeds out of the junit \"Master seed: XXXXX\" log output after killing the tests manually.  The other tests.* sys props are just constants i picked at random when trying to reproduce to ensure that the \"ant test ...\" lines i posted here would be fully reproducible)\n\nBy the looks of things, it looks the problem seems to be poping up when a refinement constraint in a multi-level pivot involves the empty string (and/or missing values?)\n\nLooking back at the log snippet i posted in my previous comment (facet.pivot=pivot_y_s,pivot_l1) and comparing that with the refinement requests in test runs that pass, i realize how none of those refinements on the pivot_l1 long values had a string prefix \u2013 so perhaps the code was getting confused about what it was supose to return, and that was then causing hte coordinator to re-request?\n\njust speculating here ... Andrew Muldowney & Brett Lucey \u2013 does that sound plausible to you?\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14046648",
            "date": "2014-06-28T02:04:13+0000",
            "content": "By the looks of things, it looks the problem seems to be poping up when a refinement constraint in a multi-level pivot involves the empty string (and/or missing values?)\n\nHmmm... both cases definitely seem to be problematic: \n\n\trefining on values that are the empty string \"\"\n\trefining against the null psuedo-values when using facet.missing\n\n\n\n(Note: TestCloudPivotFacet currently doesn't even try facet.missing \u2013 need to rememedy that in a future patch)\n\n\n\nThe attached patch update modifies DistributedFacetPivotLargeTest to add a new \"special_s\" field to a handful of docs \u2013 some of which get the value of SPECIAL (final String SPECIAL = \"\"; and it goes into a loop here...\n\n\n    // refine on empty string\n    rsp = query( \"q\", \"*:*\",\n                 \"rows\", \"0\",\n                 \"facet\",\"true\",\n                 \"facet.limit\",\"1\",\n                 FacetParams.FACET_OVERREQUEST_RATIO, \"0\", // force refine\n                 FacetParams.FACET_OVERREQUEST_COUNT, \"0\", // force refine\n                 \"facet.pivot\",\"special_s,company_t\");\n\n\n\n   [junit4]   2> 32409 T43 C21 oasc.SolrCore.execute [collection1] webapp=/po_cuf path=/select params={shard.url=[ff01::083]:33332/po_cuf|[ff01::213]:33332/po_cuf|http://127.0.0.1:37920/po_cuf&NOW=1403920234230&rows=0&isShard=true&distrib=false&wt=javabin&fpt2938=&facet.pivot.mincount=-1&facet.overrequest.count=0&q=*:*&version=2&facet.pivot={!fpt%3D2938}special_s,company_t&facet.overrequest.ratio=0&facet=true&facet.limit=1} hits=357 status=0 QTime=0 \n   [junit4]   2> 32413 T42 C21 oasc.SolrCore.execute [collection1] webapp=/po_cuf path=/select params={shard.url=[ff01::083]:33332/po_cuf|[ff01::213]:33332/po_cuf|http://127.0.0.1:37920/po_cuf&NOW=1403920234230&rows=0&isShard=true&distrib=false&wt=javabin&fpt2939=&facet.pivot.mincount=-1&facet.overrequest.count=0&q=*:*&version=2&facet.pivot={!fpt%3D2939}special_s,company_t&facet.overrequest.ratio=0&facet=true&facet.limit=1} hits=357 status=0 QTime=0 \n\n\n\n(Note the ...&fpt2938=&... and ...&fpt2939=&...)\n\nEven if you redefine SPECIAL to be some other constant (ie: SPECIAL = \"SPECIAL\";) the code still goes into a loop in the next call, where facet.missing is used and refinement is needed on the \"missing\" value...\n\n\n    // refine on empty string & facet.missing\n    rsp = query( \"q\", \"*:*\",\n                 \"fq\", \"-place_s:0placeholder\",\n                 \"rows\", \"0\",\n                 \"facet\",\"true\",\n                 \"facet.limit\",\"1\",\n                 \"facet.missing\",\"true\",\n                 FacetParams.FACET_OVERREQUEST_RATIO, \"0\", // force refine\n                 FacetParams.FACET_OVERREQUEST_COUNT, \"0\", // force refine\n                 \"facet.pivot\",\"special_s,company_t\");\n\n\n\n\n   [junit4]   2> 26798 T53 C19 oasc.SolrCore.execute [collection1] webapp=/do_ path=/select params={facet.overrequest.ratio=0&wt=javabin&facet.missing=true&facet.limit=1&facet.pivot.mincount=-1&facet.pivot={!fpt%3D2151}special_s,company_t&fpt2151=null,microsoft&distrib=false&version=2&shard.url=[ff01::083]:33332/do_|[ff01::213]:33332/do_|https://127.0.0.1:36955/do_|[ff01::114]:33332/do_&facet=true&q=*:*&rows=0&fq=-place_s:0placeholder&NOW=1403920466501&isShard=true&facet.overrequest.count=0} hits=202 status=0 QTime=0 \n   [junit4]   2> 26802 T54 C19 oasc.SolrCore.execute [collection1] webapp=/do_ path=/select params={facet.overrequest.ratio=0&wt=javabin&facet.missing=true&facet.limit=1&facet.pivot.mincount=-1&facet.pivot={!fpt%3D2153}special_s,company_t&distrib=false&version=2&shard.url=[ff01::083]:33332/do_|[ff01::213]:33332/do_|https://127.0.0.1:36955/do_|[ff01::114]:33332/do_&facet=true&q=*:*&rows=0&fpt2153=null,microsoft&fq=-place_s:0placeholder&NOW=1403920466501&isShard=true&facet.overrequest.count=0} hits=202 status=0 QTime=1 \n\n\n\n(Note the ...&fpt2151=null,microsoft&... and ...&fpt2153=null,microsoft&...)\n\n\n\nIt looks like we need to rethink how the values are encoded into a path for the purpose of refinement so we can account for and differentiate between missing values, the empty string (0 chars), and the literal string \"null\" (4 chars)\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14050898",
            "date": "2014-07-03T00:12:01+0000",
            "content": "\nIt looks like we need to rethink how the values are encoded into a path for the purpose of refinement so we can account for and differentiate between missing values, the empty string (0 chars), and the literal string \"null\" (4 chars)\n\nI've been working on this for the last few days - cleaning up how we deal with the \"refinement\" strings so that facet.missing and/or empty strings (\"\") in fields won't be problematic.\n\nIt's been slow going as i tried to be systematic about refactoring & documenting methods as i went along and started understanding more and more of the code.\n\nThe bulk of the changes i made can be summarized as:\n\n\tmake the \"valuePath\" tracking more structured via List<String> instead of building up single comma seperated refinement string right off the bat\n\trefactor the encoding/decoding of the refinement strings into a utility method thta can handle null and empty string.\n\trefactor the refinement count & subset computation so that it can actually handle facet.missing correctly (before attempts at refining facet.missing were just looking for the term \"null\" (ie: 4 characters)\n\n\n\nFull details on how this patch differs from the lsat one are listed below \u2013 but as things stand right now there is still a nasty bug somewhere in the facet.missing processing that i can't wrap my head arround...  \n\nIn short: when facet.missing is enabled in the SPECIAL test i mentioned in my last comment, it's somehow causing the refined counts of of the non-missing SPECIAL value to be wrong (even if the SPECIAL value is a regular string, and not \"\"). \n\nI can't really wrap my head arround how that's happening \u2013 it's going to involve some more manual testing & some more unit tests to get to the bottom of it, but in the mean time I wanted to get this patch posted.\n\nIf folks could review it & sanity check that i'm not doing something stupid with the refinement that would be appreciated.\n\n\n\nDetailed changes in this patch iteration...\n\n\n\tPivotFacetHelper\n\t\n\t\tadd new encodeRefinementValuePath & decodeRefinementValuePath methods\n\t\t\n\t\t\tspecial encoding to handle empty strings (should be valid when pivoting) and null values (needed for facet.missing refinement)\n\t\t\n\t\t\n\t\tadd tests in TestPivotHelperCode\n\t\n\t\n\tPivotFacetValue & PivotFacetField\n\t\n\t\tin general, make these a bit more structured\n\t\teliminate \"fieldPath\" since it's unused\n\t\treplace PivotFacetValue.field (String) with a ref to the actual parentPivot (PivotFacetField)\n\t\tadd PivotFacetField.parentValue (PivotFacetValue) to ref the value this pivot field is nested under (if any)\n\t\treplace valuePath with getValuePath() (List<String>) to track the full structure\n\t\n\t\n\tFacetComponent\n\t\n\t\tprune some big chunks of commented out code (alt approaches no longer needed it looks like?)\n\t\tuse new PivotFacetValue.getValuePath() + PivotFacetHelper.encodeRefinementValuePath instead of PivotFacetValue.valuePath\n\t\n\t\n\tSimpleFacets\n\t\n\t\tmake getListedTermCounts(String,String) private again & add javadocs clarifing that it smarSplits the list of terms\n\t\tconvert getListedTermCounts(String,String,DocSet) -> getListedTermCounts(String,DocSet,List<String>)\n\t\t\n\t\t\tie: pull the split logic out of this method, since it's confusing, and some callers don't need it.\n\t\t\tadd javadocs\n\t\t\tupdated SimpleFacets callers to do the split themselves\n\t\t\n\t\t\n\t\n\t\n\tPivotFacetProcessor\n\t\n\t\trefactor subset logic (that dealt with missing values via negatived range query) into \"getSubset\" helper method\n\t\t\n\t\t\tadd complimentary \"getSubsetSize\" method as well\n\t\t\n\t\t\n\t\tupdate previous callers of getListedTermCounts(String,String,DocSet) to use getSubsetSize instead in order to correctly handle the refinements of null (ie: facet.missing)\n\t\trefactor & cleanup processSingle:\n\t\t\n\t\t\thave caller do the field splitting & validation (eliminates redundency when refining many values)\n\t\t\tstop treating empty string as special case, switch conditionals that were looking at first value to look at list size directly\n\t\t\n\t\t\n\t\n\t\n\tmisc new javadocs on various methods throughout hte above mentioned files\n\n\n\n\n\nMisc notes for the future:\n\n\n\teven if/when we get the refinement logic fixed, we really need some safety check to ensure we've completely eliminated this possibility of an infinite loop on refinement:\n\t\n\t\tcoordinator should assert that if if asks shard for a refinement, that refinement is returned\n\t\tshard should assert that if it's asked to refine, the #vals makes sense for the #fields in the pivot\n\t\n\t\n\twe need to include more testing of facet.missing:\n\t\n\t\trandomized testing in in TestCloudPivotFacet\n\t\tmore usage of it in the Small & Large tests.\n\t\n\t\n\tin general, we need more testing that we know triggers refinement\n\t\n\t\tie: the \"Small\" test already does a bunch with facet.missing, but I guess that never caught ny of these bugs, because refinement was never needed?\n\t\trandomly set small overrequest values in TestCloudPivotFacet ?\n\t\n\t\n\tfor completeness, we should do some testing of literal string value \"null\" (4 chars) in pivot fields\n\twe aren't doing enough testing of multiple facet.pivot params in a single request - need to make sure that when refinement happens, those aren't colliding\n\t\n\t\tin particularly i'm wondering about facet.pivot={!key=aaa}foo,bar&facet.pivot={!key=bbb}foo,bar type stuff\n\t\n\t\n\n "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-14052876",
            "date": "2014-07-05T14:47:51+0000",
            "content": "Hoss,\n\nSorry we've been a little quiet lately.  I was out of town for two weeks and Andrew has been on vacation as well.  We plan on digging back into this next week.  The endless loop is definitely a concern and we will focus on that first if your changes haven't already fixed that.  What we are wondering is if you feel we could get a preliminary version of this committed if we can resolve that loop?  I have a few ideas we can do to prevent infinite looping from ever happening even if we don't have the information we expected.  We are really hoping to see this be a part of the next 4.x release in some form, and it would allow us to start getting more feedback from a broader base.\n\nThanks,\n-Brett "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14054188",
            "date": "2014-07-07T21:49:11+0000",
            "content": "Hey Hoss, I should have time this week and next to investigate the infinite loop and try to implement some of your other requests. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14054293",
            "date": "2014-07-07T23:38:20+0000",
            "content": "... \u2013 but as things stand right now there is still a nasty bug somewhere in the facet.missing processing that i can't wrap my head arround... \n\nI spent today doing some manual testing with some small amounts of data, and looking at the shard requests triggered by each request.  I then started reading through more of the refinement code (first time i've looked at a fair bit of this) and i think i've figured out what's going on (but i don't have a fix for it yet)...\n\nBasically: the PivotFacetField class, that holds a List<PivotFacetValue> doesn't do anything special as far as keeping track of the PivotFacetValue that represents the facet.missing value (ie: the PivotFacetValue where PivotFacetValue.value==null).  This means that in methods like PivotFacetField.sort() and PivotFacetField.queuePivotRefinementRequests(...) the PivotFacetValue for facet.missing is mixed in with the other values and included in considerations about what the cutoff \"countThreshold\" is for refinement, even though it's not affected by facet.limit and should always be returned\n\nThis means that in the test i added that has facet.limit=1&facet.missing=true the null vaue from facet.missing is the only value considered in the \"top 1\" of the constraints, and has a count much higher then the count for the SPECIAL value \u2013 which means SPECIAL doesn't even qualify for the processPossibleCandidateElement logic so it never gets refined at all.\n\n\u2013\n\nI think the best course of action is to cleanup PivotFacetField a bit, so that in addition to the List<PivotFacetValue> of values that are subject to the facet.limit, a specfic \"missingValue\" variable should be added to track the corrisponding PivotFacetValue \u2013 this should make the value sorting & refinement logic in queuePivotRefinementRequests() accurate as is, at the cost of slightly more complex (but accurately modeled) logic in createFromListOfNamedLists() and convertToListOfNamedLists().\n\nWhat do folks think?\n\n\n\nThe endless loop is definitely a concern and we will focus on that first if your changes haven't already fixed that. \n\nThe root cause of the infinite loop seems to be that the formating/parsing of of the refinment params wasn't in sync (ie: empty strings weren't being included at all, while facet.missing values were being encoded as \"null\" which owuld then be parsed as 4 character string literals) ... so that cause should be fixed in my latest patch.\n\nWhat still concerns me though is that there is evidently no general sanity check in the code to prevent the distributed logic in the coordinator from retrying to refine values over and over again even if the shard never responses back with a number for it (ie: if some future bug gets introduced in the refine code that runs on the shards, or if some shard has been misconfigured to have a hard coded invaraint of facet=false, etc...).  That's the sort of edge case that may be really hard to test for, but even if we can't explicit test it, we should at least have some sanity check in the distrib coordination code that says \"we already asked shardX for refinementY and still don't have it, throw 5xx error!\" instead of \"still need refinementY from shardX, ..., still need refinementY from shardX, ...\" which is what seems to be happening right now.\n\nWhat we are wondering is if you feel we could get a preliminary version of this committed if we can resolve that loop?\n\nI'm not comfortable committing features unless i know they work \u2013 particularly something like this, where it's adding distributed support to an existing core feature.  I don't want existing pivot users to see \"oh, distributed pivot support has been added\" and upgrade to SolrCloud and then start getting silently incorrect results.\n\nRest assured however: I'm dedicated to continuing to working through this issue, and helping to fix whatever bugs we find, until it's ready to be committed.  I won't leave you hanging.\n\n\n\nHey Hoss, I should have time this week and next to investigate the infinite loop and try to implement some of your other requests.\n\nThat's great \u2013 like i mentioned above, i think a sanity check on the infinite loop is important, but i suspect it should be fairly trivial (i'm just not 100% certain where it makes sense to put it yet)\n\nI think the biggest concern right now however is addressing the bugs with how facet.missing impacts refinement and kicks value values out of contention due to the modeling in PivotFacetField\n\nIf you have time, and can help out with making those changes, I can go ahead and focus on the additional tests i was describing \u2013 which is probably the best way to divide & conquer the problem since you guys already know the code internals better then me.  Then you can help review my tests, and i can help review the PivotFacetField changes.\n\nsound good?\n\n "
        },
        {
            "author": "Steve Molloy",
            "id": "comment-14056463",
            "date": "2014-07-09T17:10:15+0000",
            "content": "Quick note on PivotFacetHelper's retrieve method. I understand the desire for good performance and more than agree with it. But with some entries being optional (statistics and qcount from SOLR-3583 and SOLR-4212 for instance), this causes the lookup to start after the proper position thus not finding entries that are there. I don't have a better solution than starting from 0 currently, but I'm sure there's something that can be done to keep at least some of the speed improvement while still being able to support optional entries. Maybe force all optional to the end of the list, lookup by index for required ones (field, value, count) and starting at first optional spot for the rest? "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14056840",
            "date": "2014-07-09T22:12:53+0000",
            "content": "That sounds good Hoss "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14059477",
            "date": "2014-07-11T23:05:30+0000",
            "content": "Quick note on PivotFacetHelper's retrieve method ...\n\nI haven't really been aware of those other issues until now (although SOLR-3583 may explain some of the unused code i pruned from PivotListEntry a few patches ago) but i agree with your assessment: if/when enhancements to distributed pivots start dealing with adding optional data to each level of the pivot, the appraoch currently used will have to change.\n\n(Personally: I'm not emotionally ready to put any serious thought into that level of implementation detail in future pivot improvements - i want to focus on getting the basics of distrib pivots solid & released first)\n\n\n\nUpdated patch with most of the tests i had in mind that i mentioned before (although i'd still like to add some more facet.missing tests)...\n\n\n\tTestCloudPivotFacet\n\t\n\t\trandomize overrequest amounts\n\t\trandomize facet.mincount usage & assert never exceded\n\t\trandomize facet.missing usage & assert that null values are only ever last in list of values\n\t\t\n\t\t\tmake the odds of docs missing a field more randomized (across test runs)\n\t\t\n\t\t\n\t\tadd in the possibility of trying to pivot on a field that is in 0 docs\n\t\tDial back some constants to reduce OOM risk when running -Dtests.nightly=true\n\t\texample refine count failure from the facet.missing problem (unless there's another bug that looks really similar) with these changes:\n\t\t\n\t\t\tant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=98C12D5256897A09 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=sr -Dtests.timezone=America/Louisville -Dtests.file.encoding=UTF-8\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tDistributedFacetPivotLongTailTest\n\t\n\t\tsome data tweaks & an additional assertion to ensure refinement is happening\n\t\n\t\n\n\n\n\n\tDistributedFacetPivotSmallTest\n\t\n\t\ts/honda/null/g - help test that the 4 character string \"null\" isn't triggering any special behavior, or getting confused with a missing value in docs.\n\t\n\t\n\n\n\n\n\tDistributedFacetPivotLargeTest\n\t\n\t\tcomment & assert noting that a shard is left empty (helps with edge case testing of result merging & refinement)\n\t\tadded \"assertPivot\" helper method & did a bit of refactoring\n\t\tadded test of 2 diff pivots in the same request (swap field order)\n\t\tadded test of same bi-level pivot with & w/o a tagged fq exclusion in the same request\n\t\tadded test variants of facet.limit & facet.index used as localparam\n\t\t\n\t\t\tcurrently commented out because it doesn't work \u2013 see SOLR-6193\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nThe problem noted above with using facet.* params as local params in facet.pivot is something i discovered earlier this week while writing up these tests.  I initially set the problem set it asside to keep working on tests, with hte intention of looking into a fix once i had better coverage of the problem \u2013 but then when i came back to revisit it yesterdan and looked to the existing facet.field shard request logic for guidance, i discovered that didn't seem to work the way i expected either and realized John Gibson recently filed SOLR-6193 because facet.field does have the exact same problem.\n\ni don't think we should let this block adding distributed facet.pivot, let's tackle it holisticly for all faceting in SOLR-6193.\n\n\n\nAndrew/Brett: have you guys had a chance to look into the refinement bug when facet.missing is used?\n\n(BTW: my update patch only affected test files, so hopefully theres no collision with anything you guys have been working on \u2013 but if there is, feel free to just post whatever patch you guys come up with and i'll handle the merge)\n\n "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14062438",
            "date": "2014-07-15T18:20:43+0000",
            "content": "I've been making generally good headway on the .missing problem. We've got a new PivotFacetFieldValueCollection that should deal with the null values properly. Right now the Small and LongTail tests pass but the Long fails on the new facet.limit=1 and facet.missing=true case with SPECIAL. The control response doesn't include the null and the distributed response doesn't get the count of bbc right, it only gets 150 and I'm sure the 298 it gets for microsoft is wrong too. There is something on the shard side code that is not happy with our \"\" and null values. I'm working on that right now.\n\nMy assumption is that the facet.missing request makes it out to all the shards so we never need to refine on it since all shards responded with the full information, but I guess that isn't always the case since other fields under that null value might have limits that would need to be refined on? "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14062688",
            "date": "2014-07-15T21:37:14+0000",
            "content": "I've uploaded a new file with my facet.missing changes. It's got the small and longtail working.\n\nDistributedFacetPivotLargeTest.java\nrsp = query( \"q\", \"*:*\",\n                 \"fq\", \"-place_s:0placeholder\",\n                 \"rows\", \"0\",\n                 \"facet\",\"true\",\n                 \"facet.limit\",\"1\",\n                 \"facet.missing\",\"true\",\n                 //FacetParams.FACET_OVERREQUEST_RATIO, \"0\", // force refine\n                 //FacetParams.FACET_OVERREQUEST_COUNT, \"0\", // force refine\n                 \"facet.pivot\",\"special_s,company_t\");\n\n\nThis test gets whacky when the OVERREQUEST options are uncommented. With the OVERREQUEST options uncommented we do not get the proper bbc value and so the distributed version diverges from the non-distrib. Your second comment on this issue is exactly on point.\n\nAnother variance in that test is that on the distrib side we get \n\n{field=special_s,value=,count=3,pivot=[\n    {field=company_t,value=microsoft,count=2}, \n    {field=company_t,value=null,count=0}]}\n\n\n\nwhereas for the non-distrib we just get\n\n{field=special_s,value=,count=3,pivot=[\n    {field=company_t,value=microsoft,count=2}]}\n\n\n\nShould facet.missing respect the mincount (in this case it's 1)? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14062874",
            "date": "2014-07-15T23:43:52+0000",
            "content": "Hey Andrew, I probably won't have a chance to review this issue/patches again until monday - but some quick replies...\n\nWith the OVERREQUEST options uncommented we do not get the proper bbc value and so the distributed version diverges from the non-distrib. Your second comment on this issue is exactly on point.\n\nJust to clarify: you are saying that bbc isn't included in the \"top\" set in the distrib call because overrequest is so low, which is inconcsistent with the control where bbc is in the top \u2013 but all of the values returned by the distrib call do in fact have accurate refined counts ... correct?\n\nThe point of that check is to definitely ensure that refinement works properly on facet.missing \u2013 that's why i added it, because it wasn't before and the test didn't catch it because of the default overrequest \u2013 so we can't eliminate those OVERREQUEST params.\n\nwhat we can do is explicitly call queryServer(...) instead of query(...) to ht a random distributed server bu bypass the comparison with the control server \u2013 in that case though we want a lot of tight assertions to ensure that we aren't missing anything.\n\n(of course: we can also include another check of the same facet.missing request with the overrequest disabled if you want \u2013 no one ever complained about too many assertions in a test)\n\nShould facet.missing respect the mincount (in this case it's 1)?\n\nI think so? .. if that's what the non-distrib code is doing, that's what the distrib code should do as well. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14064082",
            "date": "2014-07-16T20:58:11+0000",
            "content": "Just to clarify: you are saying that bbc isn't included in the \"top\" set in the distrib call because overrequest is so low, which is inconcsistent with the control where bbc is in the top \u2013 but all of the values returned by the distrib call do in fact have accurate refined counts ... correct?\n\nIt was not refining properly, which I attributed to the lack of overrequest but that was incorrect. The test is actually the only one that tests the following criteria:\n\n\tA value that should be in the \"top\" elements is not because the overrequesting didn't pick it up or too many shards had values too small. (In this case \"bbc\" only has a value of 150 from the initial round, when its actual value is 445, larger than microsoft's inital value of 398)\n\tThere is a shard that has responded with an empty response, aka it has no documents (shard#3 is always empty in the long test file)\n\n\n\nWhen those two things combine we had an error in our refinement code where we would add Integer.MAX_VALUE to the possible count, overflowing the int and causing it to go negative, and we would never ask for refinement. So we would get microsoft:398 over bbc Fixed\n\nI have fixed the null issue that keeps away from counting towards the facet.limit\nI have fixed the null issue that keeps it around even when its less than facet.mincount\nI have fixed the issue where an empty response from a shard would render all values on the cusp of making it into the top values never get refined.\n\nAre you still seeing the infinite recursion problem? The seeds you provided earlier pass locally for me. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14065060",
            "date": "2014-07-17T16:10:42+0000",
            "content": "When those two things combine ... Fixed\n\nAwesome .. i love it when tests uncover bugs you never even suspected.\n\nAre you still seeing the infinite recursion problem? The seeds you provided earlier pass locally for me.\n\nChanging the encoding mechanism stoped the reliably reproducing infinite loop \u2013 my concern though is that \u2013 even if it's hard to test for \u2013 we should make sure the overall algorithm for refinement isn't susceptible to infinite looping in the event of aberrant shard behavior.\n\nAt the moment, from what i can tell, the general behavior of the refinement logic is along the lines of...\n\n\nwhile ( ! values_needing_refined.isEmpty() ) {\n  foreach (value : values_needing_refined) {\n    foreach (shard_not_responded : value) {\n      shardrequstor.enque_refinement_request(shard_not_responded, value)\n    }\n  }\n  shardrequestor.send_refinement_requests_to_shards_that_need_them()\n}\n\n\n\nNow imagine a situation where, for whatever reason, some shard will never respond back as expected from a refinement request \u2013 ie: maybe we just started a rolling config upgrade and the new solrconfig.xml permanently disables faceting via a facet=false invariant? that's going to be an infinite loop.\n\nWe need a safety valve in the refinement logic.\n\nInstead of simply sending refinement requests to a shard if there is a value whose shard count we don't know, potentially asking for hte same refinement over and over; we should instead keep track of which shards we've already asked for a refinement, and if we've already asked a shard once, and still don't have a response then we should just give up and return an error.\n\ndoes that make sense?\n "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14065242",
            "date": "2014-07-17T17:59:53+0000",
            "content": "We need a safety valve in the refinement logic.\n\nI'm with you. Brett and I have talked through some options and I think we have the requisite accounting already in place to be able to check if a shard did not respond with all the refinement values we asked for. We should be able to check the refinements and see that they have had data contributed from the shard in question, if not well throw an error.\n\nIf a shard never responds, does the searcher handle that by eventually timing out? The pivot facet code is predicated on waiting for all shards to respond before moving forward with the next level of refinement so if a shard never responds at all then it'll just wait forever. We're assuming that other processes are watching for searches that take much too long and kill them. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14065279",
            "date": "2014-07-17T18:18:01+0000",
            "content": "If a shard never responds, does the searcher handle that by eventually timing out? \n\nOff the top of my head i'm not sure \u2013 i'm pretty sure that will generate an error at a much lower level then the individual search components, so it's not something the pivot code needs to worry about.\n\nI think the main thing is that when the pivot codes is looking at a ShardResponse, it should be able to say \"this response doesn't contain a count for the refinementX we asked for in the corrisponding ShardRequest, fail!\" (as opposed to know where i believe it reQueues it) ... we can leave worrying about whether or not a ShardResponse was ever returned at all to the SearchHandler.\n\n... I think we have the requisite accounting already in place to be able to check if a shard did not respond with all the refinement values we asked for. We should be able to check the refinements and see that they have had data contributed from the shard in question, if not well throw an error.\n\nthat should be all we need. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-14065352",
            "date": "2014-07-17T18:49:40+0000",
            "content": "I think the main thing is that when the pivot codes is looking at a ShardResponse, it should be able to say \"this response doesn't contain a count for the refinementX we asked for in the corrisponding ShardRequest, fail!\" (as opposed to know where i believe it reQueues it) ... we can leave worrying about whether or not a ShardResponse was ever returned at all to the SearchHandler.\n\nWe mark a shard as having contributed a value already, and we have a list of refinements we requested from the shard.  All we'll need to do is iterate over that list after merging the shard contribution and making sure that the shard bit is set on that value.  If it isn't, then we know we got a response from that shard but that it didn't tell us what we asked it for, and therefore some sort of error has occurred.  We're working on implementing this now. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14065429",
            "date": "2014-07-17T19:40:14+0000",
            "content": "Hey Hoss. How would we test this?\n\nI've verfied it works by commenting out the mergeResponse lines and seeing it error since we expected shard contributions but failed. But how do I write a test where a shard responds in a canned way that is bad? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14065511",
            "date": "2014-07-17T20:38:23+0000",
            "content": "Hey Hoss. How would we test this?\n\nI don't know.  I don't think we can.  Like i mentioned before...\n\n...That's the sort of edge case that may be really hard to test for, but even if we can't explicit test it, we should at least have some sanity check in the distrib coordination code...\n\nIt's an extreme enough edge case that i don't think we need to jump through a crazy amount of hoops to have a test case for it, i just didn't want to leave such a dangerous trap in the code "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14065598",
            "date": "2014-07-17T21:38:36+0000",
            "content": "So I may have jumped the gun on the fix. Previously refinement requests would return counts of zero for things that it was asked about but had no values for, this is now no longer true. (It shrinks the required work to merge in refinements and limits how much data we send across the wire). This means that we cannot ask if a refinement has been fulfilled because if a shard doesn't know about a valuePath it will not include it in its response.\nIf the shard is responding properly it should still return a facet_pivot with the top level requested pivots, just with everything below that merely an empty list.\nSo really the only thing we can check is that the facet_pivot isn't null on a response, since that would indicate that something really awful happened. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14065685",
            "date": "2014-07-17T22:22:16+0000",
            "content": "Previously refinement requests would return counts of zero for things that it was asked about but had no values for, this is now no longer true. (It shrinks the required work to merge in refinements and limits how much data we send across the wire). This means that we cannot ask if a refinement has been fulfilled because if a shard doesn't know about a valuePath it will not include it in its response.\n\nSo if i'm understanding correctly:\n\nOld code:\n\n\texpected every shard to reply back with a number (at least 0) for every refinement\n\tif it didnt' have a number fro ma shard, it asked again - infinite loop risk\nNew Code:\n\texpects every shard to reply back with a number for any refinement it has a non-0 count for\n\timplicitly assumes a refinement has been fulfilled if it knows it already asked.\n\n\n\n..so it sounds like you already eliminated the underlying risk .. correct?\n\nSo really the only thing we can check is that the facet_pivot isn't null on a response, since that would indicate that something really awful happened.\n\nyeah ... sounds good: we know this shard request asked for pivot refinements, if the response doesn't at least contain a facet_pivot response then throw a server error. "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-14066383",
            "date": "2014-07-18T14:15:26+0000",
            "content": "..so it sounds like you already eliminated the underlying risk .. correct?\n\nYes.  Since we know we'll only need to send refinements once, we added some logic to ensure we don't attempt to re-refine something we have already refined.  The bonus to this change is that it should offer a minor performance bump since we won't bother to re-check all of the refined values for further refinement.  (We've already asked all the shards about all of the candidates, so there won't be a need to repeat that.  We already know we have everything we need after we've refined once.) "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14066425",
            "date": "2014-07-18T15:11:11+0000",
            "content": "Uploaded latest patch with the refinement optimizations and error check "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14072565",
            "date": "2014-07-23T23:44:51+0000",
            "content": "\nhey guys, stoked to see all these tests passing!\n\nI've been slowly working my way through Andrew's latest patch, reviewing all the code and making some tweaks/improvements as I go.  Here's a checkpointed update...\n\n\nPatch updates in attachment:\n\n\n\tfix FacetComponent to mirror refactoring done in SOLR-6216\n\tfixed up the String.format calls in various classes so they specify Locale.ROOT\n\t\n\t\tremoved some useless \"toString()\" calls in these format calls as well, particularly since it looked like they could cause NPEs\n\t\n\t\n\tPivotFacetField\n\t\n\t\tjavadocs:\n\t\t\n\t\t\tcreateFromListOfNamedLists\n\t\t\tconvertToListOfNamedLists\n\t\t\n\t\t\n\t\teliminate call to PivotFacetFieldValueCollection.contains(...) (see below)\n\t\n\t\n\tPivotFacetValue...\n\t\n\t\tjavadocs:\n\t\t\n\t\t\tclass\n\t\t\tcreateFromNamedList\n\t\t\tshardHasContributed\n\t\t\tconvertToNamedList\n\t\t\n\t\t\n\t\n\t\n\tPivotFacetFieldValueCollection...\n\t\n\t\tjavadocs:\n\t\t\n\t\t\tclass\n\t\t\trefinableCollection\n\t\t\trefinableSubList\n\t\t\trefinableSize\n\t\t\tsize\n\t\t\tget\n\t\t\tadd\n\t\t\n\t\t\n\t\tremove unused methods\n\t\t\n\t\t\tisEmpty()\n\t\t\tgetValue(Comparable)\n\t\t\tcontains(Comparable)\n\t\t\t\n\t\t\t\t(this was used, but only in a case where it was immediately followed by a call get(Comparable) so i just optimized it away and replaced it with a null check.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\trename: \"isSorted\" -> \"dirty\"\n\t\trename: \"nullValue\" -> \"missingValue\"\n\t\t\n\t\t\tit was really confusing because \"nullValue\" could be null, or it could be a PivotFacetValue whose value was null\n\t\t\n\t\t\n\t\tfix add(PivotFacetValue) to set \"dirty\" directly\n\t\tlock down some stuff...\n\t\t\n\t\t\tmethods for accessing some vars so they don't need to be public\n\t\t\tmake some things specified in constructor final\n\t\t\tmake refinableCollection and refinableSubList return immutable lists\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nSome things i'm either confused by and/or debating in my head ... comments/opinions from others would be apreciated:\n\n\n\trefinement and facet offset\n\t\n\t\tI haven't looed into this closely, but i noticed the refinement code seems to only refine things started at the \"facetFieldOffset,\" of the current collection\n\t\tdon't we need to refine all the values, starting from the beginging of the list?\n\t\tif if the offset is \"1\" and the first value X has a count of \"100\" and the second value Y has an initial count of \"50\" but a post-refinement count of \"150\" pushing itself prior to the offset and putting X into the window, then doesn't X miss out on refinement?\n\t\n\t\n\n\n\n\n\trefinableCollection()\n\t\n\t\tI think we probably want to rename refinableCollection() (and refinableSize()) to something more like \"getExplicitValuesList() (compared to the getMissingValue() method I just added) to make it more clear what you are really getting form this method ... I recognize that this name comes from the fact that we don't ever really need to refine the count for the missing value, but that seems like an implementaion detail that doesn't affect a lot of places this method is called (and particularly since the childPivots of the missing value do still need refined so even when it is relevant, it's still missleading from a recursion standpoint.)\n\t\n\t\n\n\n\n\n\ttrim\n\t\n\t\tfrom what i can understand of the trim methods - these are typically destructive operations that:\n\t\t\n\t\t\tshould only be called after all refinement is completed\n\t\t\tprune things that are no longer needed based on the limit/offset params, making the objects unusable for any future modifications/refinement so that it's only good for...\n\t\t\tshould be called just prior to asking for the final NamedList response structure\n\t\t\n\t\t\n\t\tif my understanding is correct, then it seems like it might be safer & more straight forward to instead just refactor this functionality directly into the corrisponding methods for converting to a NamedList, and clearly document those methods as destructive?\n\t\t\n\t\t\tor at the very least add a \"trimmed\" boolean and sprinkle arround some asserts in the various methods related to wether the object has/has not already been trimmed\n\t\t\n\t\t\n\t\n\t\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14075177",
            "date": "2014-07-26T00:59:58+0000",
            "content": "Making good progress (only ~1600 lines of diff left to review!)\n\nupdates in this patch...\n\n\n\tPivotFacetFieldValueCollection\n\t\n\t\tsome javadocs\n\t\trefactor away method: nonNullValueIterator()\n\t\t\n\t\t\tonly called in one place\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tPivotFacetField\n\t\n\t\tsome javaddocs\n\t\tmade createFromListOfNamedLists smart enough to return null on null input\n\t\t\n\t\t\tsimplified PivotFacetValue.createFromNamedList\n\t\t\n\t\t\n\t\tmade contributeFromShard smart enough to be a no-op on null input\n\t\t\n\t\t\tsimplified all callers (PivotFacet & PivotFacetValue)\n\t\t\n\t\t\n\t\tmade some vars final where possible via refactoring constructor & createFromListOfNamedLists\n\t\trefactor skipRefinementAtThisLevel out of the method an up to an instance var since it never changes once the facet params are set in the constructor\n\t\tconsolidate skipRefinementAtThisLevel + hasBeenRefined into a single var: needRefinementAtThisLevel\n\t\tsimplify BitSet iteration (nextSetBit is always < length)\n\t\t\n\t\t\tprocessDefiniteCandidateElement\n\t\t\tprocessPossibleCandidateElement\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tPivotFacetValue\n\t\n\t\tsome javadocs\n\t\tmade variables private and added method accessors (w/jdocs) as needed\n\t\t\n\t\t\tupdated other classes as needed to call these new methods instead of the old pub vars\n\t\t\n\t\t\n\t\tmade some vars final where possible via refactoring createFromNamedList & constructor\n\t\n\t\n\n\n\n\n\tPivotFacet\n\t\n\t\tsome javadocs\n\t\tadded getQueuedRefinements(int)\n\t\tmade some variables final where possible\n\t\trenamed noRefinementsRequired -> isRefinementsRequired\n\t\teliminate unused method: areAnyRefinementsQueued\n\t\n\t\n\n\n\n\n\tFacetComponent\n\t\n\t\tswitched direct use of PivotFacet.queuedRefinements to use PivotFacet.getQueuedRefinements\n\t\t\n\t\t\tsimplified error checking in several places\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nOne new question i want to go back and revisit later...\n\n\n\tdo we really need to track \"knownShards\" in PivotFacet ?\n\t\n\t\tResponseBuilder already maintains a String[] of all shards, getShardNum derived from it\n\t\tcan't we just loop from 0 to shards.length? does it ever matter if a shard hasn't participated?\n\t\tie: is it really important that we skip any \"unset bits\" in knownShards when looping?  (all the current usages seem safe even if a shard has no data for the current pivot)\n\t\n\t\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14077274",
            "date": "2014-07-29T01:54:33+0000",
            "content": "\nI've been focusing on more tests using facet.offset...\n\nI haven't looed into this closely, but i noticed the refinement code seems to only refine things started at the \"facetFieldOffset,\" of the current collection don't we need to refine all the values, starting from the beginging of the list?\n\nThere was in fact a bug with refinement when using facet.offset \u2013 but i was looking in the wrong place.  the code i was refering to before was involved in deciding which values to drilldown into when recursively refining the sub-pivots.  that logic was already (mostly) correct because by that point we've already refined the current levle completly, so we can skip past the offset when doing the recursion (the only glitch was a boundary check causing an IOOBE, see detials below).  Earlier on in the code however, there was a mistake where only the limit (not the limit+offset) was being used to decide the threshold value for refinement.\n\n\n\nNew improvements in this patch...\n\n\n\tTestCloudPivotFacet\n\t\n\t\tincrease the odds of overrequest==0\n\t\trandonly include a facet.offset param to sanity check refinement in that case\n\t\n\t\n\n\n\n\n\tPivotFacetField\n\t\n\t\tfix refineNextLevelOfFacets not to ask for a sublist with a start offset bigger then the size of the collection\n\t\t\n\t\t\tthis was causing an IndexOutOfBoundsException pretty quickly when offset was mixed into the random test\n\t\t\n\t\t\n\t\tfix queuePivotRefinementRequests to respect offset when picking the \"indexOfCountThreshold\"\n\t\t\n\t\t\tbefore it was only looking at limit, with offset in the randomized test this was causing failures even when pivots only had one field in them!\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nA few more things to consider in the future...\n\n\n\tPivotFacetFieldValueCollection.refinableSubList is only use to deal with offset+limit sublisting from PivotFacetField.refineNextLevelOfFacets \u2013 but PivotFacetFieldValueCollection already knows the offset&limit so maybe it should be a smarter special purpose method with 0 args: getNextLevelValuesToRefine()\n\n\n\n\n\ttrim earlier?\n\t\n\t\tthe way refinement currently works in PivotFacetField, after we've refined our values, we mark that we no longer need refinement, and then on the next call we recursively refine the subpivots of each value \u2013 and in both cases we do the offset+limit calculations and hang on to all of the values (both below offset and above limit) as we keep iterating down hte pivots \u2013 they don't get thrown away until the final trim() call just before building up the final result.\n\t\ti previously suggested folding the trim() logic into the NamedList response logic \u2013 but now i'm wondering if the trim() logic should instead be folded into refinement?  so once we're sure a level is fully refined, we go ahead and trim that level before drilling down and refining it's kids?\n\t\n\t\n\n\n\n\n\nUnfortunately, with this new patch, i did uncover a new random failure i can't easily explain (doesn't seem related ot the offset changes since facet.offset isn't evne used in these random params \u2013 but it's possible i broke something while fixing that) ...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=775F7BCA685BBC22 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=da_DK -Dtests.timezone=America/Montserrat -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 65.9s | TestCloudPivotFacet.testDistribSearch <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: {main(facet=true&facet.pivot=pivot_tl%2Cpivot_tl%2Cpivot_y_s&facet.pivot=bogus_not_in_any_doc_s%2Cpivot_l1%2Cpivot_td&facet.limit=13&facet.missing=true&facet.sort=count&facet.overrequest.count=2),extra(rows=0&q=*%3A*&fq=id%3A%5B*+TO+383%5D&_test_miss=true&_test_sort=count)} ==> bogus_not_in_any_doc_s,pivot_l1,pivot_td: {params(rows=0),defaults({main({main(rows=0&q=*%3A*&fq=id%3A%5B*+TO+383%5D&_test_miss=true&_test_sort=count),extra(fq=-bogus_not_in_any_doc_s%3A%5B*+TO+*%5D)}),extra(fq=%7B%21term+f%3Dpivot_l1%7D5098)})} expected:<7> but was:<9>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([775F7BCA685BBC22:F6B9F5D21F04DC1E]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:239)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.doTest(TestCloudPivotFacet.java:187)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:865)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:744)\n   [junit4]    > Caused by: java.lang.AssertionError: bogus_not_in_any_doc_s,pivot_l1,pivot_td: {params(rows=0),defaults({main({main(rows=0&q=*%3A*&fq=id%3A%5B*+TO+383%5D&_test_miss=true&_test_sort=count),extra(fq=-bogus_not_in_any_doc_s%3A%5B*+TO+*%5D)}),extra(fq=%7B%21term+f%3Dpivot_l1%7D5098)})} expected:<7> but was:<9>\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertNumFound(TestCloudPivotFacet.java:507)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:257)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:268)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:229)\n\n\n\n...i need to dig into this a bit more tommorow. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14078058",
            "date": "2014-07-29T17:44:10+0000",
            "content": "Quick update...\n\n...i need to dig into this a bit more tommorow.\n\na restless night sleep and semi-fresh eyes make scary bugs shallow: the problem was that PivotFacetField.queuePivotRefinementRequests had a short circuit optimization when valueCollection.refinableCollection().isEmpty() that was preventing the child pivots of the facet.missing count from being refined if there were no matching values in the field.\n\nThis patch fixes that bug and adds an explicit test for this situation to DistributedFacetPivotLargeTest. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14078739",
            "date": "2014-07-30T01:16:21+0000",
            "content": "\nI let my laptop hammer away on TestCloudPivotFacet while i was looking at some other stuff, and got a new reproducible failure...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=EE02505B2F4046AC -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=fi -Dtests.timezone=Asia/Aqtobe -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 56.9s | TestCloudPivotFacet.testDistribSearch <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: {main(facet=true&facet.pivot=pivot_y_s%2Cpivot_b&facet.pivot=pivot_tdt1&facet.limit=4&facet.offset=5&facet.pivot.mincount=17&facet.missing=false&facet.sort=index),extra(rows=0&q=id%3A%5B*+TO+786%5D&_test_min=17&_test_miss=false&_test_sort=index)} ==> pivot_y_s,pivot_b: {params(rows=0),defaults({main(rows=0&q=id%3A%5B*+TO+786%5D&_test_min=17&_test_miss=false&_test_sort=index),extra(fq=%7B%21term+f%3Dpivot_y_s%7Dg)})} expected:<22> but was:<50>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([EE02505B2F4046AC:6FE4DE43581F2690]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:239)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.doTest(TestCloudPivotFacet.java:187)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:865)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:744)\n   [junit4]    > Caused by: java.lang.AssertionError: pivot_y_s,pivot_b: {params(rows=0),defaults({main(rows=0&q=id%3A%5B*+TO+786%5D&_test_min=17&_test_miss=false&_test_sort=index),extra(fq=%7B%21term+f%3Dpivot_y_s%7Dg)})} expected:<22> but was:<50>\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertNumFound(TestCloudPivotFacet.java:507)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:257)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:229)\n   [junit4]    > \t... 42 more\n\n\n\nAt first i thought this was simply an issue in how \"needRefinementAtThisLevel\" assumed we never need refinement for sort=index \u2013 that's too general of an assertion, we can only asume no refinement is needed if mincount=0.  But fixing that still didn't solve the problem.\n\nThinking about the PivotFacetField.queuePivotRefinementRequests logic however made me realize that all of the logic in that method (and it's use of \"countThreshold\") really only works with sort=count ... for sort=index we shouldn't make any assumptions about the cutoff based on the count.\n\nBefore digging into a fix, I started working on more sort=index tests to try and better excercise this code, and quickly encountered a new (unrelated?) failure that seems to related to mincount==0 on sub pivots...\n\nI distilled the new mincount failure out into a new isolated test query (that doesn't use sort=index) in DistributedFacetPivotLargeTest:\n\n\n    rsp = query( \"q\", \"*:*\",\n                 \"rows\", \"0\",\n                 \"facet\",\"true\",\n                 \"facet.pivot\",\"place_s,company_t\",\n                 FacetParams.FACET_LIMIT, \"50\",\n                 FacetParams.FACET_PIVOT_MINCOUNT,\"0\"); \n\n\n\n...which leads to...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=DistributedFacetPivotLargeTest -Dtests.method=testDistribSearch -Dtests.seed=63DFE6A839DD2C9F -Dtests.slow=true -Dtests.locale=es_NI -Dtests.timezone=Asia/Bishkek -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 43.5s | DistributedFacetPivotLargeTest.testDistribSearch <<<\n   [junit4]    > Throwable #1: junit.framework.AssertionFailedError: .facet_counts.facet_pivot.place_s,company_t[1].pivot.length:3!=50\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([63DFE6A839DD2C9F:E23968B04E824CA3]:0)\n\n\n\n...i haven't dug into what exactly is going on here, i've been focusng on more tests for the sort=index refinement bug first (since it's easy to reproduce even w/o sub-pivots)\n\n\n\nIn addition to the above mentioned addition to DistributedFacetPivotLargeTest, this new patch also adds some new queries/assertions to DistributedFacetPivotSmallTest that seem to demo the problem with facet.sort=index as the randomized failure (at least ... i think it's the same problem).\n\ni'm going to work on fixing queuePivotRefinementRequests to account for sort=index tomorow.\n\n\n\nAndrew, Brett: I don't suppose the mincount=0 bug jumps out at you guys as something with an obvious fix? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14084842",
            "date": "2014-08-04T16:18:54+0000",
            "content": "\nAter working through the fix the the refinement logic in PivotFacetField.queuePivotRefinementRequests the previously failing seed for TestCloudPivotFacet started to pass, but some sort=index tests still weren't working, which lead me to realize 2 things:\n\n\tsome of my tests were absurd \u2013 i've gotten use to using overrequest=0 as a way to force refinement, but with facet.sort=index combined with limit (and offset) ad mincount it ment that it was impossible for the sort=index facet logic to ever find the results we're looking for.  We have to allow some overrequest when mincount>1 or the initial shard requests won't find the values (that will ultimately have a cumulative mincount high enough) in order to even try refining them.\n\toffset wasn't being added to the limit in the per-shard requests, so w/o overrequest enabled you would never get teh values you needed even in ideal situations\n\tthe shard query logic in FacetComponent was ignoring overrequest when sort=index ... this seems broken to me, but from what i can tell, it comes straight form the existing facet.field logic as well.\n\n\n\nI'll open a bug to track the existing broken logic overrequest logic in facet.field \u2013 even though i hope that once we're done with this issue, it may be fixed via refactoring and shared code with pivots (i'm not 100% certain: the FacetComponent diff is the bulk of what i still need to review more closely on this issue)\n\nThere's still a failure in DistributedFacetPivotLargeTest (mismatch comapred to control) when i tried using mincount=0 that i'm not certain if/how we can solve...\n\n\n// :nocommit: broken honda?\nrsp = query( params( \"q\", \"*:*\",\n                     \"rows\", \"0\",\n                     \"facet\",\"true\",\n                     \"facet.sort\",\"index\",\n                     \"f.place_s.facet.limit\", \"20\",\n                     \"f.place_s.facet.offset\", \"40\",\n                     FacetParams.FACET_PIVOT_MINCOUNT,\"0\",\n                     \"facet.pivot\", \"place_s,company_t\") );\n\n\n\nFrom what I can tell, the gist of the issue is that when dealing with sub-fields of the pivot, the coordination code doesn't know about some of the \"0\" values if no shard which has the value for the parent field even knows about the existence of the term.\n\nThe simplest example of this discrepency (compared to single node pivots) is to consider an index with only 2 docs...\n\n\n[{\"id\":1,\"top_s\":\"foo\",\"sub_s\":\"bar\"}\n {\"id\":2,\"top_s\":\"xxx\",\"sub_s\":\"yyy\"}]\n\n\n\nIf those two docs exist in a single node index, and you pivot on top_s,sub_s using mincount=0 you get a response like this...\n\n\n$ curl -sS 'http://localhost:8881/solr/select?q=*:*&rows=0&facet=true&facet.pivot.mincount=0&facet.pivot=top_s,sub_s&omitHeader=true&wt=json&indent=true'\n{\n  \"response\":{\"numFound\":2,\"start\":0,\"docs\":[]\n  },\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{},\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_pivot\":{\n      \"top_s,sub_s\":[{\n          \"field\":\"top_s\",\n          \"value\":\"foo\",\n          \"count\":1,\n          \"pivot\":[{\n              \"field\":\"sub_s\",\n              \"value\":\"bar\",\n              \"count\":1},\n            {\n              \"field\":\"sub_s\",\n              \"value\":\"yyy\",\n              \"count\":0}]},\n        {\n          \"field\":\"top_s\",\n          \"value\":\"xxx\",\n          \"count\":1,\n          \"pivot\":[{\n              \"field\":\"sub_s\",\n              \"value\":\"yyy\",\n              \"count\":1},\n            {\n              \"field\":\"sub_s\",\n              \"value\":\"bar\",\n              \"count\":0}]}]}}}\n\n\n\nIf however you index each of those docs on a seperate shard, the response comes back like this...\n\n\n$ curl -sS 'http://localhost:8881/solr/select?q=*:*&rows=0&facet=true&facet.pivot.mincount=0&facet.pivot=top_s,sub_s&omitHeader=true&wt=json&indent=true&shards=localhost:8881/solr,localhost:8882/solr'\n{\n  \"response\":{\"numFound\":2,\"start\":0,\"maxScore\":1.0,\"docs\":[]\n  },\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{},\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_pivot\":{\n      \"top_s,sub_s\":[{\n          \"field\":\"top_s\",\n          \"value\":\"foo\",\n          \"count\":1,\n          \"pivot\":[{\n              \"field\":\"sub_s\",\n              \"value\":\"bar\",\n              \"count\":1}]},\n        {\n          \"field\":\"top_s\",\n          \"value\":\"xxx\",\n          \"count\":1,\n          \"pivot\":[{\n              \"field\":\"sub_s\",\n              \"value\":\"yyy\",\n              \"count\":1}]}]}}}\n\n\n\nThe only solution i can think of, would be an extra (special to mincount=0) stage of logic, after each PivotFacetField is refined, that would:\n\n\titerate over all the values of the current pivot\n\tbuild up a Set of all all the known values for the child-pivots of of those values\n\titerate over all the values again, merging in a \"0\"-count child value for every value in the set\n\n\n\n...ie: \"At least one shard knows about value 'v_x' in field 'sub_field', so add a count of '0' for 'v_x' in every 'sub_field' collection nested under the 'top_field' in our 'top_field,sub_field' pivot\"\n\nI haven't thought this idea through enough to be confident it would work, or that it's worth doing ... i'm certainly not convinced that mincount=0 makes enough sense in a facet.pivot usecase to think getting this test working should hold up getting this committed \u2013 probably something that should just be committed as is, with an open Jira that it's a known bug.\n\n\n\n\n\n\nSummary Changes in this patch\n\n\tPivotFacet\n\t\n\t\tadd a new REFINE_PARAM constant for \"fpt\"\n\t\n\t\n\n\n\n\n\tPivotFacetProcessor\n\t\n\t\tjavadocs\n\t\tuse REFINE_PARAM constant\n\t\n\t\n\n\n\n\n\tPivotFacetField\n\t\n\t\tprocessDefiniteCandidateElement\n\t\t\n\t\t\tjavadocs\n\t\t\tnumberOfValuesContributedByShardWasLimitedByFacetFieldLimit can only be trusted when sort=count\n\t\t\n\t\t\n\t\tprocessPossibleCandidateElement\n\t\t\n\t\t\tmethod only useful when sort=count\n\t\t\tadded assert & javadocs making this clear\n\t\t\n\t\t\n\t\tqueuePivotRefinementRequests\n\t\t\n\t\t\tcall processDefiniteCandidateElement on all elements when using sort=index\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tFacetComponent\n\t\n\t\tapplyToShardRequests - removed this method\n\t\t\n\t\t\ta bunch of it was dead code (if limit > 0, no need to check limit>=0)\n\t\t\tmost of what wasn't dead code was also being done by the callers (ie: redundent overrequest logic)\n\t\t\tthis was also where the original mincount=0 bug lived (mincount was being forced to 1 when called from pivot cade)\n\t\t\n\t\t\n\t\tmodifyRequestForIndividualPivotFacets & modifyRequestForFieldFacets\n\t\t\n\t\t\tmade sure they were directly doing the stuff they use to depend on applyToShardRequests for\n\t\t\tfixed up limit+offset & overrequest logic\n\t\t\n\t\t\n\t\tuse REFINE_PARAM constant\n\t\n\t\n\n\n\n\n\n\tDistributedFacetPivotLargeTest\n\t\n\t\tfixed tests to be less overzealous about overrequest=0\n\t\tadded more mincount=0 testing (currently fails)\n\t\n\t\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14084867",
            "date": "2014-08-04T16:47:53+0000",
            "content": "Chris Hostetter (Unused)\n\nI confess I'm barely skimming this (it's big as you are more aware than me!). But there were two recent JIRAs, SOLR-6300 SOLR-6314 (\"facet mincount fails if distrib=true\" and \"multi-threaded facet count returns different results if shards > 1\") that sure seem like they could be related. Does that seem plausible? I realize this is pivot faceting, but...\n\nSo I'm thinking if I can get repeatable test case failures for these two JIRAs that I should apply this patch and see if this patch fixes them.\n\nThoughts? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14084887",
            "date": "2014-08-04T17:01:57+0000",
            "content": "Erick:\n\n\tSOLR-6300: appears to be specific to date/range faceting - almost certainly not related to the problem i found since there's no overrequesting logic with range faceting.\n\tSOLR-6314 seems unrelated given how it ties into the threading code, which is \"above\" the layer of changes i'm talking about ... but anything is possible.\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14084890",
            "date": "2014-08-04T17:03:54+0000",
            "content": "Rats! And here I was hoping you'd do the work for me ....\n\nGood to know though, it'll keep me from putting this off. Thanks! "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14086856",
            "date": "2014-08-05T21:56:06+0000",
            "content": "\nI've finished reviewing all the code and didn't find any new concerns.  (woot!)\n\nI was hoping that more refactoring could be done to share common logic between the facet.field distributed code and the facet.pivot distributed code (akin to what it seemed like \"applyToShardRequests()\" was aiming for in earlier patches) but between the use of the \"DistribFieldFacet\" class and the anoying discrepency between \"facet.mincount\" and \"facet.pivot.mincount\" that seemed like more trouble then it's worth.\n\nIn addition, my little-laptop-that-could has been churning away of several hundred iterations of TestCloudPivots using tests.nightly=true with this patch for the past few days, w/o any signs of bugs in the refinement code.\n\nAt this point, there are only a handful of 'nocommit' comments left in the patch, that fall into 2 basic categories:\n\n\tmethods/variables I still want to rename\n\treminders to create new jira's to track known issues / future improvements\n\n\n\nI plan to deal with those over the next 24 hours, but none of those changes should have any impact on the functionality / performance of the patch as it currently stands.\n\nBrett Lucey & Andrew Muldowney: I'd really appreciate it if you guys could take a gander at the latest version(s) of the patch and give my any thoughts you have.\n\nIn particular: i know you've been using an older patch in production for a while now, could you take this latest version for a spin using some of your real data & queries and set my mind at ease that i haven't introduced any horrible performance problems with any of hte refacotring/code cleanup / bug fixes i've made?\n\n\n\nChanges in this patch\n\n\tTestCloudPivotFacet\n\t\n\t\ta bit more logging\n\t\tdial back overrequest w/ comment (we're focused on refinement here)\n\t\tfix the num iters = 5 (no need to be higher on nightly runs, already increase the index size & num values per field)\n\t\n\t\n\n\n\n\n\tDistributedFacetPivotLargeTest\n\t\n\t\tnew commented out test of \"limit=0 + mincount=0 + missing=true\"\n\t\t\n\t\t\ti had a concern about this edge case w/refinement, but it turns out this isn't evensupported in the existing pivot code.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tFacetComponent\n\t\n\t\tminor formatting & comment cleanup\n\t\tuse PIVOT_KEY consistently throughout file\n\t\trename pivotPrefix -> PIVOT_REFINE_PREFIX; and move to top of file\n\t\tmove pivotRefinementCounter to top of file and add javadocs\n\t\ttweaked handleResponses:\n\t\t\n\t\t\tcheck PURPOSE_REFINE_FACETS and PURPOSE_REFINE_PIVOT_FACETS in seperate if blocks (instead of \"else if\"\n\t\t\tdoesn't change much at the moment, but smelled like a time bomb if/when we ever do pivot refinement in the same requests as facet.field refinement.\n\t\t\n\t\t\n\t\trefactor away sanityCheckRefinements method\n\t\t\n\t\t\tall it was doing was a single null check, so I inlined that\n\t\t\n\t\t\n\t\tuse emptyList() in createPivotFacetOutput\n\t\ttweak variable names in createPivotFacetOutput\n\t\n\t\n\n\n\n\n\tPivotFacetProcessor\n\t\n\t\tclean up nocommits related to using FieldType methods where appropriate\n\t\tjavadoc linting\n\t\n\t\n\n\n\n\n\tPivotFacetField\n\t\n\t\ttrim() javadocs & comment about future optimization\n\t\tjavadoc linting\n\t\n\t\n\n\n\n\n\tPivotFacetValueCollection\n\t\n\t\ttrim() javadocs\n\t\tjavadoc linting\n\t\n\t\n\n\n\n\n\tPivotFacet\n\t\n\t\tjavadoc linting\n\t\n\t\n\n\n\n\n\tPivotFacetValue\n\t\n\t\tjavadoc linting\n\t\n\t\n\n "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14087713",
            "date": "2014-08-06T14:26:50+0000",
            "content": "I'm on this, we'll test this against our current version and see how it shakes out. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14088456",
            "date": "2014-08-06T23:18:38+0000",
            "content": "Fingers crossed, this is the final patch.\n\nNot functional changes, just resolving hte prviously mentioned nocommits by renaming variables/methods or replacing comments about Jiras for future improvements with the actual jira numbers.\n\nant precommit passes. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14089996",
            "date": "2014-08-07T22:57:58+0000",
            "content": "No substance changes in this patch update, just needed updated to trunk since there has been a bunch of churn due to SOLR-4385. "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14093410",
            "date": "2014-08-11T22:05:26+0000",
            "content": "Inital reports are that the newest version is a fair bit slower. \n\nCaveats: We're still on 4.2 in production. So I've backported this to our 4.2 for testing. After going down that rabbit hole for a few days I've got the .wars so I can better test tomorrow but a small sample of 400 production queries on 166,343,278 documents had the following results\n\nOld Patch\nAverage Query Time: 20.56ms\n\nNew Refactor\nAverage Query Time: 63.47ms\n\nI'm using SolrMeter to run these at 200 qpm on a set of five slaves. Tomorrow I'll give each version a much larger burn in, (the query file is 651mbs of queries). I'm not sure these are statistically accurate but I wanted to share what I'm seeing at the moment. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14093449",
            "date": "2014-08-11T22:23:44+0000",
            "content": "Damn... that is unfortunate.\n\nWhich older patch are you comparing with?\nDo you have any idea where the slowdown may have been introduced? \nCan you post some details about the structure of the requests?  (any chance the speed diff is just due to legitimate bugs in the older patch that have been fixed and now result in additional refinement?) "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14094745",
            "date": "2014-08-12T21:46:01+0000",
            "content": "My previous results are crap. The logs were so full of trash their results are useless. After filtering out all refinement queries and other log lines that aren't genuine queries the results have changed significantly.\n\nOld:\naverage 125.64ms @ 10273 queries\nNew:\naverage 131.29 @ 10279 queries\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14095674",
            "date": "2014-08-13T16:32:16+0000",
            "content": "\nWhew ... ok, that diff looks much better.\n\nI'm still curious though about what exactly your comparisons look like ... how \"old\" is the version of the patch you are comparing with?  \n\nDepending on how old it is, some of the bugs we've fixed over hte last few months could totally explain the perf change (ie: it may have been fast, but the numbers may be wrong and/or prone to infinite looping)\n\nSpecific examples i'm thinking of where the gains in correctness would have definitely impacted performance...\n\n\n\tthe int overflow bug fixed ~ 16/Jul/14 13:58 prevented a bunch of refinement\n\tif you use facet.offset: not enough refinement happening until ~ 28/Jul/14 18:54\n\tif you use facet.missing + facet.mincount: sub-pivots of missing may not have been refined correctly until ~ 29/Jul/14 10:44\n\tif you ever use facet.sort=index: refinement wasn't happening until ~ 04/Aug/14 09:18\n\n\n\n\nHowever: if you're comparing my latest patch against the last patch Andrew Muldowney uploaded (~ 18/Jul/14 08:11) and if you don't use facet.offset, or facet.missing, or facet.mincount, or facet.sort=index in any of those queries ... then i'm surprised that you would see much perf difference.\n\n\n\nMy current thinking is that we should move forward with getting this committed to trunk, let it soak for a few days and get hammered by jenkins and then move from there to backport to 4x.  We can always revisit performance improvements later, now that we (in my opinion anyway) have decent confidence in the correctness of behavior.  (and it's not like the performance is abismal)\n\nDoes anyone have concerns with moving forward and revisiting questions about performance improvements in other issues?\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14095689",
            "date": "2014-08-13T16:48:22+0000",
            "content": "bq: My current thinking is that we should move forward with getting this committed to trunk....\n\nCorrect behavior always trumps performance IMO so I agree. Especially for a 5% difference in perf.....\n\nAndrew:\n\nMany thanks for reporting this info! "
        },
        {
            "author": "Andrew Muldowney",
            "id": "comment-14095786",
            "date": "2014-08-13T17:45:29+0000",
            "content": "The \"Old\" but I was using was from 6/18. So its missing a whole bunch of the latest fixes. I agree the new stuff is certainly more accurate and the performance is basically indistinguishable.\n\nAny word on a release date for 4.10? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14095837",
            "date": "2014-08-13T18:13:34+0000",
            "content": "Patch updated to trunk to deal with some minor compilation failures introduced by a (largely) unrelated commit a few hours ago (BytesRefBuilder)\n\nI'm currently running precommit - but once that's done i'll push to trunk. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14095868",
            "date": "2014-08-13T18:24:43+0000",
            "content": "Commit 1617789 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1617789 ]\n\nSOLR-2894: Distributed query support for facet.pivot "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14096270",
            "date": "2014-08-13T23:04:05+0000",
            "content": "I just hit a fail. I've attatched the log. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14096285",
            "date": "2014-08-13T23:15:20+0000",
            "content": "Spooky ... that seed doesn't reproduce for me - even though the failure looks like something that should definitely be related to pivot code, and not any sort of random query timeout or async logic that might be diff between diff runs/machines.\n\nMark Miller: does this seed's failure reproduce reliably for you?\n\nfor context here's the seed/failure from the log mark attached...\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudPivotFacet -Dtests.method=testDistribSearch -Dtests.seed=A9A19A1033FB57AD -Dtests.slow=true -Dtests.locale=sr__#Latn -Dtests.timezone=Europe/Helsinki -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 43.2s J3  | TestCloudPivotFacet.testDistribSearch <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: {main(facet=true&facet.pivot=pivot_i1%2Cbogus_not_in_any_doc_s%2Cpivot_l&facet.limit=17&facet.missing=true&facet.sort=index&facet.overrequest.ratio=0.63600624),extra(rows=0&q=id%3A%5B*+TO+511%5D&_test_miss=true&_test_sort=index)} ==> pivot_i1,bogus_not_in_any_doc_s,pivot_l: {params(rows=0),defaults({main({main({main(rows=0&q=id%3A%5B*+TO+511%5D&_test_miss=true&_test_sort=index),extra(fq=-pivot_i1%3A%5B*+TO+*%5D)}),extra(fq=-bogus_not_in_any_doc_s%3A%5B*+TO+*%5D)}),extra(fq=-pivot_l%3A%5B*+TO+*%5D)})} expected:<18> but was:<16>\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([A9A19A1033FB57AD:2847140844A43791]:0)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:248)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.doTest(TestCloudPivotFacet.java:195)\n   [junit4]    >        at org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:865)\n   [junit4]    >        at java.lang.Thread.run(Thread.java:744)\n   [junit4]    > Caused by: java.lang.AssertionError: pivot_i1,bogus_not_in_any_doc_s,pivot_l: {params(rows=0),defaults({main({main({main(rows=0&q=id%3A%5B*+TO+511%5D&_test_miss=true&_test_sort=index),extra(fq=-pivot_i1%3A%5B*+TO+*%5D)}),extra(fq=-bogus_not_in_any_doc_s%3A%5B*+TO+*%5D)}),extra(fq=-pivot_l%3A%5B*+TO+*%5D)})} expected:<18> but was:<16>\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertNumFound(TestCloudPivotFacet.java:521)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:269)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:280)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:280)\n   [junit4]    >        at org.apache.solr.cloud.TestCloudPivotFacet.assertPivotCountsAreCorrect(TestCloudPivotFacet.java:238)\n   [junit4]    >        ... 42 more\n\n\n\nEDIT: I forgot to mention that i did confirm that based on comparing mark's logs with my own, the seed in question did reproduce the same numDocs and same random pivot params (so presumably it produced the same index in between) .. one potentially interesting thing to note is that the failure mark gets is on the first random pivot request executed.\n\nThe failure message indicates that when looking at the nested \"facet.missing\" counts (ie: docs that didn't have a value in any of hte 3 specified fields fields) there were only 16 docs found by doing a filtered query, but the initial facet.pivot query said there should be 18.  If those numbers were reversed, i might guess maybe the discrepency was a timing issue of some docs being visible on some replicas when the first facet.pivot query was executed, but where later during the sanity check query \u2013 but i can't imagine how the number of matching odcs would go down (at least not in any way that wouldn't reproduce reliably) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14096322",
            "date": "2014-08-13T23:56:46+0000",
            "content": "\n\nIf those numbers were reversed, i might guess maybe the discrepency was a timing issue of some docs being visible on some replicas when the first facet.pivot query was executed, but where later during the sanity check query \u2013 but i can't imagine how the number of matching odcs would go down (at least not in any way that wouldn't reproduce reliably)\n\n1) i'm an idiot: if one shard was slow on getting the updates, then it's possible that shard might be the one consulted on the \"verification\" request, and might have fewer matching docs then what the initial request included.\n\n2) looking closer at mark's log (specifically: grep -v \"path=/select\" ~/tmp/pivotfail.log to ignore the query requests themselves) I see that after we build up the whole index, there is definitely some randomized churn from the SolrCloud test plumbing of adding new replicas, and peer sync, and RecoveryStrategy.doRecovery taking place after the initial pivot query, while the validation queries (which checks that the counts for each pivot constraint match what you get with the equivalent \"fq\" params) are happening.\n\nAll of which leads me wondering if it's possible that this test has just happened to trigger an unrelated bug where perhaps a new replica is getting put into query rotation before it's compleletely in sync with it's leader?\n\nMark Miller, Timothy Potter ... does that sound like a viable possibility given the recovery realted messages in mark's pivotfail.log ?\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14096531",
            "date": "2014-08-14T04:03:19+0000",
            "content": "FWIW: I've been letting my laptop hammer that repro line 250 times \u2013 no failures.\n\nGoing need some help trying to figure that out. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14096949",
            "date": "2014-08-14T13:25:45+0000",
            "content": "I ran the test using Mark's repro line a couple times on my Mac (OS X 10.9.4; Oracle Java 1.7.0_55) and it didn't fail for me.\n\nSo overnight I set up the suite to run 500 iterations (via -Dtests.iters=500; that option doesn't allow -Dtests.method to be supplied), otherwise same repro line, and it ran 148 iterations before a suite timeout killed the run.  Out of those 148 iterations, there were 46 failures, one error, and the suite timeout: ~30% failure rate.\n\nI'm attaching the log: 48.pivotfails.log.bz2 (warning: 130MB uncompressed). "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14097043",
            "date": "2014-08-14T15:03:40+0000",
            "content": "does that sound like a viable possibility given the recovery realted messages in mark's pivotfail.log ?\n\nThe distributed query execution logic shouldn't be selecting any replicas that don't have active status, so if it is, then the core executing the query either has out-of-date state or there's a more serious bug at play. You might try adding the following to the top of your doTest method just to make sure all the test setup is stable before adding docs or running any queries:\n\n    waitForThingsToLevelOut(30000);\n\n "
        },
        {
            "author": "SMS Chauhan",
            "id": "comment-14097562",
            "date": "2014-08-14T20:16:36+0000",
            "content": "Hi,\n\nI noticed that the code has been committed to the development branch. Thanks to all of you for closing this - many people in the community very eagerly waiting for the functionality to become available. This is exciting news! I need help in understanding two things -\n\n1. When will this become available in a stable version?\n2. I work with Solr version 4.7.1(some external dependencies). Can I patch it with this to enable the functionality?\n\nI think this will help all of us get some closure. Thank you for all of your hard work! "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14097589",
            "date": "2014-08-14T20:25:02+0000",
            "content": "\nYou might try adding the following to the top of your doTest method just to make sure all the test setup is stable before adding docs or running any queries:\n\nwaitForThingsToLevelOut(30000);\n\nI added this to TestCloudPivotFacet.doTest(), and I got zero failures out of 128 iterations using Mark's seed (again running the suite with ant using -Dtests.iters=500).\n\nWhile tailing the log from this run, after 10 iterations or so I wondered if it might be true that some other condition would make tests succeed, so I started a simultaneous run on my laptop using clean trunk in a separate checkout, and not applying Tim's change, with the same ant cmdline.  This run had 33 failures out of 117 iterations.\n\nSo it's clear to me that adding waitForThingsToLevelOut(30000); at the top of doTest() makes the failures go away.  On my laptop anyway. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14097644",
            "date": "2014-08-14T20:57:38+0000",
            "content": "So it's clear to me that adding waitForThingsToLevelOut(30000); at the top of doTest() makes the failures go away. On my laptop anyway.\n\nI'll commit that change, but i don't like it \u2013 based on tim's comments and my understanding of that method, adding that call smells like a band aid masking a larger problem unrelated to the pivot code (i'll open a new issue and try to keep investigating)\n\n1. When will this become available in a stable version?\n\nI mentioned in a previous comment...\n\nMy current thinking is that we should move forward with getting this committed to trunk, let it soak for a few days and get hammered by jenkins and then move from there to backport to 4x. \n\n...we're currently in that \"soak\" period i mentioned (to smoke out test failures exactly like the one currently being discussed)\n\n2. I work with Solr version 4.7.1(some external dependencies). Can I patch it with this to enable the functionality?\n\nThe guys who did most of hte work on this patch have backported it as far back as 4.2, so it should certaily be doable, but the patch i've posted probably won't apply trivially, it will likely take some massaging. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14097653",
            "date": "2014-08-14T21:02:14+0000",
            "content": "Commit 1618051 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1618051 ]\n\nSOLR-2894: band-aid call to waitForThingsToLevelOut seems to eliminate some failures on some systems "
        },
        {
            "author": "Brett Lucey",
            "id": "comment-14097670",
            "date": "2014-08-14T21:16:26+0000",
            "content": "sms chauhan - There are 16 hunks that fail to patch on 4.7.1.  6 of them are in the tests, 10 are in functionality.  We have backported the patch to 4.2 and the only place we had difficulty was in the tests as they used features which are not available in 4.2.  You may not run into that in 4.7.1, we don't know exactly when that stuff was introduced.  Ultimately, I don't think it's an insurmountable task.  As Hoss said, it'll take some massaging but you would be able to make it work.\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14097840",
            "date": "2014-08-14T22:44:01+0000",
            "content": "spin off SOLR-6379 with a new (non-pivot) test i wrote inspired by the weird failure mark & steve were getting "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14100037",
            "date": "2014-08-17T18:30:33+0000",
            "content": "Commit 1618490 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1618490 ]\n\nSOLR-2894: tweak test to workarround SOLR-6386 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14100039",
            "date": "2014-08-17T18:32:13+0000",
            "content": "by fluke, one of the test changes made in this issue (not really related to pivot faceting at all) brought SOLR-6386 to light.\n\nI've tightened up the test to be unaffected by this for now.  "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14100076",
            "date": "2014-08-17T19:32:14+0000",
            "content": "Commit 1618513 from hossman@apache.org in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1618513 ]\n\nSOLR-2894: Distributed query support for facet.pivot (merge r1617789, r1618051, r1618490) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14104256",
            "date": "2014-08-20T18:10:08+0000",
            "content": "This has been backported to 4x for almost 72 hours w/o any sign of problems from jenkins.  i think it's safe to call this reslved.\n\nA big thanks to everyone who contributed to this issue over the years, with code and tests and feedback and reports of problems using the various patches against various input ... and especially thanks to everyone for your patience and persistence \u2013 it's definitely paid off. "
        }
    ]
}