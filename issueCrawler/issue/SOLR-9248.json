{
    "id": "SOLR-9248",
    "title": "HttpSolrClient not compatible with compression option",
    "details": {
        "components": [
            "SolrJ"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.5,                                            5.5.1",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Since Solr 5.5, using the compression option (solrClient.setAllowCompression(true)) causes the HTTP client to quickly run out of connections in the connection pool. After debugging through this, we found that the GZIPInputStream is incompatible with changes to how the response input stream is closed in 5.5. It is at this point when the GZIPInputStream throws an EOFException, and while this is silently eaten up, the net effect is that the stream is never closed, leaving the connection open. After a number of requests, the pool is exhausted and no further requests can be served.",
    "attachments": {
        "CompressedConnectionTest.java": "https://issues.apache.org/jira/secure/attachment/12814155/CompressedConnectionTest.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-06-25T00:41:52+0000",
            "author": "Gary Lee",
            "content": "The following is a stack trace we see in the logs (note this is from 5.5, which has since changed in 5.5.1, but the same problem still occurs):\n\n2016-04-26 18:50:28,066 ERROR org.apache.solr.client.solrj.impl.HttpSolrClient  - Error consuming and closing http response stream. [source:]\njava.io.EOFException\n        at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:268)\n        at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:258)\n        at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:164)\n        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:79)\n        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:91)\n        at org.apache.solr.client.solrj.impl.HttpClientUtil$GzipDecompressingEntity.getContent(HttpClientUtil.java:356)\n        at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:87)\n        at org.apache.http.util.EntityUtils.consume(EntityUtils.java:86)\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:594)\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:240)\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:229)\n        at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:942)\n        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:957)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrClient.checkAZombieServer(LBHttpSolrClient.java:596)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrClient.access$000(LBHttpSolrClient.java:80)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrClient$1.run(LBHttpSolrClient.java:671)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n ",
            "id": "comment-15348956"
        },
        {
            "date": "2016-06-27T13:25:12+0000",
            "author": "Mark Miller",
            "content": "This is an interesting issue Mike Drob ",
            "id": "comment-15350981"
        },
        {
            "date": "2016-06-27T18:45:08+0000",
            "author": "Mike Drob",
            "content": "Gary Lee - This might be related to SOLR-8933, which was fixed in 5.5.2 and 5.6. Can you try one of those and see if the problem still persists? ",
            "id": "comment-15351588"
        },
        {
            "date": "2016-06-27T20:28:47+0000",
            "author": "Gary Lee",
            "content": "Mike Drob Looks like 5.5.2 was just released, but not sure when I'll have a chance to integrate it with our application to test this.\n\nHowever, I did look at the solr 5.5.2 source code and based on what I see, I don't believe it is resolved yet. I'm still seeing the same call in HttpSolrClient.executeMethod to close the entity and associated response input stream using Utils.consumeFully, and this is where the problem occurs. ",
            "id": "comment-15351752"
        },
        {
            "date": "2016-06-28T17:37:05+0000",
            "author": "Mike Drob",
            "content": "Ok, thanks for taking an initial look. Can you confirm that this is something that worked in earlier versions (5.4?).\n\nI'm trying to build a unit test to reproduce this, but having a little bit of trouble. I think I see where things are going wrong, but haven't been able to reproduce this in a test.\n\nWhen the call to EntityUtils.consumeFully happens, the code tries to get the stream and read the rest of the bytes from it to clear out the buffers and make servlet containers happy. However, GzipDecompressingEntity attempts to provide a new stream each time, wrapping the same underlying content (which has already been exhausted at this point).\n\nAbsent a way to reproduce this, it's hard to tell if this has been fixed. Your stack trace shows HttpSolrClient::executeMethod calling org.apache.http.util.EntityUtils::consume directly, whereas in 5.5.2 I see executeMethod -> org.apache.solr.common.util.Utils::consumeFully -> GzipDecompressingEntity::getContent (this will throw the exception) -> org.apache.http.util.EntityUtils::consumeQuietly, which will make another attempt at getContent and ignore the exception again.\n\nWhich stream are you indicating should be closed? The GZIPInputStream from the GzipDecompressingEntity was never fully constructed, and doesn't need to be closed. The connection itself should be managed by the servlet container, so we shouldn't be closing it either.\n\nI'm attaching my in-progress unit test for this. ",
            "id": "comment-15353427"
        },
        {
            "date": "2016-06-28T17:52:11+0000",
            "author": "Gary Lee",
            "content": "Hi Mike, yes this did work in 5.4. I tested it before on 5.4 and the logic was to close the response input stream directly (respBody.close()) instead of calling EntityUtils.consumeFully, so the GZIPInputStream was getting closed properly and we weren't losing connections.\n\nThe stack trace is based on 5.5, so doesn't directly correspond with 5.5.2 - sorry if that led to any confusion. But your explanation is correct and that is exactly the problem I see. The exception is now ignored (which is why it's not straightforward to get a stack trace in the logs anymore), but the end result is that the respBody input stream is never closed. I believe respBody is the GZIPInputStream that needs to be closed, because I'm seeing that the connection continues to stay leased and eventually the httpClient doesn't accept new connections anymore. \n\nYour comment on \"The GZIPInputStream from the GzipDecompressingEntity was never fully constructed\" is true when calling EntityUtils.consumeFully, but the GZIPInputStream is first constructed at the time we need to read the response, and that completes without a problem. It's the next time that we try to do the same thing where the error occurs, and the initial GZIPInputStream (respBody) never gets closed. Since the GZipDecompressingEntity is providing a new stream every time, it essentially ignores the one was previously constructed, and thus never achieves the purpose of closing out an input stream. ",
            "id": "comment-15353445"
        },
        {
            "date": "2016-07-13T14:24:36+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Working on SOLR-9290, I remembered about this issue. I think the problem here is that GzipDecompressingEntity (and DeflateDecompressingEntity) do not adhere to the contract laid out for HttpEntity#getContent which states that:\n\n     * Returns a content stream of the entity.\n     * {@link #isRepeatable Repeatable} entities are expected\n     * to create a new instance of {@link InputStream} for each invocation\n     * of this method and therefore can be consumed multiple times.\n     * Entities that are not {@link #isRepeatable repeatable} are expected\n     * to return the same {@link InputStream} instance and therefore\n     * may not be consumed more than once.\n\n\n\nSo both those classes should always return the same object from the getContent method as long as the wrapped entity is non-repeatable and a new InputStream otherwise.\n\nAlso, I'd say it is a good idea in general to avoid calling getContent a second time just to be able to consume it. So even though all entities we actually use in Solr are non-repeatable, Solr code should just make one call to getContent keep the InputStream instance around for the consumption to avoid these kind of bugs. ",
            "id": "comment-15375091"
        },
        {
            "date": "2016-07-13T14:32:10+0000",
            "author": "Mark Miller",
            "content": "Nice Shalin, thanks for looking into this. ",
            "id": "comment-15375110"
        },
        {
            "date": "2017-01-31T18:06:38+0000",
            "author": "Michael Braun",
            "content": "Is this still affecting current 6.x versions of Solr?  ",
            "id": "comment-15847235"
        },
        {
            "date": "2017-04-27T01:51:49+0000",
            "author": "Michael Braun",
            "content": "Ran the test on 6.5x and master and the problem does not appear to be visible from the same test file. Would it be fair to create another ticket to fix the implementations of [Gzip/Deflate]DecompressingEntity to respect isRepeatable and the behavior of getContent assuming that? ",
            "id": "comment-15985869"
        }
    ]
}