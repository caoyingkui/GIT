{
    "id": "SOLR-1724",
    "title": "Real Basic Core Management with Zookeeper",
    "details": {
        "affect_versions": "1.4",
        "status": "Open",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "multicore"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Though we're implementing cloud, I need something real soon I can\nplay with and deploy. So this'll be a patch that only deploys\nnew cores, and that's about it. The arch is real simple:\n\nOn Zookeeper there'll be a directory that contains files that\nrepresent the state of the cores of a given set of servers which\nwill look like the following:\n\n/production/cores-1.txt\n/production/cores-2.txt\n/production/core-host-1-actual.txt (ephemeral node per host)\n\nWhere each core-N.txt file contains:\n\nhostname,corename,instanceDir,coredownloadpath\n\ncoredownloadpath is a URL such as file://, http://, hftp://, hdfs://, ftp://, etc\n\nand\n\ncore-host-actual.txt contains:\n\nhostname,corename,instanceDir,size\n\nEverytime a new core-N.txt file is added, the listening host\nfinds it's entry in the list and begins the process of trying to\nmatch the entries. Upon completion, it updates it's\n/core-host-1-actual.txt file to it's completed state or logs an error.\n\nWhen all host actual files are written (without errors), then a\nnew core-1-actual.txt file is written which can be picked up by\nanother process that can create a new core proxy.",
    "attachments": {
        "commons-lang-2.4.jar": "https://issues.apache.org/jira/secure/attachment/12431075/commons-lang-2.4.jar",
        "hadoop-0.20.2-dev-test.jar": "https://issues.apache.org/jira/secure/attachment/12431719/hadoop-0.20.2-dev-test.jar",
        "gson-1.4.jar": "https://issues.apache.org/jira/secure/attachment/12431720/gson-1.4.jar",
        "SOLR-1724.patch": "https://issues.apache.org/jira/secure/attachment/12431031/SOLR-1724.patch",
        "hadoop-0.20.2-dev-core.jar": "https://issues.apache.org/jira/secure/attachment/12431718/hadoop-0.20.2-dev-core.jar"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Jason Rutherglen",
            "id": "comment-12800994",
            "date": "2010-01-15T23:16:16+0000",
            "content": "Additionally, upon successful completion of a core-version deployment to a set of nodes, then a customizable deletion policy like thing will be default, cleanup the old cores on the system. "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12801051",
            "date": "2010-01-16T01:20:50+0000",
            "content": "\nKatta had some interesting issues in the design of this.\n\nThese are discussed here: http://oss.101tec.com/jira/browse/KATTA-43\n\nThe basic design consideration is that failure of a node needs to automagically update the ZK state accordingly.  This allows all important updates to files to go one direction as well. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12801215",
            "date": "2010-01-16T16:39:52+0000",
            "content": "Note to self: I need a way to upload an empty core/confdir from the command line, basically into ZK, then reference that core from ZK (I think this'll work?).  I'd rather not rely on a separate http server or something... The size of a jared up Solr conf dir shouldn't be too much for ZK? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12801216",
            "date": "2010-01-16T16:47:47+0000",
            "content": "Ted,\n\nThanks for the Katta link. \n\nThis patch will likely de-emphasize the distributed search part,\nwhich is where the ephemeral node is used (i.e. a given server\nlists it's current state). I basically want to take care of this\none little deployment aspect of cores, improving on the wacky\nhackedy system I'm running today. Then IF it works, then I'll\nlook at the distributed search part, hopefully in a totally\nseparate patch.\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12801244",
            "date": "2010-01-16T18:20:49+0000",
            "content": "This'll be a patch on the cloud branch to reuse what's started, I don't see any core management code in there yet, so this looks complimentary. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12801255",
            "date": "2010-01-16T18:56:52+0000",
            "content": "\nThese are discussed here: http://oss.101tec.com/jira/browse/KATTA-43\n\nThe basic design consideration is that failure of a node needs to automagically update the ZK state accordingly. This allows all important updates to files to go one direction as well.\n\nWe actually started out that way... (when a node went down there wasn't really any trace it ever existed) but have been moving away from it.\nZK may not just be a reflection of the cluster but may also control certain aspects of the cluster that you want persistent.  For example, marking a node as \"disabled\" (i.e. don't use it).  One could create APIs on the node to enable and disable and have that reflected in ZK, but it seems like more work than simply saying \"change this znode\". "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12801296",
            "date": "2010-01-16T23:19:31+0000",
            "content": "\nWe actually started out that way... (when a node went down there wasn't really any trace it ever existed) but have been moving away from it.\nZK may not just be a reflection of the cluster but may also control certain aspects of the cluster that you want persistent. For example, marking a node as \"disabled\" (i.e. don't use it). One could create APIs on the node to enable and disable and have that reflected in ZK, but it seems like more work than simply saying \"change this znode\".\n\nI see this as a  conflation of two or three goals that leads to trouble.  All of the goals are worthy and important, but the conflation of them leads to difficult problems.  Taken separately, the goals are easily met.\n\nOne goal is the reflection of current cluster state.  That is most reliably done using ephemeral files roughly as I described.\n\nAnother goal is the reflection of constraints or desired state of the cluster.  This is best handled as you describe, with permanent files since you don't want this desired state to disappear when a node disappears.\n\nThe real issue is making sure that the source of whatever information is most directly connected to the physical manifestation of that information.  Moreover, it is important in some cases (node state, for instance) that the state stay correct even when the source of that state loses control by crashing, hanging or becoming otherwise indisposed.  Inserting an intermediary into this chain of control is a bad idea.  Replicating ZK's rather well implemented ephemeral state mechanism with ad hoc heartbeats is also a bad idea (remember how many bugs there have been in hadoop relative to heartbeats and the name node?).\n\nA somewhat secondary issue is whether the cluster master has to be involved in every query.  That seems like a really bad bottleneck to me and Katta provides a proof of existence that this is not necessary.\n\nAfter trying several options in production, what I find is best is that the master lay down a statement of desired state and the nodes publish their status in a different and ephemeral fashion.  The master can record a history or there may be general directions such as your disabled list however you like but that shouldn't be mixed into the node status because you otherwise get into a situation where ephemeral files can no longer be used for what they are good at.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12801447",
            "date": "2010-01-17T17:42:20+0000",
            "content": "A somewhat secondary issue is whether the cluster master has to be involved in every query.\n\nYeah, that's never been part of the plans AFAIK.  In fact, in this first simple/short iteration, we have no master at all (or if there is one that can direct anything, it will be customer code).\n\nAfter trying several options in production, what I find is best is that the master lay down a statement of desired state and the nodes publish their status in a different and ephemeral fashion.\n\nRight - this is captured on the solr-cloud wiki with the ideas of \"model\" and \"state\".  So far we're only dealing with state - reflecting what the current cluster looks like, and the details of how \"model\" type stuff (what state the nodes should strive for) hasn't been spelled out yet.\n\nOf course, this has hijacked Jason's issue... sorry! "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12803335",
            "date": "2010-01-21T15:40:23+0000",
            "content": "Here's the first cut... I agree, I'm not really into ephemeral\nZK nodes for Solr hosts/nodes. The reason is contact with ZK is\nhighly superficial and can be intermittent. I'm mostly concerned\nwith insuring the core operations succeed on a given server. If\na server goes down, there needs to be more than ZK to prove it,\nand if it goes down completely, I'll simply reallocate it's\ncores to another server using the core management mechanism\nprovided in this patch. \n\nThe issue is still being worked on, specifically the Solr server\nportion that downloads the cores from some location, or performs\noperations. The file format will move to json.  "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12803371",
            "date": "2010-01-21T17:25:33+0000",
            "content": "\n... I agree, I'm not really into ephemeral\nZK nodes for Solr hosts/nodes. The reason is contact with ZK is\nhighly superficial and can be intermittent. \nI have found that when I was having trouble with ZK connectivity, the problems were simply surfacing issues that I had anyway.  You do have to configure the ZK client to not have long pauses (that is incompatible with SOLR how?) and you may need to adjust the timeouts on the ZK side.  More importantly, any issues with ZK connectivity will have their parallels with any other heartbeat mechanism and replicating a heartbeat system that tries to match ZK for reliability is going to be a significant  source of very nasty bugs.  Better to not rewrite that already works.  Keep in mind that ZK connection issues are not the same as session expiration.  Katta has a fairly important set of bugfixes now to make that distinction and ZK will soon handle connection loss on its own. \n\nIt isn't a bad idea to keep shards around for a while if a node goes down.  That can seriously decrease the cost of momentary outages such as for a software upgrade.  The idea is that when the node comes back, it can advertise availability of some shards and replication of those shards should cease.\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12803518",
            "date": "2010-01-21T22:46:03+0000",
            "content": "commons-lang-2.4.jar is required "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12803943",
            "date": "2010-01-22T23:32:41+0000",
            "content": "Do we have some code that recursively downloads a tree of files from ZK?  The challenge is I don't see a way to find out if a given path represents a directory or not. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12803965",
            "date": "2010-01-23T00:37:06+0000",
            "content": "Well, a path could be both a directory and a file with the zookeeper abstraction, which doesn't really work on a standard filesystem.\n\nIf you know your going to not store file data at nodes that have children (the only way that downloading to a real file system makes sense), you could just call getChildren - if there are children, its a dir, otherwise its a file. Doesn't work for empty dirs, but you could also just do getData, and if it returns null, treat it as a dir, else treat it as a file. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12804590",
            "date": "2010-01-25T16:28:15+0000",
            "content": "If you know your going to not store file data at nodes\nthat have children (the only way that downloading to a real file\nsystem makes sense), you could just call getChildren - if there\nare children, its a dir, otherwise its a file. Doesn't work for\nempty dirs, but you could also just do getData, and if it\nreturns null, treat it as a dir, else treat it as a file.\n\nThanks Mark...  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12804655",
            "date": "2010-01-25T19:14:16+0000",
            "content": "Need to have a command line tool that dumps the state of the\nexisting cluster from ZK, out to a json file for a particular\nversion. \n\nFor my setup I'll have a program that'll look at this cluster\nstate file and generate an input file that'll be written to ZK,\nwhich essentially instructs the Solr nodes to match the new\ncluster state. This allows me to easily write my own\nfunctionality that operates on the cluster that's external to\ndeploying new software into Solr.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12804750",
            "date": "2010-01-25T22:25:40+0000",
            "content": "I did an svn update, though now am seeing the following error:\n\njava.util.concurrent.TimeoutException: Could not connect to ZooKeeper within 5000 ms\n\tat org.apache.solr.cloud.ConnectionManager.waitForConnected(ConnectionManager.java:131)\n\tat org.apache.solr.cloud.SolrZkClient.<init>(SolrZkClient.java:106)\n\tat org.apache.solr.cloud.SolrZkClient.<init>(SolrZkClient.java:72)\n\tat org.apache.solr.cloud.CoreControllerTest.testCores(CoreControllerTest.java:48) "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12804760",
            "date": "2010-01-25T22:47:22+0000",
            "content": "The ZK port changed in ZkTestServer "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12804762",
            "date": "2010-01-25T22:49:54+0000",
            "content": "The ZK port changed in ZkTestServe\n\nYeah - too easy to bump against a local ZooKeeper server with the default port, so I've switched it up. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12804773",
            "date": "2010-01-25T22:59:05+0000",
            "content": "For some reason ZkTestServer doesn't need to be shutdown any longer? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12806155",
            "date": "2010-01-28T23:44:01+0000",
            "content": "Hadoop and Gson dependencies "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12806158",
            "date": "2010-01-28T23:48:57+0000",
            "content": "Here's an update, we're onto the actual Solr node portion of the code, and some tests around that.  I'm focusing on downloading cores out of HDFS because that's my use case. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12834539",
            "date": "2010-02-16T23:08:45+0000",
            "content": "There's a wiki for this issue where the general specification is defined: \n\nhttp://wiki.apache.org/solr/DeploymentofSolrCoreswithZookeeper "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12834568",
            "date": "2010-02-17T00:27:10+0000",
            "content": "No-commit\n\nNodeCoresManager[Test] needs more work\n\nA CoreController matchHosts unit test was added to CoreControllerTest "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835458",
            "date": "2010-02-18T22:48:39+0000",
            "content": "\n\tNo-commit\n\n\n\n\n\tNodeCoresManagerTest.testInstallCores works\n\n\n\n\n\tThere's HDFS test cases using MiniDFSCluster\n\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835490",
            "date": "2010-02-18T23:26:10+0000",
            "content": "I need to figure out how integrate this with the Solr Cloud distributed search stuff... Hmm... Maybe I'll start with the Solr Cloud test cases? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835512",
            "date": "2010-02-19T00:04:20+0000",
            "content": "Updated to HEAD "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835513",
            "date": "2010-02-19T00:07:24+0000",
            "content": "I need to add the deletion policy before I can test this in a real environment, otherwise bunches of useless files will pile up in ZK. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835583",
            "date": "2010-02-19T04:43:17+0000",
            "content": "Added a way to hold a given number of host or cores files around in ZK, after which, the oldest are deleted. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835819",
            "date": "2010-02-19T17:00:57+0000",
            "content": "We need a test case for deleted and modified cores. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835871",
            "date": "2010-02-19T18:30:19+0000",
            "content": "Removing cores seems to work well, on to modified cores... I'm checkpointing progress in case things break, I can easily roll back. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835955",
            "date": "2010-02-19T21:25:52+0000",
            "content": "Also needed is the ability to move an existing core to a\ndifferent Solr server. The core will need to be copied via\ndirect HTTP file access, from a Solr server to another Solr\nserver. There is no need to zip the core first. \n\nThis feature is useful for core indexes that have been\nincrementally built, then need to be archived (i.e. the index was not\nconstructed using Hadoop). "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12835963",
            "date": "2010-02-19T21:33:28+0000",
            "content": "\nWill this http access also allow a cluster with incrementally updated cores to replicate a core after a node failure? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835965",
            "date": "2010-02-19T21:35:57+0000",
            "content": "For the above core moving, utilizing the existing Java replication will probably be suitable.  However, in all cases we need to copy the contents of all files related to the core (meaning everything under conf and data).  How does one accomplish this? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12835981",
            "date": "2010-02-19T21:54:44+0000",
            "content": "Will this http access also allow a cluster with\nincrementally updated cores to replicate a core after a node\nfailure? \n\nYou're talking about moving an existing core into HDFS? That's a\ngreat idea... I'll add it to the list!\n\nMaybe for general \"actions\" to the system, there can be a ZK\ndirectory acting as a queue that contains actions to be\nperformed by the cluster. When the action is completed it's\ncorresponding action file is deleted.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12836013",
            "date": "2010-02-19T22:34:10+0000",
            "content": "I think the check on whether a conf file's been modified, to reload the core, can borrow from the replication handler and check the diff based on the checksum of the files... Though this somewhat complicates the storage of the checksum and the resultant JSON file. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12836018",
            "date": "2010-02-19T22:46:35+0000",
            "content": "Some further notes... I can reuse the replication code, but am going to place the functionality into core admin handler because it needs to work across cores and not have to be configured in each core's solrconfig.  \n\nAlso, we need to somehow support merging cores... Is that available yet?  Looks like merge indexes is only for directories? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12836022",
            "date": "2010-02-19T22:49:55+0000",
            "content": "We need a URL type parameter to define if a URL in a core info is to a zip file or to a Solr server download point.   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12836896",
            "date": "2010-02-22T20:51:37+0000",
            "content": "I'm taking the approach of simply reusing SnapPuller and a replication handler for each core... This'll be faster to implement and more reliable for the first release (ie I won't run into little wacky bugs because I'll be reusing code that's well tested).   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12836898",
            "date": "2010-02-22T20:55:26+0000",
            "content": "Actually, I just realized the whole exercise of moving a core is pointless, it's exactly the same as replication, so this is a non-issue...\n\nI'm going to work on backing up a core to HDFS... "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12837418",
            "date": "2010-02-23T19:56:43+0000",
            "content": "We need a test case with a partial install, and cleaning up any extraneous files afterwards "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12837554",
            "date": "2010-02-24T00:56:21+0000",
            "content": "I added a test case that simulates attempting to install a bad core.\n\nStill need to get the backup a Solr core to HDFS working. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12837898",
            "date": "2010-02-24T17:30:59+0000",
            "content": "I'm not sure how we'll handle (or if we even need to) installing\na new core over an existing core of the same name, in other\nwords core replacement. I think the instanceDir would need to be\ndifferent, which means we'll need to detect and fail on the case\nof a new cores version (aka desired state) trying to install\nitself into an existing core's instanceDir. Otherwise this\npotential error case is costly in production. \n\nIt makes me wonder about the shard id in Solr Cloud and how that\ncan be used to uniquely identify an installed core, if a core of\na given name is not guaranteed to be the same across Solr\nservers. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12838689",
            "date": "2010-02-26T03:24:09+0000",
            "content": "Zipping from a Lucene directory works and has a test case\n\nA ReplicationHandler is added by default under a unique name, if one exists already, we still create our own, for the express purpose of locking an index commit point, zipping it, then uploading it to, for example, HDFS.  This part will likely be written next. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12838699",
            "date": "2010-02-26T04:08:13+0000",
            "content": "Backing a core up works, at least according to the test case... I will probably begin to test this patch in a staging environment next, where Zookeeper is run in it's own process and a real HDFS cluster is used. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12838926",
            "date": "2010-02-26T16:46:27+0000",
            "content": "In thinking about this some more, in order for the functionality\nprovided in this issue to be more useful, there could be a web\nbased UI to easily view the master cores table. There can\nadditionally be an easy way to upload the new cores version into\nZookeeper. I'm not sure if the uploading should be web based or\ncommand line, I'm figuring web based, simply because this is\nmore in line with the rest of Solr. \n\nAs a core is installed or is in the midst of some other process\n(such as backing itself up), the node/NodeCoresManager can\nreport the ongoing status to Zookeeper. For large cores (i.e. 20\nGB) it's important to see how they're doing, and if they're\ntaking too long, begin some remedial action. The UI can display\nthe statuses.  "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12839159",
            "date": "2010-02-27T04:09:26+0000",
            "content": "Are you using any of these? \n\nAn Eclipse plug-in: \nhttp://www.massedynamic.org/mediawiki/index.php?title=Eclipse_Plug-in_for_ZooKeeper \n\nA Django (Python web toolkit) app: \nhttp://github.com/phunt/zookeeper_dashboard \n\nA Swing UI \nhttp://issues.apache.org/jira/browse/ZOOKEEPER-418 \n\nAll seem to have recent activity. Maybe one of them could become a custom monitor. \n\nIf you want to monitor a horde of machines & apps via JMX, Hyperic might be the right tool: \nhttp://support.hyperic.com/display/DOC/JMX+Plugin \nhttp://support.hyperic.com/display/DOC/JMX+Plugin+Tutorial \n\nWhen I tried Hyperic out a couple of years ago I was really impressed. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12839520",
            "date": "2010-03-01T01:26:38+0000",
            "content": "Started on the nodes reporting their status to separate files that are ephemeral nodes, there's no sense in keeping them around if the node isn't up, and the status is legitimately ephemeral.  In this case, the status will be something like \"Core download 45% (7 GB of 15GB)\".   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12839937",
            "date": "2010-03-02T00:38:05+0000",
            "content": "I'm starting work on the cores file upload.  The cores file is in JSON format, and can be assembled by an entirely different process (i.e. the core assignment creation is decoupled from core deployment).  \n\nI need to figure out how Solr HTML HTTP file uploading works... There's probably an example somewhere. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12840457",
            "date": "2010-03-03T04:51:31+0000",
            "content": "Fixed the unit tests that were failing due to the switch over to using CoreContainer's initZooKeeper method.  ZkNodeCoresManager is instantiated in CoreContainer.  \n\nThere's a beginning of a UI in zkcores.jsp\n\nI think we still need a core move test.  I'm thinking of adding backing up a core as an action that may be performed in a new cores version file.   "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872531",
            "date": "2010-05-27T22:07:17+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043701",
            "date": "2011-06-03T16:46:38+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106454",
            "date": "2011-09-16T14:51:10+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234764",
            "date": "2012-03-21T18:09:14+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717198",
            "date": "2013-07-23T18:47:27+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971106",
            "date": "2014-04-16T12:57:10+0000",
            "content": "Move issue to Solr 4.9. "
        }
    ]
}