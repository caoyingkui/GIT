{
    "id": "LUCENE-1025",
    "title": "Document clusterer",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/termvectors",
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "A two-dimensional desicion tree in conjunction with cosine coefficient similarity is the base of this document clusterer. It uses Lucene for tokenization and length normalization. \n\nExample output of 3500 clustered news articles dated the thee first days of January 2004 from a number of sources can be found here: < http://ginandtonique.org/~kalle/LUCENE-1025/out_4.0.txt >. One thing missing is automatic calculation of cluster boundaries. Not impossible to implement, nor is it really needed. 4.5 in the URL above is that distance.\n\nThe example was calculated limited to the top 1000 terms from instance, divided with siblings and re-pruned all the way to the root. On my dual core it took about 100ms to insert a new document in the tree, no matter if it contained 100 or 10,000 instances. 1GB RAM held about 10,000 news articles. \n\nNext steps for this code is persistency of the tree using BDB or a even perhaps something similar to the Lucene segmented solution. Perhaps even using Lucene Directory. The plan is to keep this clusterer synchronized with the index, allowing really speedy \"more like this\" features.\n\nLater on I'll introduce map/reduce for better training speed.\n\nThis code is far from perfect, nor is the results as good as many other products.  Knowing I didn't put in more than a few handful of hours, this works quite well.\n\nBy displaying neighboring clusters (as in the example) one will definetly get more related documents at a fairly low false-positive cost. Perhaps it would be interesting to analyse user behavior to find out if any of them could be merged. Perhaps some reinforcement learning?\n\nThere are no ROC-curves, precision/recall-values nor tp/fp-rates as I have no manually clustered corpus for me to compare with.\n\n\nI've been looking for an archive of the Lucene-users forum for demonstrational use, but could not find it. Any ideas on where I can find that? It could for instance be neat to tweak this code to identify frequently asked questions and match it with an answer in the Wiki, but perhaps an SVM, NB or something-implementation would be better suited for that.\n\n\nDon't hesitate to comment on this if you have an idea, request or question.",
    "attachments": {
        "LUCENE-1025.txt": "https://issues.apache.org/jira/secure/attachment/12367455/LUCENE-1025.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-10-10T09:20:01+0000",
            "content": "Major bug in Markov Chain Token Filter detected in the port from java 1.5 > 1.4\n\nThe clustering output shouw be noticable better now. I'll update the linked example output. ",
            "author": "Karl Wettin",
            "id": "comment-12533675"
        },
        {
            "date": "2007-10-10T09:59:52+0000",
            "content": "There is now new example output here: http://ginandtonique.org/~kalle/LUCENE-1025/\n\nI recommend out_5.5.txt, but what number best demonstrate the clusterer will change as the tokenization and similarity alogrithm chages. ",
            "author": "Karl Wettin",
            "id": "comment-12533689"
        },
        {
            "date": "2007-10-11T20:20:06+0000",
            "content": "> I've been looking for an archive of the Lucene-users forum for\n> demonstrational use, but could not find it. Any ideas on where I can find that?\n\nAvailable here: http://lucene.apache.org/mail\n\n\n ",
            "author": "Karl Wettin",
            "id": "comment-12534145"
        },
        {
            "date": "2007-10-15T02:56:40+0000",
            "content": "Introduced a halfbaked Markov chain (not to be confused with the token filter in this patch with a similar name that concatenate tokens) in order to to determine a mean title from all instances in a cluster. Sort of works like the MegaHAL, the talking bot, but usually makes more sense as all titles in a cluster are similar. It needs work with limiting length, it should look further ahead than one link and it is terrible unoptimized. Still, it already behaves quite well with the news article corpus I test with.\n\nPerhaps it would make sense to rather select the title of the most central instance in a cluster, but that is not as fun. ",
            "author": "Karl Wettin",
            "id": "comment-12534722"
        },
        {
            "date": "2008-05-07T17:43:42+0000",
            "content": "MAHOUT-19 is a much better implementation. ",
            "author": "Karl Wettin",
            "id": "comment-12594978"
        }
    ]
}