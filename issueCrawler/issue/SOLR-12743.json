{
    "id": "SOLR-12743",
    "title": "Memory leak introduced in Solr 7.3.0",
    "details": {
        "labels": "",
        "priority": "Critical",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "7.3,                                            7.3.1,                                            7.4",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Reported initially by Markus Jelsma([1], [2]), but other users have had the same issue [3]. Some of the key parts:\n\nSome facts:\n* problem started after upgrading from 7.2.1 to 7.3.0;\n* it occurs only in our main text search collection, all other collections are unaffected;\n* despite what i said earlier, it is so far unreproducible outside production, even when mimicking production as good as we can;\n* SortedIntDocSet instances and ConcurrentLRUCache$CacheEntry instances are both leaked on commit;\n* filterCache is enabled using FastLRUCache;\n* filter queries are simple field:value using strings, and three filter query for time range using [NOW/DAY TO NOW+1DAY/DAY] syntax for 'today', 'last week' and 'last month', but rarely used;\n* reloading the core manually frees OldGen;\n* custom URP's don't cause the problem, disabling them doesn't solve it;\n* the collection uses custom extensions for QueryComponent and QueryElevationComponent, ExtendedDismaxQParser and MoreLikeThisQParser, a whole bunch of TokenFilters, and several DocTransformers and due it being only reproducible on production, i really cannot switch these back to Solr/Lucene versions;\n* useFilterForSortedQuery is/was not defined in schema so it was default (true?), SOLR-11769 could be the culprit, i disabled it just now only for the node running 7.4.0, rest of collection runs 7.2.1;\n\n\n\nYou were right, it was leaking exactly one SolrIndexSearcher instance on each commit. \n\n\nAnd from Bj\u00f6rn H\u00e4user ([3]):\n\nProblem Suspect 1\n\n91 instances of \"org.apache.solr.search.SolrIndexSearcher\", loaded by \"org.eclipse.jetty.webapp.WebAppClassLoader @ 0x6807d1048\" occupy 1.981.148.336 (38,26%) bytes. \n\nBiggest instances:\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u2022 org.apache.solr.search.SolrIndexSearcher @ 0x6ffd47ea8 - 70.087.272 (1,35%) bytes. \n\u00a0 \u00a0 \u00a0 \u00a0 \u2022 org.apache.solr.search.SolrIndexSearcher @ 0x79ea9c040 - 65.678.264 (1,27%) bytes. \n\u00a0 \u00a0 \u00a0 \u00a0 \u2022 org.apache.solr.search.SolrIndexSearcher @ 0x6855ad680 - 63.050.600 (1,22%) bytes. \n\n\nProblem Suspect 2\n\n223 instances of \"org.apache.solr.util.ConcurrentLRUCache\", loaded by \"org.eclipse.jetty.webapp.WebAppClassLoader @ 0x6807d1048\" occupy 1.373.110.208 (26,52%) bytes. \n\n\nMore details in the email threads.\n\n[1] http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201804.mbox/%3Czarafa.5ae201c6.2f85.218a781d795b07b1%40mail1.ams.nl.openindex.io%3E\n [2] http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201806.mbox/%3Czarafa.5b351537.7b8c.647ddc93059f68eb%40mail1.ams.nl.openindex.io%3E\n [3] http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201809.mbox/%3C7B5E78C6-8CF6-42EE-8D28-872230DEDCFB@gmail.com%3E",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-09-05T17:33:48+0000",
            "content": "Is the metrics registry holding onto these SolrIndexSearchers? Perhaps the same thing as SOLR-11882? ",
            "author": "Mark Miller",
            "id": "comment-16604708"
        },
        {
            "date": "2018-09-05T17:43:09+0000",
            "content": "Markus Jelsma, you mention in an email that you are able to reproduce this issue:\n\n\"But, i can reproduce it locally when i introduce queries, and filter queries while indexing pieces of data and committing it.\"\n\nCan you add the steps you used to reproduce, queries run, filter queries run etc...\n\n\u00a0 ",
            "author": "Joel Bernstein",
            "id": "comment-16604714"
        },
        {
            "date": "2018-09-06T08:28:03+0000",
            "content": "That is a most regrettable typo, i meant i can't/cannot reproduce it locally, even when i introduce continuous indexing and querying. That is the whole problem i have, perhaps Bj\u00f6rn can. I'll ask him on the list! ",
            "author": "Markus Jelsma",
            "id": "comment-16605460"
        },
        {
            "date": "2018-09-07T09:25:59+0000",
            "content": "Erick wrote:\nIt would be great if Markus and Bj\u00f6rn could add some environment info on the JIRA, in particular the version of Java you're both using and the op system etc...\n\nAlright. Debian GNU/Linux 9.3 Stretch, running OpenJDK 64bit 1.8.0_181-8u181-b13-1~deb9u1-b13. ",
            "author": "Markus Jelsma",
            "id": "comment-16606883"
        },
        {
            "date": "2018-09-24T05:38:18+0000",
            "content": "Hello everyone,\n\nsorry for the late answer.\n\n\u00a0\n\nWe figured it was the high auto warm counts for our most active collection. We turned them down (as suggested in the mail thread) and everything was fine again. Though I could not find any evidence, that the autowarming itself was the problem. There was nothing in the logs telling me that there are multiple open searchers at the same time.\n\n\u00a0\n\nWe are running the official solr docker images from docker hub.\n\n\u00a0\n\nThank you\n\nBj\u00f6rn\n\n\u00a0 ",
            "author": "Bj\u00f6rn H\u00e4user",
            "id": "comment-16625398"
        },
        {
            "date": "2018-09-24T15:11:04+0000",
            "content": "Bj\u00f6rn:\n\nThanks for that info, I pretty much guarantee that people trying to reproduce this wouldn't have thought of using a high autowarm count right off the bat.\n\n\u00a0\nThere was nothing in the logs telling me that there are multiple open searchers at the same time.\nThe only thing I'd expect is perhaps something like \"PERFORMANCE WARNING: Overlapping onDeckSearcher\" in the logs if your autowarm count was such that a second commit came in while autowarming was going on.\n\nI know some of the metrics were tricky to get to release all their resources, perhaps there's a path where they don't get shut down properly in this situation. ",
            "author": "Erick Erickson",
            "id": "comment-16625953"
        },
        {
            "date": "2018-09-25T11:53:11+0000",
            "content": "Hello Bj\u00f6rn H\u00e4user,\n\nAre you absolutely sure you are no longer leaking a SolrIndexSearcher instance on commit after dialing back the auto-warm counts?\n\nDespite having flooded the filter cache with entries comparable to production and continuously indexing stuff, in local tests i am still not able to reproduce the problem and so not confirm this work-around.\n\nOn the other hand, how could this solve the problem, we do not have overlapping warm up searchers, maxWarmingSearchers is set to 1 and we only do a commit roughly once every fifteen minutes when our crawler is ready to index a batch.\n\nThanks,\nMarkus ",
            "author": "Markus Jelsma",
            "id": "comment-16627213"
        },
        {
            "date": "2018-10-01T19:11:58+0000",
            "content": "Markus Jelsma I am not sure. How could I see this? ",
            "author": "Bj\u00f6rn H\u00e4user",
            "id": "comment-16634498"
        },
        {
            "date": "2018-10-03T10:23:09+0000",
            "content": "Using VisualVM's memory sampler tool, filtered on org.apache.solr.search.SolrIndexSearcher, you should see the number of instances increment by one for each commit. If you have multiple cores of the same collection on the same instance, the number of instances should grow accordingly.\n\nTake great care, on Solrs where this bug is not present, the number of instances of SolrIndexSearcher will increment too! But shortly after the commit, the number will reduce again due to searcher warming and GC delay. ",
            "author": "Markus Jelsma",
            "id": "comment-16636754"
        },
        {
            "date": "2018-10-24T09:18:08+0000",
            "content": "Hello, Bj\u00f6rn H\u00e4user, just pinging again. Could you please verify if your nodes are no longer leaking SolrIndexSearcher instances on commit? And can you confirm that it was your problem to begin with? I am still not sure if you are seeing the same problem as i have.\n\nThanks,\nMarkus ",
            "author": "Markus Jelsma",
            "id": "comment-16661981"
        }
    ]
}