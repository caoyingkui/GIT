{
    "id": "SOLR-10398",
    "title": "Multiple LIR requests can fail PeerSync even if it succeeds",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "7.3"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "I've seen a scenario where multiple LIRs happen around the same time.\nIn this case even if PeerSync succeeded we ended up failing causing a full index fetch.\n\nSequence of events:\nT1: Leader puts replica in LIR and replica's LIRState as DOWN\nT2: Replica begins PeerSync and LIRState changes\nT3: Leader puts replica in LIR again and replica's LIRState is set to DOWN\nT4: PeerSync from T1 succeeds and examines it's own LIRState which is now DOWN and fails triggering a full replication\n\nLog snippet\n\nT1 from the Leader Logs\n\nsolr.log.2:12779:2017-03-23 03:03:18.706 INFO  (qtp1076677520-9812) [c:test s:shard73 r:core_node44 x:test_shard73_replica1] o.a.s.c.ZkController Put replica core=test_shard73_replica2 coreNodeName=core_node247 on server:8993_solr into leader-initiated recovery.\n\n\n\nT2 from the replica logs:\n\nsolr.log.1:2017-03-23 03:03:26.724 INFO  (RecoveryThread-test_shard73_replica2) [c:test s:shard73 r:core_node247 x:test_shard73_replica2] o.a.s.c.RecoveryStrategy Attempting to PeerSync from http://server:8983/solr/test_shard73_replica1/ - recoveringAfterStartup=false\n\n\n\nT3 from the Leader Logs\n\nsolr.log.2:2017-03-23 03:03:43.268 INFO  (qtp1076677520-9796) [c:test s:shard73 r:core_node44 x:test_shard73_replica1] o.a.s.c.ZkController Put replica core=test_shard73_replica2 coreNodeName=core_node247 on server:8993_solr into leader-initiated recovery.\n\n\n\nT4 from the replica logs:\n\n2017-03-23 03:05:38.009 INFO  (RecoveryThread-test_shard73_replica2) [c:test s:shard73 r:core_node247 x:test_shard73_replica2] o.a.s.c.RecoveryStrategy PeerSync Recovery was successful - registering as Active.\n2017-03-23 03:05:38.012 ERROR (RecoveryThread-test_shard73_replica2) [c:test s:shard73 r:core_node247 x:test_shard73_replica2] o.a.s.c.RecoveryStrategy Error while trying to recover.:org.apache.solr.common.SolrException: Cannot publish state of core 'test_shard73_replica2' as active without recovering first!\n at org.apache.solr.cloud.ZkController.publish(ZkController.java:1179)\n at org.apache.solr.cloud.ZkController.publish(ZkController.java:1135)\n at org.apache.solr.cloud.ZkController.publish(ZkController.java:1131)\n at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:415)\n at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:227)\n\n 2017-03-23 03:05:47.014 INFO  (RecoveryThread-test_shard73_replica2) [c:test s:shard73 r:core_node247 x:test_shard73_replica2] o.a.s.h.IndexFetcher Starting download to NRTCachingDirectory(MMapDirectory@/data4/test_shard73_replica2/data/index.20170323030546697 lockFactory=org.apache.lucene.store.NativeFSLockFactory@4aa1e5c0; maxCacheMB=48.0 maxMergeSizeMB=4.0) fullCopy=true\n\n\n\nI don't know whats the best approach to tackle the problem is but I'll post suggestions after doing some research. I wanted to create the Jira to track the issue",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-04-27T20:19:07+0000",
            "content": "Cao Manh Dat we should just mark this as closed as part of\u00a0SOLR-11702 right? ",
            "author": "Varun Thacker",
            "id": "comment-16456993"
        },
        {
            "date": "2018-04-27T20:20:24+0000",
            "content": "This was addressed by the LIR redesign on\u00a0SOLR-11702 ",
            "author": "Varun Thacker",
            "id": "comment-16456997"
        },
        {
            "date": "2018-05-02T10:48:30+0000",
            "content": "Right Varun Thacker, this problem is fixed by SOLR-11702 ",
            "author": "Cao Manh Dat",
            "id": "comment-16460861"
        }
    ]
}