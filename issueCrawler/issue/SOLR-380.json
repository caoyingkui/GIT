{
    "id": "SOLR-380",
    "title": "There's no way to convert search results into page-level hits of a \"structured document\".",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "\"Paged-Text\" FieldType for Solr\n\nA chance to dig into the guts of Solr. The problem: If we index a monograph in Solr, there's no way to convert search results into page-level hits. The solution: have a \"paged-text\" fieldtype which keeps track of page divisions as it indexes, and reports page-level hits in the search results.\n\nThe input would contain page milestones: <page id=\"234\"/>. As Solr processed the tokens (using its standard tokenizers and filters), it would concurrently build a structural map of the item, indicating which term position marked the beginning of which page: <page id=\"234\" firstterm=\"14324\"/>. This map would be stored in an unindexed field in some efficient format.\n\nAt search time, Solr would retrieve term positions for all hits that are returned in the current request, and use the stored map to determine page ids for each term position. The results would imitate the results for highlighting, something like:\n\n<lst name=\"pages\">\n\u00a0\u00a0<lst name=\"doc1\">\n\u00a0\u00a0\u00a0\u00a0                <int name=\"pageid\">234</int>\n\u00a0\u00a0\u00a0\u00a0                <int name=\"pageid\">236</int>\n\u00a0\u00a0        </lst>\n\u00a0\u00a0        <lst name=\"doc2\">\n\u00a0\u00a0\u00a0\u00a0                <int name=\"pageid\">19</int>\n\u00a0\u00a0        </lst>\n</lst>\n<lst name=\"hitpos\">\n\u00a0\u00a0        <lst name=\"doc1\">\n\u00a0\u00a0\u00a0\u00a0                <lst name=\"234\">\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                        <int name=\"pos\">14325</int>\n\u00a0\u00a0\u00a0\u00a0                </lst>\n\u00a0\u00a0        </lst>\n\u00a0\u00a0        ...\n</lst>",
    "attachments": {
        "xmlpayload-src.jar": "https://issues.apache.org/jira/secure/attachment/12380806/xmlpayload-src.jar",
        "xmlpayload-example.zip": "https://issues.apache.org/jira/secure/attachment/12380808/xmlpayload-example.zip",
        "SOLR-380-XmlPayload.patch": "https://issues.apache.org/jira/secure/attachment/12369348/SOLR-380-XmlPayload.patch",
        "xmlpayload.jar": "https://issues.apache.org/jira/secure/attachment/12380807/xmlpayload.jar"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Peter Binkley",
            "id": "comment-12535288",
            "date": "2007-10-16T16:24:30+0000",
            "content": "I've been wondering about what's required to get this output added to the response. It appears that a response writer isn't the answer: those are for different formats (xml, json, etc.). Is everything we need included in the FieldType methods (write(), etc.)? The highlighting functionality is probably a good model for what we want to do.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12535290",
            "date": "2007-10-16T16:32:05+0000",
            "content": "I don't totally understand how a field type solves your problem (I'm sure it can... i just don't quite follow)\n\nBut - If you want your search results to return pages, why not just index each page as a new SolrDocument? "
        },
        {
            "author": "Peter Binkley",
            "id": "comment-12535296",
            "date": "2007-10-16T16:44:12+0000",
            "content": "The problem with the page-as-SorlDocument approach is that you then have to group the pages back under their container documents to present a unified result to the user (like this: http://tinyurl.com/yt2a25 ). I want the primary unit of granularity in search results to be the book, and the pages to be only a secondary layer. I also want to be able to do proximity searches that bridge page boundaries, have relevance ranking consider the whole book text and not just that page, etc.: i.e. treat the text as continuous for searching purposes. So I gain a lot by treating the book as the SolrDocument; I just need that extra bit of work to resolve the page positions to have it all. "
        },
        {
            "author": "Peter Binkley",
            "id": "comment-12535306",
            "date": "2007-10-16T17:43:39+0000",
            "content": "formatted the xml for clarity "
        },
        {
            "author": "Pieter Berkel",
            "id": "comment-12535426",
            "date": "2007-10-17T06:46:23+0000",
            "content": "There was a recent discussion surrounding a similar problem on solr-user:\nhttp://www.nabble.com/Structured-Lucene-documents-tf4234661.html#a12048390\n\nThe idea was to use dynamic fields (e.g. page_1, page_2, page_3... page_N) to store the text of each page in a single document.  The problem is that currently Solr does not support \"glob\" style field expansion in query parameters (e.g. qf=page_* ) so you would end up having to specify the entire list of page fields in your query, which is impractical.  There is already an open issue related to this particular problem (SOLR-247) but nobody has had time to look into it.\n\nIn terms of returning term position information, this seems somehow (albeit loosely) related to highlighting, is there any way you could use the existing functionality to achieve your goal? (definitely would be a hack though) "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12535489",
            "date": "2007-10-17T09:34:01+0000",
            "content": "> The idea was to use dynamic fields (e.g. page_1, page_2, page_3... page_N) to store the text of each page in a single document. The problem is that currently Solr does not support \"glob\" style field expansion in query parameters (e.g.\n> qf=page_* ) so you would end up having to specify the entire list of page fields in your query, which is impractical. There is already an open issue related to this particular problem (SOLR-247) but nobody has had time to look into it.\n\nIn this case, a copyField from page_* into an unstored \"contents\" would do the trick, which would also facilitate querying across pages.  A position increment gap could also prohibit phrase queries across \"pages\", optionally. "
        },
        {
            "author": "Peter Binkley",
            "id": "comment-12535648",
            "date": "2007-10-17T17:49:52+0000",
            "content": "Both these methods (page_* fields or unstored \"contents\" field) would make it difficult to discover from the search results which pages matched the query, though, wouldn't they? They would both need extra work to populate a structure like the \"pages\" and \"hitpos\" elements in the sample xml above. Would that extra work be more efficient than the document-map approach we've proposed above? \n\nThe highlighting functionality is definitely the model to follow for handling term positions. "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12535748",
            "date": "2007-10-17T21:16:50+0000",
            "content": "The discussion from http://www.nabble.com/Structured-Lucene-documents-tf4234661.html#a12048390 gives one solution (which is more of a workaround in my opinion), but I don't think it is practical.  The number of pages of the monographs we index vary greatly (10s to 1000s of pages).  So while specifying each page_* (page_1,page_2,page_3,...,page_N) as a field to highlight will work, I don't think it is the cleanest solution because you have to infer page numbers from the highlighted samples.  Furthermore, in order to get the highlighted samples you need to know the values of the * in a dynamic field which sort of defeats the purpose of the dynamic field.  If you wanted to use the position numbers themselves (for example using positions and OCR information to create highlighting on an original image), they are not available in the results.\n\nIn answer to your question Peter, one must enable highlighting and list all the page_* fields for highlighter snippets.  In the following example I have a dynamic field fulltext_*, copyfield fulltext, and defaultSearchField=fulltext:\nhttp://tinyurl.com/3xdshk\n(essentially shows the parameters and their values for this example \u2013 pay attention to the hl.fl parameter)\ngives the normal results, with the following at the end:\n\n<lst name=\"highlighting\">\n\u00a0<lst name=\"News.EFP.186500\">\n\u00a0\u00a0<arr name=\"fulltext_1\">\n\u00a0\u00a0\u00a0<str>\n\u00a0\u00a0\u00a0\u00a0 was <em>employed</em> on the G. T. R. as fireman met his death in an accident on that road some yeara ago but three\n\u00a0\u00a0\u00a0</str>\n\u00a0\u00a0</arr>\n\u00a0\u00a0<arr name=\"fulltext_4\">\n\u00a0\u00a0\u00a0<str>\n\u00a0\u00a0\u00a0\u00a0 ^f 6r-Ke.w\u00a5eaf!fl': Mr.-BradV whb is <em>employed</em> in Windsor, was also at his borne for jSew Year\n\u00a0\u00a0\u00a0</str>\n\u00a0\u00a0</arr>\n\u00a0\u00a0<arr name=\"fulltext_6\">\n\u00a0\u00a0\u00a0<str>\n\u00a0\u00a0\u00a0\u00a0 <em>employed</em> at the Walkerville brewery op to a short time ago,when illness ecessilater! his resignation. He\n\u00a0\u00a0\u00a0</str>\n\u00a0\u00a0</arr>\n\u00a0\u00a0<arr name=\"fulltext_7\">\n\u00a0\u00a0\u00a0<str>\n\u00a0\u00a0\u00a0\u00a0 . have entered intoan agreement to <em>employ</em> the powerful tug Lntz to keep th>e Detroit river between\n\u00a0\u00a0\u00a0</str>\n\u00a0\u00a0</arr>\n\u00a0</lst>\n</lst>\n\nYou will notice that only the pages with hits on them appear in the highlight section.  From this point it would take a little work to parse the /arr[@name] to get the * from fulltext_* for each document match.\n\nI agree that the highlighter is a good model of what we want to do.  But the difficulty I'm finding is the upfront part where we need to store the position to page mapping in a field while at the same time we need to analyze the full page text into another field for searching.  \n\nI don't think defining a FieldType will allow us to do this.  The FieldType looks like it is useful in controlling what the output of your defined field is (write()), and how it is sorted, but not how Fields with your FieldType will be indexed or queried.\n\nWould someone more familiar with the innards of Solr recommend I pursue the SOLR-247 problem, or continue hunting for a solution in the manner that I've been pursuing in this issue? "
        },
        {
            "author": "Peter Binkley",
            "id": "comment-12535755",
            "date": "2007-10-17T21:40:09+0000",
            "content": "Thanks for clarifying how the highlighting would let you see the page numbers. On that model, all we would need would be to enhance the highlighting report to make it show the term positions rather than (or as well a) the terms. \n\nBut I'm not ready to give up on the map idea yet. I hadn't dug far enough into FieldTypes, evidently. Could we maybe index the text in the normal way, with a token filter that ignores the milestones, and then copyfield the text to a FieldType whose only job is to build and store the map? Provided that the two were tokenizing and filtering in the same way, the position counts would remain in sync; the mapping FieldType would just require a final filter that counted the incoming tokens and took note of the milestones, and generated the map as a series of tokens in whatever format we decide to store the map in.\n\n(And Tricia, would you mind tinyfying that url, so the page doesn't get stretched?) "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12535768",
            "date": "2007-10-17T22:24:05+0000",
            "content": "In my opinion the best solution is to create one solr document per page and denormalize the container data across each page.\n\nIf I had to implement it the other way, I would probably index the pages as a multivalued field with a large position increment gap (say 1000), store term vectors, and use the position information from the term vectors to determine the page hits (e.g., pos 4668 -> page 5; pos 668 -> page 1; pos 9999 -> page 10).  Assumes < 1000 tokens per page, of course.\n\nIncidentally, this discussion doesn't really belong here.  It would be better to sketch out ideas on solr-user, then move to JIRA to track a resulting patch (if it gets that far).  I actually don't think that there is anything to add to Solr here--it seems more of a question of customization.\n "
        },
        {
            "author": "Peter Binkley",
            "id": "comment-12535941",
            "date": "2007-10-18T15:24:37+0000",
            "content": "OK, taking the discussion to solr-user until we nail down what we're doing. "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12541699",
            "date": "2007-11-12T07:43:42+0000",
            "content": "This is a draft.  Note that Payload and Token classes in particular have changed since lucene-core-2.2.0.jar.  Users of this patch will need to replace lucene-core-2.2.0.jar with lucene-core-2.3-dev.jar.  I have created a test for XmlPayloadCharTokenizer but not attached it here because LuceneTestCase is not in SOLR's classpath in any form and it will break the build.\n\n The code works in theory and passes tests to that effect.  However, in practice when I deploy the war created from the \"dist\" ant target several problems result from adding documents (which seems to work using a <![CDATA[...]]> to contain the structured document and post.jar):\n\n\n\tafter adding a XmlPayload tokenized document, q=: causes 500 error: HTTP Status 500 - read past EOF java.io.IOException: read past EOF at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:146) at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:38) at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:76) at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:153) at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:408) at org.apache.lucene.index.MultiSegmentReader.document(MultiSegmentReader.java:129) at org.apache.lucene.index.IndexReader.document(IndexReader.java:436) at ...\n\tuse of the highlight fields produces the same error.\n\tqueries that should match a XmlPayload tokenized document do not ( //result[@numFound='0'])-- though queries matching un-XmlPayload tokenized document continue to return the expected results.\n\ttrying to view the index using Luke (Lucene Index Toolbox, v 0.7.1 (2007-06-20) ) returns: Unknown format version: -4\n\tSolr Statistics confirm that all the documents have been added.\n\n\n\n\nI will continue to finish this functionality but any suggestions or other input are welcomed.  You will see how the functionality is intended to be used in src/test/org/apache/solr/highlight/XmlPayloadTest.java "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12542943",
            "date": "2007-11-16T03:38:59+0000",
            "content": "Functionality is improved.  Tests are more complete.  I have included an example (much like the example included in solr) which demonstrates the changes needed to solrconfig.xml and schema.xml.  As well as some xml documents to start playing with. \n\nTODO: \n\n\tStill have to track down what happens when filters are applied to the Tokenizer.\n\tImplement error handling for bad xml input.\n\n "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12591873",
            "date": "2008-04-23T23:43:49+0000",
            "content": "After a lengthy absence I've returned to this issue with a bit of a new perspective.  I recognize what we have described really is a customization of Solr (albeit one I have seen in at least two organizations) and as such should be built as a plug-in (http://wiki.apache.org/solr/SolrPlugins) which can reside in your solr.home lib directory.  Now that Solr has lucene 2.3 and payloads my solution is much easier to apply than before.\n\nI'll try to explain it here and then attach the src, deployable jar, and example for your use/reuse.\n\nI assume that your structured document can be represented by xml:\n\n\n<book title=\"One, Two, Three\">\n   <page label=\"1\">one</page>\n   <page label=\"2\">two</page>\n   <page label=\"3\">three</page>\n</book>\n\n\n\nBut we don't have a tokenizer that can make sense of xml.  So I wrote a tokenizer which parallels the existing WhitespaceTokenizer called XmlPayloadWhitespaceTokenizer.  XmlPayloadWhitespaceTokenizer extends XmlPayloadCharTokenizer which does the same things as CharTokenizer in Lucene, but expects that the content is wrapped in xml tags.  The tokenizer keeps track of the xpath associated with each token and stores this as a payload.  \n\nTo use my Tokenizer in Solr I add the deployable jar I created containing XmlPayloadWhitespaceTokenizer in my solr.home lib director and add a structure text field type \"text_st\" to my schema.xml:\n\n<!-- A text field that uses the XmlPayloadWhitespaceTokenizer to store xpath info about the structured document -->\n  <fieldType name=\"text_st\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n    <analyzer type=\"index\">\n      <tokenizer class=\"solr.XmlPayloadWhitespaceTokenizerFactory\"/>\n      <!-- in this example, we will only use synonyms at query time\n      <filter class=\"solr.SynonymFilterFactory\" synonyms=\"index_synonyms.txt\" ignoreCase=\"true\" expand=\"false\"/>\n      -->\n      <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\"/>\n      <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n      <filter class=\"solr.LowerCaseFilterFactory\"/>\n      <filter class=\"solr.EnglishPorterFilterFactory\" protected=\"protwords.txt\"/>\n      <filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/>\n    </analyzer>\n    <analyzer type=\"query\">\n      <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n      <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n      <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\"/>\n      <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n      <filter class=\"solr.LowerCaseFilterFactory\"/>\n      <filter class=\"solr.EnglishPorterFilterFactory\" protected=\"protwords.txt\"/>\n      <filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/>\n    </analyzer>\n  </fieldType>\n\n\n\nI also add a field \"fulltext_st\" of type \"text_st\".\n\nWe can visualize what happens to the input text above using the Solr Admin web-app analysis.jsp modified by SOLR-522.\n\n\n\n\nterm position\n1\n2\n3\n\n\nterm text\none\ntwo\nthree\n\n\nterm type\nword\nword\nword\n\n\nsource start,end\n3,6\n7,10\n11,16\n\n\npayload\n/book[title='One, Two, Three']/page[label='1']\n/book[title='One, Two, Three']/page[label='2']\n/book[title='One, Two, Three']/page[label='3']\n\n\n\n\n\nNote that I've removed the hex representation of the payload for clarity\n\nThe other side of this problem is how to present the results in a meaningful way.  Taking FacetComponent and HighlightComponent as my muse, I created a plugable SearchComponent called PayloadComponent.  This component recognizes two parameters: \"payload\" and \"payload.fl\".  If payload=true, the component will find the terms from your query in the payload.fl field, retrieve the payload in these tokens, and re-combine this information to display the xpath of a search result in a give document and the number of times that term occurs in the given xpath.  \n\nAgain, to use my SearchComponent in Solr I add the deployable jar I created containing PayloadComponent in my solr.home lib director and add a search component \"payload\" to my solrconfig.xml:\n\n\n<searchComponent name=\"payload\" class=\"org.apache.solr.handler.component.PayloadComponent\"/>\n \n  <requestHandler name=\"/search\" class=\"org.apache.solr.handler.component.SearchHandler\">\n    <lst name=\"defaults\">\n      <str name=\"echoParams\">explicit</str>\n    </lst>\n    <arr name=\"last-components\">\n      <str>payload</str>\n    </arr>\n  </requestHandler>\n\n\n\nThen the result of http://localhost:8983/solr/search?q=came&payload=true&payload.fl=fulltext_st includes something like this:\n\n\n<lst name=\"payload\">\n <lst name=\"payload_context\">\n  <lst name=\"Book.IA.0001\">\n   <lst name=\"fulltext_st\">\n    <int name=\"/book[title='Crooked Man'][url='http://ia310931.us.archive.org//load_djvu_applet.cgi?file=0/items/crookedmanotherr00newyiala/crookedmanotherr00newyiala.djvu'][author='unknown']/page[id='3']\">1</int>\n   </lst>\n  </lst>\n  <lst name=\"Book.IA.37729\">\n   <lst name=\"fulltext_st\">\n    <int name=\"/book[title='Charles Dicken's A Christmas Carol'][url=''][author='Dickens, Charles']/stave[title='Marley's Ghost'][id='One']/page[id='13']\">1</int>\n   </lst>\n  </lst>\n  <lst name=\"Book.IA.0002\">\n   <lst name=\"fulltext_st\">\n    <int name=\"/book[title='Jack and Jill and Old Dame Gill']/page[id='2']\">1</int>\n    <int name=\"/book[title='Jack and Jill and Old Dame Gill']/page[id='4']\">1</int>\n    <int name=\"/book[title='Jack and Jill and Old Dame Gill']/page[id='6']\">1</int>\n    <int name=\"/book[title='Jack and Jill and Old Dame Gill']/page[id='7']\">1</int>\n    <int name=\"/book[title='Jack and Jill and Old Dame Gill']/page[id='13']\">1</int>\n   </lst>\n  </lst>\n </lst>\n</lst>\n\n  \n\nThe documents here are borrowed from the Internet Archive and can be found in the xmlpayload-example.zip attached to this issue\n\nThen you have everything you need to write an xsl which will take your normal Solr results and supplement them with context from your structured document.\n\nThere may be some issues with filters that aren't payload aware.  The only one that concerned me to this point is the WordDelimiterFilter.  You can find a quick and easy patch at SOLR-532.\n\nThe other thing that you might run into if you use curl or post.jar is that the XmlUpdateRequestHandler is a bit anal about well formed xml, and throws an exception if it finds anything but the expected <doc> and <field> tags.  To work around either escape your structured document's xml like this:\n\n<add>\n <doc>\n  <field name=\"id\">0001</field>\n  <field name=\"title\">One, Two, Three</field>\n  <field name=\"fulltext_st\">\n   &lt;book title=\"One, Two, Three\"&gt;\n    &lt;page label=\"1\"&gt;one&lt;/page&gt;\n    &lt;page label=\"2\"&gt;two&lt;/page&gt;\n    &lt;page label=\"3\"&gt;three&lt;/page&gt;\n   &lt;/book&gt;\n  </field>\n </doc>\n</add>\n\n\nor hack XmlUpdateRequestHandler to accept your \"unexpected XML tag doc/\".\n\nCool? "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12591879",
            "date": "2008-04-24T00:09:05+0000",
            "content": "xmlpayload-src.jar contains the source files and junit test and ant build file for these plugins.\n\njar xf xmlpayload-src.jar\n\n\nwill unpack this.\n "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12591880",
            "date": "2008-04-24T00:09:55+0000",
            "content": "xmlpayload.jar is the deployable jar that can be dropped into your solr.home lib directory (it contains only .class files) "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12591883",
            "date": "2008-04-24T00:18:39+0000",
            "content": "xmlpayload-example.zip contains a specialized version of the Solr example to demonstrate the plugins. "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12591892",
            "date": "2008-04-24T00:53:07+0000",
            "content": "SOLR-532 deals with the WordDelimiterFilter that is not payload aware.\nSOLR-522 improves analysis.jsp to visualize payload savvy tokenizers and tokenfilters. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12591897",
            "date": "2008-04-24T01:40:50+0000",
            "content": "Cool?\n\nVery!   Wow Tricia - thanks for documenting that so thoroughly.  This particular feature is sure to be of great interest to many. "
        },
        {
            "author": "Laurent Hoss",
            "id": "comment-12664579",
            "date": "2009-01-16T17:37:16+0000",
            "content": "Hi Tricia\nLooks nice, I've been searching for such a feature for years in lucene (and solr)!\nBut before getting too excited, i better try to ask the correct questions before doing a real test .. as we don't even use solr yet (though I really want to  \n\nIn fact we currently have our home grown solution for similar problem:\nIn our case we want to restrain boolean searches to paragraphs or sentences of a document, and implemented this (like many others) indexing extra docs for paragraphs etc. (with duplication of many meta-data fields of the parent document)\nBesides multiplying index size, the mapping from the found paragraphs to their base documents involved a lot of custom coding.. and only recently we have at least implemented a fast counting of the base docs for the found paragraph docs, by using a 'baseDocId'-FieldCache  (essentialy a 'group by' In SQL lingo)\n\nThis leads to following requirements and questions:\n\n\tWhat is the performance of your PayloadComponent, compared to the standard SearchHandler?\nWe especially need very fast count functionality, to dynamically compute statistics/charts needing 100's of queries.\nFor this we just need the hitsCount of documents/paragraphs without the xpath payload info, which would generate a really big XML response for 100K docs resultset!\n\n\n\n\n\tWe want to find only documents where a (boolean) query matches within one of the paragraph_* fields, and not if the query matches over the combined content of multiple paragraphs, as discussed here:\nhttp://www.nabble.com/Redundant-indexing-*-4-only-solution-(for-par-sen-and-case-sensitivity)-td13684315.html#a13685041\nand\nhttp://www.nabble.com/What-is-the-best-way-to-index-xml-data-preserving-the-mark-up--td13641104.html#a13657470\n> The problem is that a search for sentence:foo AND sentence:bar is matching if foo matches in any sentence of the paragraph, and bar also matches in any sentence of the paragraph. \n\n\n\n\nDo you think this is a good option for us?\nps: We should probably put up some Wiki page for this topic, after I've seen at least 10 people asking for the possible solutions.. ok, maybe often with slightly different requirements!\n\nOne whole other way solving this would be using the SpanQuery package together with the nicelooking Qsol (http://myhardshadow.com/about.php), allthough I'm not sure about its performance especially with (really) long boolean queries! "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-12665195",
            "date": "2009-01-19T19:13:13+0000",
            "content": "Hi Laurent,\n\n    Thanks for your interest in my Solr PayloadComponent plugin.  I want to address all of the questions you pose in your comment, but won't have time until early February.  I apologize for the inconvenience but my priorities lay elsewhere right now.  Feel free to look at the code and play in the meantime.  The code that's up there is basically proof of concept.  I've been slowly working at improving the robustness of the code and improving performance so hopefully there will be a improved version before the end of March.\n\n    I'm sure there would be many people who would appreciate a Wiki page for this topic.  Why don't you go ahead and set that up?  I'll be happy to add my two cents when I'm available.\n\nAll the best,\nTricia "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12670773",
            "date": "2009-02-05T15:10:10+0000",
            "content": "Marking for 1.5 "
        },
        {
            "author": "Shairon Toledo",
            "id": "comment-12795386",
            "date": "2009-12-30T18:49:26+0000",
            "content": "I have a project that involves words extracted by OCR, each page has words, each word has its geometry to blink a highlight to end user. \nI've been trying represent this document structure by xml\n\n\n\n<document>\n   <page num=\"1\">\n    <term top='111' bottom='222' right='333' left='444'>foo</term> \n    <term top='211' bottom='322' right='833' left='944'>bar</term> \n    <term top='311' bottom='422' right='733' left='144'>baz</term> \n    <term top='411' bottom='522' right='633' left='244'>qux</term> \n   </page>\n   <page num=\"2\">\n\t<term .... />\n   </page>\n   \n</document>\n\n\n\n\nUsing the field 'fulltext_st' ,\n\n\n<field name=\"fulltext_st\">\n\t&lt;document &gt;\n\t&lt;page top='111' bottom='222' right='333' left='444' word='foo' num='1'&gt;foo&lt;/page&gt;\n\t&lt;page top='211' bottom='322' right='833' left='944' word='bar' num='1'&gt;bar&lt;/page&gt;\n\t&lt;page top='311' bottom='422' right='733' left='144' word='baz' num='1'&gt;baz&lt;/page&gt;\n\t&lt;page top='411' bottom='522' right='633' left='244' word='qux' num='1'&gt;qux&lt;/page&gt;\n\t&lt;/document&gt;\n</field>\n\n\n\nI can get all terms in my search result with them payloads.\nBut if I do search using phrase query I can't fetch any result.\n\nExample:\n\nsearch?q=foo \n\n\n<lst name=\"fulltext_st\">\n\t<int name=\"/document/page[word='foo'][num='1'][top='111'][bottom='222'][right='333'][left='444']\">1</int>\n</lst>\n\n\n\nsearch?q=foo+bar\n\n\n<lst name=\"fulltext_st\">\n\t<int name=\"/document/page[word='foo'][num='1'][top='111'][bottom='222'][right='333'][left='444']\">1</int>\n\t<int name=\"/document/page[word='baz'][num='1'][top='211'][bottom='322'][right='833'][left='944']\">1</int>\n</lst>\n\n\n\n/search?q=\"foo bar\"\n\n*nothing*\n\n\n\nI was wondering if I could get your thoughts if xmlpayload supports sort of the things or how easy is I update the code to provide a solution for do that.  \n\nthank you in advance "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12795943",
            "date": "2010-01-03T03:36:09+0000",
            "content": "Please ask this on solr-user.  Issues are for discussing implementations.\n\nLucene payloads are supported by Solr, and a rectangle per term can be stored as a payload. This allows the text to be indexed as a text field, and all queries including phrases will work as normal. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12795944",
            "date": "2010-01-03T03:37:30+0000",
            "content": "Please ask this on solr-user.  Issues are for discussing implementations.\n\nLucene payloads are supported by Solr, and a rectangle per term can be stored as a payload. This allows the text to be indexed as a text field, and all queries including phrases will work as normal. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12837035",
            "date": "2010-02-23T01:42:29+0000",
            "content": "This is an interesting patch. One current limitation seems to be that proximity search queries (PhraseQueries and SpanQueries) may result in false positives. For example, if I query\n\n\"audit trail\"~10\n\nthen I think I'd expect Solr to return only the page #s where audit and trail are near one another. (Yes, what I just said leaves some wiggle room for implementation details.) The current code, in contrast, looks like it will report all the pages where \"audit\" and \"trail\" occur, regardless of proximity to the other term.\n\nHas anyone thought about how to add proximity awareness? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872591",
            "date": "2010-05-27T22:08:25+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043644",
            "date": "2011-06-03T16:46:16+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106475",
            "date": "2011-09-16T14:51:15+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234695",
            "date": "2012-03-21T18:08:54+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717433",
            "date": "2013-07-23T18:48:14+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971031",
            "date": "2014-04-16T12:56:56+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-14221595",
            "date": "2014-11-21T23:40:10+0000",
            "content": "I think this one is probably dead/safe-to-close. Some of the functionality seems to be in the folding/parent-child code in the latest Solr.\n\nAnd if so, we can probably delete the Wiki page about Plugins that contains only this plugin so far: https://wiki.apache.org/solr/SolrPluginRepository (and the link to that from https://wiki.apache.org/solr/FrontPage under Tips) "
        },
        {
            "author": "Tricia Jenkins",
            "id": "comment-14221606",
            "date": "2014-11-21T23:47:40+0000",
            "content": "No problem here.\nOn 21 Nov 2014 16:40, \"Alexandre Rafalovitch (JIRA)\" <jira@apache.org>\n "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15394977",
            "date": "2016-07-27T02:46:54+0000",
            "content": "Let's close this. And the associated WIKI page. The original reporter confirmed it is ok to do so. "
        }
    ]
}