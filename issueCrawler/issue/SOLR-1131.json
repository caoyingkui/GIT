{
    "id": "SOLR-1131",
    "title": "Allow a single field type to index multiple fields",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.5",
            "3.1",
            "4.0-ALPHA"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "In a few special cases, it makes sense for a single \"field\" (the concept) to be indexed as a set of Fields (lucene Field).  Consider SOLR-773.  The concept \"point\" may be best indexed in a variety of ways:\n\n\tgeohash (sincle lucene field)\n\tlat field, lon field (two double fields)\n\tcartesian tiers (a series of fields with tokens to say if it exists within that region)",
    "attachments": {
        "SOLR-1131.patch": "https://issues.apache.org/jira/secure/attachment/12425699/SOLR-1131.patch",
        "diff.patch": "https://issues.apache.org/jira/secure/attachment/12428491/diff.patch",
        "SOLR-1131.Mattmann.121009.patch.txt": "https://issues.apache.org/jira/secure/attachment/12427657/SOLR-1131.Mattmann.121009.patch.txt",
        "SOLR-1131.Mattmann.121109.patch.txt": "https://issues.apache.org/jira/secure/attachment/12427758/SOLR-1131.Mattmann.121109.patch.txt",
        "SOLR-1131-IndexMultipleFields.patch": "https://issues.apache.org/jira/secure/attachment/12406593/SOLR-1131-IndexMultipleFields.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12703474",
            "date": "2009-04-28T01:29:47+0000",
            "content": "This is a sketch to see how things look if we have SchemaField/FieldType return Field[] rather then Field:\n\n\n+  /**\n+   * @deprecated use {@link #createFields(String, float)}\n+   */\n   public Field createField(String val, float boost) {\n     return type.createField(this,val,boost);\n   }\n+  \n+  public Field[] createFields(String val, float boost) {\n+    return type.createFields(this,val,boost);\n+  }\n\n\n\nI think this could work \u2013 this would let FieldType#createFields() return a list of fields.  The issues i see are:\n\n\tindexing each field adds a for loop (maybe not a big deal?)\n\tFieldType#toInternal() may or may not relate to the actual indexed value\n\tFieldType#getAnalyzer() \u2013 I guess the same analyzer would have to apply to every field?  I'm not sure what the implication is on this.\n\twhat about #getRangeQuery()\n\n\n\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12703545",
            "date": "2009-04-28T09:08:46+0000",
            "content": "Perhaps this is a case where the TeeTokenFilter and friends can come into play in Solr finally? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12703661",
            "date": "2009-04-28T14:45:08+0000",
            "content": "TeeTokenFilter look interesting.  How do you imagine it could help with letting a solr Field index multiple fields?\n\nUsing the TeeTokenFilter seems like a matter of plumbing.  Perhaps we could add some properties to FieldType that create a SinkTokenizer and then use copyField (or something similar) to use the existing SinkTokenizer.\n\nI'm not familiar enought with the Sink/Tee stuff to know what we would need \u2013 but we should make sure any SchemaField/FieldType changes open the door to this. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12706438",
            "date": "2009-05-06T14:05:23+0000",
            "content": "FieldType#getAnalyzer() - I guess the same analyzer would have to apply to every field?\n\nFields in Lucene can now be pre-analyzed (which this was available when Solr was first being developed!)\nSee Field constructors that take a TokenStream.. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12780896",
            "date": "2009-11-21T02:52:04+0000",
            "content": "Brings it up to trunk.  Still needs test cases.  All other tests pass. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12780924",
            "date": "2009-11-21T06:54:31+0000",
            "content": "Is this a good idea? This creates an extra Object (Field[]) for every Field . For a corner case we are introducing an overhead to all the field types.\n\nWhy don't we add a new interface MutlValuedFieldType which extends FieldType for this  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12780950",
            "date": "2009-11-21T11:17:10+0000",
            "content": "Is this a good idea?\n\nNot sure yet. \n\nWhy don't we add a new interface MutlValuedFieldType which extends FieldType for this \n\nAren't we just substituting a very simple construction for an instanceof check?\n\nI was possibly thinking of a couple of other options, too:\n1. add a boolean on FT for isMultiField which returns false by default, then we could check that\n2. Add a threadlocal that stores a preconstructed array of size one which could then simply be set for the single field case, which is the most common case.\n\nMy gut, however, says the object is very short lived and is likely to be of negligible cost. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12780953",
            "date": "2009-11-21T11:51:45+0000",
            "content": "dd a boolean on FT for isMultiField which returns false by default, then we could check that\n\nnot bad\n\nMy gut, however, says the object is very short lived and is likely to be of negligible cost.\n\nbut, for a huge ingestion it just means several million objects created and that much extra GC "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12780954",
            "date": "2009-11-21T11:56:54+0000",
            "content": "I'm also looking for ideas on how to handle the naming of the fields that are produced by this.  I think a FieldType that produces multiple fields should hide the logistics of the naming, which this patch doesn't even begin to scratch the surface of and also on the search side, how does one search against just one of the fields?\n\nWould appreciate thoughts on that. "
        },
        {
            "author": "Chris Male",
            "id": "comment-12780955",
            "date": "2009-11-21T12:15:48+0000",
            "content": "My initial feeling is, is searching against just one field something this functionality needs to concern itself with? If someone creates a field of type Point for example, which behind the scenes is indexed as 2 fields, from a Solr schema.xml perspective it is just 1 field, and so it should be the same at the querying level.  We are trying to encapsulate the fact that the FieldType results in multiple fields.  This then frees us up to choose a naming convention that is easy for us to implement, because we don't have to concern users with the convention.\n\nIf someone does want to be able to search against just one field, such as maybe being able to find documents at a certain x coordinate, rather than an x,y Point, then I think we can simply recommend they index that data in a separate field. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12780956",
            "date": "2009-11-21T12:25:01+0000",
            "content": "I definitely agree, Chris, the interesting part is how that manifests itself in terms of implementation, which is where I am digging in at the moment.  It means the Query parsers need to handle it as well as the ResponseWriters, etc. "
        },
        {
            "author": "Chris Male",
            "id": "comment-12780961",
            "date": "2009-11-21T12:32:57+0000",
            "content": "Those are definitely big problems.  \n\nThe ResponseWriter problem could be simplified if they used SolrDocuments rather than retrieving raw Lucene Documents.  When constructing the SolrDocuments, which is done in cooperation with an IndexSchema instance, we have the information needed to bring the multiple fields together as one.  I'm not sure of the performance impact of doing this, but it seems like having the ResponseWriters retrieve the data in a single consistent fashion is a good thing in the long run anyway. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12781123",
            "date": "2009-11-22T13:39:29+0000",
            "content": "Starting to add unit tests.  Still no support on the search/response side, but groundwork for adding multiple fields per SchemaField/FieldType is now laid.  Still need a way to know that a field/fieldtype is going to output multiple fields so that we can detect them when searching, etc. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12781948",
            "date": "2009-11-24T14:25:29+0000",
            "content": "See discussion at http://search.lucidimagination.com/search/document/d24c920ddf05b4f7/solr_1131_multiple_fields_per_field_type "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12783333",
            "date": "2009-11-29T00:00:39+0000",
            "content": "Here's a completely untested prototype patch along the lines of how I was thinking this would work with geo. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12783345",
            "date": "2009-11-29T01:30:53+0000",
            "content": "Second try - forgot to \"svn add\" the new files. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12786586",
            "date": "2009-12-06T09:27:14+0000",
            "content": "Hey Yonik,\n\nOne of the things I was debating was whether it was worthwhile to keep the single field creation or not.  I see in your patch you drop it.  I've got a patch that keeps it.  I will try to put it up this week. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12786894",
            "date": "2009-12-07T12:54:14+0000",
            "content": "One of the things I was debating was whether it was worthwhile to keep the single field creation or not. I see in your patch you drop it.\n\nNo, I kept it.  And I borrowed the isPolyField() from your email thread. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12786902",
            "date": "2009-12-07T13:08:38+0000",
            "content": "I've got a patch with search and all the existing tests working.  Still trying to work on one test that is failing due to toInternal/external conflicts.  In the patch, I hide all the details of the internal fields, thus not requiring the dynamic field stuff. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12786944",
            "date": "2009-12-07T15:27:14+0000",
            "content": "OK, here's my take on this.  I took Yonik's and merged it w/ a patch I had in the works.  It's not done, but all tests pass, including the new on I added (PolyFieldTest).  Yonik's move to put getFieldQuery in FieldType was just the key to answering the question of how to generate queries given a FieldType.\n\nNotes:\n1. I changed the Geo examples to be CoordinateFieldType (representing an abstract coordinate system) and then PointFieldType which represents a point in an n-dimensional space (default 2D).  I think from this, we could easily add things like PolygonFieldType, etc. which would allow us to create more sophisticated shapes and do things like intersections, etc.  For instance, imagine saying:  Does this point lie within this shape?  I think that might be able to be expressed as a RangeQuery\n2. I'm not sure I care for the name of the new abstract FieldType that is a base class of CoordinateFieldType called DelegatingFieldType\n3. I'm not sure yet on the properties of the generated fields just yet.  Right now, I'm delegating the handling to the sub FieldType except I'm overriding to turn off storage, which I think is pretty cool (could even work as a copy field like functionality)\n4. I'm not thrilled about creating a SchemaField every time in the createFields protected helper method, but SchemaField is final and doesn't have a setName method (which makes sense)\n\nQuestions for Yonik on his patch:\n1. Why is TextField overriding getFieldQuery when it isn't called, except possibly via the FieldQParserPlugin?\n2. I'm not sure I understand the getDistance, getBoundingBox methods on the GeoFieldType.   It seems like that precludes one from picking a specific distance (for instance, some times you may want a faster approx. and others a slower more accurate calculation)\n\n\nNeeds:\n1. Write up changes.txt\n2. More tests, including performance testing\n3. Patch doesn't support dynamic fields yet, but it should "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12786951",
            "date": "2009-12-07T15:35:43+0000",
            "content": "Patch is looking good! I'm pouring through it right now \u2013 I'll try and test this as part of work I'm doing on SOLR-1586 \u2013 maybe even update that issue if I get a sec today  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788103",
            "date": "2009-12-09T14:19:16+0000",
            "content": "1. Why is TextField overriding getFieldQuery when it isn't called, except possibly via the FieldQParserPlugin?\n\nAs you point out, it is called by FieldQParserPlugin, and it's a move to make things a bit more orthogonal - with a little more work it could even be used by the SolrQueryParser for text field types as well.  It also opened the (expert) possibility of creating a new TextField type that handled things a bit differently.\n\n2. I'm not sure I understand the getDistance, getBoundingBox methods on the GeoFieldType. It seems like that precludes one from picking a specific distance (for instance, some times you may want a faster approx. and others a slower more accurate calculation)\n\nThis decision will often be made for the user by the choice of field-types.  End users and app clients should be able to specify something like a bounding box filter and get the most performant implementation w/o having to know if it resolves to range queries, cartesian grids, or whatever.\n\nfq=\n{!gbox point=110,220 r=1.5}\n   #specify a point and a radius\n\nThis does not necessarily preclude users from calling exact functions if they know they are supported for that field type. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788113",
            "date": "2009-12-09T15:03:03+0000",
            "content": "Some minor nits about tests in general that I've also noticed in the past:\nIMO, unit tests can be too low level.  They can also be too fragile.\n\nThe test below is a pain to maintain... it essentially means you can't change the schema at all w/o breaking the test.\n+    Map<String,FieldType> polyFs = schema.getPolyFieldTypes();\n+    assertNotNull(polyFs);\n+    assertTrue(polyFs.size() == 3);\n\nI also prefer testing low level behavior as opposed to testing low level implementation.\nIt would be nice, for example, if testPointFieldType indexed a few couments (with various combinations of stored / indexed)  and then queried the index, with our high level xpath validation code to test that the field was correctly matched and had stored, or had not stored the value.\n\nRendundant null checks, trivial strings, etc:\n+    assertNotNull(\"topDocs is null\", topDocs);\n+    assertTrue(topDocs.totalHits + \" does not equal: \" + 1, topDocs.totalHits == 1)\n\nThings like the above can be replaced with the much more concise and readable:\nassertEquals(1, topDocs.totalHits)\n\nBut really, stuff like this:\n+ TopDocs topDocs = core.getSearcher().get().search(bq, 1);\n\nShould normally use the higher level search and xpath validate  functionallity.  The code above actually leads to a refcount leak.\n\nschema.xml: geo will be core... let's not add a new/different schema file in tests for this and simply add it to the latest schema12 "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12788116",
            "date": "2009-12-09T15:08:22+0000",
            "content": "Hi Yonik:\n\nI agree in general with your points above regarding unit tests. However, there seems to be a contradiction in your last statement to what you proposed above:\n\nschema.xml: geo will be core... let's not add a new/different schema file in tests for this and simply add it to the latest schema12\n\nWhy have unit tests point at the actual schema? That explicitly ties your unit tests to the shipped ops schema, and then encourages people to write unit tests against it (which could lead to the specific number checks that will break when the schema is updated as you mentioned). Instead +1 for having a separate test schema even if it causes duplication it insulates change.\n\nCheers,\nChris "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788117",
            "date": "2009-12-09T15:12:26+0000",
            "content": "Please see the DocumentBuilder changes I had added... there was starting to be too much duplicated code and I pulled it out into a utility method.\n\nSeems like SolrQueryParser should use getFieldQuery for everything (except TextField... but it could even be used for that if we make it such that we could call back to getBooleanQuery, etc).  I had this in my patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788119",
            "date": "2009-12-09T15:21:47+0000",
            "content": "Regarding polyfields... it's not clear why they are special enough to have to change the IndexSchema? (IndexSchema.isPolyField, getPolyField, getPolyFieldType, getPolyFieldTypeNoEx, etc).  Can't we just store them as normal field types? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788121",
            "date": "2009-12-09T15:29:10+0000",
            "content": "IMO, unit tests can be too low level. They can also be too fragile. \n\nI guess it all comes down to what you call a unit.  \n\nt would be nice, for example, if testPointFieldType indexed a few couments (with various combinations of stored / indexed) and then queried the index,\n\nThis is done in testIndexing()\n\nTopDocs topDocs = core.getSearcher().get().search(bq, 1);\n\nYeah, see my comment there even!  I wanted a way to validate that the correct query is created, but I don't even really need to run a search for that.\n\n\nRendundant null checks, trivial strings, etc:\n+ assertNotNull(\"topDocs is null\", topDocs);\n+ assertTrue(topDocs.totalHits + \" does not equal: \" + 1, topDocs.totalHits == 1)\n\nI need to update my IntelliJ \"Live Templates\", as I have them setup to spit out a pattern like above\n\nPlease see the DocumentBuilder changes I had added... \n\nWill do.\n\n\nSeems like SolrQueryParser should use getFieldQuery for everything (except TextField... but it could even be used for that if we make it such that we could call back to getBooleanQuery, etc). I had this in my patch.\n\nI thought I captured that, but will look again. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788123",
            "date": "2009-12-09T15:36:49+0000",
            "content": "\nRegarding polyfields... it's not clear why they are special enough to have to change the IndexSchema? (IndexSchema.isPolyField, getPolyField, getPolyFieldType, getPolyFieldTypeNoEx, etc). Can't we just store them as normal field types?\n\nMy thinking was that a Query Parser or other things might need to know look up this information, but you are right, I don't have a specific use case for them at the moment.  At the same time, poly fields feel like a hybrid between regular fields and dynamic fields and thus fit at the same level they do. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788125",
            "date": "2009-12-09T15:38:33+0000",
            "content": "I need to update my IntelliJ \"Live Templates\", as I have them setup to spit out a pattern like above\n\nlol... so that's where all that comes from...  I was going to say something like \"this looks like it came out of a code generator\" but it sounded a bit too harsh in the off chance that it wasn't   I'm very relieved to find you weren't typing out that crap by hand. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788138",
            "date": "2009-12-09T16:08:44+0000",
            "content": "poly fields feel like a hybrid between regular fields and dynamic fields and thus fit at the same level they do.\n\nThe schema needs to know about dynamic fields because it affects field names.\n\nActually, I think I just saw why you currently need some support in IndexSchema with the current way you are doing things:\nIf you have a dynamicField pt, then I think you use field names like home_pt0 and home_pt_1?\nSo in essence, it's a new type of dynamic field?  This seems like it might be hard to actually get right in all of the corner cases.\n\nWhat if, instead, dynamic fields are directly used for subfields?\nSo for a field name \"home\" instead of home_0 and home1, you would use  home_0_d, home__1_d\nNot as short, but it avoids having to add new capabilities to the IndexSchema.\n\nAnother alternative: use a prefix for subfields and the suffix for the type. _0_home_d,  _1_home_d\n\nAnother thing to keep in mind - not all subfields will always be of the same type. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788151",
            "date": "2009-12-09T16:35:58+0000",
            "content": "What if, instead, dynamic fields are directly used for subfields?\n\nThat then requires those dynamic fields to be present, which I'd rather not have to do.  Part of the goal of this issue is to hide the implementation.  Having said that, I still don't know whether that means I need to keep the IndexSchema changes.  Let me do another iteration.\n\nAnother thing to keep in mind - not all subfields will always be of the same type.\n\nAgreed, but I don't think this is baked in to the generic capabilities, just the Point stuff, where I think it is fine to have the same sub-type. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788156",
            "date": "2009-12-09T16:48:54+0000",
            "content": "That then requires those dynamic fields to be present, which I'd rather not have to do. \n\nThat's sort of a separate question: if one were allowed to register a dynamic field (not sure if this capability is present), then it could be registered if it didn't exist.   \n\nAlso, you have subFieldType=\"double\" in the schema... and that requires that the \"double\" field type be defined.  Why not have subFieldSuffix=\"_d\" and require the \"_d\" dynamic field be defined?  Seems like the same complexity level.\n\n> > Another thing to keep in mind - not all subfields will always be of the same type.\n> Agreed, but I don't think this is baked in to the generic capabilities, just the Point stuff,\n\nFor a specific point implementation, that's fine.  But if you use a point type that can do cartesian grid stuff, then you already have different field types.  But I guess subFieldType=\"double\" need only apply to some of the subfields (the ones that index the points). "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788163",
            "date": "2009-12-09T17:06:56+0000",
            "content": "\nAlso, you have subFieldType=\"double\" in the schema... and that requires that the \"double\" field type be defined. Why not have subFieldSuffix=\"_d\" and require the \"_d\" dynamic field be defined? Seems like the same complexity level\n\nI think it makes more sense for the subFieldType to be present to be tied to a type than a Field (subFieldSuffix), as it seems weird to have a field type have a dependency on a Field, whereas it seems fine for a field type to have a dependency on another field type.\n\n\nFor a specific point implementation, that's fine. But if you use a point type that can do cartesian grid stuff, then you already have different field types. But I guess subFieldType=\"double\" need only apply to some of the subfields (the ones that index the points).\nI'm not sure I see this.  If and when we implement CartesianPointType, it will still need to have a type for the sub fields (depending on the tiers specified) but I don't see why the subFieldType wouldn't be the same for all of them.  AIUI, they all have the same precision requirements.\n\nI think part of what's missing is that for some of these attributes, it would be better for them to be field properties and not fieldType properties.  For instance for the Cartesian case, you will need to declare what levels to support.  If that is specified on the FieldType, then you have a proliferation of Field Type declarations, whereas if it is on the Field, that is a lot cleaner and less verbose.  I'm just not sure how that gets implemented just yet, as having to specify startTier and endTier doesn't seem like the same level as multiValued or stored.   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788164",
            "date": "2009-12-09T17:10:09+0000",
            "content": "Perhaps we should return to the design level a bit instead of me reading code and maybe making mistakes and trying to infer intent.\n\nAssume we have this:\n  <fieldType name=\"xy\" class=\"solr.PointType\" dimension=\"2\" subFieldType=\"double\"/>\n  <field name=\"home\" type=\"xy\" indexed=\"true\" stored=\"true\"/>\n\nWhat are the exact field names that are indexed? "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12788168",
            "date": "2009-12-09T17:15:52+0000",
            "content": "\nPerhaps we should return to the design level a bit instead of me reading code and maybe making mistakes and trying to infer intent.\n\nAssume we have this:\n<fieldType name=\"xy\" class=\"solr.PointType\" dimension=\"2\" subFieldType=\"double\"/>\n<field name=\"home\" type=\"xy\" indexed=\"true\" stored=\"true\"/>\n\nWhat are the exact field names that are indexed?\n\nRegarding the fieldType subFieldType attribute \u2013 a question popped into my mind. How do we handle poly fields where each type is different? I.e., where subFieldType=\"double,tint\" or whatever... "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788170",
            "date": "2009-12-09T17:17:38+0000",
            "content": "\n<fieldType name=\"xy\" class=\"solr.PointType\" dimension=\"2\" subFieldType=\"double\"/>\n<field name=\"home\" type=\"xy\" indexed=\"true\" stored=\"true\"/>\nTwo indexed fields\nhome___0\nhome___1\n\nOne stored field:\nhome "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788228",
            "date": "2009-12-09T18:47:47+0000",
            "content": "OK... so the real issue is that this introduces a new mechanism to look up field types... not necessarily a horrible thing, but we should definitely think twice before doing so.\n\nhome__0 and home__1 are not dynamic fields as I understand it (in that there is no ___0 dynamic field.  The lookup is done by adding new support to the IndexSchema to strip off ___foo off of any field and use that as it's type?\n\nBut... that scheme seems to limit us to a single subField type (in addition to the other downsides of requiring a new lookup mechanism).\n\nI do want to separate these two issues though:\n1) field lookup mechanism (currently just exact name in schema followed by a dynamic field check)\n2) if and when fields or field types should be explicitly defined in the schema vs being created by the polyField\n\nAside: it looks like the code for getFieldOrNull isn't right?  Seems like it will return a field with both the wrong type and the wrong name?\n\n   public SchemaField getFieldOrNull(String fieldName) {\n      SchemaField f = fields.get(fieldName);\n@@ -1055,25 +1071,28 @@\n     for (DynamicField df : dynamicFields) {\n       if (df.matches(fieldName)) return df.makeSchemaField(fieldName);\n     }\n-    \n+    int idx = fieldName.indexOf(FieldType.POLY_FIELD_SEPARATOR);\n+    if (idx != -1){\n+      String fn = fieldName.substring(0, idx);\n+      f = getFieldOrNull(fn);\n+    }\n     return f;\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788298",
            "date": "2009-12-09T20:37:21+0000",
            "content": "OK... so the real issue is that this introduces a new mechanism to look up field types... not necessarily a horrible thing, but we should definitely think twice before doing so. \n\nAgreed.  I'm not wedded to this approach, just want to see the discussion through.  I do feel strongly that the goal is such that an app designer should be able to use a FieldType just as they always have, either dynamic or static.  How we get to that I don't care so much as long as it works and performs.\n\nBut... that scheme seems to limit us to a single subField type (in addition to the other downsides of requiring a new lookup mechanism).\n\nI don't follow this.  In this particular implementation, I have a single subFieldType, but I don't see why a different implementation couldn't do something like:\n\n<fieldType name=\"foo\" type=\"solr.MultiSubPointType\" dimension=\"3\" subFieldTypes=\"double,tdouble,int\"/>\n\n\n\nAside: it looks like the code for getFieldOrNull isn't right? Seems like it will return a field with both the wrong type and the wrong name?\n\nHmmm, I think it should return the \"owning\" Schema Field, i.e. the one that exists in the schema.xml file. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788307",
            "date": "2009-12-09T20:50:33+0000",
            "content": "Note, I don't think the distance function queries will work w/ my patch yet. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788319",
            "date": "2009-12-09T21:17:44+0000",
            "content": ">  Aside: it looks like the code for getFieldOrNull isn't right? Seems like it will return a field with both the wrong type and the wrong name?\n> > Hmmm, I think it should return the \"owning\" Schema Field, i.e. the one that exists in the schema.xml file.\n\nThose fields probably will be exposed at least internally to other parts of solr, so they should really return the correct field / fieldType. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788751",
            "date": "2009-12-10T15:45:13+0000",
            "content": "\nSeems like SolrQueryParser should use getFieldQuery for everything (except TextField... but it could even be used for that if we make it such that we could call back to getBooleanQuery, etc). I had this in my patch.\n\nYonik, could you elaborate on this?  It seems kind of weird to have that instanceof check in SolrQueryParser.getFieldQuery() to see if we have a TextField or not. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788778",
            "date": "2009-12-10T16:34:03+0000",
            "content": "This implements Option B as laid out at: http://search.lucidimagination.com/search/document/83a5442ab155686/solr_1131_multiple_fields_per_field_type#a600de441418a798\n\nNext up:  Implement ValueSource support for PointType. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788800",
            "date": "2009-12-10T17:32:20+0000",
            "content": "Next up: Implement ValueSource support for PointType.\n\nExactly!\n\nMy thinking was perhaps to add the ability to get lat+lon from DocValues... but for efficiency have DocValues fill in values on an object passed to it (this is inner-loop stuff.... we don't want to be creating objects per doc).\n\nDocValues getPoint(Point point)\nor perhaps\nDocValues.getPoint(double[] point) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12788802",
            "date": "2009-12-10T17:37:03+0000",
            "content": "Yonik, could you elaborate on this? It seems kind of weird to have that instanceof check in SolrQueryParser.getFieldQuery() to see if we have a TextField or not.\n\nIf you look at the impl in TextField, I had to comment out stuff like \"newTermQuery\" and replace it with \"new TermQuery\".\n\n+            // Query currentQuery = newTermQuery(new Term(field, term));\n+            Query currentQuery = new TermQuery(new Term(field, term));\n\nTo be fully back compatible, all we would need to do is check if the parser was an instance of QueryParser, and if so, delegate to newTermQuery.  Then we could use fieldType.getFieldQuery() absolutely everywhere. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12788805",
            "date": "2009-12-10T17:45:14+0000",
            "content": "DocValues.getPoint(double[] point)\n\nOK, let me see how that plays out.\n\nSee also http://www.lucidimagination.com/search/document/fd804bcd78d7bec1/solr_1131_poly_fields_and_valuesource "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789003",
            "date": "2009-12-10T23:06:27+0000",
            "content": "Hi All:\n\nHere's a cut on the patch. Some questions/comments on the existing patch(es):\n\n\n\tWhy use\n\nprivate DynamicField[] dynamicFields;\n\n\n instead of \n\nList<DynamicField>\n\n\n or \n\nCollection<DynamicField>\n\n\n in IndexSchema?\n\tThere are a bunch of useless whitespace changes (e.g., in IndexSchema, FieldType) in the existing patches. The final patch probably shouldn't include those since it makes it difficult to understand what was actually changed.\n\tIndexSchema:\n\t\n\t\twhen checking for isDuplicateDynField, if it is, nothing is done. Shouldn't this be where an exception is thrown or a message is logged? In the patch I'm attaching I took the log approach.\n\t\n\t\n\tIndexSchema:\n\t\n\t\twhat happens if subs.isEmpty() == true?\n\t\tmaybe log message that says, dyn field definition is up to you?\n\t\tI took the approach in my attached patch to log it.\n\t\n\t\n\tWhy does getPolyFieldType(String) throw an exception if the field is not a poly field type \u2013 that seems a bit brittle? Also there's the NoEx version anyways (why not just keep that one?). In the patch I've attached, I took the approach of only including a getPolyFieldType that returns null rather than throwing an ex (the NoEx version).\n\tCoordinateFieldType: why process > 1 sub field types and then throw an exception at the end? I cleaned this up to throw the Exception when it occurs.\n\tparsePoint in DistanceUtils, why use ',' as the separator \u2013 use ' ' (at least conforms to georss point then). I guess because you are supporting N-dimensional points, right?\n\tparsePoint \u2013 instead of complicated isolation loops, why not just use trim()? I've taken that approach in the patch I've attached.\n\n\n\nThis patch passes all unit tests as well. This doesn't implement option C that I proposed yet. Hopefully I'll get a chance to put that up later tonight.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12789079",
            "date": "2009-12-11T02:19:31+0000",
            "content": "Quick comment based on a spot check of the changes to IndexSchema: rather than make polyField special somehow w.r.t IndexSchema, and add a FieldType.getPolyFieldNames, etc, I had been thinking more along the lines of having an IndexSchema.registerDynamicFieldDefinition - just like the existing registerDynamicCopyField.  This would (optionally) allow any field type to add other definitions to the IndexSchema.  I continue to think it would be good to stay away of special logic for \"polyfields\" in the IndexSchema. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12789082",
            "date": "2009-12-11T02:30:09+0000",
            "content": "parsePoint in DistanceUtils, why use ',' as the separator\n\nA comma is more user friendly - spaces are often already used as delimiters in quite a few places.\nWhy did you replace more optimized code that was already written in parsePoint with less optimized code? "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789106",
            "date": "2009-12-11T04:36:50+0000",
            "content": "\nA comma is more user friendly - spaces are often already used as delimiters in quite a few places.\nWhy did you replace more optimized code that was already written in parsePoint with less optimized code?\n\nMeh, I could go either way on the comma/space issue. It would be nice to be compatible with an existing GeoPoint standard. I know georss uses space as the delimeter \u2013 do you know of any that use \",\"?\n\nRE: optimized code, can you be explicit? I would argue the code I inserted is more optimized from a readiability standpoint. It's a bit easier for your typical CS101 grad to understand. All that was being done in the prior patch is a set of forwards/backwards isolation loops to determine the start/end index to substring out, in case you have:\n\n34.333      ,100.1 OR\n34.333,100.1 OR\n34.333,       100.1\n\nAt first blush, trying to understand that code was a bit harder than simply tokenizing on the known delimeter, and then trimming each tokenized value.\n\n\n  out = externalVal.split(\",\");\n+      if(out.length != dimension){\n+        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"incompatible dimension (\" + dimension +\n+            \") and values (\" + externalVal + \").  Only \" + i + \" values specified\");        \n+      }\n+      for(int j=0; j < out.length; j++){\n+        out[j] = out[j].trim();\n+      }\n\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789240",
            "date": "2009-12-11T11:13:13+0000",
            "content": "\nQuick comment based on a spot check of the changes to IndexSchema: rather than make polyField special somehow w.r.t IndexSchema, and add a FieldType.getPolyFieldNames, etc, I had been thinking more along the lines of having an IndexSchema.registerDynamicFieldDefinition - just like the existing registerDynamicCopyField. This would (optionally) allow any field type to add other definitions to the IndexSchema. I continue to think it would be good to stay away of special logic for \"polyfields\" in the IndexSchema.\n\nSo, then the FieldType would register it's Dynamic Fields in it's own init() method by calling this method?  That can work.\n\nWhy use Dynamic Field array\n\nThe array is sorted and array access is much faster and we often have to loop over it to look it up.\n\n\nCoordinateFieldType: why process > 1 sub field types and then throw an exception at the end? I cleaned this up to throw the Exception when it occurs.\n\nOK.  Actually, this should just be in the derived class, as it may be the case some other CoordinateFieldType has multiple sub types.\n\n\n\n\tparsePoint in DistanceUtils, why use ',' as the separator - use ' ' (at least conforms to georss point then). I guess because you are supporting N-dimensional points, right?\n\tparsePoint - instead of complicated isolation loops, why not just use trim()? I've taken that approach in the patch I've attached.\n\n\n\nI think comma makes sense.  As for the optimization stuff, I agree w/ Yonik, this is code that will be called a lot.\n\n\n\twhen checking for isDuplicateDynField, if it is, nothing is done. Shouldn't this be where an exception is thrown or a message is logged? In the patch I'm attaching I took the log approach.\n\n\n\n\n\nIt is logged, but for the poly fields, if the dyn field is already defined, that's just fine. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789241",
            "date": "2009-12-11T11:13:31+0000",
            "content": "I've got a patch almost ready that brings in the ValueSource stuff. "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789330",
            "date": "2009-12-11T14:47:12+0000",
            "content": "Hi Grant:\n\nThe array is sorted and array access is much faster and we often have to loop over it to look it up.\n\nOK, fair enough.\n\nI think comma makes sense. \n\nOK, so you think it makes sense \u2013 why? Because it's an N-dimensional array and spaces are less \"user friendly\" as Yonik put it? \n\nAs for the optimization stuff, I agree w/ Yonik, this is code that will be called a lot.\n\nI'm wondering what there is to agree with since \"optimization\" was never defined. Are you talking speed? Are you talking memory efficiency? Code readability? Maintainability? Some combination of all of those? There are tradeoffs in everything. You could rewrite some of the provided java runtime methods to squeeze out extra performance, but what's the point of libraries or reusable functions then? The prior code that was in there basically rewrote exactly what split() and trim() do, so why not reuse what's there? If you throw up the performance flag, I would push back on readability and maintainability. \n\nIt is logged, but for the poly fields, if the dyn field is already defined, that's just fine.\n\nWhere is it logged? It wasn't in the most up-to-date patch, provided on 2009-12-10 04:34 PM. Here was the code snipped that was there:\n\n\n+    //For each poly field, go through and add the appropriate Dynamic field\n+      for (FieldType fieldType : polyFieldTypes.values()) {\n+        if (fieldType instanceof DelegatingFieldType){\n+          List<FieldType> subs = ((DelegatingFieldType) fieldType).getSubTypes();\n+          if (subs.isEmpty() == false){\n+            //add a new dynamic field for each sub field type\n+            for (FieldType type : subs) {\n+              log.debug(\"dynamic field creation for sub type: \" + type.typeName);\n+              SchemaField df = SchemaField.create(\"*\" + FieldType.POLY_FIELD_SEPARATOR + type.typeName,\n+                      type, type.args);//TODO: is type.args right?\n+              if (isDuplicateDynField(dFields, df) == false){\n+                addDynamicFieldNoDupCheck(dFields, df);\n+              }\n               // NOTE: there is no else here, so I added an else and a log message\n+            }\n+          }\n            // NOTE: there is no else here, so I added an else and a log message\n+        }\n+      }\n+\n\n\n\nHere's what I added:\n\n\n+    //For each poly field, go through and add the appropriate Dynamic field\n+      for (FieldType fieldType : polyFieldTypes.values()) {\n+        if (fieldType instanceof DelegatingFieldType){\n+          List<FieldType> subs = ((DelegatingFieldType) fieldType).getSubTypes();\n+          if (!subs.isEmpty()){\n+            //add a new dynamic field for each sub field type\n+            for (FieldType type : subs) {\n+              log.debug(\"dynamic field creation for sub type: \" + type.typeName);\n+              SchemaField df = SchemaField.create(\"*\" + FieldType.POLY_FIELD_SEPARATOR + type.typeName,\n+                      type, type.args);//TODO: is type.args right?\n+              if (!isDuplicateDynField(dFields, df)){\n+                addDynamicFieldNoDupCheck(dFields, df);\n+              }\n+              else{\n+                log.debug(\"dynamic field creation avoided: dynamic field: [\"+df.getName()+\"] \" +\n+                \t\t\"already defined in the schema!\");\n+              }\n+              \n+            }\n+          }\n+          else{\n+            log.debug(\"field type: [\"+fieldType.getTypeName()+\"]: no sub fields defined\");\n+          }\n+        }\n+      }\n+\n\n\n\nAlso, I get that it's fine for the poly fields if the dyn field is already defined (in fact, based on my mailing lists comments http://old.nabble.com/SOLR-1131:-disconnect-between-fields-created-by-poly-fields-td26736431.html I think this should always be the case), but whether it's fine or not, it's still worthy to log to provide someone more information. \n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789339",
            "date": "2009-12-11T15:11:57+0000",
            "content": "\nI'm wondering what there is to agree with since \"optimization\" was never defined. Are you talking speed? Are you talking memory efficiency? Code readability? Maintainability? Some combination of all of those?\n\nSpeed and memory.  \n\nAs for logging, that code is all going away in the next patch, I think "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789342",
            "date": "2009-12-11T15:18:03+0000",
            "content": "Speed and memory. \n\nUnfortunately, it's at the cost of readability and maintainability. \n\nAs for logging, that code is all going away in the next patch, I think\n\nI'll take a look when you throw it up, thanks.\n\nCheers,\nChris\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789351",
            "date": "2009-12-11T15:45:59+0000",
            "content": "Unfortunately, it's at the cost of readability and maintainability. \n\nMaybe.  It took me all of 30 seconds to figure out what it was doing.  I'll put some comments on it.  While readability is important, Solr's goal is not to make a product that a CS101 grad can read, it's too build a blazing fast search server.  That call could hit millions of times when indexing points. "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789354",
            "date": "2009-12-11T15:50:44+0000",
            "content": "Maybe. It took me all of 30 seconds to figure out what it was doing. I'll put some comments on it. While readability is important, Solr's goal is not to make a product that a CS101 grad can read, it's too build a blazing fast search server. That call could hit millions of times when indexing points.\n\nSure, maybe the goal isn't for a CS101 grad to be able to easily understand the code, but SOLR's goal should include being open to ideas from the community that involve reusing standard Java library functions and not rewriting based on perception of speed and memory without empirical proof. Where's the evidence that using things like split and trim are so much more costly than rewriting those basic capabilities that they warrant not using them? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789406",
            "date": "2009-12-11T17:28:00+0000",
            "content": "OK, this is getting a lot closer to ready to commit.\n\nChanges:\n\n\tIntroduced a MultiValueSource - ValueSource that abstractly represents ValueSources for poly fields, and other things.\n\tIntroduced PointValueSource - point(x,y,z) - a MultiValueSource that wraps other value sources (could be called something else, I suppose)\n\tImplemented PointTypeValueSource to represent ValueSource for the PointType class.\n\tHooked in multivalue callbacks to DocValues.  In addition to making functions work with Points (et. al) it should be possible to write functions that work on multivalued fields, but I did not undertake this work.\n\tAdd in SchemaAware callback mechanism so that Field Types and other schema stuff can register dynamic fields, etc. after the schema has been created\n\tUpdated the example to have spatial information in the docs, etc.  See http://wiki.apache.org/solr/SpatialSearch\n\tModified the distance functions to work with MultiValueSources\n\tcleaned up the tests\n\tIncorporated various comments from Chris and Yonik.\n\n "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789484",
            "date": "2009-12-11T19:38:26+0000",
            "content": "Hi All,\n\nUpdated patch:\n\n\n\n\tIntroduced a MultiValueSource - ValueSource that abstractly represents ValueSources for poly fields, and other things.\n\n\n\nI added javadoc to this and the ASF license header.\n\n\n\n\tIntroduced PointValueSource - point(x,y,z) - a MultiValueSource that wraps other value sources (could be called something else, I suppose)\n\n\n\nI put the ASF header before the package decl, to be consistent with the other SOLR java files.\n\n\n\n\tAdd in SchemaAware callback mechanism so that Field Types and other schema stuff can register dynamic fields, etc. after the schema has been created\n\n\n\nAdded more javadoc here, and ASF license.\n\n\n\n\tIncorporated various comments from Chris and Yonik.\n\n\n\nThanks, I appreciate it. I'm still -1 on the way this patch deals with the \"optimization\" issue. I'd like to see evidence that it makes sense to not use split and trim.\n\nCheers,\nChris\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789520",
            "date": "2009-12-11T20:54:10+0000",
            "content": "I'm still -1 on the way this patch deals with the \"optimization\" issue. I'd like to see evidence that it makes sense to not use split and trim.\n\nMy tests show it to be at least 7 times faster.  But this should be obvious from static analysis, too.  First of all, String.split() uses a regex which then makes a pass through the underlying character array.  Then, trim has to go back through and analyze the char array too, not to mention the extra String creations.  The optimized version here makes one pass and deals solely at the char array level and only has to do the substring, which I think can be optimized by the JVM to be a copy on write.\n\n\n  public void testDistPerf() throws Exception {\n    String [] input = new String[1000000];\n    Random random = new Random();\n    for (int i = 0; i < input.length; i++){\n      input[i] = random.nextInt() + \", \" + random.nextInt();\n    }\n    String [] out = new String[2];\n    long time = 0;\n    long start = System.currentTimeMillis();\n    for (int j = 0; j < 50; j++) {\n      for (int i = 0; i < input.length; i++){\n        split(input[i], out, 2);\n      }\n    }\n    time = (System.currentTimeMillis() - start);\n    System.out.println(\"Time: \" + time);\n    time = 0;\n    start = System.currentTimeMillis();\n    for (int j = 0; j < 50; j++) {\n      for (int i = 0; i < input.length; i++){\n        DistanceUtils.parsePoint(out, input[i], 2);\n      }\n    }\n    time = (System.currentTimeMillis() - start);\n    System.out.println(\"Time: \" + time);\n  }\n\n  private String[] split(String externalVal, String[] out, int dimension) {\n    out = externalVal.split(\",\");\n    if (out.length != dimension) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"incompatible dimension (\" + dimension +\n              \") and values (\" + externalVal + \").  Only \" + out.length + \" values specified\");\n    }\n    for (int j = 0; j < out.length; j++) {\n      out[j] = out[j].trim();\n    }\n    return out;\n  }\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789526",
            "date": "2009-12-11T21:00:40+0000",
            "content": "I think this is ready to commit.  I'd like to do so on Monday or Tuesday of next week, so that should give plenty of time for further review "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789561",
            "date": "2009-12-11T22:02:50+0000",
            "content": "Hi Grant:\n\n\nMy tests show it to be at least 7 times faster. But this should be obvious from static analysis, too. First of all, String.split() uses a regex which then makes a pass through the underlying character array. Then, trim has to go back through and analyze the char array too, not to mention the extra String creations. The optimized version here makes one pass and deals solely at the char array level and only has to do the substring, which I think can be optimized by the JVM to be a copy on write.\n\nGot it. A couple of points:\n\n1. 7x faster is great, but could end up being noise if x = 2 ms. It matters if x is say 2 minutes, agreed. If it's on the ms end then the expense of more lines of (uncommented) code isn't worth it.\n2. This code is likely to get called heavily on the indexing side, so performance, though still an issue, is not as hugely important as say on the searching side.\n3. If you feel strongly about an optimized version of this magic splitAndTrim function, how ability a utility function and refactor then? I would guess this code could be used elsewhere, and that would help to satisfy my hunger for reusability. I'll even javadoc the function and do the refactor if you'd like.\n\nCheers,\nChris\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789802",
            "date": "2009-12-12T22:00:50+0000",
            "content": "Missing an & in DistanceUtils.parsePoint "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12789833",
            "date": "2009-12-13T01:20:02+0000",
            "content": "Hi Grant:\n\nThanks. Your latest patch omits class-level javadoc I wrote for DelegatingFieldType and for the #inform method in SchemaAware.\n\n\n+/**\n+ * An interface for {@link FieldType}s that are poly fields, as defined in <a\n+ * href=\"http://issues.apache.org/jira/browse/SOLR-1131\">SOLR-1131</a>, so that\n+ * poly fields can declare the {@link FieldType}s of their sub-fields.\n+ * \n+ * @since SOLR-1131\n+ * \n+ **/\n+public interface DelegatingFieldType {\n+\n+  /**\n+   * \n+   * Returns the {@link FieldType}s of the sub-fields for this poly-field.\n+   * \n+   * @return A {@link List} of {@link FieldType}s for the sub-fields of a poly\n+   *         field.\n+   */\n+  public List<FieldType> getSubTypes();\n+}\n\n\n\n\n+public interface SchemaAware {\n+\n+  /**\n+   * Informs the {@link IndexSchema} provided by the <code>schema</code>\n+   * parameter of an event (e.g., a new {@link FieldType} was added, etc.\n+   * \n+   * @param schema\n+   *          The {@link IndexSchema} instance that inform of the update to.\n+   * \n+   * @since SOLR-1131\n+   */\n+  public void inform(IndexSchema schema);\n+}\n\n\n\nOther than that +1. Thanks for seeing this through to a great patch.\n\nCheers,\nChris "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12789883",
            "date": "2009-12-13T12:14:08+0000",
            "content": "Added Chris's comments. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12790237",
            "date": "2009-12-14T16:58:45+0000",
            "content": "Given the concerns of some people around automatically registering a dynamic field, perhaps we should optionally allow a subFieldSuffix to be passed instead of subFieldTypes.  \n\nminor nit: why is \"subFieldTypes\" plural?  We're only specifying a single type, right? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12790330",
            "date": "2009-12-14T20:22:56+0000",
            "content": "minor nit: why is \"subFieldTypes\" plural? We're only specifying a single type, right?\n\nThat's inherited from a more general mechanism that would allow multiple field types.  So, the PointType only allows one, but other implementations may allow more.\n\nGiven the concerns of some people around automatically registering a dynamic field, perhaps we should optionally allow a subFieldSuffix to be passed instead of subFieldTypes.\n\nThat's fine by me.  Given my workload, I will try to do this and commit on Thursday of this week, unless someone wants to take it before then. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12790357",
            "date": "2009-12-14T20:45:35+0000",
            "content": "That's inherited from a more general mechanism that would allow multiple field types.\n\nBut multiple sub-field types would be used for different things, and should hence be separate parameters?  So if a field type indexed points separately and indexed a field that contains a list of cartesian tiers, I could see the params being\n\n<fieldType ..., coordinateType=\"...\"  cartesianTierType=\"...\"/>\n(or coordinateSuffix / cartesianTierSuffix) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12790635",
            "date": "2009-12-15T09:51:35+0000",
            "content": "in FieldType#createFields(SchemaField field, FieldType delegatedType, String storageVal, boost, String ... externalVals)\n\nString name = field.getName();\n      Map<String, String> props = new HashMap<String, String>();\n      //Just set these, delegate everything else to the field type\n      props.put(\"indexed\", \"true\");\n      props.put(\"stored\", \"false\");\n      //props.put(\"omitNorms\", \"true\");\n      //props.put(\"tokenized\", \"false\");\n      if (field.indexed()) {\n\n        for (int j = 0; j < externalVals.length; j++) {\n          //SchemaField is final, as is name, so we need to recreate each time\n          //put the counter before the separator, b/c dynamic fields can't be asterisks on both the front and the end of the String\n          SchemaField sf = SchemaField.create(name  + \"_\" + j + POLY_FIELD_SEPARATOR + delegatedType.typeName, delegatedType, props);\n          //QUESTION: should we allow for vectors, etc?  Not sure that it makes sense\n          results[j] = delegatedType.createField(sf, externalVals[j], boost);\n        }\n      }\n\n\n\nIt is not clear as to why can't the 'sf' instance be cached and reused? \n\n we can also  avoid creating the synthetic field name at query time in PointField#.getFieldQuery\n\n\nWhy do we have a map for flags why not use a bitset? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12790715",
            "date": "2009-12-15T12:08:58+0000",
            "content": "It is not clear as to why can't the 'sf' instance be cached and reused? \n\nBecause there is no way to change the name on a SchemaField w/o changing SchemaField to be non-final.  I don't think SchemaField should be non-final.\n\nWhy do we have a map for flags why not use a bitset?\n\nYeah, we could add a new method that takes a bitset, b/c I believe that is what is used under the hood anyway. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12790720",
            "date": "2009-12-15T12:25:21+0000",
            "content": "Because there is no way to change the name on a SchemaField w/o changing SchemaField to be non-final. I don't think SchemaField should be non-final.\n\nEven if SchemaField is final we can precreate and cache the SchemaField objects because the properties of the synthetic field is known in advance. For instance, if you have a dimension of 2 ,the PointType instance will always have 2 well known synthetic names and types that can be created well in advance and they can be reused "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12790728",
            "date": "2009-12-15T13:07:56+0000",
            "content": "Even if SchemaField is final we can precreate and cache the SchemaField objects because the properties of the synthetic field is known in advance. For instance, if you have a dimension of 2 ,the PointType instance will always have 2 well known synthetic names and types that can be created well in advance and they can be reused\n\nTrue, but you need to also be able to change the name and it needs to be able to rely on the existing createField signature, which uses these values on the SchemaField.  Earlier patches had a separate, internal createField() method that took in all the options (thus not requiring the SF at all) but they don't work for the delegation.  \n\nI'm open to ideas, though, so throw up some code. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12790744",
            "date": "2009-12-15T13:44:33+0000",
            "content": "I guess Noble was referring to something like what is done in this patch.\n\n\n\tDelegatingFieldType has a new method:\n\npublic SchemaField[] getSubFields(SchemaField mainField);\n\n\n\tPointType and PlusMinusField implement this new method. It is not the prettiest way but this is one way to do it.\n\tWith this approach, we can get the names from the subFields wherever the name is used (not implemented in this patch).\n\n\n\nThe PlusMinusField is actually a field type and not a field so we should probably rename it to PlusMinusFieldType. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12790745",
            "date": "2009-12-15T13:55:52+0000",
            "content": "OK, I see what you mean.  I don't think we should add it onto the interface, though.  I think it can just be handled by changing the signature of the createField method that takes in the delegatedFieldType. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12791235",
            "date": "2009-12-16T06:57:25+0000",
            "content": "Modified the Query creation to use the cached SchemaField names.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12791414",
            "date": "2009-12-16T15:45:56+0000",
            "content": "I'm spot-checking mutiple different patches at this point... but in general, we should strive to not expose the complexity further up the type hierarchy, and we should not limit what subclasses can do.\n\nisPolyField() returns true if more than one Fieldable can be returned from createFields()\ncreateFields() is free to return whatever the heck it likes.\nAnd from SchemaField and FieldType's perspective,that's it. Implementation details are up to subclasses and we shouldn't add assumptions in base classes.  There should be no concept of subFieldTypes or whatever baked into anything.\n\nSo, from Noble's patch: we shouldn't try caching subfields in SchemaField... and esp not via \"if (type instanceof DelegatingFieldType)\"... it really doesn't belong there.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12791505",
            "date": "2009-12-16T18:24:52+0000",
            "content": "we shouldn't try caching subfields in SchemaField\n\nI believe The SchemaField is an ideal place to cache the 'synthetic' field info. \n\nand esp not via \"if (type instanceof DelegatingFieldType)\"... it really doesn't belong there.\n\ntrue. It was a quick and dirty way to demo the idea.  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791665",
            "date": "2009-12-16T23:09:19+0000",
            "content": "I have a new patch in the works that makes creating the SchemaField lighter weight.  I agree w/ Yonik, I don't think this can be cached in general.  Also, I've done away with the Delegating Field Type. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12791789",
            "date": "2009-12-17T05:12:35+0000",
            "content": "I guess we need to revamp the API.\n\nThe FieldType should act as a factory of SchemaField. And SchemaField does not have to be a final class. Solr Should do all the operations through that SchemaField "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12791831",
            "date": "2009-12-17T07:35:38+0000",
            "content": "I have opened an issue for the same SOLR-1664 "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791964",
            "date": "2009-12-17T15:12:03+0000",
            "content": "Cleaned up the field creation a bit, more documentation in the example.  I think this is ready to go. "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12792052",
            "date": "2009-12-17T18:30:07+0000",
            "content": "Hey Grant:\n\nLet me give this a quick review. Won't take longer than 20 mins. Thanks for pushing forward on this.\n\nCheers,\nChris "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12792055",
            "date": "2009-12-17T18:34:02+0000",
            "content": "+1. Looks good, Grant. Let's get this sucker committed...\n\nThanks!\n\nCheers,\nChris "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792405",
            "date": "2009-12-18T09:14:50+0000",
            "content": "Cleaned up the field creation a bit, more documentation in the example. I think this is ready to go.\n\nAre you sure that we want to create a new String for every query/field creation? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792448",
            "date": "2009-12-18T12:09:31+0000",
            "content": "Are you sure that we want to create a new String for every query/field creation?\n\nI don't see anyway around it.  The caching doesn't work b/c there may be field types where the number of SchemaFields may not be known.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792495",
            "date": "2009-12-18T14:49:46+0000",
            "content": "it is possible to cache the objects in FieldType as \n\nprivate final Map<SchemaField, List<SchemaField>> subFields;\n\n\n\nWe can lazily initialize this Map for each SchemaField . "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792522",
            "date": "2009-12-18T16:29:23+0000",
            "content": "It doesn't look like subFieldSuffix actually works correctly - I've made a lot of little changes already, so I'll just go ahead and take a shot at fixing it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792669",
            "date": "2009-12-18T22:14:36+0000",
            "content": "Here's an updated patch that fixes a lot of little bugs - hopefully others can use this as a base so we don't lose all the little changes.  I also attached a diff of the patch to the previous patch to help people see what's changed (yuck... doesn't seem that readable though).  This isn't finished though - I only got to the FieldType / IndexSchema changes, and some of the ValueSource stuff.  I didn't get to distance and value source parsing stuff.\n\nSome of the changes:\n\n\tsmall javadoc cleanups\n\tfix subFieldSuffix so that it actually uses that suffix\n\tmake any utility methods on FieldType/IndexSchema dealing with \"poly\" field creation package protected - I don't think we want these public... it's specific for a field that adds only to other fields that it defines (with a specific naming convention) all of the same type.  They probably don't even belong on the base classes, but I don't care so much if they aren't public or protected, we can remove later.\n\tmake PointType actually delegate to the subFieldType... before it was assuming thinks like TermQuery and TermRangeQuery... this would have actually disabled NumericRangeQuery speedups!\n\tremove SchemaField creation from PointType - we should get fields from the schema\n\tfixed some value sources that didn't weight correctly\n\tfix createFields() to return Fieldable instead of Field\n\n\n\nWhen I fixed up point type, I did so in many places by assuming 2 points (so it will break for other dimensions).\n\nI had been working off the assumption that we wanted a geo specific base class to delegate some things to (like the most efficient way to get a bounding box, etc).  If so, we need to decide what that class will be.  Making it point or coordinate already bakes in a lot if implementation details (subType stuff).  Do we want geo to just work off of a generic n dimentional point class, or should we have a 2d lat/lon?  It does feel like we're loosing something by trying to over-generalize.  The PointTypeValueSource is inner-loop stuff, so I did specialize that for lat/lon. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792675",
            "date": "2009-12-18T22:28:18+0000",
            "content": "Do we want geo to just work off of a generic n dimentional point class, or should we have a 2d lat/lon?\n\nI think we want generic.  Some geo stuff will want 3D (elevation).  \n\nremove SchemaField creation from PointType - we should get fields from the schema\n\nI thought about that, but why should we have to figure out what the dynamic field is every time when we already know it?\n\nbq, make PointType actually delegate to the subFieldType... before it was assuming thinks like TermQuery and TermRangeQuery... this would have actually disabled NumericRangeQuery speedups!\n\nWhere do you mean?  In createField?  Or in the getFieldQuery? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792725",
            "date": "2009-12-19T00:41:10+0000",
            "content": "Actually, we should probably move the creation of the actual PointType off to SOLR-1586, anyway.  The reason I want n-dimensional is b/c this stuff is useful for more than just traditional lat/lon.   "
        },
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12792751",
            "date": "2009-12-19T01:43:33+0000",
            "content": "\nActually, we should probably move the creation of the actual PointType off to SOLR-1586, anyway. The reason I want n-dimensional is b/c this stuff is useful for more than just traditional lat/lon. \n\n+1, I was going to suggest the same thing. BTW, the geohash field type is ready when you have a chance to take a look.\n\nCheers,\nChris "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792848",
            "date": "2009-12-19T14:50:28+0000",
            "content": "I think we want generic. Some geo stuff will want 3D (elevation).\n\nI imagine 99% of the users of Point will be those wanting straight 2D geo search (limit by simple distance, or sort by simple distance).\nI don't see how elevation would help in isolation since more accurate distance measures would presumably need map info for driving distance.\nI could see people wanting to store 3D points, etc, but they can do that today.\n\nI thought about that, but why should we have to figure out what the dynamic field is every time when we already know it?\n\nIt's more of an issue of directly creating fields... that's currently up to the schema (or the FieldType if some form of SOLR-1664 goes in).\n\n> make PointType actually delegate to the subFieldType... before it was assuming thinks like TermQuery and TermRangeQuery... this would have actually disabled NumericRangeQuery speedups!\n\nWhere do you mean? In createField? Or in the getFieldQuery?\n\nIn getFieldQuery and getRangeQuery... I changed to actually delegate to the subFieldType.getFieldQuery and getRangeQuery. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792864",
            "date": "2009-12-19T17:08:00+0000",
            "content": "I imagine 99% of the users of Point will be those wanting straight 2D geo search (limit by simple distance, or sort by simple distance).\nI don't see how elevation would help in isolation since more accurate distance measures would presumably need map info for driving distance.\nI could see people wanting to store 3D points, etc, but they can do that today.\n\nBut it's not like the code is any faster or more complicated and neither is the interface the user sees (I'm sure most people know what a point is).  If I have a generic Point, now I have a Vector.  With a Vector, I can do all kinds of interesting things.  But rather than fret over it, we can have both.  LatLonFieldType and PointFieldType. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-12792875",
            "date": "2009-12-19T17:42:02+0000",
            "content": "Maybe Point -> \n{ Point2D, Point3D }\n ? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12793313",
            "date": "2009-12-21T19:43:32+0000",
            "content": "It's more of an issue of directly creating fields... that's currently up to the schema \n\nI'm not following.  Is the problem in:\n\n@Override\n  public Fieldable[] createFields(SchemaField field, String externalVal, float boost) {\n    String[] point = DistanceUtils.parsePoint(null, externalVal, dimension);\n    return createFields(field, dynFieldProps, subType, externalVal, boost, point);\n  }\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12793320",
            "date": "2009-12-21T19:53:06+0000",
            "content": "fix createFields() to return Fieldable instead of Field\n\nThis seems a bit weird (even though I understand why) due to the fact that the other createField methods actually return Field and not Fieldable.   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793482",
            "date": "2009-12-22T03:25:04+0000",
            "content": "> >    It's more of an issue of directly creating fields... that's currently up to the schema \n> I'm not following. Is the problem in:\n[...]\nNo, I meant directly creating SchemaFields doesn't seem great.  If we put a cache in, it would bypass that too.\n\nThis seems a bit weird (even though I understand why) due to the fact that the other createField methods actually return Field and not Fieldable. \n\nIf it weren't for back compat, we would have already changed createField to return Fieldable (and I think there's a SOLR issue somewhere that either does this or depends on it).  Fields are a lot more limiting than Fieldables (but createField was in Solr before there even was a Fieldable).\n\nif SOLR-1664 goes ahead it might be a natural place to make everything in SchemaField deal in Fieldabes? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12793521",
            "date": "2009-12-22T06:00:21+0000",
            "content": "if SOLR-1664 goes ahead it might be a natural place to make everything in SchemaField deal in Fieldabes?\n\nWhat stops us from resolving SOLR-1664 ?  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12793679",
            "date": "2009-12-22T17:20:58+0000",
            "content": "Updated to include some of Yonik's concerns about implementation.  I didn't get his Javadoc changes b/c they were too hard to determine the differences.\n\nOther notes:\n\n\tRemoved PlusMinusField and just used the PointType for those tests.  This then allowed me to move the registerPoly static method to the CoordinateFieldType\n\tI kept the n-dimensional point.  All of our distances work on vectors, I see no reason not to keep them.  Performance wise, most people w/ dimension of 2 or 3 will see little if any difference between this and specifically calling out a lat/lon field type.\n\tI believe I cleaned up the public method stuff so that helper methods are now package private.\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12793699",
            "date": "2009-12-22T18:08:21+0000",
            "content": "Note, I also changed PointValueSource to be ToMultiValueSource, as it really isn't just a point, but I'm not married to that name either. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12794054",
            "date": "2009-12-23T14:08:36+0000",
            "content": "Added some range tests. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12794413",
            "date": "2009-12-24T13:04:51+0000",
            "content": "Committed revision 893746.\n\nLeaving open for a little while to deal with any side effects. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12794439",
            "date": "2009-12-24T15:02:59+0000",
            "content": "OK, I'll try to diff what I had done before and re-make those changes.\nedit: I also see bug fixes I had made that got lost... I'll do a full re-review.\nedit: It appears the subFieldSuffix is broken again too. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12794445",
            "date": "2009-12-24T15:30:44+0000",
            "content": "OK, I thought I had got them all, but feel free to commit as you see fit. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12794842",
            "date": "2009-12-28T18:15:04+0000",
            "content": "Changed toMultiVS to vector(): Committed revision 894183.  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872676",
            "date": "2010-05-27T22:57:43+0000",
            "content": "Correcting Fix Version based on CHANGES.txt, see this thread for more details...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13013092",
            "date": "2011-03-30T15:45:31+0000",
            "content": "Bulk close for 3.1.0 release "
        }
    ]
}