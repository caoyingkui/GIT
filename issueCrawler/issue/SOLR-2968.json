{
    "id": "LUCENE-5468",
    "title": "Hunspell very high memory use when loading dictionary",
    "details": {
        "affect_versions": "3.5",
        "status": "Closed",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Hunspell stemmer requires gigantic (for the task) amounts of memory to load dictionary/rules files. \nFor example loading a 4.5 MB polish dictionary (with empty index!) will cause whole core to crash with various out of memory errors unless you set max heap size close to 2GB or more.\nBy comparison Stempel using the same dictionary file works just fine with 1/8 of that (and possibly lower values as well).\n\nSample error log entries:\nhttp://pastebin.com/fSrdd5W1\nhttp://pastebin.com/Lmi0re7Z",
    "attachments": {
        "patch.txt": "https://issues.apache.org/jira/secure/attachment/12507572/patch.txt",
        "LUCENE-5468.patch": "https://issues.apache.org/jira/secure/attachment/12631599/LUCENE-5468.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Robert Muir",
            "id": "comment-13169314",
            "date": "2011-12-14T13:12:30+0000",
            "content": "\nBy comparison Stempel using the same dictionary file works just fine with 1/8 of that (and possibly lower values as well).\n\nI imagine Stempel's Trie is good, but have you also compared Morfologik (http://svn.apache.org/repos/asf/lucene/dev/trunk/modules/analysis/morfologik/) ?\nIts precompiled FST might be the most space-efficient for polish.\n\nBut really I think Hunspell's dictionary structure should be more efficient, we could build the FST on-the-fly (if case-insensitive mode is off). But when \nthis is on, entries must be merged.\n\nInstead it might be better for the hunspell stuff to support loading FSTs (where we would do any case-sensitivity tweaking/merging of entries, then build FST).\nIt might be possible to re-use some of the same code from SOLR-2888 that does a similar thing to build a suggester FST.\n\nIn my opinion its worth it to build the FST not just for the words, but also the affixes (in some files these are humungous too!)\n\nFor lucene I think we would just allow HunspellDictionary to also be instantiated from these FST inputstreams. The solr factory / configuration would need\nto be tweaked to make this easy and intuitive. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13169347",
            "date": "2011-12-14T13:40:28+0000",
            "content": "Morfologik will be exactly the same size in memory as its unzipped dictionary, so about 1.8MB + 3.5MB if you use both pl (morfologik) and pl-sgjp (morfeusz) dictionaries. These are fixed dictionaries (that is unknown words won't be stemmed) but the coverage is decent for contemporary Polish.\n\nIf you explain what you're trying to do/ achieve then perhaps we'll be able to give you some more hints. "
        },
        {
            "author": "Chris Male",
            "id": "comment-13169353",
            "date": "2011-12-14T13:47:04+0000",
            "content": "+1 to your idea Robert.  I've been thinking along the same lines that FSTs might help us out here. "
        },
        {
            "author": "Maciej Lisiewski",
            "id": "comment-13169356",
            "date": "2011-12-14T13:47:34+0000",
            "content": "The last time I checked Morfologik was just mentioned as a possible new stemmer - I have used it before and I prefer it to Stempel/Hunspell, so I guess this solves my problem for now, thanks \n\nAs for Hunspell IMHO 2GB heap just to load dictionary makes it borderline unusable for some languages. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13169358",
            "date": "2011-12-14T13:51:53+0000",
            "content": "\nAs for Hunspell IMHO 2GB heap just to load dictionary makes it borderline unusable for some languages.\n\nRight but honestly the original motivation was to get something up quickly when you have no other choice: for minority languages, etc. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13169362",
            "date": "2011-12-14T13:55:52+0000",
            "content": "You know what they say these days \u2013 just buy more ram and get rid of the problem by covering it with money  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13169371",
            "date": "2011-12-14T14:05:34+0000",
            "content": "yeah but the HunspellDictionary really is ridiculous if you try to use a large dictionary with it, \neven without cutting over to an FST it could probably be improved.\n\nfor minority languages without really nice dictionaries it probably doesnt matter much, but for\nthe languages with really nice dictionaries you also tend to have language-specific options available.\n\njust another crazy idea: I don't know how much of morfologik is dependent upon polish itself, but \nif it already knows how to compile ispell/hunspell into an efficient form and work with it, maybe\nwe should just be seeing if we can 'generalize' that and work it from that angle. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13169390",
            "date": "2011-12-14T14:24:51+0000",
            "content": "I must disappoint you here \u2013 morfologik simply compiles a list of inflected-base-tag triples, it has no logic for generating these forms from lexical flags/ base dictionaries. Marcin Mi\u0142kowski has a set of scripts for that and he, as far as I recall, used aspell/ ispell to \"dump\" all of their forms by feeding the input dictionary basically. I think hunspell provides more intelligent handling of words outside of the dictionary so there's value in it that morfologik doesn't have. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13169394",
            "date": "2011-12-14T14:29:39+0000",
            "content": "\nMarcin Mi\u0142kowski has a set of scripts for that and he, as far as I recall, used aspell/ ispell to \"dump\" all of their forms by feeding the input dictionary basically. I think hunspell provides more intelligent handling of words outside of the dictionary so there's value in it that morfologik doesn't have.\n\nI think what you describe is essentially at a highlevel exactly what the hunspellfilter does. Theoretically there is more intelligent handling possible (correcting spelling), but this isn't implemented, not interesting for search anyway for the most part, and there is definitely no OOV mechanism.  "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13169527",
            "date": "2011-12-14T17:46:26+0000",
            "content": "You're probably right \u2013 my opinion was based on my inspection of hunspell's source code that I did once or twice in the past \u2013 I remember there's logic to perform more advanced stuff than dictionary lookup, but I never got the full picture if or how it's used. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170312",
            "date": "2011-12-15T16:06:41+0000",
            "content": "I'm working on a quick 80/20 stab here. I think it will help a lot. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170453",
            "date": "2011-12-15T20:01:12+0000",
            "content": "here's a patch cutting this thing over to use less ram once its started. but it probably uses more initially when parsing, mainly because we cannot guarantee the input is in sorted order. I think we should fix that, so that jumping thru hoops is the exception rather than the rule:\n\n\twe allow multiple dictionary files... is this really needed?\n\tif you use ignoreCase it means entries can be out of sorted order too.\n\tin some strange encodings the order in the original file could differ from binary order.\n\n\n\nthe building could just do the 2-phase thing it does now for the crazy cases and be efficient for the 90% case if we clean up.\n\nThe remaining problems:\n\n\tfix existing confusion in the dictionary api (like multiple input files) so that most of the time we can rely upon sorted order.\n\tsolr should never instantiate more than one of the same dictionary across different fields (thats a factory issue, i'm not going to deal with it here, but its just stupid if the factory does this).\n\tanything in the patch with nocommit, TODO, or bogus should be fixed.\n\n "
        },
        {
            "author": "Chris Male",
            "id": "comment-13170725",
            "date": "2011-12-16T03:07:19+0000",
            "content": "Hey, patch looks cool Robert.\n\nwe allow multiple dictionary files... is this really needed?\nI don't think so.\n\nsolr should never instantiate more than one of the same dictionary across different fields (thats a factory issue, i'm not going to deal with it here, but its just stupid if the factory does this)\nThats a really good point actually.  Makes me wonder whether there are other files / datastructures in analysis factories that are in the same boat? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170741",
            "date": "2011-12-16T03:38:28+0000",
            "content": "\nMakes me wonder whether there are other files / datastructures in analysis factories that are in the same boat?\n\nMaybe synonyms too? I dunno, just seems like if factories implement ResourceLoaderAware, \ninstead of calling init() and inform() on all of them, instead they should be able to parse \ntheir params in init(), override equals/hashcode based on their parameters, and some mechanism\nwould just then reuse existing ones instead of creating duplicates. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13170817",
            "date": "2011-12-16T07:54:27+0000",
            "content": "Looks good to me from looking at the diff. Btw., we really should pull out the getOutputForInput(FST, input) logic currently present in lookupOrd somewhere where it's reusable \u2013 I've seen it in a few places (or needed it a few times)...  "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13170865",
            "date": "2011-12-16T09:43:50+0000",
            "content": "Background for supporting multiple dictionaries is here: http://code.google.com/p/lucene-hunspell/issues/detail?id=4 and is invaluable for adding local customizations or overrides without touching the official dictionaries. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170886",
            "date": "2011-12-16T10:24:34+0000",
            "content": "at least the local override/customizations files can surely require sorted order?  "
        },
        {
            "author": "Maciej Lisiewski",
            "id": "comment-13170906",
            "date": "2011-12-16T11:24:02+0000",
            "content": "Those overrides/customizations will be tiny when compared to the main dictionary - is the sorting really an issue here?\nSimple example: default PL dictionary is close to 200k words. Largest custom dictionaries (legal, military, medical) will be 5-10k words (I'm basing those estimates on the best sources that I have found to generate those dictionaries from). In most cases we should expect <1k words.  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170911",
            "date": "2011-12-16T11:27:34+0000",
            "content": "\nThose overrides/customizations will be tiny when compared to the main dictionary - is the sorting really an issue here?\n\nDoesn't matter here, our FST requires that it be built in-order. doesn't matter if even one single word is out of order.\n\nbecause of this, we can't build the data structure efficiently. "
        },
        {
            "author": "Maciej Lisiewski",
            "id": "comment-13170917",
            "date": "2011-12-16T11:38:08+0000",
            "content": "What I was trying to say is that the custom dictionaries are small enough to be loaded and sorted in memory before building FST. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170918",
            "date": "2011-12-16T11:39:26+0000",
            "content": "Also, its required by the hunspell format itself. So this is not crazy to enforce. "
        },
        {
            "author": "Chris Male",
            "id": "comment-13170921",
            "date": "2011-12-16T11:41:25+0000",
            "content": "I don't see any problem mandating that overrides/customizations adhere to a sorted order.  I don't think we can assume custom dictionaries are going to be small - there's nothing in the APIs which force that.  Using FSTs gives us the performance benefit we're seeking in this issue, I think the small sacrifice is worth the huge benefit. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13170926",
            "date": "2011-12-16T11:59:14+0000",
            "content": "You can always sort inside your application if you're not sure if the words come or not in sorted order, Maciej. Lucene/Solr now even has on-disk merge sort which you can use for large(r) data sets \u2013 this code is along FSTCompletion in trunk. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170927",
            "date": "2011-12-16T11:59:33+0000",
            "content": "note: in some cases we will still have to use the throwaway treemap or similar like the patch i uploaded does.\n\nbut we could then know these two cases up front:\n\n\tsomeone enables ignoreCase=true\n\twhen binary sort order of the charset != utf8 binary order\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13170928",
            "date": "2011-12-16T12:03:10+0000",
            "content": "\nYou can always sort inside your application if you're not sure if the words come or not in sorted order, Maciej\n\nWell someone has to sort to 'test' any dictionary customizations with hunspells tools anyway.\n\nSo i assume people are already doing 'sort foo.dic my_foo_customizations.dic > combined.dic' then using 'analyze'\nand other commands to test... otherwise how are they testing their customizations ?!  "
        },
        {
            "author": "Mathias H.",
            "id": "comment-13559680",
            "date": "2013-01-22T14:42:57+0000",
            "content": "Dictionaries with the same file location should be shared across all field and all indexes. This would minimize the problem if you're using multiple indexes. \n\nCurrently I can't use Solr because I have 10 indexes with 5 field and for each field a DictionaryCompoundWordTokenFilterFactory is assigned. So the dictionary will be loaded 50 times. This is too much for my RAM. "
        },
        {
            "author": "Mathias H.",
            "id": "comment-13804030",
            "date": "2013-10-24T10:10:05+0000",
            "content": "I now solved the problem in my special case. I wrote a custom TokenFilterFactory that wraps the DictionaryCompoundWordTokenFilterFactory / HunspellStemFilterFactory and caches the factories, so they will be reused across indexes and fieldtypes. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13909908",
            "date": "2014-02-23T21:29:47+0000",
            "content": "I don't think we should let some esoteric options like multiple dictionaries keep this stuff unusable.\n\nSo I'm happy to just fork the entire stuff into a different package (hunspell2 or something), so we have a reasonably efficient version that doesnt have these esoteric options. The old stuff can stay as is, i do not care. "
        },
        {
            "author": "Chris Male",
            "id": "comment-13909943",
            "date": "2014-02-24T00:17:28+0000",
            "content": "Multiple dictionaries was never in the original design either.  Having an efficient and usable design seems to be of higher priority so +1 to not forking and doing this in place. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13909949",
            "date": "2014-02-24T00:31:56+0000",
            "content": "Well, I don't want the whole issue to get hung up on that stuff. Basically i'm working on a number of changes (especially tests though, to ensure the stuff is really working correctly). If we want, we can just lay down my new files on top of the existing stuff, or we can keep it/deprecate it, whatever we want to do. \n\nI just want to make some progress on a few improvements I've been investigating to try to make this thing more usable  "
        },
        {
            "author": "Chris Male",
            "id": "comment-13909952",
            "date": "2014-02-24T00:35:19+0000",
            "content": "Sounds good "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13910015",
            "date": "2014-02-24T04:41:05+0000",
            "content": "Commit 1571137 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571137 ]\n\nLUCENE-5468: commit current state "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13910017",
            "date": "2014-02-24T04:45:05+0000",
            "content": "I brought the previous FST patch up to speed, and then built a test to parse many dictionaries and compare memory. When it says FAIL, thats because the current code can't parse the dictionary (i fixed all the issues here).\n\nIn general, RAM use is better, but in some cases its still bad because of how the affixes are represented. I still havent removed my Treemap yet either (i wanted to have a way to test all the dictionaries like this before really locking things down).\n\n\n\n\ndict\nold RAM\nnew RAM\n\n\naf_ZA.zip\n18 MB\n899 KB\n\n\nak_GH.zip\n1.5 MB\n71 KB\n\n\nbg_BG.zip\nFAIL\n1.1 MB\n\n\nca_ANY.zip\n28.9 MB\n1.2 MB\n\n\nca_ES.zip\n15.1 MB\n1.2 MB\n\n\ncop_EG.zip\n2.1 MB\n489.3 KB\n\n\ncs_CZ.zip\n50.4 MB\n2.8 MB\n\n\ncy_GB.zip\nFAIL\n1.6 MB\n\n\nda_DK.zip\nFAIL\n750.8 KB\n\n\nde_AT.zip\n1.3 MB\n293.1 KB\n\n\nde_CH.zip\n12.6 MB\n895.6 KB\n\n\nde_DE.zip\n12.6 MB\n895 KB\n\n\nde_DE_comb.zip\n102.2 MB\n4.8 MB\n\n\nde_DE_frami.zip\n20.9 MB\n1.2 MB\n\n\nde_DE_neu.zip\n101.5 MB\n4.8 MB\n\n\nel_GR.zip\n74.3 MB\n1.1 MB\n\n\nen_AU.zip\n8.1 MB\n1.2 MB\n\n\nen_CA.zip\n9.8 MB\n436.7 KB\n\n\nen_GB-oed.zip\n8.2 MB\n1.2 MB\n\n\nen_GB.zip\n8.3 MB\n1.2 MB\n\n\nen_NZ.zip\n8.4 MB\n1.2 MB\n\n\neo.zip\n4.9 MB\n1.3 MB\n\n\neo_EO.zip\n4.9 MB\n1.3 MB\n\n\nes_AR.zip\n14.8 MB\n3.9 MB\n\n\nes_BO.zip\n14.8 MB\n3.9 MB\n\n\nes_CL.zip\n14.7 MB\n3.9 MB\n\n\nes_CO.zip\n14.3 MB\n3.8 MB\n\n\nes_CR.zip\n14.8 MB\n3.9 MB\n\n\nes_CU.zip\n14.7 MB\n3.9 MB\n\n\nes_DO.zip\n14.7 MB\n3.9 MB\n\n\nes_EC.zip\n14.8 MB\n3.9 MB\n\n\nes_ES.zip\n15.1 MB\n4.1 MB\n\n\nes_GT.zip\n14.8 MB\n3.9 MB\n\n\nes_HN.zip\n14.8 MB\n3.9 MB\n\n\nes_MX.zip\n14.3 MB\n3.8 MB\n\n\nes_NEW.zip\n15.5 MB\n4.2 MB\n\n\nes_NI.zip\n14.8 MB\n3.9 MB\n\n\nes_PA.zip\n14.8 MB\n3.9 MB\n\n\nes_PE.zip\n14.2 MB\n3.8 MB\n\n\nes_PR.zip\n14.7 MB\n3.9 MB\n\n\nes_PY.zip\n14.8 MB\n3.9 MB\n\n\nes_SV.zip\n14.8 MB\n3.9 MB\n\n\nes_UY.zip\n14.8 MB\n3.9 MB\n\n\nes_VE.zip\n14.3 MB\n3.8 MB\n\n\net_EE.zip\n53.6 MB\n5.9 MB\n\n\nfo_FO.zip\n18.6 MB\n485.7 KB\n\n\nfr_FR-1990_1-3-2.zip\n14 MB\n636.4 KB\n\n\nfr_FR-classique_1-3-2.zip\n14 MB\n743.1 KB\n\n\nfr_FR_1-3-2.zip\n14.5 MB\n755.2 KB\n\n\nfy_NL.zip\n4.2 MB\n272.8 KB\n\n\nga_IE.zip\n14 MB\n674.8 KB\n\n\ngd_GB.zip\n2.7 MB\n111 KB\n\n\ngl_ES.zip\nFAIL\n1.2 MB\n\n\ngsc_FR.zip\nFAIL\n1.4 MB\n\n\ngu_IN.zip\n20.3 MB\n914.9 KB\n\n\nhe_IL.zip\n53.3 MB\n1.8 MB\n\n\nhi_IN.zip\n2.7 MB\n136.9 KB\n\n\nhil_PH.zip\n3.4 MB\n164.8 KB\n\n\nhr_HR.zip\n29.7 MB\n564.8 KB\n\n\nhu_HU.zip\nFAIL\n17.6 MB\n\n\nhu_HU_comb.zip\nFAIL\n19.9 MB\n\n\nia.zip\n4.9 MB\n211.9 KB\n\n\nid_ID.zip\n3.9 MB\n218.4 KB\n\n\nit_IT.zip\n15.3 MB\n1.6 MB\n\n\nku_TR.zip\n1.6 MB\n147.6 KB\n\n\nla.zip\n5.1 MB\n2.5 MB\n\n\nlt_LT.zip\n15 MB\n2.8 MB\n\n\nlv_LV.zip\n36.3 MB\n1.9 MB\n\n\nmg_MG.zip\n2.9 MB\n131.7 KB\n\n\nmi_NZ.zip\nFAIL\n171.2 KB\n\n\nmk_MK.zip\nFAIL\n436.9 KB\n\n\nmos_BF.zip\n13.3 MB\n210 KB\n\n\nmr_IN.zip\nFAIL\n115.5 KB\n\n\nms_MY.zip\n4.1 MB\n221.6 KB\n\n\nnb_NO.zip\n22.9 MB\n1.4 MB\n\n\nne_NP.zip\n5.5 MB\n495.6 KB\n\n\nnl_NL.zip\n22.9 MB\n1.1 MB\n\n\nnl_med.zip\n1.2 MB\n60.2 KB\n\n\nnn_NO.zip\n16.5 MB\n1 MB\n\n\nnr_ZA.zip\n3.1 MB\n171.1 KB\n\n\nns_ZA.zip\n1.7 MB\n85.8 KB\n\n\nny_MW.zip\nFAIL\n69.6 KB\n\n\noc_FR.zip\n9.1 MB\n690.5 KB\n\n\npl_PL.zip\n43.9 MB\n4.9 MB\n\n\npt_BR.zip\nFAIL\n3.9 MB\n\n\npt_PT.zip\n5.8 MB\n773.4 KB\n\n\nro_RO.zip\n5.1 MB\n226.2 KB\n\n\nru_RU.zip\n21.7 MB\n1.4 MB\n\n\nru_RU_ye.zip\n43.7 MB\n1.6 MB\n\n\nru_RU_yo.zip\n21.7 MB\n1.4 MB\n\n\nrw_RW.zip\n1.6 MB\n70.1 KB\n\n\nsk_SK.zip\n25.1 MB\n2.3 MB\n\n\nsl_SI.zip\n38.3 MB\n806.6 KB\n\n\nsq_AL.zip\n28.9 MB\n654.6 KB\n\n\nss_ZA.zip\n3.1 MB\n176.3 KB\n\n\nst_ZA.zip\n1.7 MB\n86.5 KB\n\n\nsv_SE.zip\n9.5 MB\n668.8 KB\n\n\nsw_KE.zip\n6.3 MB\n286 KB\n\n\ntet_ID.zip\n2 MB\n92.4 KB\n\n\nth_TH.zip\nFAIL\n377.4 KB\n\n\ntl_PH.zip\n2.6 MB\n116.5 KB\n\n\ntn_ZA.zip\n1.5 MB\n61.6 KB\n\n\nts_ZA.zip\n1.6 MB\n81 KB\n\n\nuk_UA.zip\n17.6 MB\n3 MB\n\n\nve_ZA.zip\nFAIL\n108.8 KB\n\n\nvi_VN.zip\n1.7 MB\n53.6 KB\n\n\nxh_ZA.zip\n3 MB\n158.9 KB\n\n\nzu_ZA.zip\n24.5 MB\n13.5 MB\n\n\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13910413",
            "date": "2014-02-24T15:45:09+0000",
            "content": "Commit 1571321 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571321 ]\n\nLUCENE-5468: factor OfflineSorter out of suggest "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13910509",
            "date": "2014-02-24T17:11:09+0000",
            "content": "Commit 1571356 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571356 ]\n\nLUCENE-5468: sort dictionary data with offline sorter "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13911914",
            "date": "2014-02-25T19:18:10+0000",
            "content": "Commit 1571788 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571788 ]\n\nLUCENE-5468: deduplicate patterns used by affix condition check "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13911953",
            "date": "2014-02-25T19:44:33+0000",
            "content": "Commit 1571802 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571802 ]\n\nLUCENE-5468: remove redundant 'append' in Affix "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13911985",
            "date": "2014-02-25T20:07:07+0000",
            "content": "Commit 1571807 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571807 ]\n\nLUCENE-5468: Stem -> CharsRef "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13912169",
            "date": "2014-02-25T22:29:29+0000",
            "content": "Commit 1571844 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1571844 ]\n\nLUCENE-5468: make Affix fixed-width "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13914696",
            "date": "2014-02-27T16:19:22+0000",
            "content": "Commit 1572643 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572643 ]\n\nLUCENE-5468: don't create unnecessary objects "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13914755",
            "date": "2014-02-27T17:19:17+0000",
            "content": "Commit 1572660 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572660 ]\n\nLUCENE-5468: encode affix data as 8 bytes per affix, before cutting over to FST "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13914795",
            "date": "2014-02-27T17:53:31+0000",
            "content": "Commit 1572666 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572666 ]\n\nLUCENE-5468: convert affixes to FST "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13914799",
            "date": "2014-02-27T17:57:35+0000",
            "content": "I am finished compressing for now. I think its pretty reasonable across all the languages.\n\nI will cleanup and try to add back the multiple dictionary/ignore case stuff and clean up some other things.\n\n\n\n\ndict\nold RAM\nnew RAM\n\n\naf_ZA.zip\n18 MB\n917.1 KB\n\n\nak_GH.zip\n1.5 MB\n103.2 KB\n\n\nbg_BG.zip\nFAIL\n465.7 KB\n\n\nca_ANY.zip\n28.9 MB\n675.4 KB\n\n\nca_ES.zip\n15.1 MB\n639.8 KB\n\n\ncop_EG.zip\n2.1 MB\n144.5 KB\n\n\ncs_CZ.zip\n50.4 MB\n1.5 MB\n\n\ncy_GB.zip\nFAIL\n627.4 KB\n\n\nda_DK.zip\nFAIL\n669.8 KB\n\n\nde_AT.zip\n1.3 MB\n123.9 KB\n\n\nde_CH.zip\n12.6 MB\n725.4 KB\n\n\nde_DE.zip\n12.6 MB\n726 KB\n\n\nde_DE_comb.zip\n102.2 MB\n4.2 MB\n\n\nde_DE_frami.zip\n20.9 MB\n1023.5 KB\n\n\nde_DE_neu.zip\n101.5 MB\n4.2 MB\n\n\nel_GR.zip\n74.3 MB\n1 MB\n\n\nen_AU.zip\n8.1 MB\n521 KB\n\n\nen_CA.zip\n9.8 MB\n450.5 KB\n\n\nen_GB-oed.zip\n8.2 MB\n526.6 KB\n\n\nen_GB.zip\n8.3 MB\n527.3 KB\n\n\nen_NZ.zip\n8.4 MB\n532.4 KB\n\n\neo.zip\n4.9 MB\n310.5 KB\n\n\neo_EO.zip\n4.9 MB\n310.5 KB\n\n\nes_AR.zip\n14.8 MB\n734.9 KB\n\n\nes_BO.zip\n14.8 MB\n735 KB\n\n\nes_CL.zip\n14.7 MB\n734.9 KB\n\n\nes_CO.zip\n14.3 MB\n722.1 KB\n\n\nes_CR.zip\n14.8 MB\n733.9 KB\n\n\nes_CU.zip\n14.7 MB\n732.8 KB\n\n\nes_DO.zip\n14.7 MB\n731.9 KB\n\n\nes_EC.zip\n14.8 MB\n733.5 KB\n\n\nes_ES.zip\n15.1 MB\n743 KB\n\n\nes_GT.zip\n14.8 MB\n734.5 KB\n\n\nes_HN.zip\n14.8 MB\n735.2 KB\n\n\nes_MX.zip\n14.3 MB\n723.8 KB\n\n\nes_NEW.zip\n15.5 MB\n768.5 KB\n\n\nes_NI.zip\n14.8 MB\n734.5 KB\n\n\nes_PA.zip\n14.8 MB\n733.8 KB\n\n\nes_PE.zip\n14.2 MB\n721.3 KB\n\n\nes_PR.zip\n14.7 MB\n732.4 KB\n\n\nes_PY.zip\n14.8 MB\n734.1 KB\n\n\nes_SV.zip\n14.8 MB\n733.6 KB\n\n\nes_UY.zip\n14.8 MB\n736.9 KB\n\n\nes_VE.zip\n14.3 MB\n722.7 KB\n\n\net_EE.zip\n53.6 MB\n473.6 KB\n\n\nfo_FO.zip\n18.6 MB\n517.9 KB\n\n\nfr_FR-1990_1-3-2.zip\n14 MB\n526.7 KB\n\n\nfr_FR-classique_1-3-2.zip\n14 MB\n539.2 KB\n\n\nfr_FR_1-3-2.zip\n14.5 MB\n550.4 KB\n\n\nfy_NL.zip\n4.2 MB\n265.6 KB\n\n\nga_IE.zip\n14 MB\n460.6 KB\n\n\ngd_GB.zip\n2.7 MB\n143.1 KB\n\n\ngl_ES.zip\nFAIL\n479.4 KB\n\n\ngsc_FR.zip\nFAIL\n1.3 MB\n\n\ngu_IN.zip\n20.3 MB\n947 KB\n\n\nhe_IL.zip\n53.3 MB\n539.2 KB\n\n\nhi_IN.zip\n2.7 MB\n169 KB\n\n\nhil_PH.zip\n3.4 MB\n197 KB\n\n\nhr_HR.zip\n29.7 MB\n573 KB\n\n\nhu_HU.zip\nFAIL\n1.2 MB\n\n\nhu_HU_comb.zip\nFAIL\n5.4 MB\n\n\nia.zip\n4.9 MB\n222.9 KB\n\n\nid_ID.zip\n3.9 MB\n226.3 KB\n\n\nit_IT.zip\n15.3 MB\n612.9 KB\n\n\nku_TR.zip\n1.6 MB\n118.7 KB\n\n\nla.zip\n5.1 MB\n199.3 KB\n\n\nlt_LT.zip\n15 MB\n682.5 KB\n\n\nlv_LV.zip\n36.3 MB\n763.9 KB\n\n\nmg_MG.zip\n2.9 MB\n163.8 KB\n\n\nmi_NZ.zip\nFAIL\n191.4 KB\n\n\nmk_MK.zip\nFAIL\n469.1 KB\n\n\nmos_BF.zip\n13.3 MB\n242.2 KB\n\n\nmr_IN.zip\nFAIL\n147.7 KB\n\n\nms_MY.zip\n4.1 MB\n226.9 KB\n\n\nnb_NO.zip\n22.9 MB\n1.2 MB\n\n\nne_NP.zip\n5.5 MB\n328.1 KB\n\n\nnl_NL.zip\n22.9 MB\n1.1 MB\n\n\nnl_med.zip\n1.2 MB\n92.3 KB\n\n\nnn_NO.zip\n16.5 MB\n914 KB\n\n\nnr_ZA.zip\n3.1 MB\n203.3 KB\n\n\nns_ZA.zip\n1.7 MB\n118 KB\n\n\nny_MW.zip\nFAIL\n101.8 KB\n\n\noc_FR.zip\n9.1 MB\n401.5 KB\n\n\npl_PL.zip\n43.9 MB\n1.7 MB\n\n\npt_BR.zip\nFAIL\n2.1 MB\n\n\npt_PT.zip\n5.8 MB\n379.4 KB\n\n\nro_RO.zip\n5.1 MB\n256.3 KB\n\n\nru_RU.zip\n21.7 MB\n882 KB\n\n\nru_RU_ye.zip\n43.7 MB\n1.5 MB\n\n\nru_RU_yo.zip\n21.7 MB\n897.3 KB\n\n\nrw_RW.zip\n1.6 MB\n102.3 KB\n\n\nsk_SK.zip\n25.1 MB\n1.2 MB\n\n\nsl_SI.zip\n38.3 MB\n604 KB\naf_ZA.zip\n18 MB\n917.1 KB\n\n\nak_GH.zip\n1.5 MB\n103.2 KB\n\n\nbg_BG.zip\nFAIL\n465.7 KB\n\n\nca_ANY.zip\n28.9 MB\n675.4 KB\n\n\nca_ES.zip\n15.1 MB\n639.8 KB\n\n\ncop_EG.zip\n2.1 MB\n144.5 KB\n\n\ncs_CZ.zip\n50.4 MB\n1.5 MB\n\n\ncy_GB.zip\nFAIL\n627.4 KB\n\n\nda_DK.zip\nFAIL\n669.8 KB\n\n\nde_AT.zip\n1.3 MB\n123.9 KB\n\n\nde_CH.zip\n12.6 MB\n725.4 KB\n\n\nde_DE.zip\n12.6 MB\n726 KB\n\n\nde_DE_comb.zip\n102.2 MB\n4.2 MB\n\n\nde_DE_frami.zip\n20.9 MB\n1023.5 KB\n\n\nde_DE_neu.zip\n101.5 MB\n4.2 MB\n\n\nel_GR.zip\n74.3 MB\n1 MB\n\n\nen_AU.zip\n8.1 MB\n521 KB\n\n\nen_CA.zip\n9.8 MB\n450.5 KB\n\n\nen_GB-oed.zip\n8.2 MB\n526.6 KB\n\n\nen_GB.zip\n8.3 MB\n527.3 KB\n\n\nen_NZ.zip\n8.4 MB\n532.4 KB\n\n\neo.zip\n4.9 MB\n310.5 KB\n\n\neo_EO.zip\n4.9 MB\n310.5 KB\n\n\nes_AR.zip\n14.8 MB\n734.9 KB\n\n\nes_BO.zip\n14.8 MB\n735 KB\n\n\nes_CL.zip\n14.7 MB\n734.9 KB\n\n\nes_CO.zip\n14.3 MB\n722.1 KB\n\n\nes_CR.zip\n14.8 MB\n733.9 KB\n\n\nes_CU.zip\n14.7 MB\n732.8 KB\n\n\nes_DO.zip\n14.7 MB\n731.9 KB\n\n\nes_EC.zip\n14.8 MB\n733.5 KB\n\n\nes_ES.zip\n15.1 MB\n743 KB\n\n\nes_GT.zip\n14.8 MB\n734.5 KB\n\n\nes_HN.zip\n14.8 MB\n735.2 KB\n\n\nes_MX.zip\n14.3 MB\n723.8 KB\n\n\nes_NEW.zip\n15.5 MB\n768.5 KB\n\n\nes_NI.zip\n14.8 MB\n734.5 KB\n\n\nes_PA.zip\n14.8 MB\n733.8 KB\n\n\nes_PE.zip\n14.2 MB\n721.3 KB\n\n\nes_PR.zip\n14.7 MB\n732.4 KB\n\n\nes_PY.zip\n14.8 MB\n734.1 KB\n\n\nes_SV.zip\n14.8 MB\n733.6 KB\n\n\nes_UY.zip\n14.8 MB\n736.9 KB\n\n\nes_VE.zip\n14.3 MB\n722.7 KB\n\n\net_EE.zip\n53.6 MB\n473.6 KB\n\n\nfo_FO.zip\n18.6 MB\n517.9 KB\n\n\nfr_FR-1990_1-3-2.zip\n14 MB\n526.7 KB\n\n\nfr_FR-classique_1-3-2.zip\n14 MB\n539.2 KB\n\n\nfr_FR_1-3-2.zip\n14.5 MB\n550.4 KB\n\n\nfy_NL.zip\n4.2 MB\n265.6 KB\n\n\nga_IE.zip\n14 MB\n460.6 KB\n\n\ngd_GB.zip\n2.7 MB\n143.1 KB\n\n\ngl_ES.zip\nFAIL\n479.4 KB\n\n\ngsc_FR.zip\nFAIL\n1.3 MB\n\n\ngu_IN.zip\n20.3 MB\n947 KB\n\n\nhe_IL.zip\n53.3 MB\n539.2 KB\n\n\nhi_IN.zip\n2.7 MB\n169 KB\n\n\nhil_PH.zip\n3.4 MB\n197 KB\n\n\nhr_HR.zip\n29.7 MB\n573 KB\n\n\nhu_HU.zip\nFAIL\n1.2 MB\n\n\nhu_HU_comb.zip\nFAIL\n5.4 MB\n\n\nia.zip\n4.9 MB\n222.9 KB\n\n\nid_ID.zip\n3.9 MB\n226.3 KB\n\n\nit_IT.zip\n15.3 MB\n612.9 KB\n\n\nku_TR.zip\n1.6 MB\n118.7 KB\n\n\nla.zip\n5.1 MB\n199.3 KB\n\n\nlt_LT.zip\n15 MB\n682.5 KB\n\n\nlv_LV.zip\n36.3 MB\n763.9 KB\n\n\nmg_MG.zip\n2.9 MB\n163.8 KB\n\n\nmi_NZ.zip\nFAIL\n191.4 KB\n\n\nmk_MK.zip\nFAIL\n469.1 KB\n\n\nmos_BF.zip\n13.3 MB\n242.2 KB\n\n\nmr_IN.zip\nFAIL\n147.7 KB\n\n\nms_MY.zip\n4.1 MB\n226.9 KB\n\n\nnb_NO.zip\n22.9 MB\n1.2 MB\n\n\nne_NP.zip\n5.5 MB\n328.1 KB\n\n\nnl_NL.zip\n22.9 MB\n1.1 MB\n\n\nnl_med.zip\n1.2 MB\n92.3 KB\n\n\nnn_NO.zip\n16.5 MB\n914 KB\n\n\nnr_ZA.zip\n3.1 MB\n203.3 KB\n\n\nns_ZA.zip\n1.7 MB\n118 KB\n\n\nny_MW.zip\nFAIL\n101.8 KB\n\n\noc_FR.zip\n9.1 MB\n401.5 KB\n\n\npl_PL.zip\n43.9 MB\n1.7 MB\n\n\npt_BR.zip\nFAIL\n2.1 MB\n\n\npt_PT.zip\n5.8 MB\n379.4 KB\n\n\nro_RO.zip\n5.1 MB\n256.3 KB\n\n\nru_RU.zip\n21.7 MB\n882 KB\n\n\nru_RU_ye.zip\n43.7 MB\n1.5 MB\n\n\nru_RU_yo.zip\n21.7 MB\n897.3 KB\n\n\nrw_RW.zip\n1.6 MB\n102.3 KB\n\n\nsk_SK.zip\n25.1 MB\n1.2 MB\n\n\nsl_SI.zip\n38.3 MB\n604 KB\n\n\nsq_AL.zip\n28.9 MB\n581.7 KB\n\n\nss_ZA.zip\n3.1 MB\n208.5 KB\n\n\nst_ZA.zip\n1.7 MB\n118.7 KB\n\n\nsv_SE.zip\n9.5 MB\n535.4 KB\n\n\nsw_KE.zip\n6.3 MB\n318.2 KB\n\n\ntet_ID.zip\n2 MB\n124.5 KB\n\n\nth_TH.zip\nFAIL\n409.6 KB\n\n\ntl_PH.zip\n2.6 MB\n148.7 KB\n\n\ntn_ZA.zip\n1.5 MB\n93.7 KB\n\n\nts_ZA.zip\n1.6 MB\n113.1 KB\n\n\nuk_UA.zip\n17.6 MB\n979.1 KB\n\n\nve_ZA.zip\nFAIL\n140.9 KB\n\n\nvi_VN.zip\n1.7 MB\n85.8 KB\n\n\nxh_ZA.zip\n3 MB\n191.1 KB\n\n\nzu_ZA.zip\n24.5 MB\n827.1 KB\n\n\n\n\n\n\n\n\nsq_AL.zip\n28.9 MB\n581.7 KB\n\n\nss_ZA.zip\n3.1 MB\n208.5 KB\n\n\nst_ZA.zip\n1.7 MB\n118.7 KB\n\n\nsv_SE.zip\n9.5 MB\n535.4 KB\n\n\nsw_KE.zip\n6.3 MB\n318.2 KB\n\n\ntet_ID.zip\n2 MB\n124.5 KB\n\n\nth_TH.zip\nFAIL\n409.6 KB\n\n\ntl_PH.zip\n2.6 MB\n148.7 KB\n\n\ntn_ZA.zip\n1.5 MB\n93.7 KB\n\n\nts_ZA.zip\n1.6 MB\n113.1 KB\n\n\nuk_UA.zip\n17.6 MB\n979.1 KB\n\n\nve_ZA.zip\nFAIL\n140.9 KB\n\n\nvi_VN.zip\n1.7 MB\n85.8 KB\n\n\nxh_ZA.zip\n3 MB\n191.1 KB\n\n\nzu_ZA.zip\n24.5 MB\n827.1 KB\n\n\n\n "
        },
        {
            "author": "Chris Male",
            "id": "comment-13914954",
            "date": "2014-02-27T19:38:19+0000",
            "content": "Those are some pretty amazing reductions, well done! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13914960",
            "date": "2014-02-27T19:41:39+0000",
            "content": "I have the previous options added back too locally. so i will fix up tests and so on and just copy over the old filter and make a patch.  "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13915004",
            "date": "2014-02-27T20:19:29+0000",
            "content": "Commit 1572718 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572718 ]\n\nLUCENE-5468: hunspell2 -> hunspell (with previous options and tests) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13915025",
            "date": "2014-02-27T20:39:36+0000",
            "content": "Commit 1572724 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572724 ]\n\nLUCENE-5468: fix precommit+test "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13915028",
            "date": "2014-02-27T20:42:53+0000",
            "content": "Commit 1572727 from Robert Muir in branch 'dev/branches/lucene5468'\n[ https://svn.apache.org/r1572727 ]\n\nLUCENE-5468: add additional change "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915033",
            "date": "2014-02-27T20:48:46+0000",
            "content": "I think the change is ready. There are other improvements that can be done (for example, maybe an option for the factory to cache these things in case you use same ones across multiple fields, and more efficient affix handling against the FST, and so on), but it would be better on different issues I think?\n\nHere is a patch (from diff-sources), sorry its not so useful, as I renamed some things. I tried making one from svn diff after reintegration, but it was equally useless. If you want you can also review my commits on this issue to the branch, too.\n\nhere is CHANGES entry:\n\nAPI Changes:\n\n\n\tLUCENE-5468: Move offline Sort (from suggest module) to OfflineSort. (Robert Muir)\n\n\n\nOptimizations:\n\n\n\tLUCENE-5468: HunspellStemFilter uses 10 to 100x less RAM. It also loads\n  all known openoffice dictionaries without error, and supports an additional\n  longestOnly option for a less aggressive approach.  (Robert Muir)\n\n\n "
        },
        {
            "author": "Chris Male",
            "id": "comment-13915045",
            "date": "2014-02-27T21:05:22+0000",
            "content": "Is the longestOnly option a standard Hunspell thing? (more a question of general interest) "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915051",
            "date": "2014-02-27T21:11:28+0000",
            "content": "No, but when testing relevance, outputting all the stems leads to slower indexing, a much larger index, and significantly impacts precision for some languages.\n\nSo after reading CLEF experiments done with hungarian (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.8036&rep=rep1&type=pdf) where they suggest a simple disambiguation heuristic (shortest stem for most aggressive), i experimented with the opposite, and found it was quite useful  "
        },
        {
            "author": "Chris Male",
            "id": "comment-13915053",
            "date": "2014-02-27T21:14:31+0000",
            "content": "Awesome, sounds like a great addition then. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13915083",
            "date": "2014-02-27T21:42:19+0000",
            "content": "These are incredible reductions on RAM usage from cutting over to\nFSTs.  And it's nice that you are using IntSequenceOutputs, and\nthat you are now able to load dictionaries that failed before!\n\nI'm not sure it matters here, but do you handle the FST Builder returning\nnull for the built FST (when there was nothing added)?  Just a common\ngotchya...\n\nDo you have any sense of how the lookup speed changed? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915094",
            "date": "2014-02-27T21:47:43+0000",
            "content": "\nI'm not sure it matters here, but do you handle the FST Builder returning\nnull for the built FST (when there was nothing added)? Just a common\ngotchya...\n\nDo you have any sense of how the lookup speed changed?\n\nMany dictionaries have either no prefixes or no suffixes: the code comment below this also answers your other question about NULL FST I think.\nAdmittedly its probably no faster now, but it can be faster if we make the Stemmer smarter when walking the possibilities in the word:\n\n  // TODO: this is pretty stupid, considering how the stemming algorithm works\n  // we can speed it up to be significantly faster!\n  IntsRef lookupAffix(FST<IntsRef> fst, char word[], int offset, int length) {\n    if (fst == null) {\n      return null;\n    }\n\n\n\nGiven the fact this thing takes sometimes 100MB per field and makes it nearly unusable, I made such larger changes a TODO for a separate issue? "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13915095",
            "date": "2014-02-27T21:50:39+0000",
            "content": "Definitely +1 to commit this and worry about speedups separately! "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13915115",
            "date": "2014-02-27T22:13:29+0000",
            "content": "Commit 1572754 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1572754 ]\n\nLUCENE-5468: reduce RAM usage of hunspell "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13915152",
            "date": "2014-02-27T22:51:56+0000",
            "content": "Commit 1572774 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1572774 ]\n\nLUCENE-5468: reduce RAM usage of hunspell "
        },
        {
            "author": "Lukas Vlcek",
            "id": "comment-13915171",
            "date": "2014-02-27T23:12:14+0000",
            "content": "Amazing improvement!\n\nWhile we are on Hunspell I would like to make a proposal for additional enhancements but first I would like to ask if you would be interested in seeing such improvements in the code. I would be happy to open a new ticket for this in such case.\n\n1) AFAIR Hunspell token filter has an option to setup level of recursion. Originally hardcoded to 2 if I am not mistaken. But the level of recursion counts for both prefix and postfix rules - meaning if it is set to 2 and 1 prefix rule is applied, then we can only apply 2-1 suffix rules. What I would like to propose is adding an option to explicitly specify recursion level for both the prefix and for postfix rules. This probably depends a lot on how the affix rules are constructed but I can clearly see this would help in case of Czech dictionary - hopefully this might be found useful for other languages too.\n\n2) Case sensitivity is a tricky part. Czech dictionary is case sensitive and it can deliver very nice results but users can not always fully benefit from this. The biggest problem I remember are tokens at the beginning of sentences. They start with capitals and thus they may not be found in dict where only lowercased variation is recorded.\nI was thinking that one useful solution to this issue can be adding an option to lowercase given token if it hasn't been found in dict and making a second pass through the filter again with lowercased token (it is costly but would be only optional so user is the one to decide if this is worth the indexing time).\n\n3) Also it would be really useful if Hunspell token filter provided an option to output all terms that are the result of application of relevant rules to input token (so in essence quite opposite transformation to what is used during stemming). Such functionality would be useful if users want to add custom extension to existing dictionary (having an option to load several dict files is really useful IMO) and they want to check that they constructed valid rules for specific words. Having Lucene directly supporting them via exposed API would be great I think (especially when thinking about later applications in Solr and Elasticsearch). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915178",
            "date": "2014-02-27T23:15:36+0000",
            "content": "All 3 of these options are still supported by both the filter/dictionary and the factory. look at 'recursionCap',  'ignoreCase', and dictionaries is a List<InputStream>. And by default it outputs all terms (unless you supply longestMatch=true). So I'm not really sure what is needed here? "
        },
        {
            "author": "Lukas Vlcek",
            "id": "comment-13915222",
            "date": "2014-02-27T23:46:56+0000",
            "content": "Robert,\n\nI did not check the latest code so please forgive my ignorance but let me try to explain:\n\nrecursionCap does not distinguishes between how many prefix and suffix rules were applied. Does it? It counts just the total. If I set recursionCap to 1 it actually includes all the following options:\n\n\t2 prefix rules, 0 suffix rules\n\t1prefix rule, 1 suffix rule\n\t0 prefix rules, 2 suffix rules\n\n\n\nThis may not play well with some affix rule dictionaries. For example the czech dictionary is constructed in such a way that only one suffix rule can be applied otherwise the filter can generate irrelevant tokens. So the recursionCap MUST be set to 0.\nHowever, if this recursion level is consumed on removal of prefix then it can not continue and manipulate also the suffix. So what I am proposing is having an option to set recursionCap separately for prefix and suffix. In case of czech dict I would say: you can apply only one prefix rule and only one suffix rule (meaning you can NEVER apply two prefix rules or two affix rules).\n\nAs for ignoreCase - how does it work if the dictionary contains terms like \"Xx\" and \"xx\" and each is allowed to use different set of rules? I need to distinguish between them. But at the same hand if the dictionary contains only \"yy\" but I get \"Yy\" as input (because it was the first word of the sentence) would it be able to process it correctly and still distinguish between \"Xx\" and \"xx\"?\n\nAs for the last feature I probably confused you. What I am looking for is not output of all possible root words for given term but all possible inflections for given (root) word. For example: input is \"tell\" and based on loaded dictionary the output would be [\"tell\",\"tells\",\"telling\", ...]. I think it wold not be hard to expose such API and I believe users would appreciate this when constructing custom dictionaries (I tried that and I was missing such feature, sure I can implement it myself but I believe having it in Solr and Elasticsearch would be great, definitely this is not useful for indexing process but as a part of tuning your dictionary this would be helpful). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915230",
            "date": "2014-02-27T23:53:58+0000",
            "content": "ok, i was a little confused. I thought perhaps you referred to the previous discussion above about removing things \n\nI just want to make it clear i kept all the additional options we already had!\n\n\nSo what I am proposing is having an option to set recursionCap separately for prefix and suffix. In case of czech dict I would say: you can apply only one prefix rule and only one suffix rule (meaning you can NEVER apply two prefix rules or two affix rules).\n\n+1, can you open an issue for this?\n\n\nAs for ignoreCase - how does it work if the dictionary contains terms like \"Xx\" and \"xx\" and each is allowed to use different set of rules? I need to distinguish between them. \n\nRight, thats why it does nothing by default \n\n\nBut at the same hand if the dictionary contains only \"yy\" but I get \"Yy\" as input (because it was the first word of the sentence) would it be able to process it correctly and still distinguish between \"Xx\" and \"xx\"?\n\nIn my opinion, this is not the responsibility of this filter (it simply has ignoreCase on or off). This has more to do with your analysis chain? So if you want to put a lowercase filter first always, thats one approach. If you want to use some rule/heuristic for sentence tokenization or other fancy stuff, you can selectively lowercase and get what you want. But this filter knows nothing about that \n\n\nI think it wold not be hard to expose such API and I believe users would appreciate this when constructing custom dictionaries (I tried that and I was missing such feature, sure I can implement it myself but I believe having it in Solr and Elasticsearch would be great, definitely this is not useful for indexing process but as a part of tuning your dictionary this would be helpful).\n\nWhy not just use the hunspell command-line tools like 'unmunch', 'analyze', etc for that? "
        },
        {
            "author": "Chris Male",
            "id": "comment-13915231",
            "date": "2014-02-27T23:56:05+0000",
            "content": "I dont think we should make the recusionCap anymore complex.  I put it in there simply to prevent languages from getting into infinite loops. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915236",
            "date": "2014-02-27T23:59:21+0000",
            "content": "Chris but Lukas has a real use-case and its probably like 5 total lines of code to split that out? I dunno, it seems fine to me. "
        },
        {
            "author": "Chris Male",
            "id": "comment-13915240",
            "date": "2014-02-28T00:01:30+0000",
            "content": "Yeah I guess.  We can go over that in a new issue. "
        },
        {
            "author": "Lukas Vlcek",
            "id": "comment-13915266",
            "date": "2014-02-28T00:23:45+0000",
            "content": "OK, I will open a new ticket for the recursionCap tomorrow (it is late on my end now).\n\nJust a real quick comments on my two other suggestions:\n\nLowercasing in Hunspell - Robert, when you think about it there is really no simple solution to this using existing Lucene analysis flow AFAIK. If you apply lowercase BEFORE Hunspell you lose the option to correctly stem the uppercased token (if there is any record for it in the dictionary). If you apply if after Hunspell you have the problem with first token in sentences (in most cases). The other option is (as you mentioned) employ some more sophisticated analysis chain (but is there any suitable in Lucene out of the box or do I have to go down the road and setup complex language library or framework?)\nSo the option to allow lowercasing for second pass is IMO nice compromise that can help a lot with really minimal effort (and it is also easy to explain to users what it does and when to use it). It is not perfect solution but may be good enough to solve 80/20 principle.\n\nGetting all inflections - yes, there are CL tools for this. But this is really more about user experience comfort, and again, it is easy to explain how to use it, what it does and users do not have to mess with CL tools and things like that. Not sure how hard it would be to implement this with what is in Hunspell now.\nAlso one thing is some CL tool used against some dictionary files and other thing can be using Lucene code on dictionary loaded into memory by Lucene. If there are issues in the code these two approaches can give different results (yes, they should be the same...) "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13915281",
            "date": "2014-02-28T00:37:38+0000",
            "content": "\nLowercasing in Hunspell - Robert, when you think about it there is really no simple solution to this using existing Lucene analysis flow AFAIK. If you apply lowercase BEFORE Hunspell you lose the option to correctly stem the uppercased token (if there is any record for it in the dictionary). If you apply if after Hunspell you have the problem with first token in sentences (in most cases). The other option is (as you mentioned) employ some more sophisticated analysis chain (but is there any suitable in Lucene out of the box or do I have to go down the road and setup complex language library or framework?)\nSo the option to allow lowercasing for second pass is IMO nice compromise that can help a lot with really minimal effort (and it is also easy to explain to users what it does and when to use it). It is not perfect solution but may be good enough to solve 80/20 principle.\n\nThere may not be, but its about where the responsibility should be. Its more than the first token in sentences: named entities etc are involved too. If you want to get this right, yes, you need a more sophisticated analysis chain! That being said, I'm not against your 80/20 heuristic, I'm just not sure how 80/20 it is \n\n\nGetting all inflections - yes, there are CL tools for this. But this is really more about user experience comfort, and again, it is easy to explain how to use it, what it does and users do not have to mess with CL tools and things like that. Not sure how hard it would be to implement this with what is in Hunspell now.\nAlso one thing is some CL tool used against some dictionary files and other thing can be using Lucene code on dictionary loaded into memory by Lucene. If there are issues in the code these two approaches can give different results (yes, they should be the same...)\n\nOn this one i honestly do disagree. I dont mean to sound rude, but if you are smart enough to make a custom dictionary, I don't think I need to baby such users around and make them comfortable by duplicating command line tools they can install themselves in java  The tools provided by hunspell are the best here, and if someone is making a custom dictionary they already need to be digging into these tools/docs to know what they are doing. I don't see a value in duplicating this stuff and providing morphological generation and other super-advanced esoteric stuff, when there are more basic things needed (like decomposition).  As far as if things differ, then those are bugs that should be fixed... "
        },
        {
            "author": "Lukas Vlcek",
            "id": "comment-13917384",
            "date": "2014-03-02T12:31:11+0000",
            "content": "Hi Robert,\n\nI created a new ticket LUCENE-5484 for distinct recursion levels per pre/suffix rules.\n\n\nThere may not be, but its about where the responsibility should be. Its more than the first token in sentences: named entities etc are involved too. If you want to get this right, yes, you need a more sophisticated analysis chain! That being said, I'm not against your 80/20 heuristic, I'm just not sure how 80/20 it is \n\nI fully understand YPOW. The question of responsibility is important. But if I consider that workaround like lowercasing for optional second pass could be easier than telling user to setup complicated analysis chain (or employ external system) then I believe it might make sense to do a qualified exception.  Heh...\n\nBut seriously. How about if I open a ticket for this to allow to fly this idea around. WDYT?\n\nI would like to try to implement it as well (if no one else will do it) though I will not get to it soon. As for the 80/20 aspect the good thing about this feature is that it could be measured (precision, recall, ...). And may be only implementing this feature cold tell us if it is useful or not.\n\n\n[...] if you are smart enough to make a custom dictionary, I don't think I need to baby such users around and make them comfortable by duplicating command line tools they can install themselves in java [...]\n\nShort: I agree\n\nLong: Creating a new dictionary is very hard. It is for wizards... but the thing here Robert is that creating a new dictionary from scratch is something completely different than extending existing dictionary. At least for average users (like me), they probably can hardly do the former but can relatively easy do the later. The former involves creating the affix rules, the later means using given affix rules and build on top of it.\n\nWhen I was trying to extend existing dictionary then in fact I had to do the following:\n1) identify words that were missing in the dict file (or files)\n2) assigning some of existing rules to each of them\n3) verify #2 was done right\n\nAs for 1) that is easy (the only trick when creating a new file with missing words is to stick to encoding defined in aff file)\nAs for 2) that is harder but in my case I was building on top of relatively large dictionary so I could bet on the fact that language morphology has been already cover well in affix rules (so I assumed I was not introducing words with new/unique morphology to the dictionary). So in fact instead of trying to understand the rules (see my note about this below) I searched for words that should have similar morphology features and used their rules (for example if I were to add a word \"fail\" I would search for \"sail\" and use the same rules).\nAs for 3) this in fact means expanding token in root form according to all possible valid rules and check it all makes sense. As you pointed out, there are CL tools for this but I simply did not want to learn them (I did not feel like a wizard). And the good question is if Lucene should be able to provide API that could be used for this task. In the end of the day Lucene is said to be a IR library and has language analysis capabilities, so why not? But I am fine to leave this feature out now. Just wanted to explain some of my motivations for this feature.\n\n\nNote:\nAs for understanding the affix rules - this is probably complex topic and I did not have a time to dig deep enough to say anything qualified about it yet. However, as far as I understand various *spell systems have various limitations. For example in case of the Czech dictionary, it is an ispell which allowed only limited number of affix rules (that is what I understood from conversation with an author of Czech dictionary). Which means that if the number of rules is limited then what we see being shipped in aff file is more a result of some preprocessing that takes set of rules that are understandable to human and produces more compact set that might not be easily understood by humans.\n\nBut this is unrelated topic, except that it can illustrate the situation of average user who just want to add some new words into existing dictionary and do not have the capacity to become an expert on ispell (or myspell, or aspell, ... or ... you name it). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13917408",
            "date": "2014-03-02T13:25:43+0000",
            "content": "\nI fully understand YPOW. The question of responsibility is important. But if I consider that workaround like lowercasing for optional second pass could be easier than telling user to setup complicated analysis chain (or employ external system) then I believe it might make sense to do a qualified exception.\n\nThis responsibility is really important though. Maybe you should break away from the czech dictionary and look at the others before you decide that its \"easiest\" here. For example the german dictionary has lots of complex casing rules encoded in the affix file itself for decompounding purposes. This feature already is plenty complicated. If you can do ANYTHING and I mean ANYTHING outside of it in any way, we should keep it out of here.\n\n\nAs you pointed out, there are CL tools for this but I simply did not want to learn them (I did not feel like a wizard). And the good question is if Lucene should be able to provide API that could be used for this task. In the end of the day Lucene is said to be a IR library and has language analysis capabilities, so why not? But I am fine to leave this feature out now. Just wanted to explain some of my motivations for this feature.\n\nBecause its an IR library, not a tool for building lexical resources. We just dont have the resources to \"compete\" with that, we don't have people that need it, and why waste our time when there are perfectly good tools available? I don't know why you refuse to \"learn\" the hunspell tools, they are trivial to learn!\n\nBesides the commandline tools, quick searches reveal GUI tools too, such as http://marcoagpinto.cidadevirtual.pt/proofingtoolgui.html. Quote from the page: \"My tool is so intuitive that even a 6-year-old kid can use it.\"\n\nI don't think such work should be duplicated inside the apache lucene project. "
        },
        {
            "author": "Lukas Vlcek",
            "id": "comment-13918508",
            "date": "2014-03-03T19:48:44+0000",
            "content": "Good points, I did not know that about the german dictionary. In this perspective my suggestion sounds really hack-ish and should be left out.\n\n[...] even a 6-year-old kid can use it.\nI am always amazed about what kids these days can achieve...\n\nThanks for your time Robert! "
        },
        {
            "author": "Torsten Krah",
            "id": "comment-13934860",
            "date": "2014-03-14T10:51:10+0000",
            "content": "Just for interest - are multiple dictionaries still supported with this change (after reading all comments its not clear if it was dropped or not)?\nThis option is nice to have, because you can make local modifications and can update the main dictionary from upstream (libreoffice etc.) without need for merging or something.\nIf not - is there already a ticket to get this working again? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13982557",
            "date": "2014-04-27T23:25:43+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}