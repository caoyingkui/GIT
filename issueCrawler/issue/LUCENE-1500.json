{
    "id": "LUCENE-1500",
    "title": "Highlighter throws StringIndexOutOfBoundsException",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Using the canonical Solr example (ant run-example) I added this document (using exampledocs/post.sh):\n\n<add><doc>\n  <field name=\"id\">Test for Highlighting StringIndexOutOfBoundsExcdption</field>\n  <field name=\"name\">Some Name</field>\n  <field name=\"manu\">Acme, Inc.</field>\n  <field name=\"features\">Description of the features, mentioning various things</field>\n  <field name=\"features\">Features also is multivalued</field>\n  <field name=\"popularity\">6</field>\n  <field name=\"inStock\">true</field>\n</doc></add>\n\nand then the URL http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception.\n\nI have a patch.  I don't know if it is completely correct, but it avoids this exception.",
    "attachments": {
        "Lucene-1500-NewException.patch": "https://issues.apache.org/jira/secure/attachment/12401778/Lucene-1500-NewException.patch",
        "LUCENE-1500.patch": "https://issues.apache.org/jira/secure/attachment/12400890/LUCENE-1500.patch",
        "patch.txt": "https://issues.apache.org/jira/secure/attachment/12396645/patch.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-12-23T03:34:00+0000",
            "content": "I think the bug in Solr, not Lucene. Please see SOLR-925, a patch is there. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12658735"
        },
        {
            "date": "2008-12-23T04:08:46+0000",
            "content": "That makes sense, thank-you Koji.\n\nI think it may be worth incorporating this patch anyway, since it is a small change which makes the method more robust.  It just eliminates the possibility of running off the end of the text string. ",
            "author": "David Bowen",
            "id": "comment-12658740"
        },
        {
            "date": "2009-02-24T21:35:21+0000",
            "content": "I have run into this issue over the last couple days.  Also using Solr, but the error is triggered by content that has multi-byte characters (such as German).\n\nIt seems that somewhere Lucene is counting bytes instead of characters, so each substring the highlighter tries to select is offset further forward in the string being matched.\n\nhere's an example trying to highlight the string 'Drupaltalk' with \"strong\" tags\n\n<p class=\"search-snippet\">\n Community ist - und dieses Portal Dr<strong>upaltalk.d</strong>e samt seinem schon eifrigen Benutzer- und G\u00e4stezulauf ( ... \n nter \"Dru<strong>paltalk001</strong>\" k\u00f6nnt Ihr die erste Konferenz noch mal nachh\u00f6ren und erfahren, wie Selbstorganisation in der Drupal Szene funktioniert.  \n \"Dru<strong>paltalk002</strong>\" ist dann der Talk vom Dienstag zum Thema \"Drupal Al</p>\n\n\n\nSo the attached patch would probably avoid the exception (and is a good idea) but would not fix the bug I'm seeing. ",
            "author": "Peter Wolanin",
            "id": "comment-12676421"
        },
        {
            "date": "2009-02-24T22:06:27+0000",
            "content": "Actually, looking at the Lucene source and the trace:\n\n\njava.lang.StringIndexOutOfBoundsException: String index out of range: 2822\n\tat java.lang.String.substring(String.java:1765)\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:274)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:313)\n\tat org.apache.solr.handler.component.HighlightComponent.process(HighlightComponent.java:84)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n       ...\n\n\n\nI see now that getBestTextFragments() takes in a token stream - and each token in this steam already has start/end positions set.  So, this patch would mitigate the exception, but looks like the real bug is in Solr, or perhaps elsewhere in Lucene where the token stream is constructed. ",
            "author": "Peter Wolanin",
            "id": "comment-12676426"
        },
        {
            "date": "2009-02-24T23:20:51+0000",
            "content": "Actually - the initial patch does not avoid the exception I'm seeing, since the start of the token is ok, but the end is beyond the string's end.  Here is a slightly enhanced version that checks both the start and end of the token. ",
            "author": "Peter Wolanin",
            "id": "comment-12676449"
        },
        {
            "date": "2009-02-25T10:56:37+0000",
            "content": "I think the defensive coding (the current patch) makes sense, to avoid out-of-bounds substring call.\n\nBut what is the root cause here?  It seems likely the analyzer chain is not producing the right (original) start/end offset in the tokens?  What analyzer chain are you using? ",
            "author": "Michael McCandless",
            "id": "comment-12676611"
        },
        {
            "date": "2009-02-25T12:46:32+0000",
            "content": "Hmmm. I'm not so sure that this \"defensive coding\" patch is the right thing to do here. \n\nOne could argue that it is obscuring an error condition further upstream (as you suggest, Mike - a dodgy analyzer). Commiting this will only make these errors harder to detect e.g. we'd get forum posts saying \"why doesn't my term get highlighted?\"\n\nPerhaps we can turn this around and ask \"under what conditions is it acceptable to provide a TokenStream with tokens whose offsets exceed the length of the text provided?\". \nNot sure I see a justifiable case for supporting that as a legitimate scenario and I would prefer the reporting of an error in this case.\n\n ",
            "author": "Mark Harwood",
            "id": "comment-12676633"
        },
        {
            "date": "2009-02-25T13:36:33+0000",
            "content": "Yes - this patch is not a fix - but a work-around.\n\nThe root cause is clearly somewhere in the code generating the token stream - tokens seem to be getting positions in bytes rather than characters.\n\nDefaultSolrHighlighter.java has this code:\n\n\n\n\nimport org.apache.lucene.search.highlight.TokenSources;\n\n...\n\n            // create TokenStream\n            try {\n              // attempt term vectors\n              if( tots == null )\n                tots = new TermOffsetsTokenStream( TokenSources.getTokenStream(searcher.getReader(), docId, fieldName) );\n              tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n            }\n            catch (IllegalArgumentException e) {\n              // fall back to anaylzer\n              tstream = new TokenOrderingFilter(schema.getAnalyzer().tokenStream(fieldName, new StringReader(docTexts[j])), 10);\n            }\n\n\n ",
            "author": "Peter Wolanin",
            "id": "comment-12676648"
        },
        {
            "date": "2009-02-25T15:44:27+0000",
            "content": "Can you post your document and schema so that others can reproduce your problem? ",
            "author": "Koji Sekiguchi",
            "id": "comment-12676682"
        },
        {
            "date": "2009-02-25T17:37:49+0000",
            "content": "Perhaps we can turn this around and ask \"under what conditions is it acceptable to provide a TokenStream with tokens whose offsets exceed the length of the text provided?\". Not sure I see a justifiable case for supporting that as a legitimate scenario and I would prefer the reporting of an error in this case.\n\n+ 1 to reporting an error ... but StringIndexOutOfounds is not a helpful error.  Code should certainly check the offsets before calling substring and throw a specific exception identifying which token has an invalid offset ",
            "author": "Hoss Man",
            "id": "comment-12676714"
        },
        {
            "date": "2009-02-25T18:15:27+0000",
            "content": "\nThe thing is, since it's an unchecked exception, you could be well\ninto production, serving searches, etc, when suddenly unexpectedly\nthis exception kills the entire search request.  It's brittle.\n\n(vs highlighting the wrong parts but still providing search results to\nthe user \u2013 graceful degradation).\n\nSo I'm still torn on \"brittle\" (what we have now) vs \"graceful\"\n(current \"workaround\" patch) handling.  For exceptions that can strike\narbitrarily deep into production/searching, I usually lean towards\nbeing \"graceful\".\n\nOr... maybe we could introduce a new exception to call this case out\n(IncorrectTokenOffsetsException or something?), but: we make it a\nchecked exception so at compile time you need to handle it?\n\n(I still don't like that we only catch the error intermittantly... but there's\nnothing we could really do about that, so javadocs should spell that\nout). ",
            "author": "Michael McCandless",
            "id": "comment-12676728"
        },
        {
            "date": "2009-02-25T19:23:56+0000",
            "content": "So to be consistent, where else in Lucene might an \"IncorrectTokenOffsetsException\" be a possibility - IndexWriter.addDocument(..)? ",
            "author": "Mark Harwood",
            "id": "comment-12676745"
        },
        {
            "date": "2009-02-25T19:56:01+0000",
            "content": "So to be consistent, where else in Lucene might an \"IncorrectTokenOffsetsException\" be a possibility - IndexWriter.addDocument(..)?\n\nGood question!\n\nI don't think we can throw it from addDocument, because we cannot\nassume/assert that the int startOffset & endOffset are in fact\ncharacter offsets into the String/Reader we had been given?\n\nTechnically, to Lucene the start/end offsets are somewhat opaque (I\nthink?).  It's not until you actually do something with them (eg, call\nthe highlighter) that you are saying \"these are really supposed to be\ncharacter offsets into the specific text I just provided you\". ",
            "author": "Michael McCandless",
            "id": "comment-12676754"
        },
        {
            "date": "2009-02-27T13:57:45+0000",
            "content": "Mark, do you want/have time to take this one?  I'd like to resolve it before 2.4.1 if we can... ",
            "author": "Michael McCandless",
            "id": "comment-12677372"
        },
        {
            "date": "2009-02-27T20:10:52+0000",
            "content": "OK - choices are:\n\n1) Throw a RuntimeException with a more useful diagnostic message\n2) Throw a new checked Exception (introducing possible compile errors in existing client code)\n3) Check for the error condition and ignore (as done in the current patch)\n\nThis feels to me like one of those \"there's something seriously wrong with the codebase\" problems rather than an invalid bit of data or user input which is external to the system so my personal preference is to lean towards 1). \n ",
            "author": "Mark Harwood",
            "id": "comment-12677507"
        },
        {
            "date": "2009-02-27T20:34:45+0000",
            "content": "Well, this patch does not (obviously) solve the real bug.  Is it possible to combine #1 and #3, but possibly revert #3 later when we solve the real bug in the highlighter code?   ",
            "author": "Peter Wolanin",
            "id": "comment-12677517"
        },
        {
            "date": "2009-02-27T20:54:45+0000",
            "content": "\nThis feels to me like one of those \"there's something seriously wrong with the codebase\" problems\n\nMy guess is it can happen quite easily and not be so obvious until the\n\"right\" text goes through the analyzer.  If Solr's analysis (for\nhighlighting multi-valued fields) can hit this my guess is it's (wrong\noffsets coming out of analyzer) not uncommon...\n\nI'd still prefer #2, but I don't feel super strongly about it, so if\nthere's no consensus otherwise, it's fine with me to go forward\nwith #1. ",
            "author": "Michael McCandless",
            "id": "comment-12677524"
        },
        {
            "date": "2009-02-27T21:11:12+0000",
            "content": "The bug we are seeing now happens on pretty much every document that contains multi-byte characters, but only sometime was it going past the end of the full string and hitting the exception.   With the patch, the bug is still very evident, it jsut prevents the exception.  I's a serious flaw in the highlighter - maybe using some only non-utf-8 aware method to calculate string lengths? ",
            "author": "Peter Wolanin",
            "id": "comment-12677531"
        },
        {
            "date": "2009-02-27T21:19:19+0000",
            "content": "I thought the bug was in the analyzer (producing out-of-bounds token offsets), not highlighter?\n\nThe original Solr bug (SOLR-925), which I think was the analyzer producing invalid offsets, is now fixed.  Peter are you using Solr, or are you seeing this in a different context? ",
            "author": "Michael McCandless",
            "id": "comment-12677535"
        },
        {
            "date": "2009-02-27T21:27:45+0000",
            "content": "I am using Solr, but with a single value field.  I'm using the current Solr build (includes the fix), so the bug I'm describing, which triggers the same exception as the prior Solr bug did, is still present and unrelated to SOLR-925.\n\nThe extent of my tracing suggests it's coming when the token stream is generated, which looks to be part of the lucene highlighter:  org.apache.lucene.search.highlight.TokenSources\n ",
            "author": "Peter Wolanin",
            "id": "comment-12677540"
        },
        {
            "date": "2009-02-27T21:49:24+0000",
            "content": "The extent of my tracing suggests it's coming when the token stream is generated, which looks to be part of the lucene highlighter: org.apache.lucene.search.highlight.TokenSources\n\nwith my limited knowledge of solr highlighting, that really doesn't sound right.\n\nIf the field has TermVectors, then the TokenStream used comes from there, otherwise it coems from analyzing the stored field value \u2013 either way the analyzer configured in Solr should be setting the offset values, correct? \n\nhence my question on the solr thread where this first came up...\nhttp://www.nabble.com/Error-with-highlighter-and-UTF-8-chars--to22156161.html#a22207917\n\nso what does the analysis screen tell you about each token produced with that input text given your configuration?  in verbose mode it will show the start/end offsets for every token, so it should be fairly easy to identify where the bug is.  ",
            "author": "Hoss Man",
            "id": "comment-12677549"
        },
        {
            "date": "2009-02-27T21:52:40+0000",
            "content": "But that class either uses TermVectors stored in the index, or re-analyzes using the analyzer passed to it, I think?  Are you using TermVectors? ",
            "author": "Michael McCandless",
            "id": "comment-12677551"
        },
        {
            "date": "2009-02-27T22:10:51+0000",
            "content": "I'm still trying to get a handle on how these pieces fit together., so sorry if I've jumped to the wrong conclusion.  If the analyzer is where the offsets are calculated, then that sounds like the place to look.\n\nThe field does use term vectors.  The field uses this type from the Solr schema:\n\n\n    <fieldType name=\"text\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n\n\n\nThe full schema is\nhttp://cvs.drupal.org/viewvc.py/drupal/contributions/modules/apachesolr/schema.xml?revision=1.1.2.1.2.30&pathrev=DRUPAL-6--1\n\nthe field is\n\n<field name=\"body\" type=\"text\" indexed=\"true\" stored=\"true\" termVectors=\"true\"/>\n\n\n\nin case it's relevant, the solrconfig is:\nhttp://cvs.drupal.org/viewvc.py/drupal/contributions/modules/apachesolr/solrconfig.xml?revision=1.1.2.15&pathrev=DRUPAL-6--1 ",
            "author": "Peter Wolanin",
            "id": "comment-12677561"
        },
        {
            "date": "2009-02-28T00:27:12+0000",
            "content": "Peter: i tried some experiments with teh analyzer specified in the schema+fieldType you referenced, and i couldn't reproduce any problem.\n\nI suggest you open a new Jira issue against Solr and attach some sort of reproducible test (either a junit test OR a patch against the example configs + an example doc to index and an example query that either triggers an error or highlights an incorrect substring) so people have a starting point for trying to figure where the incorrect offsets are coming from.\n\nLUCENE-1500 can/should stay open, but should focus specifically on the issue of what Highlighter's behavior should be when dealing with offsets that go past the end of the string (there seems to be consensus that it should do something different then IndexOutOfBounds, it just doesn't seem to be clear what the new behavior should be) ",
            "author": "Hoss Man",
            "id": "comment-12677608"
        },
        {
            "date": "2009-02-28T01:49:31+0000",
            "content": "Ah, it occurs to me that we first saw this bug recently - and it seems likely it was only after starting to use :\n\n <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"mapping-ISOLatin1Accent.txt\"/>\n\n\n\nfor that field type.  I will investigate more and post a SOLR issue. ",
            "author": "Peter Wolanin",
            "id": "comment-12677620"
        },
        {
            "date": "2009-02-28T02:08:02+0000",
            "content": "for that field type. I will investigate more and post a SOLR issue.\n\nIf you use charFilter, your Tokenizer must be a \"CharStreamAware\" Tokenizer. We have only two CharStreamAwareTokenizers - CharStreamAwareWhitespaceTokenizer and CharStreamAwareCJKTokenizer in Solr. Please consider using CharStreamAwareWhitespaceTokenizer in your case. To know what CharStreamAware Tokenizer is, see SOLR-822 . ",
            "author": "Koji Sekiguchi",
            "id": "comment-12677626"
        },
        {
            "date": "2009-02-28T02:35:03+0000",
            "content": "Koji - thanks - I was aware that not all worked with the mapping filter, but I was apparently misinformed since I was told that the \"solr.HTMLStripWhitespaceTokenizerFactory\" was also suitable for CharFilter.  Indeed your e-mail thread linked from SOLR-822 describes exactly the problem I have:\n\nAs you can see, if you use CharFilter, Token offsets could be incorrect because CharFilters may convert 1 char to 2 chars or the other way around.\n\nIn the thread you suggest that this API could be aded to lucene java? ",
            "author": "Peter Wolanin",
            "id": "comment-12677629"
        },
        {
            "date": "2009-02-28T04:04:16+0000",
            "content": "Peter, thank you.\n\nIn the thread you suggest that this API could be aded to lucene java?\n\nI've proposed CharFilter feature for Lucene in LUCENE-1466 . If you like it, please vote for it.  ",
            "author": "Koji Sekiguchi",
            "id": "comment-12677639"
        },
        {
            "date": "2009-03-01T23:52:00+0000",
            "content": "Attached a patch with new checked exception.\nThis will have a knock-on effect on all Highlighter client code (Solr?) as it introduces a new checked exception that must be handled. ",
            "author": "Mark Harwood",
            "id": "comment-12677865"
        },
        {
            "date": "2009-03-02T10:21:32+0000",
            "content": "Patch looks great, thanks Mark!\n\nDo you also need to check that startOffset is within bounds?  EG if both start & end offset == length of input text, what does substring do?\n\nIf endOffset is < startOffset (seriously invalid token offsets) what happens?\n\nMaybe just add javadoc to the @throws  explaining why this new exception would be thrown? ",
            "author": "Michael McCandless",
            "id": "comment-12677948"
        },
        {
            "date": "2009-03-02T11:08:54+0000",
            "content": "My thoughts were that this exception solely traps inconsistencies with Tokens in relation to a particular provided chunk of text.\n\nI think internal inconsistencies within a Token (e.g. endOffset <startOffset) should ideally be handled by Token (throwing something like an IllegalArgumentException in it's constructor).\nI guess an open question there is can startOffset=endOffset in a Token? Either way, String.substring simply returns an empty string so I think that's probably OK in highlighter. ",
            "author": "Mark Harwood",
            "id": "comment-12677956"
        },
        {
            "date": "2009-03-02T11:39:55+0000",
            "content": "The problem is, this hits a StringIndexOutOfBoundsException:\n\n\nString s = \"abcd\";\ns.substring(3, 2);\n\n\n\n\njava.lang.StringIndexOutOfBoundsException: String index out of range: -1\n    at java.lang.String.substring(String.java:1938)\n\n\n\nI think highlighter should guard against this? ",
            "author": "Michael McCandless",
            "id": "comment-12677963"
        },
        {
            "date": "2009-03-02T11:41:17+0000",
            "content": "I think given the addition of a checked exception to the Highlighter API, we should not back-port this to 2.4.1. ",
            "author": "Michael McCandless",
            "id": "comment-12677964"
        },
        {
            "date": "2009-03-02T11:55:23+0000",
            "content": "Isn't your example predicated on being given an invalid Token with end<start?\n\nWhat did you think of my suggestion to fix this problem at it's source - i.e. Token should never be in a state with end<start in the first place?\n\nAcheiving this goal is complicated by the fact that offsets are not only set in the constructor - there are independent set methods for start and end offsets which can be called in any order.\nOne solution would be to deprecate Token.setStartOffset and Token.endOffset and replacing with a Token.setExtent(int startOffset, int endOffset) with the appropriate checks.\n\n\n ",
            "author": "Mark Harwood",
            "id": "comment-12677968"
        },
        {
            "date": "2009-03-02T13:38:38+0000",
            "content": "What did you think of my suggestion to fix this problem at it's source - i.e. Token should never be in a state with end<start in the first place?\n\nI'm wary of this, for the same reason as cited above.  I think the\nstart/end offsets are rather opaque to Lucene, and I would lean\ntowards asserting as little as possible about them during indexing.\n\n(Not to mention the challenges, as you enumerated, of actually doing\nthis checking with the current API).\n\nI think at point-of-use (highlighting) we should assert the offsets\nare valid, for highlighting. ",
            "author": "Michael McCandless",
            "id": "comment-12677983"
        },
        {
            "date": "2009-03-02T14:19:12+0000",
            "content": "I struggle to see why endOffset<startOffset should ever be acceptable but also share your concerns about the disruption of changing the Token API to enforce this.\n\nSo, I'll add code to the patch to check for bad startOffsets too. If we had more \"points of use\" for Token offsets outside of highlighting I'd be more concerned, but things being the way they are this seems like the most pragmatic option. ",
            "author": "Mark Harwood",
            "id": "comment-12677991"
        },
        {
            "date": "2009-03-09T10:36:24+0000",
            "content": "Handing this one off to you Mark! ",
            "author": "Michael McCandless",
            "id": "comment-12680121"
        },
        {
            "date": "2009-03-09T10:48:18+0000",
            "content": "Will submit a new patch tonight. ",
            "author": "Mark Harwood",
            "id": "comment-12680125"
        },
        {
            "date": "2009-03-09T21:53:21+0000",
            "content": "Added support for testing both Token start or end offset >text.length.\n\nAdded javadoc comments for new exception ",
            "author": "Mark Harwood",
            "id": "comment-12680292"
        },
        {
            "date": "2009-03-10T00:07:04+0000",
            "content": "The InvalidTokenOffsetsException.java just needs a copyright header; otherwise patch looks good! ",
            "author": "Michael McCandless",
            "id": "comment-12680330"
        },
        {
            "date": "2009-03-13T19:23:24+0000",
            "content": "With updated Apache license header.\n\nI'll commit soon if no objections ",
            "author": "Mark Harwood",
            "id": "comment-12681868"
        },
        {
            "date": "2009-03-25T23:06:53+0000",
            "content": "Committed in revision: 758460  ",
            "author": "Mark Harwood",
            "id": "comment-12689267"
        },
        {
            "date": "2009-04-29T07:43:38+0000",
            "content": "Hi! with the risk of being a noob, I just have to ask...\nThe patch Lucene-1500-NewException.patch, is it incorporated in the Lucene nightly builds as of 2009-04-27?\n\nI am now running my code against the nightly builds of Lucene20090427, and those libs give me the new error \"InvalidTokenOffsetsException\" at the exact same spot as the 2.4.1 libs return a StringIndexOutOfBounds exception. Should it not solve the issue?\n\nIf not, could you maybe suggest a client coding which circumvents it?\n\nWhen using the Tika test-documents with search query \"tika\"\nThe resulting 9 documents return 5 occurences of the exception. \n\nPS: I'm not running Solr, just POJO with lucene and Tika\n\n ",
            "author": "Lars Olson",
            "id": "comment-12704022"
        },
        {
            "date": "2009-04-29T09:22:00+0000",
            "content": "Should it not solve the issue?\n\nThe change to highlighter (which is committed, on trunk) is simply to throw a more meaningful, checked exception (InvalidTokenOffsetsException) when the Token has out-of-bounds offsets.  On upgrading to this, this is an exception your code needs to handle.\n\nBut this issue doesn't fix the root causes that actually produce Tokens with incorrect offsets \u2013 that's very app specific.  EG, the particular case that led to this issue was SOLR-925 (which is now fixed).\n\nAre you actually hitting this exception at runtime, or noting that you need to handle it at compilation time? ",
            "author": "Michael McCandless",
            "id": "comment-12704047"
        }
    ]
}