{
    "id": "LUCENE-973",
    "title": "Token of  \"\" returns in CJKTokenizer + new TestCJKTokenizer",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.3",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The \"\" string returns as Token in the boundary of two byte character and one byte character. \n\nThere is no problem in CJKAnalyzer. \nWhen CJKTokenizer is used with the unit, it becomes a problem. (Use it with \nSolr etc.)",
    "attachments": {
        "CJKTokenizer20070807.patch": "https://issues.apache.org/jira/secure/attachment/12363325/CJKTokenizer20070807.patch",
        "with-patch.jpg": "https://issues.apache.org/jira/secure/attachment/12387210/with-patch.jpg",
        "without-patch.jpg": "https://issues.apache.org/jira/secure/attachment/12387209/without-patch.jpg",
        "LUCENE-973.patch": "https://issues.apache.org/jira/secure/attachment/12390960/LUCENE-973.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-08-07T13:32:09+0000",
            "content": "patch attached. ",
            "author": "Toru Matsuzawa",
            "id": "comment-12518155"
        },
        {
            "date": "2008-02-17T09:13:53+0000",
            "content": "I don't fully understand the problem report.  What problem are you seeing and does the unit test you included in the patch (thank you!) cover that case? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12569681"
        },
        {
            "date": "2008-02-17T15:09:49+0000",
            "content": "The current CJKTokenizer returns a redundant empty string at the end of token stream when it tokenizes CJK characters.\n\nString str = \"C1C2C3\";\nTokenizer tokenizer = new CJKTokenizer( new StringReader( str ) );\nfor( Token token = tokenizer.next(); token != null; token = tokenizer.next() )\n    System.out.println( \"token = \\\"\" + token.termText() + \"\\\"\" );\n\nThis should be:\n\ntoken = \"C1C2\"\ntoken = \"C2C3\"\n\nbut the current CJKTokenizer outputs:\n\ntoken = \"C1C2\"\ntoken = \"C2C3\"\ntoken = \"\"\n\nThe attached test case reproduce this problem and the patch solves it. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12569706"
        },
        {
            "date": "2008-07-30T16:36:49+0000",
            "content": "I attached Solr analysis screen to show the problem and the result of the patch. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12618428"
        },
        {
            "date": "2008-07-30T18:38:30+0000",
            "content": "Hi Koji,\n\nThe test class in your patch is a nice addition.\n\nThere is no problem in CJKAnalyzer. \n\nThe only reason that CJKAnalyzer doesn't have this problem is that the empty string is one of the stopwords it filters out from CJKTokenizer's output!\n\nThe following part of your patch appears to address a problem that you haven't covered in your comments - is this so?  If it is a problem separate from the empty-string issue, can you describe the effects of this change?:\n\n\n@@ -175,8 +175,9 @@\n                             length = 0;\n                             preIsTokened = false;\n \n-                            break;\n+                            continue;\n                         } else {\n+                            tokenType = \"double\";\n                             break;\n                         }\n                     }\n\n\n\n\nThe other part of your patch reads:\n\n\n@@ -236,8 +237,13 @@\n             }\n         }\n \n-        return new Token(new String(buffer, 0, length), start, start + length,\n+        String tokenString = new String(buffer, 0, length) ;\n+        if( dataLen == -1 && \"\".equals(tokenString)) {\n+          return null ;\n+        } else {\n+          return new Token(tokenString, start, start + length,\n                          tokenType\n                         );\n+        }\n\n\n\nWouldn't it be simpler/clearer to test length for zero instead of constructing a String and testing it for equality with the empty string?:\n\n\nif (length > 0) {\n  String tokenString = new String(buffer, 0, length);\n  return new Token(tokenString, start, start + length, tokenType);\n} else {\n  return null;\n}\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12618462"
        },
        {
            "date": "2008-07-30T22:27:56+0000",
            "content": "Hi Steven,\n\nThe test class in your patch is a nice addition.\n\nThanks, but the attached patch was written by Toru. \nMatsuzawa-san, can you follow up the comments? ",
            "author": "Koji Sekiguchi",
            "id": "comment-12618534"
        },
        {
            "date": "2008-07-30T22:37:17+0000",
            "content": "Sorry Toru, I saw Koji's two most recent comments on the issue and assumed that he was the reporter, and didn't scroll up and check . ",
            "author": "Steve Rowe",
            "id": "comment-12618540"
        },
        {
            "date": "2008-09-25T02:55:11+0000",
            "content": "Thank you for Sekiguchi-san and Steven comment. \nI am sorry for slow comment . \n\n\nThe following part of your patch appears to address a problem that you haven't covered in your comments - is this so? If it is a problem separate from the empty-string issue, can you describe the effects of this change?:\nIn current CJKTokenizer, \"C3\" becomes \"Single\" of non-ascii as shown by the following examples. \n\n \n// C1C2C3 is non-ascii\nString str = \"C1C2abcC3def\" ;\nTokenizer tokenizer = new CJKTokenizer( new StringReader( str ) );\nfor( Token token = tokenizer.next(); token != null; token = tokenizer.next() )\nSystem.out.println( \"token=\\\"\" + token.termText() + \"\\\"\" + \" type=\\\"\"+ token.type() + \"\\\"\");\n\n \ncurrent CJKTokenizer outputs:\n\n \ntoken=\"C1C2\" type=\"double\"\ntoken=\"\" type=\"single\"\ntoken=\"abc\" type=\"single\"\ntoken=\"C3\" type=\"single\"\ntoken=\"def\" type=\"single\"\n\n \napplying patch:\n\n \ntoken=\"C1C2\" type=\"double\"\ntoken=\"C2\" type=\"double\"\ntoken=\"abc\" type=\"single\"\ntoken=\"C3\" type=\"double\"\ntoken=\"def\" type=\"single\"\n\n \n\n\nWouldn't it be simpler/clearer to test length for zero instead of constructing a String and testing it for equality with the empty string?:\nI think that your correction is better.  ",
            "author": "Toru Matsuzawa",
            "id": "comment-12634363"
        },
        {
            "date": "2008-09-25T21:45:10+0000",
            "content": "I think that your correction is better. \n\nCool, the LUCENE-973.patch file incorporates this change - I had to add a recursion in order to handle this properly.\n\nAlso:\n\n\n\tAdded a test for a single CJK character stream\n\tSync'd with the current trunk (mostly reusable Token changes)\n\tCleaned up formatting a little\n\tSwitched token types to be constant ints instead of hard-coded strings\n\n\n\nNote that this patch is nice not just because of the bug it fixes, but also (mainly?) because of the unit tests it adds \u2013 currently CJKTokenizer has no tests. ",
            "author": "Steve Rowe",
            "id": "comment-12634645"
        },
        {
            "date": "2008-10-20T02:46:22+0000",
            "content": "A patch attached which uses unicode character representations ('\\uNNNN' style) to avoid Japanese characters in the test code. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12640899"
        },
        {
            "date": "2009-06-15T16:45:09+0000",
            "content": "You guys looking for this for 2.9?\n\nIf so, any volunteers? If I assign myself any more, I won't likely get to them all. ",
            "author": "Mark Miller",
            "id": "comment-12719635"
        },
        {
            "date": "2009-06-15T21:11:00+0000",
            "content": "+1 from me for inclusion in 2.9.\n\nMark, as you wrote a couple of hours ago on java-dev, in response to Robert Muir's complaint about the lack of tests in contrib:\n\nwe should probably push for tests or write them before committing more often.\n\nHere's a chance to improve the situation: this issue adds a test to a contrib module where there currently are none! ",
            "author": "Steve Rowe",
            "id": "comment-12719776"
        },
        {
            "date": "2009-06-15T21:19:00+0000",
            "content": "very nice. although it might be a tad trickier to convert to the new API, anything with tests is easier!\n\nin other words, i have the existing cjktokenizer converted, but whose to say I did it right  ",
            "author": "Robert Muir",
            "id": "comment-12719781"
        },
        {
            "date": "2009-06-16T14:52:39+0000",
            "content": "So the latest patch is ready to go in? I guess I could take this one then. ",
            "author": "Mark Miller",
            "id": "comment-12720169"
        },
        {
            "date": "2009-06-16T15:01:13+0000",
            "content": "I'll take it Mark!  Fixes a bug and adds tests for CJKAnalyzer/Tokenizer where there were none before.... big step forward! ",
            "author": "Michael McCandless",
            "id": "comment-12720177"
        },
        {
            "date": "2009-06-16T15:57:25+0000",
            "content": "Does anyone know if the added recursive call to next() is \"bounded\"?  Ie, is there any way that the next() method could hit length==0 an unbounded number of times in a row (for some \"unlucky\" input text)?  I don't want to run out of stack... ",
            "author": "Michael McCandless",
            "id": "comment-12720200"
        },
        {
            "date": "2009-06-16T16:02:29+0000",
            "content": "sounds like another good test case, add a few thousand 0-length tokens to the stream and see what happens... ",
            "author": "Robert Muir",
            "id": "comment-12720204"
        },
        {
            "date": "2009-06-16T16:07:41+0000",
            "content": "Well, my question is: is there any input text that would cause an arbitrary number of such 0-length tokens in a row?\n\nEg the original cause of that was just at the boundary of two byte character and one byte character... so if that's the only case that hits 0-length token, then we are OK.  But if there are other cases, such that one could chain any number of such tokens in sequence, we're not, and we have to translate recursion into iteration. ",
            "author": "Michael McCandless",
            "id": "comment-12720207"
        },
        {
            "date": "2009-06-16T16:16:04+0000",
            "content": "Michael i don't see anything obvious, but a test case for two byte char + one byte char + [tons of english text and/or whitespace and/or punctuation] would make me feel better ",
            "author": "Robert Muir",
            "id": "comment-12720212"
        },
        {
            "date": "2009-06-16T16:25:25+0000",
            "content": "Or... how about we just switch to iteration not recursion?  I attached a patch w/ that change... ",
            "author": "Michael McCandless",
            "id": "comment-12720220"
        },
        {
            "date": "2009-06-16T16:32:35+0000",
            "content": "Or... how about we just switch to iteration not recursion? I attached a patch w/ that change... \n\n+1 from me - thanks, Mike! ",
            "author": "Steve Rowe",
            "id": "comment-12720224"
        },
        {
            "date": "2009-06-16T16:33:53+0000",
            "content": "michael, thanks.  ",
            "author": "Robert Muir",
            "id": "comment-12720225"
        },
        {
            "date": "2009-06-16T16:35:52+0000",
            "content": "OK I will commit shortly!  Another one down!\n\nThanks for persisting on this Toru, Koji, Steven, Mark, Robert... this one was open for far toooo long. ",
            "author": "Michael McCandless",
            "id": "comment-12720226"
        }
    ]
}