{
    "id": "LUCENE-2219",
    "title": "improve BaseTokenStreamTestCase to test end()",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9.2",
            "3.0.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "2.9,                                            3.0",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.\n\nIn my opinion you currently have to write too much code to test this.\n\nThis patch does the following:\n\n\tadds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents\n\tin assertAnalyzesTo, automatically fill this with the String length()\n\n\n\nIn my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.\n\nfor assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.\n\nthe tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.",
    "attachments": {
        "LUCENE-2219.patch": "https://issues.apache.org/jira/secure/attachment/12430541/LUCENE-2219.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-17T09:29:14+0000",
            "content": "this fixes contrib too, as long as you apply the CJKTokenizer fix from LUCENE-2207.\n\nend() was incorrect for ChineseTokenizer, SmartChinese, and Wikipedia ",
            "author": "Robert Muir",
            "id": "comment-12801351"
        },
        {
            "date": "2010-01-17T10:00:26+0000",
            "content": "Wikipedia does not call super.end().\n\nLooks good! ",
            "author": "Uwe Schindler",
            "id": "comment-12801357"
        },
        {
            "date": "2010-01-17T10:02:55+0000",
            "content": "Wikipedia does not call super.end(). \n\nUwe, thanks for taking a look...\n\neven StandardTokenizer does not call super.end()!\n\nshould we really do this? ",
            "author": "Robert Muir",
            "id": "comment-12801358"
        },
        {
            "date": "2010-01-17T11:48:36+0000",
            "content": "i merged Koji's fix and tests to CJK from LUCENE-2207 into this patch, and improved CJKTokenizer's tests to always use assertAnalyzesTo, for better checking.\n\ni plan to commit soon ",
            "author": "Robert Muir",
            "id": "comment-12801378"
        },
        {
            "date": "2010-01-17T15:51:10+0000",
            "content": "I am fine with this patch! ",
            "author": "Uwe Schindler",
            "id": "comment-12801425"
        },
        {
            "date": "2010-01-17T19:28:03+0000",
            "content": "Committed revision 900196 to trunk. ",
            "author": "Robert Muir",
            "id": "comment-12801485"
        }
    ]
}