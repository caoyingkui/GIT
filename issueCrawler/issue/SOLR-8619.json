{
    "id": "SOLR-8619",
    "title": "A new replica should not become leader when all current replicas are down as it leads to data loss",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5"
        ],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Here's what I'm talking about:\n\n\tStart a 2 node solrcloud cluster\n\tCreate a 1 shard/1 replica collection\n\tAdd documents\n\tShut down the node that has the only active shard\n\tADDREPLICA for the shard/collection, so Solr would attempt to add a new replica on the other node\n\tSolr waits for a while before this replica becomes an active leader.\n\tIndex a few new docs\n\tBring up the old node\n\tThe replica comes up, with it's old index and then syncs to only contain the docs from the new leader.\nAll old documents are lost in this case\n\n\n\nHere are a few things that might work here:\n1. Reject an ADDREPLICA call if all current replicas for the shard are down. Considering the new replica can not sync from anyone, it doesn't make sense for this replica to even come up\n2. The replica shouldn't become active/leader unless either it was the last known leader or active before it went into recovering state\nunless there are no other replicas in the clusterstate.\n\nThis might very well be related to SOLR-8173 but we should add a check to ADDREPLICA as well.",
    "attachments": {
        "SOLR-8619-test.patch": "https://issues.apache.org/jira/secure/attachment/12788519/SOLR-8619-test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-01-28T23:27:53+0000",
            "author": "Erick Erickson",
            "content": "I'd also not want to even try to ADDREPLICA if there were no active leaders.\n\nAlthough I understand that you could start the ADDREPLICA command and then all the other replicas could go down before it synched, so it looks like a belt-and-suspenders kind of thing.\n\nHmmm, does that play nice with\n1> creating a collection? there are no replicas by definition\n2> adding shards (implicit router)\n\nJust random thoughts.... ",
            "id": "comment-15122520"
        },
        {
            "date": "2016-01-28T23:36:46+0000",
            "author": "Mark Miller",
            "content": "This is expected behavior given the design of the system?\n\nYou can't expect not to lose data with these cases - it's a large part of what the min replication param is for. If you want to ensure your data is not lost, you need to ensure it hits more than one one replica as a minimum. At least a replication factor of 3 is probably best to avoid data loss. ",
            "id": "comment-15122534"
        },
        {
            "date": "2016-01-28T23:39:28+0000",
            "author": "Mark Miller",
            "content": "1. Reject an ADDREPLICA call if all current replicas for the shard are down. Considering the new replica can not sync from anyone, it doesn't make sense for this replica to even come up\n\nThat's probably true in this case though. I don't know if we should reject it, but at least make it aware it probably should not become the leader until it has sync'd from a leader? ",
            "id": "comment-15122538"
        },
        {
            "date": "2016-01-29T01:42:00+0000",
            "author": "Anshum Gupta",
            "content": "What I have in mind should play nice with both of those things else it's not even a solution  ",
            "id": "comment-15122707"
        },
        {
            "date": "2016-01-29T01:45:10+0000",
            "author": "Anshum Gupta",
            "content": "Sure, I strongly think we need to be intelligent in electing leaders. That would solve this problem but why would we want a new replica to get added up that can't do anything but consume resources for a core? Not a ton of resources but still. I guess you'll agree. ",
            "id": "comment-15122721"
        },
        {
            "date": "2016-01-29T02:20:28+0000",
            "author": "Jason Gerlowski",
            "content": "Throwing in my 2 cents.  New to SolrCloud, so feel free to ignore...\n\n+1 for having a check to ensure that a replica isn't marked as a leader unless it's had a chance to sync with a leader.\n\n+1 for having ADDREPLICA calls fail if there are no active replicas.  I'd be fine with allowing API users to create not-ready-for-leadership replicas if there was a great way of conveying that caveat to them.  But short of adding a replica-state option to CLUSTERSTATUS to convey this caveat, I can't think of a good way to do this.  IMO, it seems cleaner conceptually to prevent users up front from getting into this state.  Bit hand-wavy though, so take this rationale with a grain of salt. ",
            "id": "comment-15122781"
        },
        {
            "date": "2016-01-29T03:06:05+0000",
            "author": "Mark Miller",
            "content": "Sure, I strongly think we need to be intelligent in electing leaders. That would solve this problem but why would we want a new replica to get added up that can't do anything but consume resources for a core? Not a ton of resources but still. I guess you'll agree.\n\nBecause it seems if you want to add a replica, you want to add a replica.\n\nLet's say I do add replica right when the first replica goes down for some reason - like it loses it's zk connection due to a GC event. But then it connects again. It almost seems preferable to me that my add replica call still works, but it won't become the leader - then when the first replica quickly re-establishes its connection to Zk, it will recover from it.\n\nMy thinking is, if I want to add a replica, I don't care that it has no one to recover from at any given moment. I want to add a replica to the shard now. Let the system work out when it's safe and possible to sync up with the shard. Otherwise, I have to process the fail, go look at why it happened, try and get that straightened out, try the call again, repeat, etc.\n\nThere doesn't seem to be a strong reason to fail - the call can easily work and when the other replicas come back on line, everything will settle out. We just want to make sure it won't become the leader without recovering first. ",
            "id": "comment-15122860"
        },
        {
            "date": "2016-01-29T03:30:12+0000",
            "author": "Erick Erickson",
            "content": "Perhaps a new state? \"new\"? \"never_syncd\"? \"not_eligible_for_leader_election\"? Whatever. Point is just a flag saying \"I don't care what else you do, I shouldn't be leader yet\".\n\nStill, you'd have to reconcile such a state with collection creation, but that doesn't seem like a big deal. ",
            "id": "comment-15122882"
        },
        {
            "date": "2016-01-29T06:10:47+0000",
            "author": "Mark Miller",
            "content": "It may not stick long term, but currently, if you simply set the lastPublished state we track to anything but ACTIVE it won't become leader. ",
            "id": "comment-15123037"
        },
        {
            "date": "2016-01-29T15:36:18+0000",
            "author": "Anshum Gupta",
            "content": "Sure, this should work. Might have to add a few checks there for the conditions that Erick mentioned though. ",
            "id": "comment-15123603"
        },
        {
            "date": "2016-01-29T15:38:12+0000",
            "author": "Anshum Gupta",
            "content": "I think we could just reuse the lastPublished state instead of adding a new state, while adding conditions that let things flow smoothly in cases of collection creation and custom routed shard creation. ",
            "id": "comment-15123606"
        },
        {
            "date": "2016-01-29T15:44:07+0000",
            "author": "Anshum Gupta",
            "content": "But the user wouldn't get back a useable replica. We could add retries but fail if it's not just a short event. The idea here being, if a user is expecting traffic, typically the case where a user would want to add a replica, the response from the addreplica call should assure him that a usable replica was added. If that wasn't the case, ask him to retry while also communicating about the reason for error. If we don't do that, the user would have to check the clusterstatus to confirm if the new replica is actually usable or not. ",
            "id": "comment-15123616"
        },
        {
            "date": "2016-01-29T22:18:50+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Typical cloud issues lead to all replicas of a shard going to recovery/down/recovery_failed, and the only way is to cold start it by shutting all down and bringing them back up. Will checking lastPublished for ACTIVE interfere with that?\n\nAn another cue which can be useful is the index generation being zero.. ",
            "id": "comment-15124331"
        },
        {
            "date": "2016-01-29T23:34:46+0000",
            "author": "Varun Thacker",
            "content": "Typical cloud issues lead to all replicas of a shard going to recovery/down/recovery_failed, and the only way is to cold start it by shutting all down and bringing them back up. Will checking lastPublished for ACTIVE interfere with that?\n\nYeah I've always found it very tricky to help clients bring up a shard when all replicas got into recovery/down/recovery_failed state. I guess now we have forceLeaders to do so ",
            "id": "comment-15124439"
        },
        {
            "date": "2016-01-30T06:12:03+0000",
            "author": "Shai Erera",
            "content": "There are two separate issues here:\n\n\n\tA new replica added via ADDREPLICA\n\tWhich replica should become a leader\n\n\n\nIn the first case, I don't think that the replica should ever become the leader, until it has had a chance to sync w/ a leader and first published its state as ACTIVE. I also thought along the lines of Erick's suggestion, i.e. add an INITIALIZING state to a Replica. Then, replicas can transition from INITIALIZING -> RECOVERING -> ACTIVE, but never INITIALIZING -> DOWN. Then, the DOWN -> ACTIVE transition is \"safe\" in that only non-initializing replicas can become active leaders in the case of conflicts, or a whole cluster restart, because we know they were once ACTIVE.\n\nIn case of a new collection, where all REPLICAS are new, then we have two choices: either we note that in the internal ADDREPLICA call, so they are added in DOWN state, or (which is simpler I guess), since all replicas will be INITIALIZING, one can become the leader since they're all equal.\n\nFor the second case, which replica should become the leader, the proposals made here make sense, but IMO they belong to a separate issue. Using the index version, the commit point info etc. are good. But the problem that we've hit is that the ONLY live replica at the moment was the new (empty) one, there were two others in DOWN state, but their nodes did not belong to the cluster (ZK issues, network splits ...) and then that replica decided to become the leader. When the 2 others later joined the cluster, they replicated \"empty\" index from the leader, and data was lost.\n\nIf we added the new replica in the INITIALIZING state, it would stay that, and when the two others returned to the cluster, they would re-compete for leadership, using all the proposals made above, and no data would be lost. ",
            "id": "comment-15124761"
        },
        {
            "date": "2016-01-30T22:17:28+0000",
            "author": "Mark Miller",
            "content": "Typical cloud issues lead to all replicas of a shard going to recovery/down/recovery_failed, and the only way is to cold start it by shutting all down and bringing them back up. Will checking lastPublished for ACTIVE interfere with that?\n\nI'm going back to improving that this week! ",
            "id": "comment-15125104"
        },
        {
            "date": "2016-01-31T23:53:59+0000",
            "author": "Anshum Gupta",
            "content": "This makes sense, along with a check for new collection creation i.e. when a Replica is in INITIALIZING state, it can get to ACTIVE if there's no other replica in the cluster state. Or we could add this check and allow for the replica to get to RECOVERING in such a case, and then allow the transition from RECOVERING -> ACTIVE. ",
            "id": "comment-15125572"
        },
        {
            "date": "2016-02-18T20:19:15+0000",
            "author": "Anshum Gupta",
            "content": "A test that shows what's going on. ",
            "id": "comment-15153012"
        }
    ]
}