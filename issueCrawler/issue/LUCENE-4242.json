{
    "id": "LUCENE-4242",
    "title": "UnInverted cache uses term freq to filter out terms (but deleted docs are included in the freq count)",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "4.0-BETA",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "TermEnum.docFreq() count is used to compute uninverted index\n(DocTermOrds.uninvert()). The code goes like:\n\n      final int df = te.docFreq();\n      if (df <= maxTermDocFreq) {\n\nSo, if there are deleted documents in the index and maxTermDocFreq is\nlow, then the term will be excluded (even if the freq of the livedocs\nis OK). Most likely, the cache will be incomplete.",
    "attachments": {
        "LUCENE-4242.patch": "https://issues.apache.org/jira/secure/attachment/12537408/LUCENE-4242.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-20T20:51:08+0000",
            "content": "An attempt to offset the freq by the number of deleted docs in the index ",
            "author": "Roman Chyla",
            "id": "comment-13419553"
        },
        {
            "date": "2012-07-20T21:40:00+0000",
            "content": "Thanks roman!\n\nI think the math is backwards?  EG if I have 0 deleted docs I think this patch will then allow all terms regardless of their docFreq vs maxTermDocFreq?\n\nI think the ratio you want to multiply the terms docFreq by is reader.numDocs() / reader.maxDoc()?  That's 1.0 if there are no deletions, and 0.0 if all docs are deleted. ",
            "author": "Michael McCandless",
            "id": "comment-13419593"
        },
        {
            "date": "2012-07-20T23:29:47+0000",
            "content": "Thanks for looking at it! I intended to say \"1.0f - ratioDeleted\" - but i forgot the substraction\n\nTaht was essentially the same as you suggest, just more convoluted\n\nI will add a new patch \"Math.min(1.0f, (float) reader.numDocs() / reader.maxDoc())\"\n\nassuming: \n\nreader.maxDoc() never returns 0\nreader.maxDoc() = number of documents in the index (not the highest lucene id)\n\n\nstill learning lucene... and I thought that maxDoc() would return the highest lucene int, and so if (for whatever reasons) the reader contained only the last segment of a two-segment index, ie. a segment with docs: 50..100, and 5 were deleted, numDoc=45, maxDoc=101\n\noffsetDeletedDocs = min(1.0f, 45 / 101) ?\n\n\nBut this doesn't seem to be the case... ",
            "author": "Roman Chyla",
            "id": "comment-13419682"
        },
        {
            "date": "2012-07-20T23:30:20+0000",
            "content": "improved version ",
            "author": "Roman Chyla",
            "id": "comment-13419683"
        },
        {
            "date": "2012-07-21T13:45:20+0000",
            "content": "maxDoc is indeed a confusing name... but it is 1+ the max docID in that reader.  A better name would have been numDocsIgnoringDeletions.\n\nNot sure why you see maxDoc=101 if your segment has 50 docs ... that's odd.  Were you testing against the top-level reader?\n ",
            "author": "Michael McCandless",
            "id": "comment-13419838"
        },
        {
            "date": "2012-07-21T13:57:57+0000",
            "content": "I cleaned up the patch a bit ... only issue now is it causes failures in TestRandomFaceting. ",
            "author": "Michael McCandless",
            "id": "comment-13419843"
        },
        {
            "date": "2012-07-21T14:22:38+0000",
            "content": "This patch doesn't seem to make sense.\nliveDocsRatio will be the same for every term - if you want to take into account deleted docs, then just do it when you set maxTermDocFreq.\n\nAlso, this code comes from Solr, where maxTermDocFreq is set as a percentage of maxDoc - so things are already scaled by the number of deleted docs.\n\nI don't think there's anything to fix here. ",
            "author": "Yonik Seeley",
            "id": "comment-13419855"
        },
        {
            "date": "2012-07-21T15:08:02+0000",
            "content": "liveDocsRatio will be the same for every term - if you want to take into account deleted docs, then just do it when you set maxTermDocFreq.\n\nGood!  I'll move it up front.\n\nAlso, this code comes from Solr\n\nTrue, but it matters not where the code came from \n\nI think what you meant to say is \"where Solr invokes DocTermOrds ...\".  So then the question is what contract should we have (should caller be expected to pro-rate by deletes themselves, or should DocTermOrds do so (= this patch)).\n\nwhere maxTermDocFreq is set as a percentage of maxDoc - so things are already scaled by the number of deleted docs.\n\nWait, how is that taking deleted docs into account?  maxDoc doesn't reflect deletions.  Looks to me like deleted docs are not factored in now by Solr, either? ",
            "author": "Michael McCandless",
            "id": "comment-13419860"
        },
        {
            "date": "2012-07-22T07:21:51+0000",
            "content": "maxDoc doesn't reflect deletions.\n\nHolding nDocs steady, as deletions rise so does maxDoc.  But now that I think about it, this depends on the semantics of \"taking deletes into account\".  It depends on the use-case and exactly what you're using DocTermOrds for (and why you're skipping some terms).  It may make sense for Solr to pro-rate for deletions... I'm not sure.\n\nSo then the question is what contract should we have (should caller be expected to pro-rate by deletes themselves, or should DocTermOrds do so\n\nI think the current behavior is easier to understand and explain - it's a direct check of the term docFreq.\nA pro-rate is just a guess since we wouldn't be guaranteeing that the \"real\" docFreq would be below the max. ",
            "author": "Yonik Seeley",
            "id": "comment-13420117"
        },
        {
            "date": "2012-07-22T22:43:00+0000",
            "content": "Not sure why you see maxDoc=101 if your segment has 50 docs ... that's odd. Were you testing against the top-level reader?\n\nI haven't seen it in real, it was more a question. I didn't know if it was possible to construct a composite reader (with some segments missing) and used it to get the uninverted cache. As maxDoc was abstract and I wasn't sure about all the implementations, it made me wonder what maxDoc() would be. 1.0f-(numDeletedDocs()/numDocs()) seemed somewhat less ambiguous then ",
            "author": "Roman Chyla",
            "id": "comment-13420354"
        },
        {
            "date": "2012-07-23T10:42:48+0000",
            "content": "A pro-rate is just a guess since we wouldn't be guaranteeing that the \"real\" docFreq would be below the max.\n\nIt is a guess and we should document that it's only approximate.\n\nThe basic idea seems simple: the deleted docs won't actually be stored in DTO yet the docFreq is counting them, so you're rejecting terms that would have been below the cutoff.  I think it makes sense to pro-rate, but I agree the caller can easily do so ... \n\nroman maybe you can shed some more light on your use case? ",
            "author": "Michael McCandless",
            "id": "comment-13420548"
        },
        {
            "date": "2012-07-24T05:14:36+0000",
            "content": "My use case was to get an un-inverted index of a multiValued field - I use it to make 2nd-pass queries (e.g. find all docs that are referencing docs found in the 1st pass). I had my own code to create the univerted index (in the 3.x release) but wanted st standard - so this is how I noticed the difference between the branches.\n\nHowever, if I let DocTermOrds manage the un-inverted cache, I can fully switch (I hope).\n\nBut as this un-inverted index is used for faceting, isn't it possible there will be errors in the faceting? Some terms missing? ",
            "author": "Roman Chyla",
            "id": "comment-13421185"
        }
    ]
}