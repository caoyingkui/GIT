{
    "id": "LUCENE-2701",
    "title": "Factor maxMergeSize into findMergesForOptimize in LogMergePolicy",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "LogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.\n\nAs part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.\n\nI'll attach a patch shortly.",
    "attachments": {
        "LUCENE-2701-2.patch": "https://issues.apache.org/jira/secure/attachment/12468507/LUCENE-2701-2.patch",
        "LUCENE-2701.patch": "https://issues.apache.org/jira/secure/attachment/12457138/LUCENE-2701.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-10-14T09:28:36+0000",
            "content": "Patch adds maxMergeMB handling to optimize as well. If there are no segments exceeding the threshold, then only maxNumSegments constraint is taken into account. Basically I've created two private methods findMergesForOptimizeMaxMergeSize and findMergesForOptimizeMaxNumSegments (the original logic). findMergesForOptimize calls the relevant one.\n\nI've also changed some members to protected and methods as well, for really easy extension of LMP. As a result, I removed two methods from BalancedSegmentsMP that were copied over from LMP.\n\nI took the opportunity to change OneMerge.segments and userCompoundfile to public - they are final so no risk of changing from the outside. But otherwise, if you would like to write a MP which queries the OneMerge objects, you can't. I added totalSize() to return the total size in bytes of that merge.\n\nTest + CHANGES entry as well. ",
            "author": "Shai Erera",
            "id": "comment-12920897"
        },
        {
            "date": "2010-10-15T10:20:37+0000",
            "content": "Added support for maxMergeDocs as well. Also, I created a test class for size bounded optimize and added several test cases.\n\nI think it's ready to commit, but I'll wait a few days for some reviews. ",
            "author": "Shai Erera",
            "id": "comment-12921299"
        },
        {
            "date": "2010-10-15T12:46:19+0000",
            "content": "Patch looks good!\n\nMaybe rename OneMerge.totalSize -> totalSizeInBytes?  Hmm does anyone\nactually call this new method?\n\nMaybe note somewhere that now optimize (when there's a maxMergeDocs/MB\nconstraint) is able to merge fewer than mergeFactor segments at a\ntime?\n\nThis code is a bit confusing:\n\n\n       if (last - start - 1 > 1) {\n         // there is more than 1 segment to the right of this one.\n         spec.add(new OneMerge(infos.range(start + 1, last), useCompoundFile));\n       } else if (start != last - 1 && !isOptimized(infos.info(start + 1))) {\n          spec.add(new OneMerge(infos.range(start + 1, last), useCompoundFile));\n       }\n\n\n\nBoth if clauses are doing the same thing right?  (Ie merging the chunk\nof segs to the right). Maybe put a comment explaining the 2nd one?  (I\nthink it's for the case where there's 1 segment to our right but it's\nnot optimized, eg the CFS differs?).  Or maybe consolidate into a single\nif? ",
            "author": "Michael McCandless",
            "id": "comment-12921329"
        },
        {
            "date": "2010-10-15T13:20:46+0000",
            "content": "You're right about the code - the 'else if' is in case there is one not optimized segment to the right. Added a comment and combined them into one OR-ed if. Also added a test case.\n\nOneMerge.totalSizeInBytes \u2013 no one calls it now, but I would like to write a MP which will, and remove merges that exceed a specified total size. It's just a service method, instead of you needing to write it on your own. I renamed it to totalBytesSize. And on the way added totalNumDocs, doing the same for the number of docs.\n\nMaybe note somewhere that now optimize (when there's a maxMergeDocs/MB constraint) is able to merge fewer than mergeFactor segments at a time?\n\nWasn't it able to do so even before? E.g. if maxNumSegments < numSegments < mergeFactor? ",
            "author": "Shai Erera",
            "id": "comment-12921344"
        },
        {
            "date": "2010-10-20T14:01:56+0000",
            "content": "Committed revision 1025544 (3x).\nCommitted revision 1025577 (trunk). ",
            "author": "Shai Erera",
            "id": "comment-12922968"
        },
        {
            "date": "2011-01-14T12:04:04+0000",
            "content": "This change together with LUCENE-2773 caused a change of the IW#optimize() and friends semantics.\nIW#optimize() says:\n\n /**\n   * Requests an \"optimize\" operation on an index, priming the index\n   * for the fastest available search. Traditionally this has meant\n   * merging all segments into a single segment as is done in the\n   * default merge policy, but individual merge policies may implement\n   * optimize in different ways.\n   *\n\n\n\n\nWhich is not entirely true anymore since default now uses \n\n\n  /** Default maximum segment size.  A segment of this size\n   *  or larger will never be merged.  @see setMaxMergeMB */\n  public static final double DEFAULT_MAX_MERGE_MB = 2048;\n\n\n\nthis is not what I would expect from optimize() even if it would be documented that way. A plain optimize call should by default result in a single segment IMO. Yet, we could make this set by a flag in LogMergePolicy maybe something like LogMergePolicy#useMasMergeSizeForOptimize = false; as a default? ",
            "author": "Simon Willnauer",
            "id": "comment-12981730"
        },
        {
            "date": "2011-01-14T15:51:46+0000",
            "content": "I agree that there should not be a defaults for the max merge segment size for optimize, though it's good to have the option. ",
            "author": "Jason Rutherglen",
            "id": "comment-12981803"
        },
        {
            "date": "2011-01-14T16:25:22+0000",
            "content": "I don't think we need a useDefaultMaxMergeMb. Instead, we can default the member to Long.MAX_VAL. That way, if no one sets it, all segments will be considered for merge, and if one wants, he can set it.\n\nI expect that if I use IW with a LMP that sets maxMergeMB, then even if I call optimize() this setting will take effect.\n\nBTW, I don't remember introducin this defaul as part of this issue. This issue only changed LMP to take the already existed setting into account. So maybe reverting this default should be handled within the issue I was changed in? ",
            "author": "Shai Erera",
            "id": "comment-12981813"
        },
        {
            "date": "2011-01-14T16:38:57+0000",
            "content": "BTW, I don't remember introducin this defaul as part of this issue. This issue only changed LMP to take the already existed setting into account. So maybe reverting this default should be handled within the issue I was changed in?\nTrue this was done in here:  LUCENE-2773  - but this seemed to be more related?!\nI don't think we need a useDefaultMaxMergeMb. Instead, we can default the member to Long.MAX_VAL. That way, if no one sets it, all segments will be considered for merge, and if one wants, he can set it.\n\nI think mike did that on purpose to prevent large segs from merging during indexing.... so what is wrong with disable that limit during optimize? ",
            "author": "Simon Willnauer",
            "id": "comment-12981817"
        },
        {
            "date": "2011-01-14T17:29:27+0000",
            "content": "I think mike did that on purpose to prevent large segs from merging during indexing.\n\nRight \u2013 large merges are really quite nasty \u2013 mess up searches, NRT turnaround, IW.close() suddenly unexpectedly takes like an hour, etc.\n\nBut, really the best fix, which I'd love to do at some point, is to fix our merge policy so that insanely large merges are done w/ fewer segments (eg only 2 segments at once).  I think BalancedMP does this. ",
            "author": "Michael McCandless",
            "id": "comment-12981836"
        },
        {
            "date": "2011-01-16T19:24:23+0000",
            "content": "Patch fixes some jdocs + adds a separate setting for maxMergeSizeForOptimize which is applied only during optimize(). ",
            "author": "Shai Erera",
            "id": "comment-12982366"
        },
        {
            "date": "2011-01-16T19:39:27+0000",
            "content": "Patch fixes some jdocs + adds a separate setting for maxMergeSizeForOptimize which is applied only during optimize().\npatch looks good to me! +1 to commit\n\nthanks shai ",
            "author": "Simon Willnauer",
            "id": "comment-12982368"
        },
        {
            "date": "2011-01-17T04:47:25+0000",
            "content": "Thanks Simon !\n\nCommitted revision 1059750 (3x).\nCommitted revision 1059751 (trunk). ",
            "author": "Shai Erera",
            "id": "comment-12982477"
        },
        {
            "date": "2011-03-30T15:50:16+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013418"
        }
    ]
}