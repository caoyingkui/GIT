{
    "id": "SOLR-9811",
    "title": "Make it easier to manually execute overseer commands",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Sometimes solrcloud will get into a bad state w.r.t. election or recovery and it would be useful to have the ability to manually publish a node as active or leader. This would be an alternative to some current ops practices of restarting services, which may take a while to complete given many cores hosted on a single server.\n\nThis is an expert operator technique and readers should be made aware of this, a.k.a. the \"I don't care, just get it running\" approach.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-11-29T20:19:22+0000",
            "author": "Kevin Risden",
            "content": "Just curious how this is different than something like:\n\nSOLR-7569\nhttps://cwiki.apache.org/confluence/display/solr/Collections+API#CollectionsAPI-ForceLeader ",
            "id": "comment-15706418"
        },
        {
            "date": "2016-11-29T20:33:23+0000",
            "author": "Mark Miller",
            "content": "I don't think this will really add anything on top of the force leader api - if you cannot force a leader (already a data loss dangerous last resort operation), something like this has extremely little, if any, chance of helping the situation.\n\nRegardless, rather than exposing something so internal, the force leader api should probably be bulked up to try even more than it does now if something is missing. ",
            "id": "comment-15706450"
        },
        {
            "date": "2016-11-29T20:43:11+0000",
            "author": "Mike Drob",
            "content": "I was thinking also that when you have a leader and one of the shards is stuck in a recovery loop of some kind, it would be good to be able to clear out only that replica's state instead of having to restart the whole server that it is on. Maybe you can do that through existing core admin commands? ",
            "id": "comment-15706473"
        },
        {
            "date": "2016-11-29T21:46:18+0000",
            "author": "Scott Blum",
            "content": "I've had cases where a single replica ended up stuck in the DOWN state, but all the other replicas on that machine were fine, and the replica itself was physically present and able to serve queries.  The only 'fix' in that situation is to reboot the node, but then you're interrupting service for collections that have no problems.\n\nIn those cases I've manually shoved a state update operation into ZK like this.\n\nFrom zk-shell\ncreate /solr/overseer/queue/qn- '{\"core\":\"FOO_shard1_replica1\",\"core_node_name\":\"core_node1\",\"base_url\":\"http://1.1.1.1:8983/solr\",\"node_name\":\"1.1.1.1:8983_solr\",\"state\":\"active\",\"shard\":\"shard1\",\"collection\":\"FOO\",\"operation\":\"state\"}' false true\n\n\n\nThe correct values can be filled in from the result of querying CLUSTERSTATE on the affected collection.  Would be nice to have a tool for this. ",
            "id": "comment-15706632"
        },
        {
            "date": "2016-11-30T14:59:19+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Scott, have you tried issuing the requestrecovery coreadmin API in such a case? Also, any idea about the root cause? ",
            "id": "comment-15708803"
        },
        {
            "date": "2016-11-30T17:46:49+0000",
            "author": "Scott Blum",
            "content": "Didn't try that, I wasn't aware of that!  I wouldn't have thought to look in the coreadmin API for collection-state related issues.\n\nUnsure about the root cause, it could have been from me having to nuke nuke a runaway state update queue but I've seen it a few times during times of cluster turbulence. ",
            "id": "comment-15709241"
        },
        {
            "date": "2016-11-30T18:43:36+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "It is an internal API so not publicly documented but useful to know for fixing misbehaved clusters. ",
            "id": "comment-15709384"
        },
        {
            "date": "2016-11-30T18:49:31+0000",
            "author": "Mark Miller",
            "content": "Scott Blum, I'm not saying there are not cases trying something drastic won't help in an emergency - I'm saying those types of efforts should be put into the forceLeader api that we already have. That command itself really should have been named something more like, try to make things work, don't worry if data might be lost. ",
            "id": "comment-15709398"
        },
        {
            "date": "2016-11-30T19:07:46+0000",
            "author": "Scott Blum",
            "content": "Seems fine to me.  I was mostly posting what I'd done for Mike Drob who needs to do something similiar.  I've tried FORCELEADER a few times but for me it never puts a replica erroneously marked as DOWN into an ACTIVE state. ",
            "id": "comment-15709449"
        },
        {
            "date": "2016-11-30T19:12:37+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Scott Blum, I think FORCELEADER is supposed to recover from situations where a shard has lost a leader and a new leader is not elected due to some race condition. To fix a DOWN replica and bring to ACTIVE, there is REQUESTRECOVERY; can you try that to see if it fixes the replica? ",
            "id": "comment-15709458"
        },
        {
            "date": "2016-11-30T19:14:25+0000",
            "author": "Scott Blum",
            "content": "Yeah I'll give that a shot next time, didn't know it existed before today. ",
            "id": "comment-15709464"
        },
        {
            "date": "2016-12-02T18:04:48+0000",
            "author": "Scott Blum",
            "content": "REQUESTRECOVERY did not work:\n\n\n2016-12-02 18:03:02.611 ERROR (recoveryExecutor-3-thread-10-processing-n:10.240.0.69:8983_solr x:24VFQ_shard1_replica0 s:shard1 c:24VFQ r:core_node1) [c:24VFQ s:shard1 r:core_node1 x:24VFQ_shard1_replica0] o.a.s.c.RecoveryStrategy Error while trying to recover. core=24VFQ_shard1_replica0:org.apache.solr.common.SolrException: Cloud state still says we are leader.\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:320)\n\n ",
            "id": "comment-15715835"
        },
        {
            "date": "2016-12-02T18:06:49+0000",
            "author": "Scott Blum",
            "content": "The replica is marked by LEADER and DOWN.\n\nBasically, I can't FORCELEADER because the replica isn't active, and I can't force recovery because the replica is already leader. ",
            "id": "comment-15715840"
        },
        {
            "date": "2016-12-02T18:17:06+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Scott Blum do you know how to reproduce or what's the cause? ",
            "id": "comment-15715858"
        },
        {
            "date": "2016-12-02T18:20:57+0000",
            "author": "David Smiley",
            "content": "FWIW I also recently just tried REQUESTRECOVERY and it didn't work; and I was very patient.  Eventually I restarted the node and it was then happy.  I don't recall seeing the message Scott saw, but I don't have the logs anymore (I think) to be sure. ",
            "id": "comment-15715867"
        },
        {
            "date": "2016-12-02T19:35:21+0000",
            "author": "Scott Blum",
            "content": "I'm not sure, but it might have something to with race conditions when \"moving\" a replica.\n\nAn operation we do a lot of is create a new replica on a new machine, wait for it to become active, then delete the old replica.  It's possible that this process is what sometimes leaves us with a single replica marked both \"DOWN\" and \"LEADER\". ",
            "id": "comment-15716043"
        }
    ]
}