{
    "id": "LUCENE-2947",
    "title": "NGramTokenizer shouldn't trim whitespace",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "3.0.3",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Before I tokenize my strings, I am padding them with white space:\n\nString foobar = \" \" + foo + \" \" + bar + \" \";\n\nWhen constructing term vectors from ngrams, this strategy has a couple benefits.  First, it places special emphasis on the starting and ending of a word.  Second, it improves the similarity between phrases with swapped words.  \" foo bar \" matches \" bar foo \" more closely than \"foo bar\" matches \"bar foo\".\n\nThe problem is that Lucene's NGramTokenizer trims whitespace.  This forces me to do some preprocessing on my strings before I can tokenize them:\n\nfoobar.replaceAll(\" \",\"$\"); //arbitrary char not in my data\n\nThis is undocumented, so users won't realize their strings are being trim()'ed, unless they look through the source, or examine the tokens manually.\n\nI am proposing NGramTokenizer should be changed to respect whitespace.  Is there a compelling reason against this?",
    "attachments": {
        "LUCENE-2947.patch": "https://issues.apache.org/jira/secure/attachment/12473206/LUCENE-2947.patch",
        "NGramTokenizerTest.java": "https://issues.apache.org/jira/secure/attachment/12472548/NGramTokenizerTest.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-03-03T15:21:30+0000",
            "content": "Hi Dave, in my opinion there are a lot of problems with our current NGramTokenizer (yours is just one) and it would be a good idea to consider creating a new one. We could rename the old one to ClassicNGramTokenizer or something for people that need the backwards compatibility.\n\nA lot of the problems already have open JIRA issues: i gave my opinion on some of the broken-ness here: LUCENE-1224 . The largest problem being that these tokenizers only examine the first 1024 chars of the document. They shouldn't just discard anything after 1024 chars. There is no need to load the 'entire document' into memory... n-gram tokenization can work on a \"sliding window\" across the document.\n\nIn my opinion part of n-gram character tokenization is being able to configure what is a token character and what is not. (Note I don't mean java character here, but in the more abstract sense, e.g. a character might have diacritics and be treated as a single unit). For some applications maybe this is just 'alphabetic letters', for other apps perhaps even punctuation could be considered 'relevant'. So it should somehow be flexible.  Furthermore, in the case of word-spanning n-grams, you should be able to collapse runs of \"Non-characters\" into a single marker (e.g. _), and usually you would want to do this for the start and end of string too.\n\nhere's visual representation of how things should look when you use these tokenizers in my opinion:\nhttp://www.csee.umbc.edu/~nicholas/601/SIGIR08-Poster.pdf ",
            "author": "Robert Muir",
            "id": "comment-13002019"
        },
        {
            "date": "2011-03-03T15:35:41+0000",
            "content": "A simple failing JUnit test illustrating the problem. ",
            "author": "David Byrne",
            "id": "comment-13002022"
        },
        {
            "date": "2011-03-03T15:43:36+0000",
            "content": "Thanks for the feedback Robert.  I'll give it a shot and try and write a new one.  I wanted to write a tokenizer to support skip-grams anyways. ",
            "author": "David Byrne",
            "id": "comment-13002026"
        },
        {
            "date": "2011-03-03T15:58:09+0000",
            "content": "Thank you... by the way if you want to do skip-grams as a separate tokenizer or whatever, you know whatever makes sense...\n\nI could imagine some of the n-gram variations might need to be their own tokenizers to prevent things from being too complicated but perhaps they could still share some code.\n\n(But maybe you have some way to fit skipgrams in there easily) ",
            "author": "Robert Muir",
            "id": "comment-13002031"
        },
        {
            "date": "2011-03-03T16:03:32+0000",
            "content": "Yeah I was originally planning to implement skip-grams as a seperate tokenizer.  Since we are re-evaluating ngram tokenization in general, maybe I can come up with an elegant solution.  Support for positional ngrams is another thing to consider.  ",
            "author": "David Byrne",
            "id": "comment-13002035"
        },
        {
            "date": "2011-03-09T22:02:27+0000",
            "content": "I've finished my first attempt at a patch.  The code could benefit from a bit of refactoring, but I wanted to make sure everyone agrees to the changes in principal before working on refining it.  The test cases (hopefully) illustrate most of the nuances.\n\nBenefits:\n\n\taccepts strings of any length (not just 1024 chars)\n\tcollapses consecutive whitespace characters\n\ttakes custom sets of chars to be treated as whitespace\n\t\"pads\" the beginning and end of the string\n\tFollows the same format as the PDF Robert linked above\n\n\n\nQuirks (examples in the test cases):\n\n\tUnigrams aren't \"padded\"...it just made the most sense\n\tbecause of the format, underscores will look identical to whitespace\n\tleading or trailing whitespace can result in weird looking ngrams (i.e. \"__\")\n\toffset values for ngrams with collapsed whitespace can be unintuitive (but consistent)\n\n ",
            "author": "David Byrne",
            "id": "comment-13004821"
        },
        {
            "date": "2011-03-14T13:12:37+0000",
            "content": "Has anybody had a chance to take a look at this patch?\n\nHere's my real world example of this patch in action: https://github.com/dbyrne/NameMatching ",
            "author": "David Byrne",
            "id": "comment-13006399"
        },
        {
            "date": "2013-04-26T22:17:15+0000",
            "content": "NGramTokenizer doesn't trim whitespaces anymore (LUCENE-4955). ",
            "author": "Adrien Grand",
            "id": "comment-13643300"
        }
    ]
}