{
    "id": "SOLR-3589",
    "title": "Edismax parser does not honor mm parameter if analyzer splits a token",
    "details": {
        "affect_versions": "3.6,                                            4.0-BETA",
        "status": "Closed",
        "fix_versions": [
            "3.6.2",
            "4.1",
            "6.0"
        ],
        "components": [
            "search"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "With edismax mm set to 100%  if one of the tokens is split into two tokens by the analyzer chain (i.e. \"fire-fly\"  => fire fly), the mm parameter is ignored and the equivalent of  OR query for \"fire OR fly\" is produced.\nThis is particularly a problem for languages that do not use white space to separate words such as Chinese or Japenese.\n\nSee these messages for more discussion:\nhttp://lucene.472066.n3.nabble.com/edismax-parser-ignores-mm-parameter-when-tokenizer-splits-tokens-hypenated-words-WDF-splitting-etc-tc3991911.html\n\nhttp://lucene.472066.n3.nabble.com/edismax-parser-ignores-mm-parameter-when-tokenizer-splits-tokens-i-e-CJK-tc3991438.html\n\nhttp://lucene.472066.n3.nabble.com/Why-won-t-dismax-create-multiple-DisjunctionMaxQueries-when-autoGeneratePhraseQueries-is-false-tc3992109.html",
    "attachments": {
        "testSolr3589.xml.gz": "https://issues.apache.org/jira/secure/attachment/12542189/testSolr3589.xml.gz",
        "SOLR-3589.patch": "https://issues.apache.org/jira/secure/attachment/12551654/SOLR-3589.patch",
        "SOLR-3589_test.patch": "https://issues.apache.org/jira/secure/attachment/12551648/SOLR-3589_test.patch",
        "SOLR-3589-3.6.PATCH": "https://issues.apache.org/jira/secure/attachment/12552500/SOLR-3589-3.6.PATCH"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Joel Rosen",
            "id": "comment-13418320",
            "date": "2012-07-19T14:22:02+0000",
            "content": "A user in this thread reports this is a bug introduced in 3.6: \n\nhttp://lucene.472066.n3.nabble.com/Dismax-Question-td3992446.html\n\nHe says they reverted to 3.5 and it went away.\n\nHowever I just tried the same setup with Solr versions 3.5, 3.4, and 3.1, and I can verify that the behavior is the same in each, so now I doubt it was a bug introduced in 3.6.  Could it be something in the default configuration that changed between versions? "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13418537",
            "date": "2012-07-19T18:26:59+0000",
            "content": "I didn't see enough configuration information in that thread to determine whether they were reporting the same bug or some different bug or configuration issue. After reading that thread  I also verified that the problem reported here occurs with version 3.4.  So I think that the thread you cite may refer to a different issue.  If you do happen to find some configuration change that fixes the problem, please let me know.\n\n\nTom   "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13435558",
            "date": "2012-08-15T21:55:06+0000",
            "content": "The root problem is that with automatic phrase query generation turned off, by default and for the text_general field in particular, the core Lucene query parser is generating a query for the tuple of sub-terms using the default query operator, which is \"OR\" by default. There is no notion of an \"mm\" or min-match parameter down at that level in Lucene, which knows nothing about Solr or edismax or request parameters.\n\nAs things stand, the only option is to set the default query operator, \"q.op\", to \"AND\".\n\nYou can of course also turn on autoGeneratePhraseQueries or select an analyzer than doesn't split terms.\n\nAt this point, I would advise resolving this issue as \"Won't Fix\", although it could also be spun off into a Lucene issue to add support for min-match down at that level, which edismax can then also communicate with.\n "
        },
        {
            "author": "Joel Rosen",
            "id": "comment-13435720",
            "date": "2012-08-16T02:17:14+0000",
            "content": "It's not just mm.  You set q.op to AND and it does the same thing.\n\nThe issue is that the query parser should treat the split tokens as separate tokens just as if they were separated by whitespace, but it doesn't.  If I use a smart Chinese tokenizer to split up a Chinese sentence into words, why can't the query parser treat those words exactly the same way it treats words from an English sentence? "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13435921",
            "date": "2012-08-16T12:15:22+0000",
            "content": "It's not just mm. You set q.op to AND and it does the same thing.\n\nJoel, you're right. Upon closer inspection of the code, I see that the reason is that edismax never sets the Lucene default operator directly. Instead, it sets the default value of \"mm\" parameter to 100% if \"q.op\" is \"AND\", and set's BooleanQuery.minNrShouldMatch to the number of optional terms. That is equivalent to setting the default Lucene query operator at the top-level boolean level, but has no effect for terms that get split down at the analyzer level. Oh well. Scratch that suggestion.\n\nI think I'm back to wanting to suggest that edismax should actually set the Lucene-level default query operator if \"mm\" is 100%. I think that would fix the original problem and allow the user to choose whether to user \"mm\" or \"q.op\" to control AND/OR. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13436045",
            "date": "2012-08-16T15:45:43+0000",
            "content": "If I use a smart Chinese tokenizer to split up a Chinese sentence into words, why can't the query parser treat those words exactly the same way it treats words from an English sentence?\n\nIndexing of whole documents can in fact treat text as if it were words from an English sentence, and split tokens do in fact behave as such in that context, but a query is not an English sentence or sentence in any natural language. Rather, a query is a structured expression composed of terms and operators, typically separated by whitespace or special operators such as parentheses. Portions of queries may look like natural language phrases or even whole sentences, but in reality they are sequences of terms and operators.\n\nIn addition to being parsed according to the syntax of queries, as opposed to natural language processing or the raw token stream processing of an indexer, each of the query terms must be \"analyzed\" before the final form of the term can be generated into a Lucene Query structure. That analysis is performed separate form the \"parsing\" of the structured user query expression. That means that the processing of sub-terms that result from analysis is handled at a different level than source-level query terms that happen to \"look\" like English words. In other words, the sub-terms are processed by the \"query generator\" while the source terms were processed by the \"query parser\". We loosely refer to the combination of (user) query parsing and (Lucene) query generation as \"the query parser\", but it is important to distinguish (user query) \"parsing\" from (Lucene Query) \"generation\".\n\nThe query parser does its best to handle sub-terms reasonably, but expecting that they will magically handled the same exact way as source terms is somewhat impractical. That doesn't mean that there can't be improvement, but simply that a dose of realism is needed when considering the potential, challenges, and limits of query parsing/processing/generation. "
        },
        {
            "author": "Joel Rosen",
            "id": "comment-13436069",
            "date": "2012-08-16T16:20:16+0000",
            "content": "Sounds to me like this is an English-centric design flaw with dismax.  The point of dismax is to intelligently process simple user-entered phrases, right?  If I understand correctly, it does this by looking at the terms entered and making some decisions about how to join them with AND or OR.  But it assumes that a term is a whitespace-delimited string, yes?  This is an incorrect assumption for Chinese.  If instead of making this assumption, dismax ran the analyzers first to determine what is and isn't a term, then I imagine you would get more predictable behavior across both whitespace delimited and non-whitespace delimited languages, and you wouldn't need any \"magical\" handling for different languages. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13436082",
            "date": "2012-08-16T16:40:09+0000",
            "content": "Be careful not to confuse dismax and edismax. They are two different query parsers, with different goals.\n\nOne of edismax's goals was to support \"fielded queries\" (e.g., \"title:abc AND date:123\") and the full Lucene query syntax. No typical analyzer will be able to tell you that title and date are field names.\n\nNot \"English-centric\", but European/Latin-centric for sure. The edismax and classic Lucene query parsers share that heritage, based on whitespace, but the dismax query parser doesn't \"suffer\" from that same need to parse field names and operators.\n\nThere is no question that better query parser support is needed for non-European/Latin languages, but that requires careful, high-level, overall design, which is a tall order for a fast-paced open source community where features tend to be looked at in isolation.\n\nOne clarification...\n\nassumes that a term is a whitespace-delimited string\n\nYes and no. We need to be careful about distinguishing a \"source term\" - what the parser recognizes, which is whitespace delimited, from \"analyzed terms\" which are recognized and output by the field type analyzers. There is no requirement that the output terms be whitespace-delimited or that the input to an anlyzer be whitespace-delimited. So, the theory has been that even a whitespace-centric complex-structure query parser can also handle, for example, Chinese text. Obviously that hasn't worked out as cleanly as desired and more work is needed. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13436454",
            "date": "2012-08-17T00:04:36+0000",
            "content": "My proposal is for edismax to set the Lucene default query operator to \"AND\" if either: 1) \"q.op\" is \"AND\", or 2) \"mm\" is \"100%\".\n\nI think that will address the stated problem.\n\nAny objection?\n\nI'll try to come up with a patch, but a committer will be needed to apply it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13436468",
            "date": "2012-08-17T00:22:28+0000",
            "content": "My proposal is for edismax to set the Lucene default query operator to \"AND\"\n\nHmmm, I dunno.  mm=100% is really only meant to apply to top level query terms, not structured lucene queries.\n\nFor example, in (foo:x foo:(a b c))\nIt doesn't seem like a b c should all be mandatory just because there happens to be a default mm of 100% (and they are not today). "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13436484",
            "date": "2012-08-17T00:45:04+0000",
            "content": "I could back off and simply say that edismax should set the Lucene default query operator to \"AND\" if \"q.op\" is \"AND\", but that would not address this particular issue, which is complaining that mm won't force the split terms to be ANDed.\n\nIf we really want to say that mm CAN'T be used to force split terms to be ANDED, then we should really resolve this issue asinvalid/won't fix.\n\nI should probably file a separate issue for the fact that q.op is not obeyed for any but the top-level query.\n\nAnd, the wiki makes no mention of \"mm\" being intended only for the top level query. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13436486",
            "date": "2012-08-17T00:48:37+0000",
            "content": "I was not saying that this issue shouldn't be fixed, but merely commenting on the negative consequences of one proposed solution. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13437426",
            "date": "2012-08-18T22:23:35+0000",
            "content": "[\nSee SOLR-3636, it's the same problem space but with synonym expansion. If \"Monkeyhouse\" expands to \"monkey house\", then a dismax or edismax query finds words with either (\"monkey\" OR \"house\"). Must-match defaults to 100% so you would expect this to mean \"monkey\" AND \"house\".\n\nThis seems to be a multi-part problem.\n] retracted as per below. Yes, synonyms are another box'o'fun.  "
        },
        {
            "author": "Bernd Fehling",
            "id": "comment-13437674",
            "date": "2012-08-20T06:19:06+0000",
            "content": "I would not mix synonyms into this because they need a special seperate treatment.\nIt might work for \"monkeyhouse => monkey house\" but what if you have synonyms like \"nuclear fission, kernspaltung, fissione nucleare\"?\nYou would expect to get a search like (nuclear AND fission) OR (kernspaltung) OR (fissione AND nucleare).\nThis is a simplified example just to show that if you include synonyms into this issue you also have to detect/parse/obey the kind of synonym mapping. "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13440583",
            "date": "2012-08-23T19:30:03+0000",
            "content": "Just repeated tests in Solr 4.0Beta and the bug behaves the same. "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13440667",
            "date": "2012-08-23T21:11:54+0000",
            "content": "File is gzipped. Unix line endings. Put document in solr/example/exampledocs.  Queries listed in file. "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13440669",
            "date": "2012-08-23T21:12:17+0000",
            "content": "I'm not at the point where I understand the test cases for Edismax enough to write unit tests. If someone can point me to an example unit test somewhere that I could use to model a test please do.\n  In the meantime, attached is a file which can be put in the Solr exampledocs directory and indexed.  Sample queries demonstrating the problem with English hyphenated words and with CJK are included  "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13440670",
            "date": "2012-08-23T21:12:47+0000",
            "content": "See above note "
        },
        {
            "author": "Naomi Dushay",
            "id": "comment-13454533",
            "date": "2012-09-13T01:00:02+0000",
            "content": "(comment redacted! \u2013 I can't repeat the results today.  Perhaps I was missing a Solr commit ... since the index is behaving differently, and I didn't change the index, though I did restart Solr a few times.)\n\n(below left for historical purposes)\n\nI may have stumbled into something.   Try setting q.op explicitly.\n\n(baseurl)/select?q=fire-fly\n\ngives me a lot more results than\n\n(baseurl)/select?q=fire-fly&q.op=AND\n\n\noddly,   q.op=OR   gives me the same results as setting it to AND.\n\n\nWhy did I stumble into this?\n\nfrom http://wiki.apache.org/solr/DisMaxQParserPlugin#mm_.28Minimum_.27Should.27_Match.29\n\n\"In Solr 1.4 and prior, you should basically set mm=0 if you want the equivilent of q.op=OR, and mm=100% if you want the equivilent of q.op=AND. In 3.x and trunk the default value of mm is dictated by the q.op param (q.op=AND => mm=100%; q.op=OR => mm=0%). Keep in mind the default operator is effected by your schema.xml <solrQueryParser defaultOperator=\"xxx\"/> entry. In older versions of Solr the default value is 100% (all clauses must match)\"\n\nI have q.op set in my schema, thus:\n\n<solrQueryParser defaultOperator=\"AND\" />\n\nbut when I use the q.op parameter, I experience something different.  Wild! \n\nDoes this give us any insights? "
        },
        {
            "author": "Naomi Dushay",
            "id": "comment-13487310",
            "date": "2012-10-30T22:25:25+0000",
            "content": "Would this bug be addressed if this one is addressed?  https://issues.apache.org/jira/browse/LUCENE-3833  (add operator to lucene queryparser for term quorum) "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488370",
            "date": "2012-11-01T00:01:02+0000",
            "content": "Simple unit test, based on Tom's example. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488380",
            "date": "2012-11-01T00:35:15+0000",
            "content": "here's my hack patch.\n\nno idea if its right... in particular i dont really know what is going on with this query parser in general.\n\nthe high level idea is that we can detect if the \"multiple tokens\" are synonyms versus CJK by the coord setting of the BQ, since coord will be enabled in the e.g. CJK case (same as if it were whitespace), but disabled for synonyms.\n\nbut it could be totally wrong: at least tests pass and it might give someone else some ideas or be a useful workaround   "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488449",
            "date": "2012-11-01T04:31:42+0000",
            "content": "I traced through the logic here, and added additional tests (e.g. multi-field aliasing for this CJK case).\n\nActually I needed the logic in a different place, anyway I think this patch is significantly more baked. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488469",
            "date": "2012-11-01T05:06:58+0000",
            "content": "More tests: I think this patch is ready actually.\n\nIts well contained, we only apply this when the analyzer splits a token (e.g. CJK), not to any structured queries and not for the synonyms case. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488932",
            "date": "2012-11-01T18:57:22+0000",
            "content": "I pinged hossman on IRC for some feedback, ill update the tests to show we aren't changing behavior with synonyms: this isnt tested today.\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13488988",
            "date": "2012-11-01T20:07:05+0000",
            "content": "patch with the added synonyms test. "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13491918",
            "date": "2012-11-06T22:50:12+0000",
            "content": "Back-port to 3.6 branch "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13491922",
            "date": "2012-11-06T22:57:54+0000",
            "content": "I back-ported to 3.6 branch.  Forgot to change the name from SOLR-3589.patch, so the 6/Nov/12 patch is the 3.6 patch against yesterdays svn version of 3.6.\n\nMain difference I saw between 3.6 and 4.0 is that Solr 4.0 uses DisMaxQParser.parseMinShouldMatch() to set the default at 0% if q.op=OR and %100 if q.op =AND\n\nI just kept the 3.6 behavior which uses 3.6 default of 100% (if mm is not set)\n\nI'll test the 3.6 patch against a production index tomorrow.\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13492048",
            "date": "2012-11-07T01:47:22+0000",
            "content": "Hi Tom: thanks for working on the 3.6 backport!\n\nI'll commit the trunk/4.x patch first, and wait for your testing and review your patch before looking at 3.6! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13492095",
            "date": "2012-11-07T03:40:40+0000",
            "content": "Committed to trunk/4.x.\n\nWill look tomorrow at 3.6. "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13492487",
            "date": "2012-11-07T16:41:07+0000",
            "content": "Forgot to work from your latest patch with the synonyms test.   I'll post a new backport of the patch with the synonyms test and against the latest 3.6x in svn shortly "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13492535",
            "date": "2012-11-07T17:41:00+0000",
            "content": "Backport to 3.6 r1406713. Includes synonyms test.\n\nWill test in against production later today  "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-13492591",
            "date": "2012-11-07T19:01:04+0000",
            "content": "Hi Robert,\n\nI just put the backport to 3.6 up on our test server and pointed it to one of our production shards.  The improvement for Chinese queries  are dramatic.  (Especially for longer queries like the TREC 5 queries, see examples below)\n\nWhen you have time, please look over the backport of the patch.  I think it is fine but I would appreciate you looking it over.  My understanding of your patch is that it just affects a small portion of the edismax logic, but I don't understand the edismax parser well enough to be sure there isn't some difference between 3.6 and 4.0 that I didn't account for in the patch.\n\nThanks for working on this.   Naomi and I are both very excited about this bug finally being fixed and want to put the fix into production soon.\n\u2014\nExample TREC 5 Chinese queries:\n\n<num> Number: CH4\n<E-title> The newly discovered oil fields in China.\n<C-title> \u4e2d\u56fd\u5927\u9646\u65b0\u53d1\u73b0\u7684\u6cb9\u7530   \n40,135 items found for \u4e2d\u56fd\u5927\u9646\u65b0\u53d1\u73b0\u7684\u6cb9\u7530 with current implementation (due to dismax bug)\n78 items found for \u4e2d\u56fd\u5927\u9646\u65b0\u53d1\u73b0\u7684\u6cb9\u7530 with patch\n\n<num> Number: CH10\n<E-title> Border Trade in Xinjiang\n<C-title> \u65b0\u7586\u7684\u8fb9\u5883\u8d38\u6613  \n20,249 items found for \u65b0\u7586\u7684\u8fb9\u5883\u8d38\u6613  current implementation (with bug)\n243 items found for \u65b0\u7586\u7684\u8fb9\u5883\u8d38\u6613      with patch. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13493133",
            "date": "2012-11-08T12:03:15+0000",
            "content": "Backported to 3.6 branch in case we do a 3.6.2\n\nThanks Tom! "
        },
        {
            "author": "Naomi Dushay",
            "id": "comment-13493397",
            "date": "2012-11-08T19:09:56+0000",
            "content": "any chance this fix can be applied to dismax as well? "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610588",
            "date": "2013-03-22T16:19:39+0000",
            "content": "[branch_4x commit] Robert Muir\nhttp://svn.apache.org/viewvc?view=revision&revision=1406439\n\nSOLR-3589: Edismax parser does not honor mm parameter if analyzer splits a token "
        }
    ]
}