{
    "id": "SOLR-6787",
    "title": "API to manage blobs in  Solr",
    "details": {
        "components": [],
        "type": "Sub-task",
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "A special collection called .system needs to be created by the user to store/manage blobs. The schema/solrconfig of that collection need to be automatically supplied by the system so that there are no errors\n\nAPIs need to be created to manage the content of that collection\n\n\n\n#create your .system collection first\nhttp://localhost:8983/solr/admin/collections?action=CREATE&name=.system&replicationFactor=2\n#The config for this collection is automatically created . numShards for this collection is hardcoded to 1\n\n#create a new jar or add a new version of a jar\n\ncurl -X POST -H 'Content-Type: application/octet-stream' --data-binary @mycomponent.jar http://localhost:8983/solr/.system/blob/mycomponent\n\n#  GET on the end point would give a list of jars and other details\ncurl http://localhost:8983/solr/.system/blob \n# GET on the end point with jar name would give  details of various versions of the available jars\ncurl http://localhost:8983/solr/.system/blob/mycomponent\n# GET on the end point with jar name and version with a wt=filestream to get the actual file\ncurl http://localhost:8983/solr/.system/blob/mycomponent/1?wt=filestream > mycomponent.1.jar\n\n# GET on the end point with jar name and wt=filestream to get the latest version of the file\ncurl http://localhost:8983/solr/.system/blob/mycomponent?wt=filestream > mycomponent.jar\n\n\n\nPlease note that the jars are never deleted. a new version is added to the system everytime a new jar is posted for the name. You must use the standard delete commands to delete the old entries",
    "attachments": {
        "SOLR-6787.patch": "https://issues.apache.org/jira/secure/attachment/12684901/SOLR-6787.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2014-12-03T14:16:09+0000",
            "author": "Noble Paul",
            "content": "Feature complete. But no testcases ",
            "id": "comment-14233022"
        },
        {
            "date": "2014-12-08T06:02:59+0000",
            "author": "Shawn Heisey",
            "content": "This sounds very cool.\n\nI do have a couple of questions after looking at the patch, probably showing a lot of ignorance:\n\n\n\tIs this SolrCloud specific?  If so, could it work on non-cloud setups?  There seems to be ZK-specific code, and you mentioned collection, not core.\n\tI see a MAX_SZ constant set to 2MB, although it doesn't appear to be used anywhere in the patch.  One of the jars that I use with Solr is the icu4j jar, which is over 9 MB.  Would the the max size be configurable?\n\n ",
            "id": "comment-14237482"
        },
        {
            "date": "2014-12-08T09:28:05+0000",
            "author": "Noble Paul",
            "content": "Is this SolrCloud specific? If so, could it work on non-cloud setups? \n\nNo. It works in non-cloud mode. But , we give emphasis to solrcloud on all documentation\n\nI see a MAX_SZ constant set to 2MB, although it doesn't appear to be used anywhere in the patch...\n\nIt is used and that is the limit\n\nOne of the jars that I use with Solr is the icu4j jar which is over 9 MB.\n\nI want to keep a sensible max size so that people don't screw their system. BTW, I would make it configurable for the handler  using the config API ",
            "id": "comment-14237670"
        },
        {
            "date": "2014-12-08T11:10:31+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "A special collection called .system needs to be created by the user to store/manage blobs.\n\nThat 'dot' might be easy to miss and then people will get weird 404 errors. Perhaps we should keep system just like version or route or even route? ",
            "id": "comment-14237770"
        },
        {
            "date": "2014-12-08T12:07:13+0000",
            "author": "Noble Paul",
            "content": "The period looked like a logical name for users failiar with *nix where , hidden/syste directories are named so. Anyway I'm open to suggestions ",
            "id": "comment-14237813"
        },
        {
            "date": "2014-12-09T13:53:55+0000",
            "author": "Noble Paul",
            "content": "final patch with testcases. \nThe collection name is still open for change ",
            "id": "comment-14239405"
        },
        {
            "date": "2014-12-09T15:58:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1644095 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1644095 ]\n\nSOLR-6787 API to manage blobs in Solr ",
            "id": "comment-14239574"
        },
        {
            "date": "2014-12-10T06:42:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1644341 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1644341 ]\n\nSOLR-6787 adding CHANGES.TXT entry ",
            "id": "comment-14240723"
        },
        {
            "date": "2014-12-10T11:30:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1644376 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1644376 ]\n\nSOLR-6787 API to manage blobs in Solr ",
            "id": "comment-14240967"
        },
        {
            "date": "2014-12-10T13:10:15+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "The period looked like a logical name for users failiar with *nix where , hidden/syste directories are named so. Anyway I'm open to suggestions\n\nYeah, on further thinking, I've warmed up to .system. Looking forward to what comes next! ",
            "id": "comment-14241060"
        },
        {
            "date": "2014-12-10T17:32:38+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1644489 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1644489 ]\n\nSOLR-6787 removing the dependency on zk jar for post.jar ",
            "id": "comment-14241415"
        },
        {
            "date": "2014-12-10T17:37:15+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1644494 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1644494 ]\n\nSOLR-6787 removing the dependency on zk jar for post.jar ",
            "id": "comment-14241429"
        },
        {
            "date": "2014-12-10T18:41:08+0000",
            "author": "Yonik Seeley",
            "content": "I'm not sure if there was a previous discussion that covers some of my questions here... so I'll just shoot:\n\nI think I understand some of the motivations around a special collection... but I don't understand why there needs to be special APIs.\nWe should be able to support binary field types, and if our support is lacking, we should fix that.  Same with more REST-like APIs to query the collection or retrieve data, and same for document versioning.\n\n As far as the motivations for this issue... I see SOLR-6801 being linked.  But can a request handler / component in solrconfig.xml make use of a jar in .system?  Does this mean that .system will somehow need to come up before every other collection in a cloud setup (or even in a stand-alone setup).  Or is persistence being handled some other way?\n\nIn some ways it feels like we're starting from the bottom up (which can be a fine approach) without the use-cases / high level designs / goals being documented or hashed out (unless I've missed some of these discussions... pointers welcome).  Does this stuff relate at all to the goal of providing a smaller download and having an easier plugin mechanism for the stuff that's in contrib? ",
            "id": "comment-14241513"
        },
        {
            "date": "2014-12-11T02:47:13+0000",
            "author": "Noble Paul",
            "content": "but I don't understand why there needs to be special APIs.\n\nThese are not really \"special\" APIs. It is just another requesthandler which anyone can register in any core. \n\nWe should be able to support binary field types, and if our support is lacking\n\nyes. But it also needs to do more things to make this usable. This requesthandler really makes that very convenient. I don't see how can a generic binary field API be as useful. Suggestions are welcome\n\nBut can a request handler / component in solrconfig.xml make use of a jar in .system? Does this mean that .system will somehow need to come up before every other collection in a cloud setup \n\nAll the handlers loaded from .system  will be automatically be startup=\"lazy . So , any request fired to those handlers must  respond with \" .system collection not yet available \" , till .system is loaded\n\nDoes this stuff relate at all to the goal of providing a smaller download and having an easier plugin mechanism for the stuff that's in contrib\n\nNo , this is not conceived for a smaller download.  I don't yet plan to make the contribs plugged in through this\n\nThe usecase is this.\n\nI have a fairly large solrcloud cluster where I deploy a custom component. The current solution is to go to all the nodes and put a jar file there and do a rolling restart of the entire cluster. And, for every new version of the component , the user has to go through the same  steps. For Lucidworks , it is a fairly common usecase and will make our product easier to manage\n\nThe other usecase is to manage other files like synonyms / stopwords (or any other files required by any other component) in Solr so that we don't load very large files into Zookeeper \n\nIn some ways it feels like we're starting from the bottom up (which can be a fine approach) without the use-cases / high level designs / goals\n\nWe are rethinking the way Solr is being used. The objective is to make it less painful to do what we experts can do with Solr.  I'm glad that people are asking . NO , you haven't missed anything . This JIRA is the first piece of documentation ever to happen on this topic and all questions are welcome . \nLet's build it together ",
            "id": "comment-14242041"
        },
        {
            "date": "2014-12-17T19:46:27+0000",
            "author": "Yonik Seeley",
            "content": "These are not really \"special\" APIs.\nI was responding to this:  \"APIs need to be created to manage the content of that collection\"\nAnd I was wondering since binary field and \"blob\" seem synonymous, why there would be a separate/different API to get/set the value of such a field.\n\nAll the handlers loaded from .system will be automatically be startup=\"lazy\" .\n\nBut request handlers are one of the only things that have support for \"lazy\".\nWhat's the plan to support custom SearchComponents, Update processors, QParsers, or ValueSourceParsers (all of those are very common)?\n\nAlso, a big question is persistence.  What happens when you add a request handler via API, and then the server is bounced?\n\nWe are rethinking the way Solr is being used. \n\nThat's great, but please do so in public forums so everyone can participate in the discussion. ",
            "id": "comment-14250397"
        },
        {
            "date": "2014-12-18T06:24:19+0000",
            "author": "Noble Paul",
            "content": "why there would be a separate/different API to get/set the value of such a field.\n\nTell me how can I do that today . I may need to use SolrJ or something. If I let users do it themselves , they will use their own schema and their own tools to achieve the same. Eventually we will have a system which \"anyone can use but no one would want to use\" \n\nAnd what about versioning? I don't want different nodes to run different versions of the jar\n\nWe should stop building \"ready to cook\" stuff and start making \"ready to eat\" stuff if we want to survive in this space.\n\nSo. I am starting with the user and ask the question , \"how would they use it\". Then I think of how we can achieve it in Solr (and not the other way around) If I can't put a simple coherent way of using something I don't wan't to recommend anyone to use it.\n\nBut request handlers are one of the only things that have support for \"lazy\".\n\nRequestHandlers are being added first. They are the first class citizens. Others will have to be loaded lazily as well. Will be taken up in due course\n\nWhen I said \"lazy\" it does not mean the current implementation of \"lazy\" . It is a new implementation of laziness required for  dynamic loading\n\nAlso, a big question is persistence. What happens when you add a request handler via API, and then the server is bounced?\n\nThey are all persisted . Actually any new component added causes a core reload automatically . refer SOLR-6607\n\nThat's great, but please do so in public forums so everyone can participate in the discussion.\n\nI'm going solo. So , there are not many discussions. I would like others to start discussions in JIRA and I can participate\n\nI used \"we\" because it is a goal for Lucidworks to make this possible. \"How\" is not discussed \n ",
            "id": "comment-14251277"
        },
        {
            "date": "2014-12-18T09:40:47+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1646418 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1646418 ]\n\nSOLR-6787 refactored a method out ",
            "id": "comment-14251427"
        },
        {
            "date": "2014-12-18T09:49:10+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1646419 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1646419 ]\n\nSOLR-6787 refactored a method out ",
            "id": "comment-14251434"
        },
        {
            "date": "2014-12-18T15:00:00+0000",
            "author": "Alexandre Rafalovitch",
            "content": "Why do you need a user to create the collection first? Especially since the config/etc is hardcoded and it needs to be present on every node? Wouldn't it make sense for collection to be just auto-created on first access? Or is there a specific flexibility that only user can decide on?\n\nAlso, if you have heterogeneous environment, the current implementation would require a union of all blobs, right? As in, every node will need to have all possible blobs/jars, not just the jars the collections on that node have.\n\nFinally, does the .system collection show up on the Admin UI? Are there implications/restrictions of that? ",
            "id": "comment-14251750"
        },
        {
            "date": "2014-12-18T17:34:02+0000",
            "author": "Yonik Seeley",
            "content": "\n> why there would be a separate/different API to get/set the value of such a field.\nTell me how can I do that today.\n\nThat's my entire point... if it's hard to do X, look first into making X easier rather than creating a different method.\nAt the end of the day, there may still be some little things that warrant separate API calls to make it easier.  But if anything is identified to make generic APIs more usable, that's a win.\n\nAnother way to look at it: It appears you've created an API specific to \".system\"... but why should it be specific?  If it's easier to manage blobs this way, shouldn't users be able to use this on whatever collections / fields they want? ",
            "id": "comment-14251925"
        },
        {
            "date": "2014-12-18T18:05:21+0000",
            "author": "Yonik Seeley",
            "content": "\nWe should stop building \"ready to cook\" stuff and start making \"ready to eat\" stuff if we want to survive in this space.\nSo. I am starting with the user and ask the question , \"how would they use it\". \n\nI've always had that philosophy and I think the majority of other committers do also.\nTrying to reuse and improve existing APIs before creating new ones, or making new APIs as generic as possible does not run counter to that (and produces a better system over the long haul).\n\n\nI'm going solo. So , there are not many discussions. I would like others to start discussions in JIRA and I can participate\n\nAnother option would have been to do it in a branch first, esp if it's exploratory. ",
            "id": "comment-14251967"
        },
        {
            "date": "2014-12-18T19:29:24+0000",
            "author": "Noble Paul",
            "content": "That's my entire point... if it's hard to do X, look first into making X easier \n\nWe should do that too. \n\nBut it does not solve the problem\n\nIf it's easier to manage blobs this way\n\nIt won't be. Why would anyone want to store a blob and all the past versions of it normally? \n\nI'm trying to dedupe a blob here , is it required normally ?\n\nI'm trying to ensure that there is always a name for a blob  , is it required normally ?\n\nAnother option would have been to do it in a branch first, esp if it's exploratory.\n\nIt is not exploratory . This is a feature we need \n\nTrying to reuse and improve existing APIs before creating new ones, or making new APIs as generic as possible does not run counter to that \n\nAs I said earlier I'm fully convinced about the need of this API. At the same time I'm open to suggestions to improve it. \nWe do an exploratory branch when we have a doubt on how to implement it. In this case , the question is about the interface which requires discussion not another branch\n\nI guess you are confused by the title of the ticket. It is not an API to jus manage \"binary\" fields . I need certain semantics for storing large objects in a special collection. There are more use cases , such as large synonyms, stopwords etc and other storage for the system where ZK is not suitable ",
            "id": "comment-14252097"
        },
        {
            "date": "2014-12-18T19:41:16+0000",
            "author": "Noble Paul",
            "content": "hi Alexandre Rafalovitch I postyed a reply to you , but it got lost\n\nWhy do you need a user to create the collection first? Especially since the config/etc is hardcoded and it needs to be present on every node?\n\nMost users will not use this feature . When it becomes common place we should create it y default. \nAlso, if you have heterogeneous environment, the current implementation would require a union of all blobs, right? As in, every node will need to have all possible blobs/jars, not just the jars the collections on that node have.\n\nThat is not true. The jars are fetched over the http API and used on demand . It is never fetched from a local collection\n\n\nFinally, does the .system collection show up on the Admin UI? Are there implications/restrictions of that?\n\nIt is a normal collection where we have a predefined schema/config and it shows up in the admin UI ",
            "id": "comment-14252121"
        },
        {
            "date": "2014-12-21T15:48:45+0000",
            "author": "Yonik Seeley",
            "content": "\n> That's my entire point... if it's hard to do X, look first into making X easier\nWe should do that too.\nBut it does not solve the problem\n\nIt doesn't have to solve the whole top level use case you're thinking of... that's not what I'm talking about at all.\nI'm talking about API design in general.  If X is hard, look to make it easier in a way that benefits more than just the specific \"problem\" you're trying to solve.\n\n\n> If it's easier to manage blobs this way\nIt won't be. Why would anyone want to store a blob and all the past versions of it normally?\nPeople have asked to store blobs (binary fields), and people have asked for versioning.\n\nI'm trying to dedupe a blob here , is it required normally ?\nYes, people have asked for deduping (that's why we have an update processor for it now)\n\nI'm trying to ensure that there is always a name for a blob , is it required normally ?\nYes, this is why required fields were added to the schema.\n\nThis is a feature we need\n\nThe need for the functionalltiy of a feature says nothing about what form the API/implementation should take.\nAnyway, it feels like we're completely talking past each-other or something at this point... it seemed pretty obvious to me that \"blob\" == \"binary field\".\n ",
            "id": "comment-14255192"
        },
        {
            "date": "2014-12-22T15:05:06+0000",
            "author": "Noble Paul",
            "content": "If these were the requirements , I fail to see them captured in a ticket. \n\nBut , it is still possible to offload most of the code in this handler to a generic API without impacting this set of APIs. \n\nThe questions I would like answered are \n\n\tis it required to change the public interfaces of this API? If yes, please elaborate\n\tWe may need a better binary field API. I totally agree. I would like to see the specific requirements . I'll be glad to make this generic\n\n ",
            "id": "comment-14255809"
        },
        {
            "date": "2014-12-22T15:32:24+0000",
            "author": "Alexandre Rafalovitch",
            "content": "Looking at the patch, it seems an improvement of couple of universal pieces of code plus hard-coding a specific collection setup and URL structure. This is a very basic view, as I am not totally familiar with the codebase, but perhaps I can help to find the middle ground.\n\nThe universal changes seem to be about allow post tool and other code to better handle binary content. So, that's already generic.\n\nThe hardcoded schema/solrconfig look to me like they should be something like a special-case configset that's perhaps stored in the distribution jar. Which would be separate new, but I am sure useful, functionality. The hardcoded shard requirements look to me like something that would be a documentation issue, especially since we require the user to create the collection anyway. Or maybe it should be a variable somewhere that can be carried along with the configset (core.properties?)\n\nThe other part is about loading and serving a binary blob. We already have BinaryField and in fact that's what's used under the covers in the hard-coded schema. How do we populate/extract things from it now? This implementation looks useful, it could just have an assertion/requirement that whatever the query string is, the result should be a single item, single field which would then be binary streamed to the user. \n\nThere was some de-duplication code. Could it not be an UpdateRequestProcessor of some sort? \n\nFinally, there is the convenience API that hardcodes blob name and version as URL parameters. This may be hard to make generic but is it really that big a deal given that this particular use-case is internal (right?). The internal code could generate query parameters just as easily as the REST style.  ",
            "id": "comment-14255840"
        },
        {
            "date": "2014-12-22T15:49:02+0000",
            "author": "Mark Miller",
            "content": "I'm going solo.\n\nBefore going solo, I wish you would learn how the project formats code, tags commits, and focus a little more on getting your test solid before committing. These mistakes are all too common. ",
            "id": "comment-14255853"
        },
        {
            "date": "2014-12-22T15:55:58+0000",
            "author": "Noble Paul",
            "content": "point taken Mark Miller \n\n And all inputs on this API design are welcome ",
            "id": "comment-14255862"
        },
        {
            "date": "2014-12-22T17:21:05+0000",
            "author": "Yonik Seeley",
            "content": "If these were the requirements , I fail to see them captured in a ticket.\n\nI think you're maybe confusing open source development with commercial software development \nWe don't really work off of \"requirements\", and we certainly strive to do better than just meet requirements.\nWe need to thinking about the potential requirements/desires of the thousands of users (and potential new users) of Solr.\nWe need to think hard about APIs since they tend to stick around a long time.\n\nI was essentially asking \"can we do better?\".  It doesn't mean the answer is \"yes\"... but you should have hopefully thought about it.\n\nHere's some stupid simple hypothetical examples of what I'm talking about:\n\nHypothetical:  I need to add three numbers together in a function query... I'm going to add a \"add3(x,y,z)\".  Can we do better?  yeah... maybe we should just make the existing \"add()\" take any number of arguments.  There was no \"requirement\" to do more than 3... but that doesn't matter in the slightest.\n\nHypothetical: My common function add(mul(a,b),mul(c,d)) was too slow for my requirements, so I added a custom add_prods(a,b,c,d) that gets rid of the overhead and is now faster.  Can we do better?  Yes... let's not change the API for this special case and instead look for these patterns and automatically optimize them.  ",
            "id": "comment-14255951"
        },
        {
            "date": "2014-12-22T17:52:23+0000",
            "author": "Noble Paul",
            "content": "I was essentially asking \"can we do better?\". It doesn't mean the answer is \"yes\"... but you should have hopefully thought about it. \n\nI totally understand where you are coming from. I agree with you on most of it in principle.  \n\nMy point was this\n\n\n\tI would like to know if the public API of this ticket can be improved\n\tCan we totally do away with this and make it some kind of generic feature. honestly , I don't see that as a good option.\n\n\n\nI'm very much in agreement with you on the point that we need to get some of it to a generic API.  ",
            "id": "comment-14255977"
        },
        {
            "date": "2015-01-01T12:47:24+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1648836 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1648836 ]\n\nSOLR-6787 changed the schema of blob store. now, the id is blobName/version and not the md5 ",
            "id": "comment-14262542"
        },
        {
            "date": "2015-01-01T14:46:57+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1648848 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1648848 ]\n\nSOLR-6787 changed the schema of blob store. now, the id is blobName/version and not the md5 ",
            "id": "comment-14262565"
        },
        {
            "date": "2015-01-04T15:07:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1649349 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1649349 ]\n\nSOLR-6787  more logging to trace errors ",
            "id": "comment-14263885"
        },
        {
            "date": "2015-01-07T10:49:40+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650030 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1650030 ]\n\nSOLR-6787 hardening tests ",
            "id": "comment-14267504"
        },
        {
            "date": "2015-01-07T10:51:55+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650032 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650032 ]\n\nSOLR-6787 hardening tests ",
            "id": "comment-14267507"
        },
        {
            "date": "2015-01-08T06:33:30+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650212 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1650212 ]\n\nSOLR-6787 commit right away instead of waiting ",
            "id": "comment-14268891"
        },
        {
            "date": "2015-01-08T06:45:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650214 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650214 ]\n\nSOLR-6787 commit right away instead of waiting ",
            "id": "comment-14268904"
        },
        {
            "date": "2015-01-08T11:06:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650251 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1650251 ]\n\nSOLR-6787 more logging ",
            "id": "comment-14269195"
        },
        {
            "date": "2015-01-08T11:07:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650252 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650252 ]\n\nSOLR-6787 more logging ",
            "id": "comment-14269197"
        },
        {
            "date": "2015-01-09T06:52:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650448 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1650448 ]\n\nSOLR-6787 A simple class to mask a handler defined in same path ",
            "id": "comment-14270657"
        },
        {
            "date": "2015-01-09T06:57:07+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650449 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650449 ]\n\nSOLR-6787 A simple class to mask a handler defined in same path ",
            "id": "comment-14270666"
        },
        {
            "date": "2015-01-10T10:32:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650729 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1650729 ]\n\nSOLR-6787 enable autocommit maxDocs=1 for .system collection ",
            "id": "comment-14272434"
        },
        {
            "date": "2015-01-10T10:40:54+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1650731 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1650731 ]\n\nSOLR-6787 enable autocommit maxDocs=1 for .system collection ",
            "id": "comment-14272437"
        },
        {
            "date": "2015-01-10T14:20:18+0000",
            "author": "Mark Miller",
            "content": "Please develop these things to a decent level outside of svn and then put up a patch. You tend to commit early half baked patches that fail a lot and then fire in a bunch of random commits after with no patches. I'm really unhappy about it and I'm going to start -1 more of these issues. It's become a common pattern. This is not how we develop on Lucene and Solr. Use a branch if you need to do this. You can't easily follow this style of dev if it's done regularly, it makes reviews and back ports difficult, and everyone has to deal with the frequently failing tests. There is no need to put your stuff in way before it's ready.\n ",
            "id": "comment-14272515"
        },
        {
            "date": "2015-01-11T13:22:59+0000",
            "author": "Noble Paul",
            "content": "Mark\nHow do you know something is fully developed? You build a feature , add a test do manual testing and if everything is passing commit it . \nThis did not fail any old test and it is only failing the new tests in Jenkins. So, I'm fixing what I think is the issue and checking in. Even the original commit does not have any obvious problem that I know of. Sometimes I'm trying a different approach . \nThere are dozens of tests failing every day, are we going to remove all of them? There were replication issues due to which tests were failing and it could have been a reason for this failure too ",
            "id": "comment-14272898"
        },
        {
            "date": "2015-01-21T07:12:51+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653451 from Noble Paul in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1653451 ]\n\nSOLR-6787, SOLR-6801 use realtime get to verify that the versions do not collide ",
            "id": "comment-14285276"
        },
        {
            "date": "2015-01-21T07:15:31+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653452 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1653452 ]\n\nSOLR-6787, SOLR-6801 use realtime get to verify that the versions do not collide ",
            "id": "comment-14285279"
        },
        {
            "date": "2015-01-21T07:17:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653453 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1653453 ]\n\nSOLR-6787, SOLR-6801 use realtime get to verify that the versions do not collide ",
            "id": "comment-14285282"
        },
        {
            "date": "2015-02-03T14:44:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1656747 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1656747 ]\n\nSOLR-6787: .system collection create fails if /configs dir is not present in ZK ",
            "id": "comment-14303351"
        },
        {
            "date": "2015-02-03T14:46:27+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1656748 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1656748 ]\n\nSOLR-6787: .system collection create fails if /configs dir is not present in ZK ",
            "id": "comment-14303357"
        },
        {
            "date": "2015-02-23T05:00:54+0000",
            "author": "Anshum Gupta",
            "content": "Bulk close after 5.0 release. ",
            "id": "comment-14332601"
        },
        {
            "date": "2015-03-10T01:06:32+0000",
            "author": "Yonik Seeley",
            "content": "I just ran onto this new method on SolrQueryRequest:\n\n\n+\n+  /** Forward the request to another handler. DO a return after this call if\n+   * no other operations need to be performed\n+   * @param handler the name of the handler\n+   * @param params The new set of parameter\n+   */\n+  public void forward(String handler, SolrParams params,  SolrQueryResponse rsp);\n\n\n\nThat feels like a really odd method to have on SolrQueryRequest... is there some advantage to having it there? ",
            "id": "comment-14354021"
        },
        {
            "date": "2015-03-10T01:19:25+0000",
            "author": "Yonik Seeley",
            "content": "The default implementation is also buggy... it creates a request object that is never closed.\nIt's also not clear what \"DO a return after this call if no other operations need to be performed\" means.\n\nI wouldn't want to encourage people to use this... perhaps this refactor should just be reverted or moved to BlobManager (as it's currently the only user). ",
            "id": "comment-14354038"
        },
        {
            "date": "2015-03-10T01:19:30+0000",
            "author": "Yonik Seeley",
            "content": "The default implementation is also buggy... it creates a request object that is never closed.\nIt's also not clear what \"DO a return after this call if no other operations need to be performed\" means.\n\nI wouldn't want to encourage people to use this... perhaps this refactor should just be reverted or moved to BlobManager (as it's currently the only user). ",
            "id": "comment-14354040"
        },
        {
            "date": "2015-03-10T05:34:23+0000",
            "author": "Noble Paul",
            "content": "The default implementation is also buggy... it creates a request object that is never closed.\n\nIt is done in a try finally block. So, it will be closed\n\n\n try(LocalSolrQueryRequest r = new LocalSolrQueryRequest(getCore(), params)) {\n      getCore().getRequestHandler(handler).handleRequest(r, rsp);\n    }\n\n\n\nIt's also not clear what \"DO a return after this call if no other operations need to be performed\" means.\n\nIt means the caller can just return if the forwarded request will do all the work\n\nI wouldn't want to encourage people to use this... perhaps this refactor should just be reverted or moved to BlobManager (as it's currently the only user).\n\nThe problem I see with our internal APIs is that they are mostly expert only and easy to screw up.It is easy to forget to close the request here. That is why I created a \"non-expert\" method which anyone can use.\n\n ",
            "id": "comment-14354368"
        },
        {
            "date": "2015-03-12T13:17:26+0000",
            "author": "Yonik Seeley",
            "content": "The problem I see with our internal APIs is that they are mostly expert only and easy to screw up.It is easy to forget to close the request here. That is why I created a \"non-expert\" method which anyone can use.\n\nRight... but I'm not sure at all that this won't screw up in the general case, so I don't think it's ready for \"non-expert\" use.  I don't think handleRequest was really written to be recursive (i.e be called from handleRequest itself). ",
            "id": "comment-14358638"
        },
        {
            "date": "2015-03-12T13:40:22+0000",
            "author": "Noble Paul",
            "content": "I agree with you that recursive screw up is possible. Instead of removing that API itself , we should add safeguard the caller  and prevent recursive calls in that method itself.\n\nIMHO recursive loops are caught almost immediately but resource leaks are not found until it's too late ",
            "id": "comment-14358673"
        },
        {
            "date": "2015-03-12T13:55:53+0000",
            "author": "Yonik Seeley",
            "content": "Even with a different handler though... \"forwarding\" to another request handler was never a first class supported operation before.  The whole thing feels a bit squirrelly.  For example: the new request object that is being created... it's closed automatically when the method returns, but the response object is still there and will presumably either be used/looked at by the caller, or used to ultimately write the response.  That breaks a previously held invariant - don't use a response object after the request has been closed (it may contain state tied to the request object). ",
            "id": "comment-14358693"
        },
        {
            "date": "2015-03-12T14:24:08+0000",
            "author": "Noble Paul",
            "content": "don't use a response object after the request has been closed .(it may contain state tied to the request object).\n\nif the caller wants to see the output of another handler , what is the solution? serialize the response an deserialize it?\n ",
            "id": "comment-14358720"
        },
        {
            "date": "2015-03-12T17:29:31+0000",
            "author": "Yonik Seeley",
            "content": "what is the solution? \n\nI'm pointing out the possible issues and why my gut fee was to not encourage the use of this API.  The answer is to develop a solid API if we want this feature.\n\nAnother big issue off the top of my head (that would be much harder to catch via testing):\nA different searcher may be used by the sub-request than is used by the parent request.  That's going to cause all sorts of problems.\nAlso less likely, the schema can change and be different from parent to sub request.\nThere's also the question of lost \"context\", and anything that may use that (I see you use that to only do useParams once, for example.  is that OK to do again?) ",
            "id": "comment-14359029"
        },
        {
            "date": "2015-03-13T12:22:49+0000",
            "author": "Noble Paul",
            "content": "I looked into the code of LocalSolrQueryRequest\n\nIf uses the core that was there in the parent request. So, unless the parent request is closed , this one is safe. As long as the forward() happens in the same thread , it is safe.\n\nThe only risk I see is if the schema changes in flight\n\nThere's also the question of lost \"context\", and anything that may use that\n\nThis is probably the only one I see as a problem. But, don't you think we should fix this by having a special ForwardableSolrRequest which uses the same core, schema and context from the parent request\n\n(I see you use that to only do useParams once, for example. is that OK to do again?)\n\nThat is used for a moment and removed immediately before anyone can see it  ",
            "id": "comment-14360269"
        },
        {
            "date": "2015-03-13T12:30:06+0000",
            "author": "Yonik Seeley",
            "content": "The only risk I see is if the schema changes in flight\nThe searcher changing is both more likely and higher risk of very bad failures.  FYI, I opened SOLR-7238. ",
            "id": "comment-14360276"
        },
        {
            "date": "2015-08-27T16:15:26+0000",
            "author": "Yonik Seeley",
            "content": "Urg... while working on SOLR-7957, I've just been bit by this bad implementation of forward() that closes a request before the response has been written. ",
            "id": "comment-14716965"
        },
        {
            "date": "2015-08-27T20:23:31+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1698226 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1698226 ]\n\nSOLR-6787: fix BlobHandler.forward to not close request until after response has been written ",
            "id": "comment-14717455"
        }
    ]
}