{
    "id": "SOLR-8531",
    "title": "ZK leader path changed in 5.4",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.4.1"
        ],
        "affect_versions": "5.4",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "While doing a rolling upgrade from 5.3 to 5.4 of a solrcloud cluster, I observed that upgraded nodes would not register their shards as active unless they were elected the leader for the shard.\nThere were no errors, the shards were fully up and responsive, but would not  publish any change from the \"down\" state.\n\nThis appears to be because the recovery process never happens, because the ZK node containing the current leader can't be found, because the ZK path has changed.\n\nSpecifically, the leader data node changed from:\n<collection>/leaders/<shard>\nto\n<collection>/leaders/<shard>/leader\n\nIt looks to me like this happened during SOLR-7844, perhaps accidentally. \n\nAt the least, the \"Migrating to Solr 5.4\" section of the README should get updated with this info, since it means a rolling upgrade of a collection with multiple replicas will suffer serious degradation in the number of active replicas as nodes are upgraded. It's entirely possible this will reduce some shards to a single active replica.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-01-09T08:49:20+0000",
            "author": "Shawn Heisey",
            "content": "Jeff Wartes, what README information are you thinking should be updated?  If it's one of the text files included in the download, those can't change.  A 5.4.1 release would need to be made.\n\nAssuming that the path change wasn't necessary for some functionality in SOLR-7844, I think we should revert the change and push a 5.4.1 release out as quickly as possible.  Alternately (assuming it's even possible), 5.4.1 could detect older releases in the cluster and work in a compatibility mode. ",
            "id": "comment-15090505"
        },
        {
            "date": "2016-01-09T17:26:04+0000",
            "author": "Jeff Wartes",
            "content": "I was imagining a note https://lucene.apache.org/solr/5_4_0/changes/Changes.html#v5.4.0.upgrading_from_solr_5.3\nBut I could understand that being driven off of an immutable release tag.\n\nI haven't fully read the SOLR-7844 patch for comprehension, but the change to ZkStateReader.java looks like the reason:\nhttps://github.com/apache/lucene-solr/commit/65cb72631b0833f8ddcf34dfa3d4a91f2c5091c4#diff-8f54b814c3da916328992910b1ad9163\n\nI don't immediately see the change being necessary, so I suspect it could be reverted or made reverse-compatible without too much trouble.\n\nIf it's the former, then I'll presumably hit the same issue again in reverse moving from 5.4 to 5.4.1, which could be ok now that I know to expect it. ",
            "id": "comment-15090706"
        },
        {
            "date": "2016-01-09T18:05:16+0000",
            "author": "Erick Erickson",
            "content": "Jeff:\n\nThanks for \n1> persisting in figuring out what happened.\n2> reporting it this comprehensively.\n\nWhat did you do to fix this? I'm assuming that when the last old node is upgraded somebody takes over leadership and if not the FORCELEADER could fix that. And that if you aren't doing a rolling upgrade, i.e. you take the whole cluster down, upgrade and bring the whole cluster up there aren't any problems.\n\nI agree that you shouldn't have to do either of the above, mostly just making sure I understand the ramifications.\n\n[~markrmiller] any comments?\n ",
            "id": "comment-15090720"
        },
        {
            "date": "2016-01-09T18:19:15+0000",
            "author": "Jeff Wartes",
            "content": "1. A fully upgraded cluster behaves normally.\n2. The problem is only occurs for collections with replicationFactor > 1, but by definition, this means you only have problems if you're trying an HA upgrade.\n\nUpgraded nodes got in line for leader election as normal, but could not figure out the current leader on start, and never executed replication recovery and became active. If I restarted 5.3 nodes for a given shard, the 5.4 shard would eventually get elected leader, and publish active state without intervention, but restarting the 5.4 shard again would mean a 5.3 shard got elected, and the 5.4 node would be stuck in 'down' state again. I did not test restarting a 5.3 shard while the 5.4 shard was leader.\n\nIn my case I had sufficient production capacity to upgrade half my cluster, create a new collection in 5.4, copy the data into it, and then upgrade the rest of the cluster, so I did that. \nAs mentioned, taking downtime and upgrading the whole cluster at once would also have worked. ",
            "id": "comment-15090732"
        },
        {
            "date": "2016-01-09T21:58:04+0000",
            "author": "Mark Miller",
            "content": "Surface level, if I remember, I thought for backcompat on 5x, we continued writing the leader to both spots. ",
            "id": "comment-15090798"
        },
        {
            "date": "2016-01-11T04:46:39+0000",
            "author": "Jeff Wartes",
            "content": "I just looked again, and 5.4 is indeed writing the leader data to both places. Perhaps 5.4 is only looking in the new place?\nThis is speculation, but if so, a possible upgrade path might have been to try to get the first 5.4 node for each shard to be the leader, (preferredLeader property?) and then the rest of the rollout would work.  \nAs I mentioned, I didn't check what happened when I restarted a 5.3 node while 5.4 was leader though. ",
            "id": "comment-15091427"
        },
        {
            "date": "2016-01-26T17:05:42+0000",
            "author": "Jeff Wartes",
            "content": "Looks like this was resolved in SOLR-8561 ",
            "id": "comment-15117568"
        }
    ]
}