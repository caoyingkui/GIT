{
    "id": "SOLR-7490",
    "title": "Update by query feature",
    "details": {
        "components": [],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "An update feature similar to the deleteByQuery would be very useful. Say, the user wants to update a field of all documents in the index that match a given criteria. I have encountered this use case in my project and it looks like it could be a useful first class solr/lucene feature. I want to check if this is something we would want to support in coming releases of Solr and Lucene, are there scenarios that will prevent us from doing this, feasibility, etc.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-04-30T04:02:45+0000",
            "author": "Erick Erickson",
            "content": "Problem here is that you don't necessarily have any source to re-index from. Solr doesn't (yet) have any update-in-place, atomic updates only work if all source fields are stored. So this could only possibly work if all the fields in the docs were stored, and could well result in a massive amount of work being done by Solr as the result of a single call.\n\nI guess it might be possible to accommodate this with the updatable docValues, but again that would be a restricted use-case.\n\nIt feels like the restrictions are too onerous to make this something that would generate a lot of interest. I'm not opposed to the idea, mind you, but I do need to be persuaded that it's worth the complexity.\n\nAnd how that would all play with distributed updates and optimistic locking I don't know. ",
            "id": "comment-14520812"
        },
        {
            "date": "2015-04-30T17:32:05+0000",
            "author": "Praneeth",
            "content": "Thanks for the points. Its true that the all the document fields need to be stored in order to implement this feature through atomic updates.\n\ncould well result in a massive amount of work being done by Solr as the result of a single call\n\nWould this be significantly more work for Solr than what it does for deleteByQuery? \n\nI'm not very much familiar with working of DocValues and cannot at the moment comment on doing it through updatable DocValues. I'll look further into it. As you mentioned, optimistic locking is a primary concern here and could result in a lot of work for Solr.\n\nI think now I understand some of the primary concerns and I will look into these areas and post back here. ",
            "id": "comment-14521907"
        },
        {
            "date": "2015-04-30T19:31:55+0000",
            "author": "Erick Erickson",
            "content": "bq: Would this be significantly more work for Solr than what it does for deleteByQuery?\n\nAbsolutely. deleteByQuery just marks each doc as deleted, which is a much cheaper operation than re-indexing each and every one of the affected docs. e.g. updating by a q=: would re-index every document in the corpus, possibly billions.\n\nDocValues still wouldn't be cheap in this case, but not nearly as bad as arbitrary fields. And do note that DocValues are limited to non-text types (string, numeric and the like). But that's most often the use-case here I think. ",
            "id": "comment-14522109"
        },
        {
            "date": "2015-04-30T20:30:23+0000",
            "author": "Praneeth",
            "content": "Ya but isn't update also effectively a mark for delete and then index the document ? So, I thought it wouldn't cost much on the solr side to index a stream of documents. \n\nConsidering a query that qualifies everything, Solr ends up re-importing the whole data from itself which is basically an optimize operation I think, where we end up rewriting the whole index? But it seems to make it easy to change the schema without having to do anything after (basically change the schema and issue an update by query qualifying the whole index) which basically supports uptime re-indexing of a solr collection with new schema I guess.\n\nWith atomic updates, as you say, we will be exposing the freedom of updating a huge set of documents in one request. We will be pushing Solr too much unless it is used wisely. ",
            "id": "comment-14522198"
        },
        {
            "date": "2015-04-30T22:10:19+0000",
            "author": "Erick Erickson",
            "content": "bq: Considering a query that qualifies everything, Solr ends up re-importing the whole data from itself which is basically an optimize operation I think\n\nNot at all. An optimize does, not for instance, re-analyze all of the documents. How could it? Unless the data has stored=\"true\", the original content is just gone. It just copies some binary bits around, a much simpler task. Perhaps not fast on a large corpus, but much faster then re-analyzing everything.\n\n\nbq: With atomic updates, as you say, we will be exposing the freedom of updating a huge set of documents in one request. We will be pushing Solr too much unless it is used wisely.\n\nNot really the same thing at all IMO. It's much less surprising to write a program that re-indexes a bunch of data than to write a single statement that's the equivalent of SQL \"update blah where blah\" and doesn't return for, perhaps, hours.\n\nbq: But it seems to make it easy to change the schema without having to do anything after (basically change the schema and issue an update by query qualifying the whole index) which basically supports uptime re-indexing of a solr collection with new schema I guess.\n\nI think you're still missing the point. There's no data to re-index from unless the fields have stored=\"true\".\n\nbq: But it seems to make it easy to change the schema without having to do anything after (basically change the schema and issue an update by query qualifying the whole index) which basically supports uptime re-indexing of a solr collection with new schema I guess.\n\nOn any large size corpus, this will essentially have down-time. Your server will be so hammered that it won't be serving any queries. Or at least not quickly. But it is an interesting idea if (and only if) you have all the data stored.\n\nIf you're making the argument that if all fields are stored and if you want to update a particular value for all docs that satisfy a query and if you're willing to accept the risk of huge operations, then the work difference between a update-by-query and atomic updates is roughly equal, I'll agree with you. But frankly the benefit is very marginal in my view, so specialized that I'd be reluctant to push it forward.\n\nFeel free to disagree of course, maybe others have a different opinion.  ",
            "id": "comment-14522371"
        },
        {
            "date": "2015-05-01T13:58:15+0000",
            "author": "Praneeth",
            "content": "Yes. I do understand that atomic updates require the fields to be stored. And ya, your conclusion in the above comment is what I basically meant. I do see your concern about the huge operation. ",
            "id": "comment-14523222"
        }
    ]
}