{
    "id": "SOLR-12408",
    "title": "Introduce parallelShards() in Streaming Expressions",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "streaming expressions"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "parallel() uses hash filter partitioning, which doesn't work in some edge cases with high cardinality facets since they kill coordinator on merge phase. \nI propose to introduce parallelShards() which will accepts a collection, and spawns per-shard substreams (I'm not sure wether to use distrib=false or shards=foo). So, far it's not clear whether workerCollection is useful for it at all.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-05-27T20:21:17+0000",
            "content": "I believe that the JSON facet api has a streaming mode for facets which does not work in distributed mode. The idea was to have Streaming Expressions perform the merge of the streaming facets. Does that scenario resolve what you are describing?\n\nOr is there another high cardinality faceting use case that you have in mind? ",
            "author": "Joel Bernstein",
            "id": "comment-16492146"
        },
        {
            "date": "2018-05-27T20:31:11+0000",
            "content": "The idea was to have Streaming Expressions perform the merge of the streaming facets. Does that scenario resolve what you are describing?\n\nWell, yes. We can say so. \n\nHowever, we'll need to go further. From my experience, it's not possible to merge 30 sorted 100M streams with heap and synchronising these input streams. The solution I've checked so far is like tweaking update() to send in-place update to temporary collection, which accumulates those per-shard responses.    ",
            "author": "Mikhail Khludnev",
            "id": "comment-16492154"
        }
    ]
}