{
    "id": "SOLR-8611",
    "title": "Incorrect logging in ZkController",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.4",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "When a new Zookeeper Session is created all cores publish a \"down\" status.\nThen a call to \"waitForLeaderToSeeDownState\" is made.\nUnfortunately, the logged info inside this method is not correct, keeping the last published core:\n\n2016-01-28 10:19:36.296 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [   ] o.a.s.c.ZkController ZooKeeper session re-connected ... refreshing core states after session expiration.\n2016-01-28 10:19:36.296 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_marketing s:shard1 r:core_node3 x:offers_suggest_marketing_shard1_replica5] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.300 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_topsearch s:shard1 r:core_node4 x:offers_suggest_topsearch_shard1_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.301 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_storage s:shard3 r:core_node8 x:offers_storage_shard3_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.302 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_lean s:shard3 r:core_node6 x:offers_lean_shard3_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.305 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_lean s:shard3 r:core_node6 x:offers_lean_shard3_replica2] o.a.s.c.ZkController Replica core_node3 NOT in leader-initiated recovery, need to wait for leader to see down state.\n2016-01-28 10:19:36.313 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_lean s:shard3 r:core_node6 x:offers_lean_shard3_replica2] o.a.s.c.ZkController Replica core_node4 NOT in leader-initiated recovery, need to wait for leader to see down state.\n2016-01-28 10:19:36.317 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_lean s:shard3 r:core_node6 x:offers_lean_shard3_replica2] o.a.s.c.ZkController Replica core_node8 NOT in leader-initiated recovery, need to wait for leader to see down state.\n\n\n\n\nIt should be\n\n2016-01-28 10:19:36.296 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [   ] o.a.s.c.ZkController ZooKeeper session re-connected ... refreshing core states after session expiration.\n2016-01-28 10:19:36.296 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_marketing s:shard1 r:core_node3 x:offers_suggest_marketing_shard1_replica5] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.300 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_topsearch s:shard1 r:core_node4 x:offers_suggest_topsearch_shard1_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.301 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_storage s:shard3 r:core_node8 x:offers_storage_shard3_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.302 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_lean s:shard3 r:core_node6 x:offers_lean_shard3_replica2] o.a.s.c.ZkController publishing state=down\n2016-01-28 10:19:36.305 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_marketing s:shard1 r:core_node3 x:offers_suggest_marketing_shard1_replica5] o.a.s.c.ZkController Replica core_node3 NOT in leader-initiated recovery, need to wait for leader to see down state.\n2016-01-28 10:19:36.313 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_suggest_topsearch s:shard1 r:core_node4 x:offers_suggest_topsearch_shard1_replica2] o.a.s.c.ZkController Replica core_node4 NOT in leader-initiated recovery, need to wait for leader to see down state.\n2016-01-28 10:19:36.317 INFO  (zkCallback-3-thread-37-processing-n:node054:8983_solr) [c:offers_storage s:shard3 r:core_node8 x:offers_storage_shard3_replica2] o.a.s.c.ZkController Replica core_node8 NOT in leader-initiated recovery, need to wait for leader to see down state.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-09-28T09:11:27+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Could not reproduce this. Can you provide instructions on how to reproduce from scratch?\nAlso please state what exactly you mean is wrong in the log, is it the content of the [c:offers_suggest_topsearch s:shard1...] section you mean? Think that is produced from MDC logging, so the issue could be lack of updating MDCLoggingContext.setCoreDescriptor(descriptor); in the for-loop of core descriptors? ",
            "id": "comment-15528997"
        },
        {
            "date": "2016-09-28T09:17:15+0000",
            "author": "Stephan Lagraulet",
            "content": "That's right, core node information is not updated.\nAs you see in my example, there a 4 lines in the log for r:core_node6 and there should be one for each node, as stated in the fixed log I provided (core_node3, code_node4, core_node8) ",
            "id": "comment-15529013"
        },
        {
            "date": "2016-09-28T09:44:52+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Can you help with some steps to reproduce in the simplest possible way, from a stock Solr?\n\nThis is what I tried, but I may be mistaken for what condition that triggers this code path?\n\ncd solr\nbin/solr start -e cloud -noprompt\nbin/solr create -c testcoll -shards 2 replicationFactor 2\nbin/post -c testcoll2 example/exampledocs/\nbin/solr stop -p 8983 # takes down ZK on node1\nbin/solr start -cloud -p 8983 -s \"example/cloud/node1/solr\" #start again\nbin/solr stop -all\ngrep \"NOT in leader-initiated recovery\" example/cloud/node?/logs/solr.log\n\n\n ",
            "id": "comment-15529093"
        },
        {
            "date": "2016-09-28T11:30:54+0000",
            "author": "Stephan Lagraulet",
            "content": "This only occured on our production node when a massive recovery was forced on all nodes of one shard.\nUnfortunately we did not have a simple case to reproduce the problem. \nMaybe a static analysis of the code should provide a hint to where to find the bug? ",
            "id": "comment-15529332"
        }
    ]
}