{
    "id": "SOLR-12519",
    "title": "Support Deeply Nested Docs In Child Documents Transformer",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Sub-task",
        "fix_versions": [
            "7.5"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "As discussed in SOLR-12298, to make use of the meta-data fields in SOLR-12441, there needs to be a smarter child document transformer, which provides the ability to rebuild the original nested documents' structure.\n In addition, I also propose the transformer will also have the ability to bring only some of the original hierarchy, to prevent unnecessary block join queries. e.g.\n\n  {\"a\": \"b\", \"c\": [ {\"e\": \"f\"}, {\"e\": \"g\"} , {\"h\": \"i\"} ]} \n\n Incase my query is for all the children of \"a:b\", which contain the key \"e\" in them, the query will be broken in to two parts:\n 1. The parent query \"a:b\"\n 2. The child query \"e:*\".\n\nIf the only children flag is on, the transformer will return the following documents:\n \n[ {\"e\": \"f\"}, {\"e\": \"g\"} ]\n\n\nIn case the flag was not turned on(perhaps the default state), the whole document hierarchy will be returned, containing only the matching children:\n\n{\"a\": \"b\", \"c\": [ {\"e\": \"f\"}, {\"e\": \"g\"} ]",
    "attachments": {
        "SOLR-12519-no-commit.patch": "https://issues.apache.org/jira/secure/attachment/12931194/SOLR-12519-no-commit.patch",
        "SOLR-12519-fix-solrj-tests.patch": "https://issues.apache.org/jira/secure/attachment/12936767/SOLR-12519-fix-solrj-tests.patch",
        "SOLR-12519.patch": "https://issues.apache.org/jira/secure/attachment/12936464/SOLR-12519.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-07-09T18:42:21+0000",
            "content": "my query is for all the children of \"a:b\", which contain the key \"e\" in them\n\nThat's one use-case, \"all children that have a certain key\" but there are perhaps more use-cases to be addressed in this issue?\n\n\tall child docs matching some custom query   (e.g. your example above of e:*)\n\tall child docs of a certain key, e.g. all \"c\" docs.\n\t... and all their descendants Y/N\n\tall child docs at a certain path\n\t... and all their descendants Y/N\n\n\n\nIn all these cases, we must always retrieve all ancestors up to the root document I think.\n\nPerhaps some path syntax/language could articulate this?\n\nUltimately we'll want to utilize PathHierarchyTokenizer in some way. ",
            "author": "David Smiley",
            "id": "comment-16537381"
        },
        {
            "date": "2018-07-10T12:05:27+0000",
            "content": "Ultimately we'll want to utilize PathHierarchyTokenizer in some way.\nI have been playing around using this tokenizer, which is rather useful.\nThe problem I am currently experiencing is I have not been able to find a way to get only children of a certain path, excluding the path. e.g.\nAll descendants of toppings, excluding toppings.\n\n!field f=_NEST_PATH_}toppings/\n\nThe following filter returns descendants including toppings, even though I have appended the delimiter after the path.\nEven tried the following filters to no avail:\n\n((NOT _NEST_PATH_:\"toppings\") AND ({!field f=_NEST_PATH_}toppings))\n\n\n((NOT _NEST_PATH_:\"*toppings\") AND ({!field f=_NEST_PATH_}toppings))\n ",
            "author": "mosh",
            "id": "comment-16538477"
        },
        {
            "date": "2018-07-10T14:11:10+0000",
            "content": "Judging from TestPathHierarchyTokenizer (e.g. testStartOfCharEndOfDelimiter), if you put a trailing '/' at the end of the input, then it's retained in the indexed token for the full path and you can then query by this.  In this way, if your query includes the trailing '/' then it means an exact match vs any descendent.  We could add this trailing '/' in the URP, though it feels more like this addition belongs elsewhere since we don't need this trailing '/' in the stored form which is what would happen.  Still; not a big deal if we do it in the URP \u2013 just one char.  With this change, your query would look like this:  \n\n-_NEST_PATH_:\"toppings/\" +_NEST_PATH_:\"toppings\"\n\n\nThis query assumes \"toppings\" is a top level child.  If we need any depth then both can be wildcards with a leading '*' (actually more complicated to differentiate a \"stoppings\" from \"toppings\").\n\nI feel like a lot of this sort of logic could go into a new FieldType.  The FieldType would be responsible for encapsulating the analysis indexing config/logic, and thus it becomes a one-liner in anyone's schema.  More importantly, it could encapsulate simple query parsing rules and accept some local-param options.  So imagine something like this:\n\nFind any comment element: (not necessarily at root)\n\n{!field f=myPathField}comment\n\n\n\nFind the comments at this path:  (this example is a comment of a comment of the top doc)\n\n{!field f=myPathField}/comment/comment\n\n\n\nFind the comments at this path and all descendants:\n\n{!field f=myPathField descendants=true}/comment/comment\n\n\n\nFind the comments at this path and all descendants but not itself:\n\n{!field f=myPathField self=false descendants=true}/comment/comment\n\n\n\nFind the comments at this path and all ancestors:\n\n{!field f=myPathField ancestors=true}/comment/comment\n\n\n\nThis is possible because a FieldType implements getFieldQuery with the QParser passed in exposing local params.  Most spatial fields use this technique to e.g. pass distErrPct in the query. ",
            "author": "David Smiley",
            "id": "comment-16538657"
        },
        {
            "date": "2018-07-10T14:18:39+0000",
            "content": "This needn't be seen as a bunch of extra work; although there will be some extra.  It's more about packaging \u2013 where do we put logic we'll need such that it could be used by not only ChildDocTransformer but really anyone.  ChildDocTransformer would then not have much complexity with respect to formulating a query to match children against input options/specification as that part would be delegated to the new FieldType.  I can create an issue for it. ",
            "author": "David Smiley",
            "id": "comment-16538671"
        },
        {
            "date": "2018-07-10T15:23:15+0000",
            "content": "If we want end-users (of an analytical sort, mind you) to express paths, then we may want to embed the ancestors/descendants,self toggling directly into the path expression in some concise way.  A leading/trailing \"**\" could handle ancestors/descendants.  A trailing '/' could handle self.   In this way we have a nice compact syntax that isn't inextricably tied to Solr (e.g. you might present this path input with  help on valid paths). ",
            "author": "David Smiley",
            "id": "comment-16538769"
        },
        {
            "date": "2018-07-11T14:34:54+0000",
            "content": "if you put a trailing '/' at the end of the input, then it's retained in the indexed token for the full path and you can then query by this. In this way, if your query includes the trailing '/' then it means an exact match vs any descendent\nI have tried using this trick to no avail, even when I added a trailing / using the URP or the PatternReplaceTokenizer. even when my childFilter is NEST_PATH:toppings/, the query returns toppings' descendants.\n Perhaps I have misconfigured the PathHierarchyTokenizer?\n I will try and look in to this further, hopefully we won't have to add a new feature to the PathHierarchyTokenizer. ",
            "author": "mosh",
            "id": "comment-16540169"
        },
        {
            "date": "2018-07-11T15:59:28+0000",
            "content": "Feel free to throw up a nocommit patch that just demonstrates something not working, ideally via a test.  In such a patch, you'd edit whatever is convenient to demonstrate the problem and not worry about it's suitability as a contribution. ",
            "author": "David Smiley",
            "id": "comment-16540285"
        },
        {
            "date": "2018-07-11T16:16:19+0000",
            "content": "I have modified the Nested URP in this patch to add a trailing '/'.\nIn addition to the above change, I have added the test TestDeeplyNestedChildDocTransformer#testExactPath, which show cases 2 queries, the latter one fails. I have also added a new field type to the schema. ",
            "author": "mosh",
            "id": "comment-16540320"
        },
        {
            "date": "2018-07-11T17:22:37+0000",
            "content": "I can see right away that the issue is that you are using the PathHierarchyTokenizer on both your index analyzer and query analyzer.  It's one or the other \u2013 as shown in javadocs for PathHierarchyTokenizerFactory and also in the schema for the the test of that factory.  If we use PathHierarchyTokenizerFactory at index time then the query time would be a literal string/term match and would find the exact match with the trailing '/'.  Path tokenizing at index time allows quick/fast descendant matches.  If we also want to use the same field for finding ancestors, then this may be a fun challenge but probably doable.  First thing that comes to mind would be the transformer (or whatever client) manually path tokenizing the input and ensuring each token has a trailing '/', thus an ancestor query of \"Books/NonFic/Science/\" from the javadoc example would become a Boolean OR query for \"Books/\" and \"Books/NonFic/\" and Books/NonFic/Science/\". ",
            "author": "David Smiley",
            "id": "comment-16540412"
        },
        {
            "date": "2018-07-12T11:22:04+0000",
            "content": "Thanks a lot David Smiley, I have uploaded a WIP pull request, which is far from ready, just to see we are all on the same track.\nConsider it an alpha version of the transformer.\nIf we also want to use the same field for finding ancestors\nOne major obstacle I have yet to conquer is a way return only the ancestors of children that matched the child filter. Currently I have only thought of doing a reverse lookup using the nest_parent field, iterating over the ancestor keys' values, but that seems slow and cumbersome.\nHopefully we will be able to find a better way. ",
            "author": "mosh",
            "id": "comment-16541487"
        },
        {
            "date": "2018-07-14T03:20:52+0000",
            "content": "Oh I understand now.  My suggestion to use PathHierarchyTokenizerFactory was centered around use-cases of querying for child docs purely by this path (e.g. all paths that look like this, etc.).  If the query is find all child docs that match some arbitrary query (which is what \"childFilter\" is), and furthermore their ancestors, then PathHierarchyTokenizerFactory may not be so useful in that.  Sorry for the wild goose chase; though I suspect we'll revisit the use of PathHierarchyTokenizerFactory in the near future.\n\nI think we can do this with DocValues to store the nest path, and with modifications to ChildDocTransformer's loop over matching child documents.  Recognize first how Lucene/Solr actually sequence the arrangement of nested child documents.  Any given child document always comes before it's parent (and thus recursively so).  Therefore, what can be done is to look at all documents after a matching child document to see which of those is an ancestor of a matching child document.  Detecting if child doc X has an ancestor of doc X + N is a matter of comparing if the path at X + N is a prefix of the path at X.  You stop looping forward once you reach the root document \u2013 tracked in parentsFilter bits.  If that's not enough information for you to implement this, I can post a patch modification to ChildDocTransformer that will do this, and maybe you could take it further from there (e.g. restructure the ancestors into a nice hierarchy). ",
            "author": "David Smiley",
            "id": "comment-16543976"
        },
        {
            "date": "2018-07-15T12:52:54+0000",
            "content": "Detecting if child doc X has an ancestor of doc X + N is a matter of comparing if the path at X + N is a prefix of the path at X. You stop looping forward once you reach the root document \u2013 tracked in parentsFilter bits\nparentFilter bits is generated in ToChildBlockJoinWeight#scorer and is a private member of the ToChildBlockJoinScorer class. Would this change be implemented at the transformer level, thus creating the parent BitSet twice, or would it be better to accumulate each BitSet and have a getParentsBitSet method? ",
            "author": "mosh",
            "id": "comment-16544538"
        },
        {
            "date": "2018-07-16T13:30:19+0000",
            "content": "You're correct that we might inadvertently have a situation where the bits get produced twice, though I don't think we need to modify Lucene (e.g. ToChildBlockJoinWeight) to address that.  \nQueryBitSetProducer internally has a cache, but it wont be leveraged unless we retain QueryBitSetProducer.  We could either address that \u2013 cache these things (Query->QBSP) somewhere, or don't use QueryBitSetProducer and instead leverage Solr's own filter cache.  There's even a TODO in ChildDocTransformer to this effect.  Oh yeah, I added that TODO June 1st   See org.apache.solr.search.join.BlockJoinParentQParser#getCachedFilter for a clue. ",
            "author": "David Smiley",
            "id": "comment-16545208"
        },
        {
            "date": "2018-07-19T10:44:59+0000",
            "content": "\u00a0\nSee org.apache.solr.search.join.BlockJoinParentQParser#getCachedFilter for a clue.\nThat sounds like the next thing on my todo list, right after implementing the filtering using Lucene iterations we have discussed prior.\n\nBTW,\n I have just pushed to the WIP pull request, I have made some progress and it is starting to get there.\n\n\u00a0 ",
            "author": "mosh",
            "id": "comment-16549116"
        },
        {
            "date": "2018-07-29T06:21:38+0000",
            "content": "It has dawned upon me that we do not get matching children's descendant documents when there is a child Filter for a specific field.\n I was thinking perhaps we should use the paths in the filter, rewind to the document proceeding the previous parent, and check each document if its path contains the filter. That way we can add the matching child documents descendants.\n WDYT? ",
            "author": "mosh",
            "id": "comment-16561031"
        },
        {
            "date": "2018-07-30T04:57:13+0000",
            "content": "I see what you're saying... though some apps/use-cases\u00a0won't care about descendants. \u00a0A boolean flag could toggle this. \u00a0Lets add support for that in a follow-up issue? \u00a0Your implementation proposal makes sense to me. ",
            "author": "David Smiley",
            "id": "comment-16561432"
        },
        {
            "date": "2018-08-01T11:11:37+0000",
            "content": "I'm not sure whether this belongs here, but I have been toying with the thought of using this transformer in conjunction with NestedUpdateProcessor and AtomicUpdate to allow SOLR to completely re-index the entire nested structure. Would this be acceptable, or would the performance costs be too high? This is just a thought, I am still thinking about implementation details. Hopefully I will be able to post a more concrete proposal soon. ",
            "author": "mosh",
            "id": "comment-16565139"
        },
        {
            "date": "2018-08-01T14:00:29+0000",
            "content": "That sounds useful\u00a0\u2013 definitely a separate issue. \u00a0Feel free to file it; even if you're not sure it'll go anywhere. \u00a0Wether it's \"too high\" or not is application/scenario dependent. \u00a0Most apps need to update their documents, and supporting block-join configurations would be a nice convenience over making a client have to resend the block. ",
            "author": "David Smiley",
            "id": "comment-16565364"
        },
        {
            "date": "2018-08-10T04:34:49+0000",
            "content": "Commit 96cd3074e1e3c8bd5317de2cb243a78133ba68eb in lucene-solr's branch refs/heads/SOLR-12519 from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=96cd307 ]\n\nSOLR-12519: ChildDocTransformer, redo for hierarchy\nTook PR #416 (Moshe) and went further with it ",
            "author": "ASF subversion and git services",
            "id": "comment-16575749"
        },
        {
            "date": "2018-08-10T04:50:14+0000",
            "content": "mosh I pushed a feature branch, as you can see. \u00a0I took your PR, and went a bit further with it. \u00a0\n\nThere's no \"hierarchy\" transformer param, or anonChildDocs transformer param either; it's driven by the presence of the nest path in the schema. \u00a0No new params from before.\n\nOne of the biggest changes you'll notice is I ripped out ToChildBlockJoinQuery which was really unnecessary for a scenario like this where we're really just doing an ID lookup.  But the silly thing is that we already have the low-level Lucene document ID for the root document in a parameter.  So we don't even need the uniqueKey field either.  It scans from first child ID (even if it didn't match the childFilter).  This sets us up well for a future feature you've spoken of to get all prior sibling child documents.  It's unfortunate we fetch the path only to potentially not need it if we haven't reached the first child matching the filter but that should be a relatively minor cost (DocValues are designed for fast access).\n\nThere's a nocommit about supporting \"limit\".  If there is a limit... we may want to scan backwards from the root ID to detect which of them are in the child filter so we know how far back to go, and then perhaps initialize the loop from that point.  That doesn't sound too bad.  It's also an improvement over the previous limit processing which would go forward from the first... which seems worse then starting from the root.  This ought to be tested.\n\nI definitely made other changes too.  Maybe you can see these changes easiest in your IDE by comparing the branches.  At least I can do so easily in IntelliJ. ",
            "author": "David Smiley",
            "id": "comment-16575751"
        },
        {
            "date": "2018-08-14T08:33:36+0000",
            "content": "mosh I pushed a feature branch, as you can see.  I took your PR, and went a bit further with it.\n\nDavid I have forked your branch and started looking at the comments, making some final adjustments.\nShould the \"limit\" feature be addressed in this ticket? ",
            "author": "mosh",
            "id": "comment-16579437"
        },
        {
            "date": "2018-08-14T13:52:40+0000",
            "content": "Should the \"limit\" feature be addressed in this ticket?\n\nYes definitely. ",
            "author": "David Smiley",
            "id": "comment-16579839"
        },
        {
            "date": "2018-08-22T01:22:42+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n -1 \n  patch  \n   0m  6s \n  SOLR-12519 does not apply to SOLR-12519. Rebase required? Wrong Branch? See https://wiki.apache.org/solr/HowToContribute#Creating_the_patch_file for help.  \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12519 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12936464/SOLR-12519.patch \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/166/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16588215"
        },
        {
            "date": "2018-08-22T03:57:36+0000",
            "content": "Yetus thought the patch should be applied to the branch of the same name as this feature, but the patch is actually for master, and so it failed.  I just deleted the branch.  I ran the full test suite + precommit and found these failures in SolrJ:\n\n\u00a0\u00a0 [junit4] Tests with failures [seed: D1E2EA5B85C1A1D2]:\n\u00a0\u00a0 [junit4] \u00a0 - org.apache.solr.client.solrj.SolrExampleBinaryTest.testChildDoctransformer\n\u00a0\u00a0 [junit4] \u00a0 - org.apache.solr.client.solrj.SolrExampleXMLTest.testChildDoctransformer\n\u00a0\u00a0 [junit4] \u00a0 - org.apache.solr.client.solrj.embedded.SolrExampleStreamingTest.testChildDoctransformer\n\u00a0\u00a0 [junit4] \u00a0 - org.apache.solr.client.solrj.embedded.SolrExampleStreamingBinaryTest.testChildDoctransformer\n\n\nCan you investigate mosh?\n\nSeparately, I wonder if it's a big deal to change the semantics of \"limit\" in a minor release.  AFAIK Hoss Man you originally added the ChildDocTransformer and implemented \"limit\" as starting from the furthest child document from the root.  I think that's an odd choice.  Wouldn't the \"top\" of the of child docs be closest to the root document; wouldn't that be more useful?  I suspect that most users don't actually want to limit child docs so configure it not to limit... but I don't know for sure, that's just my hunch. ",
            "author": "David Smiley",
            "id": "comment-16588354"
        },
        {
            "date": "2018-08-22T04:12:20+0000",
            "content": "found these failures in SolrJ\nOn it! ",
            "author": "mosh",
            "id": "comment-16588364"
        },
        {
            "date": "2018-08-22T04:30:46+0000",
            "content": "When I run the tests based off the latest master, I get this rather odd stacktrace when running SolrJ tests(including the ones you listed):\n\n2351 ERROR (coreContainerWorkExecutor-2-thread-1) [    ] o.a.s.c.CoreContainer Error waiting for SolrCore to be loaded on startup\norg.apache.solr.common.SolrException: Unable to create core [collection1]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1164) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.lambda$load$13(CoreContainer.java:689) ~[java/:?]\n\tat com.codahale.metrics.InstrumentedExecutorService$InstrumentedCallable.call(InstrumentedExecutorService.java:197) ~[metrics-core-3.2.6.jar:3.2.6]\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_172]\n\tat org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:209) ~[java/:?]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_172]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_172]\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_172]\nCaused by: org.apache.solr.common.SolrException: Could not load conf for core collection1: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:96) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\nCaused by: org.apache.solr.common.SolrException: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:582) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\nCaused by: org.apache.solr.common.SolrException: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:182) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:474) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\nCaused by: org.apache.solr.common.SolrException: Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:182) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.readAnalyzer(FieldTypePluginLoader.java:376) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:104) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:53) ~[java/:?]\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:474) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\nCaused by: org.apache.solr.common.SolrException: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.core.SolrResourceLoader.findClass(SolrResourceLoader.java:557) ~[java/:?]\n\tat org.apache.solr.core.SolrResourceLoader.newInstance(SolrResourceLoader.java:626) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:356) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:349) ~[java/:?]\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.readAnalyzer(FieldTypePluginLoader.java:376) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:104) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:53) ~[java/:?]\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:474) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\nCaused by: java.lang.ClassNotFoundException: solr.KoreanTokenizerFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_172]\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_172]\n\tat java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:814) ~[?:1.8.0_172]\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_172]\n\tat java.lang.Class.forName0(Native Method) ~[?:1.8.0_172]\n\tat java.lang.Class.forName(Class.java:348) ~[?:1.8.0_172]\n\tat org.apache.solr.core.SolrResourceLoader.findClass(SolrResourceLoader.java:541) ~[java/:?]\n\tat org.apache.solr.core.SolrResourceLoader.newInstance(SolrResourceLoader.java:626) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:356) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:349) ~[java/:?]\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.readAnalyzer(FieldTypePluginLoader.java:376) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:104) ~[java/:?]\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:53) ~[java/:?]\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:474) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173) ~[java/:?]\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45) ~[java/:?]\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118) ~[java/:?]\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192) ~[java/:?]\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139) ~[java/:?]\n\t... 7 more\n2361 INFO  (TEST-SolrExampleXMLTest.testChildDoctransformer-seed#[1F7342CAC3794D78]) [    ] o.a.s.SolrTestCaseJ4 ###Starting testChildDoctransformer\n2497 ERROR (qtp1592093469-22) [    ] o.a.s.s.HttpSolrCall null:org.apache.solr.core.SolrCoreInitializationException: SolrCore 'collection1' is not available due to init failure: Could not load conf for core collection1: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.core.CoreContainer.getCore(CoreContainer.java:1598)\n\tat org.apache.solr.servlet.HttpSolrCall.init(HttpSolrCall.java:249)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:469)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:377)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:323)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\n\tat org.apache.solr.client.solrj.embedded.JettySolrRunner$DebugFilter.doFilter(JettySolrRunner.java:139)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\n\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:674)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:531)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.solr.common.SolrException: Could not load conf for core collection1: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:96)\n\tat org.apache.solr.core.CoreContainer.getConfigSet(CoreContainer.java:1192)\n\tat org.apache.solr.core.CoreContainer.createFromDescriptor(CoreContainer.java:1139)\n\tat org.apache.solr.core.CoreContainer.lambda$load$13(CoreContainer.java:689)\n\tat com.codahale.metrics.InstrumentedExecutorService$InstrumentedCallable.call(InstrumentedExecutorService.java:197)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:209)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.solr.common.SolrException: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:582)\n\tat org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:180)\n\tat org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:105)\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:173)\n\tat org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45)\n\tat org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:70)\n\tat org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:118)\n\tat org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:91)\n\t... 9 more\nCaused by: org.apache.solr.common.SolrException: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:182)\n\tat org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:474)\n\t... 16 more\nCaused by: org.apache.solr.common.SolrException: Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:182)\n\tat org.apache.solr.schema.FieldTypePluginLoader.readAnalyzer(FieldTypePluginLoader.java:376)\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:104)\n\tat org.apache.solr.schema.FieldTypePluginLoader.create(FieldTypePluginLoader.java:53)\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152)\n\t... 17 more\nCaused by: org.apache.solr.common.SolrException: Error loading class 'solr.KoreanTokenizerFactory'\n\tat org.apache.solr.core.SolrResourceLoader.findClass(SolrResourceLoader.java:557)\n\tat org.apache.solr.core.SolrResourceLoader.newInstance(SolrResourceLoader.java:626)\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:356)\n\tat org.apache.solr.schema.FieldTypePluginLoader$2.create(FieldTypePluginLoader.java:349)\n\tat org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:152)\n\t... 21 more\nCaused by: java.lang.ClassNotFoundException: solr.KoreanTokenizerFactory\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:814)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.solr.core.SolrResourceLoader.findClass(SolrResourceLoader.java:541)\n\t... 25 more\n\n2549 INFO  (TEST-SolrExampleXMLTest.testChildDoctransformer-seed#[1F7342CAC3794D78]) [    ] o.a.s.SolrTestCaseJ4 ###Ending testChildDoctransformer\nNOTE: reproduce with: ant test  -Dtestcase=SolrExampleXMLTest -Dtests.method=testChildDoctransformer -Dtests.seed=1F7342CAC3794D78 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=th-TH -Dtests.timezone=GMT0 -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:34359/solr/collection1: SolrCore 'collection1' is not available due to init failure: Could not load conf for core collection1: Can't load schema /tmp/solr.client.solrj.SolrExampleXMLTest_1F7342CAC3794D78-001/tempDir-001/collection1/conf/managed-schema: Plugin init failure for [schema.xml] fieldType \"text_ko\": Plugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.KoreanTokenizerFactory'\n\n\tat __randomizedtesting.SeedInfo.seed([1F7342CAC3794D78:6CA95D504F613A7E]:0)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:643)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:255)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:244)\n\tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:194)\n\tat org.apache.solr.client.solrj.SolrClient.deleteByQuery(SolrClient.java:895)\n\tat org.apache.solr.client.solrj.SolrClient.deleteByQuery(SolrClient.java:858)\n\tat org.apache.solr.client.solrj.SolrClient.deleteByQuery(SolrClient.java:873)\n\tat org.apache.solr.client.solrj.SolrExampleTests.testChildDoctransformer(SolrExampleTests.java:1780)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1737)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:934)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:970)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:984)\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:57)\n\tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n\tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:943)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:829)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:879)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:890)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:57)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n\tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\nThere seems to be an issue with the KoreanAnalyzer.\nIs this error similar to the ones you faced? ",
            "author": "mosh",
            "id": "comment-16588374"
        },
        {
            "date": "2018-08-22T14:55:09+0000",
            "content": "I presume you hit this error when running a test in IntelliJ.  I hit the very same error just now.  From time to time, it's necessary to keep your IntelliJ configuration up to date since there are occasional changes to modules/dependencies.  You could do \"ant idea\" to regenerate your configs.  That didn't quite fix it for me, so I manually \"imported\" the \"nori\" module into the Analysis group.   The surefire way to work is to \"ant clean-idea\" first which I almost never do since I'm afraid of what it might remove from my tuned settings.\n\nNote the seed is necessary to reproduce the error.  It's triggering an assertion failure inside the child doc transformer. ",
            "author": "David Smiley",
            "id": "comment-16588972"
        },
        {
            "date": "2018-08-22T17:31:22+0000",
            "content": "AFAIK Hoss Man you originally added the ChildDocTransformer and implemented \"limit\" as starting from the furthest child document from the root. I think that's an odd choice. Wouldn't the \"top\" of the of child docs be closest to the root document; wouldn't that be more useful? I suspect that most users don't actually want to limit child docs so configure it not to limit... but I don't know for sure, that's just my hunch.\n\nI'm not really following what you mean, the code doesn't go out of it's way to try and get the \"top\" children or the bottom children \u2013 it just executes a ToChildBlockJoinQuery (fltered by the childFilterQuery) and let's the docs be returned in the score order of that query w/a limit on the qunaiy \u2013 perhaps you're refering to the sort by score? does ToChildBlockJoinQuery score the \"farthest\" queries higher for some reason?\n\ni don't think we've ever documented how the limit was applied, so i don't think it's a huge problem to change it ... but it would also probably be trivial to add a \"sort\" local param to that transformer if you were worried about giving affected users an easy way to force the olde behavior.\n\nIIRC that limit is largely as a safety valve to prevent massive responses, i doubt anyone would be too affecte by the order changing. ",
            "author": "Hoss Man",
            "id": "comment-16589155"
        },
        {
            "date": "2018-08-22T17:52:00+0000",
            "content": "To be clear, the current logic has the effect of index order and thus furthest from the root.  It's very un-obvious this is true \u2013 heck, this would make for a good interview question or \"Stump the Chump\"   , but it is, and the tests we had to modify reflect that as well.  As we discussed in IRC, you didn't make a deliberate choice on this matter; you just wanted a \"safety value\" option.\n\nSo lets just change it.  I'll call out this fact in the upgrade notes.  It's an \"improvement\" IMO. ",
            "author": "David Smiley",
            "id": "comment-16589179"
        },
        {
            "date": "2018-08-23T06:15:44+0000",
            "content": "Can you investigate mosh?\nAfter further investigation it seems like those tests were failing because there was no check whether no children matched the childFilter or whether the parent matched the parentsFilter, causing an assertionError.\nI added a couple more conditions to fix this in the patch file\u00a0SOLR-12519-fix-solrj-tests.patch, which I just uploaded. ",
            "author": "mosh",
            "id": "comment-16589744"
        },
        {
            "date": "2018-08-23T12:24:56+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 22 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   2m 16s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   2m  3s \n  the patch passed  \n\n\n +1 \n  javac  \n   2m  3s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   0m 36s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   0m 17s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   0m 17s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n +1 \n  unit  \n   0m 53s \n  facet in the patch passed.  \n\n\n -1 \n  unit  \n   0m  5s \n  tools in the patch failed.  \n\n\n +1 \n  unit  \n   0m 46s \n  dataimporthandler in the patch passed.  \n\n\n +1 \n  unit  \n   0m 28s \n  dataimporthandler-extras in the patch passed.  \n\n\n +1 \n  unit  \n  50m 31s \n  core in the patch passed.  \n\n\n -1 \n  unit  \n   2m 36s \n  solrj in the patch failed.  \n\n\n  \n   \n  62m 58s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n solr.client.solrj.embedded.SolrExampleStreamingBinaryTest \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12519 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12936767/SOLR-12519-fix-solrj-tests.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  \n\n\n uname \n Linux lucene1-us-west 4.4.0-130-generic #156~14.04.1-Ubuntu SMP Thu Jun 14 13:51:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / 8cde127 \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on July 24 2018 \n\n\n Default Java \n 1.8.0_172 \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/170/artifact/out/patch-unit-lucene_tools.txt \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/170/artifact/out/patch-unit-solr_solrj.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/170/testReport/ \n\n\n modules \n C: lucene/facet lucene/tools solr solr/contrib/dataimporthandler solr/contrib/dataimporthandler-extras solr/core solr/solrj U: . \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/170/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16590156"
        },
        {
            "date": "2018-08-23T13:36:34+0000",
            "content": "\n\n\n\nReason\nTests\n\n\nFailed junit tests\nsolr.client.solrj.embedded.SolrExampleStreamingBinaryTest\n\n\n\n\n\nI am having trouble finding the test seed, and without providing one this test seems to pass.\nForgive my lack of knowledge, where might I be able to find the test seed which was specifically used in the failed test run? ",
            "author": "mosh",
            "id": "comment-16590216"
        },
        {
            "date": "2018-08-23T14:25:59+0000",
            "content": "Next time I suggest pushing a new commit to the PR so that it's easy for me to see what change you did. I used a diff tool on your latest patch file with the one I uploaded before. BTW your latest patch includes tons of changes unrelated to this issue.\n\nThinking out-lout.... I think\u00a0I like the check for the rootDocId needing to be in the parents bit filter. Was this an actual problem shown by the test? Could this happen \"normally\"? I suspect this could only happen if the user/app isn't following the rules of nested docs \u2013 like they updated a child doc by itself, or something like that. If I'm right, I think we should throw a helpful exception in this scenario to alert them they are doing something wrong. \u00a0Or perhaps this may happen if you do a query for docs that are not exclusively root docs, and you use the child transformer. \u00a0Yeah... but is that actually a problem? \u00a0I think we should be able to support that just fine. \u00a0In this case you want the child docs below the matching child doc being transformed and not it's parents. \u00a0Can we explicitly test/support this? \u00a0I think we should.\n\nI think we can avoid the addedChildDocs flag by checking if \"pendingParentPathsToChildren\" is empty; no? Can you add a test triggering this scenario? I'm aware the example tests caught it but those are more sanity checks on our shipped configs and/or SolrJ interaction, and not meant to be the canonical tests for specific Solr features.\nI am having trouble finding the test seed\nNo prob. I think this is the easiest way: Click the \"test results\" link in the table. On the page that shows in Jenkins, the failed tests show at the top as links. Click the link for the failing test. On the page that shows, you'll see a stack trace. The stack trace will nearly always contain the seed near the top like so: __randomizedtesting.SeedInfo.seed([BC02A7658A1C547C:CFD8B8FF0604237A]:0) ",
            "author": "David Smiley",
            "id": "comment-16590297"
        },
        {
            "date": "2018-08-27T04:31:00+0000",
            "content": "I think I like the check for the rootDocId needing to be in the parents bit filter. Was this an actual problem shown by the test? Could this happen \"normally\"?\nYes, it was shown by tests.\nIn this case you want the child docs below the matching child doc being transformed and not it's parents.  Can we explicitly test/support this?  I think we should.\nIt should work, I'll add a small test to verify this. ",
            "author": "mosh",
            "id": "comment-16593178"
        },
        {
            "date": "2018-08-27T10:32:53+0000",
            "content": "Hey, there was a nasty bug which showed up in the tests, but they now all seem to pass, even when the last failing seed was used.\nI even added a test to ensure ChildDocTransformer works with non root fields, which only required one small change.\nNext time I suggest pushing a new commit to the PR so that it's easy for me to see what change you did.\nAs requested, I pushed the new commits to the PR . ",
            "author": "mosh",
            "id": "comment-16593457"
        },
        {
            "date": "2018-08-28T13:56:12+0000",
            "content": "The PR looks good; I'm glad we can support adding nested child docs even on a search result document that is not itself the root doc (and is tested).  I'll do some precommit & tests and commit later today.  I think we'll both be relieved that this issue is going to be done soon  ",
            "author": "David Smiley",
            "id": "comment-16595001"
        },
        {
            "date": "2018-08-28T19:07:01+0000",
            "content": "As I start to write out the notes on the change in semantics of \"limit\", and look back at the test, I think the limit interpretation is actually worse now.  My bad (palm to face!). \n\nA documents's children come first and are left-to-right (low to high).  It's the intermediate parents that get placed after, and so it's not quite as simple as strictly left-right or right-left when wanting an ideal \"limit\".  I don't think the semantics of \"limit\" should be changed for existing users; there is no path metadata and we might as well start at the lowest.  For a simple flat list of child docs, it's the right thing to do.\n\n (made up syntax of a nested docA with some nested children)\n\ndocA:{ docB, docC:{ docC.1, docC.2}, docD}\n\n\nWill get serialized/flattened like so:\n\ndocB, docC.1, docC.2, docC, docD, docA\n\n\n\nLets say we match all child docs (not filtered).\nConsider a limit of 1.  Arguably, docB ought to be the sole child added.  That's what happens currently, but soon will be docD.  :-/\nConsdier a limit of 2.  Arguably, docB then docC ought to be added. That's not what happens currently (docB & docC.1), and soon won't do that either (docC & docD).  But since we have the metadata, we are in a position to do it right.\n\nDisclaimer: I didn't test-out the above; it's all from intuition.\n\nIt's kinda embarrassing we didn't see this after discussing it a bit and \"correcting\" tests.  Maybe the testing methodology doesn't make this in-your-face enough?  I've advocated before about the virtues of testing an entire document structure as a string because all is laid bare to see \u2013 it's very direct; less to think about.  This goes hand-in-hand with indexing a simple document literally in the same test method as the test, instead of algorithmically generating documents (perhaps complex ones) in some other method.  There are certainly pros/cons both ways.\n\nWhat might the fix be?  I think we should loop from the lowest docID underneath the parent (as it was before).  And as we go, we can accumulate a counter of how many docs have been added.  If we've reached that counter, then from that point forward, we only want intermediate docs to already-accumulated docs (i.e. only collect ancestors).  The actual number of docs returned could be more than the limit but it shouldn't be more than the number of intermediate parents.  In the example above with limit 2, we'd get docB and docC with child docC.1  WDYT mosh? ",
            "author": "David Smiley",
            "id": "comment-16595456"
        },
        {
            "date": "2018-08-29T04:07:53+0000",
            "content": "The actual number of docs returned could be more than the limit but it shouldn't be more than the number of intermediate parents. In the example above with limit 2, we'd get docB and docC with child docC.1 WDYT mosh?\nSure thing,\njust pushed new commits with this new logic. ",
            "author": "mosh",
            "id": "comment-16595903"
        },
        {
            "date": "2018-08-29T14:02:30+0000",
            "content": "Commit 5a0e7a615a9b1e7ac97c6b0f9e5604dcc1aeb03f in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5a0e7a6 ]\n\nSOLR-12519: child doc transformer can now produce a nested structure.\nFixed SolrDocument's confusion of field-attached child documents in addField()\nFixed AtomicUpdateDocumentMerger's confusion of field-attached child documents in isAtomicUpdate() ",
            "author": "ASF subversion and git services",
            "id": "comment-16596374"
        },
        {
            "date": "2018-08-29T14:05:15+0000",
            "content": "Commit 171cfc8e8e4d4e3f0061aa181c28c14e967a350f in lucene-solr's branch refs/heads/branch_7x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=171cfc8 ]\n\nSOLR-12519: child doc transformer can now produce a nested structure.\nFixed SolrDocument's confusion of field-attached child documents in addField()\nFixed AtomicUpdateDocumentMerger's confusion of field-attached child documents in isAtomicUpdate()\n\n(cherry picked from commit 5a0e7a615a9b1e7ac97c6b0f9e5604dcc1aeb03f) ",
            "author": "ASF subversion and git services",
            "id": "comment-16596376"
        },
        {
            "date": "2018-08-29T14:12:36+0000",
            "content": "This morning I spent some time carefully indenting the document string literal in TestChildDocTransformerHierarchy#generateDocHierarchy and a couple of it's nearby methods so that I could more clearly see what was going on.  I also enhanced testParentFilterLimitJSON to test that it does not return the \"toppings\", because they follow the \"lonely\" stuff which is returned.  (a quick try of increasing the limit caused the assertion to fail so I think it's right).  I also simplified the limit/match logic, and used \">=\" which I think is more clearly correct than \"==\" since we collect more matches due to needing ancestors.\n\nPlease close the PR. ",
            "author": "David Smiley",
            "id": "comment-16596383"
        },
        {
            "date": "2018-08-29T17:36:36+0000",
            "content": "Commit cae91b1eaf15d15f5cd6db792b33df5a26d6f2bc in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cae91b1 ]\n\nSOLR-12519: fix testGrandChildFilterJSON\nSimplified differentiating random docs we don't care about from those we do by using IDs less than 0 ",
            "author": "ASF subversion and git services",
            "id": "comment-16596638"
        },
        {
            "date": "2018-08-29T17:37:40+0000",
            "content": "Commit bcbdeedbad507c99ce5a8d8756ebda40de0779e8 in lucene-solr's branch refs/heads/branch_7x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bcbdeed ]\n\nSOLR-12519: fix testGrandChildFilterJSON\nSimplified differentiating random docs we don't care about from those we do by using IDs less than 0\n\n(cherry picked from commit cae91b1eaf15d15f5cd6db792b33df5a26d6f2bc) ",
            "author": "ASF subversion and git services",
            "id": "comment-16596640"
        },
        {
            "date": "2018-08-30T04:45:44+0000",
            "content": "Would adding an \"fl\" param to ChildDocTransformer be a part of this ticket, or is it a whole new one? ",
            "author": "mosh",
            "id": "comment-16597058"
        }
    ]
}