{
    "id": "SOLR-469",
    "title": "Data Import RequestHandler",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.3"
        ],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We need a RequestHandler Which can import data from a DB or other dataSources into the Solr index .Think of it as an advanced form of SqlUpload Plugin (SOLR-103).\n\nThe way it works is as follows.\n\n\n\tProvide a configuration file (xml) to the Handler which takes in the necessary SQL queries and mappings to a solr schema\n\n\n\tIt also takes in a properties file for the data source configuraution\n\n\n\tGiven the configuration it can also generate the solr schema.xml\n\tIt is registered as a RequestHandler which can take two commands do-full-import, do-delta-import\n\n\n\tdo-full-import - dumps all the data from the Database into the index (based on the SQL query in configuration)\n\tdo-delta-import - dumps all the data that has changed since last import. (We assume a modified-timestamp column in tables)\n\n\n\tIt provides a admin page\n\n\n\twhere we can schedule it to be run automatically at regular intervals\n\tIt shows the status of the Handler (idle, full-import, delta-import)",
    "attachments": {
        "SOLR-469.patch": "https://issues.apache.org/jira/secure/attachment/12375149/SOLR-469.patch",
        "xpath-stream.patch": "https://issues.apache.org/jira/secure/attachment/12387326/xpath-stream.patch",
        "SOLR-469-contrib.patch": "https://issues.apache.org/jira/secure/attachment/12381499/SOLR-469-contrib.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12566202",
            "date": "2008-02-06T17:03:31+0000",
            "content": "A patch out of our (Noble Paul's and Shalin Shekhar Mangar's) work on this issue. Please refer to http://wiki.apache.org/solr/DataImportHandler for a user guide.\n\nOur design philosophy for data imports is based on templatized SQL which gives the user of this tool a lot of flexibility. It can generate schemas, do full-imports and delta-imports. Please note that this is work in progress and there's a lot to be done for it to be committed. We plan to write more documentation and tests as we go on.\n\nStart by looking at changes to solrconfig.xml and then to DataImportHandler.java The central class is DataImporter.java which uses DocBuilder to do the actual full-dump and delta-dump operations.\n\nWe expose a powerful API for applications to do custom tasks. This API was needed because even in our own tasks, there was frequent need to perform custom operations on rows/columns before they could be indexed. Assuming that other users may face the same problems, we expose Context.java, DataSource.java, EntityProcessor.java, Transformer.java as interfaces which can be used to provide custom data sources or transformations on column values before indexing. In our own project, we have used these interfaces to do tasks such as reading XML from a column and extracting relevant items to be indexed.\n\nLooking forward to your feedback and comments. Let us know what will it take to get this feature into SOLR.\n\n\n\tNoble Paul & Shalin Shekhar Mangar\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12566217",
            "date": "2008-02-06T17:29:27+0000",
            "content": "Hi-  thanks for posting this.\n\nI have not had a chance to look at this in depth, but a couple things jump out at me.\n\n1.  It looks like the model here is to treat \"data-config.xml\" as the master and generate schema,xml from that.  To me this seems a bit strange and difficult to support long term.  In my view, \"schema.xml\" should always be the place to define fields and indexing properties.  \"data-config.xml\" should just be the place that maps SQL to the schema.\n\n2. why not just use the standard copyField stuff rather then rolling your own?\n\n        <field name=\"text\">\n            <copyFrom>cat</copyFrom>\n            <copyFrom>name</copyFrom>\n            <copyFrom>manu</copyFrom>\n            <copyFrom>features</copyFrom>\n        </field>\n\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12566246",
            "date": "2008-02-06T18:11:17+0000",
            "content": "hi ,\nthanks for the inputs\n!) for very simple use cases we can avoid people touching the\nschema.xml altogether. because we usually have a standard schema.xml\nbut for the <field > tags. People can choose to edit the schema after\nit is created but if both are totally different, both may not be in\nsync and can throw errors. Anyway, we are open to suggestions\n2)We can use <copyField> instead of <copyFrom>\n--thanks\nNoble Paul\n\n\n\n\n\u2013 \n--Noble Paul "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12566599",
            "date": "2008-02-07T13:59:27+0000",
            "content": "We are planning to eliminate the schema creation step. So we may not need to put in those details which are already present in schema.xml and we can simplify the data-config and eliminate the <copyField> also. So we must introduce a verifier which ensures that the data-config is in sync with the schema.xml.  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12567311",
            "date": "2008-02-09T13:40:45+0000",
            "content": "It seems my earlier patch wasn't generated in the correct way. It had absolute paths to all files instead of having relative paths. This new patch corrects it. Also, it removes a test which had got in by mistake the previous patch. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12567313",
            "date": "2008-02-09T13:48:08+0000",
            "content": "Sorry, this new patch is the correct one. Still learning the ropes  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12567447",
            "date": "2008-02-10T16:23:46+0000",
            "content": "Changes\n\n\tEliminated schema creation step as per Ryan's suggestions.\n\tNo need to put field attributes such as type, multiValued, indexed, stores etc. in data-config.xml, those are now read directly from the Solr IndexSchema\n\tNo need to put copyField information in data-config.xml since copy fields are managed by Solr\n\n\n\nThe only attributes needed to be provided in data-config.xml for a field are:\n\n\tcolumn (The column in the db from which the field's value comes from, Required)\n\tname (Optional, if the field name differs from the column name, the field name needs to be given)\n\tboost (Optional, if the field needs to be boosted)\n\n\n\nI'll update the wiki document to reflect the above changes. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12568295",
            "date": "2008-02-12T19:02:36+0000",
            "content": "The wiki page for DataImportHandler now has instructions for running an example for a full-import process. We've used the same data provided by example in Solr and created a hsqldb database out of it. Would love to have some feedback at this point.\n\nWe'll add examples for delta-import soon. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12569572",
            "date": "2008-02-16T14:49:40+0000",
            "content": "Changes\n\n\tSupport for deleted rows detection (Details will be added to Wiki soon)\n\tNumerous bug fixes\n\tMerged DataImporter and DataImporterContext together\n\tImproved response format showing status messages of operation\n\tDataImportHandler is now SolrCoreAware\n\tCode refactorings\n\tA Verifier which checks data-config.xml against the solr schema.xml to make sure that all fields defined in data-config.xml are defined in schema.xml and all (required) fields defined in solr schema.xml are mentioned in data-config.xml\n\n\n\nWe recently indexed around 1.7 million documents using this tool. The documents had mostly sint and sdouble fields in it (since we wanted to see the performance of this patch and not lucene's speed). We were able to index 1.7 million documents in 166 seconds on our production hardware.\n\nNote: Details of the API exposed in our work is now added to our Wiki. Also, an example solr home is provided in the Wiki page (under \"Full Import Example\" section to try this out. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12569979",
            "date": "2008-02-18T17:49:43+0000",
            "content": "Add a facility to delete documents from Solr index on the basis of a solr query.\nIt is useful if you wish to expire the documents after a certain period of time.  "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12570588",
            "date": "2008-02-20T06:18:14+0000",
            "content": "Haven't looked at the patch, but I've read most of http://wiki.apache.org/solr/DataImportHandler\n\nSmall comment: don't name that config file \"data-config.xml\".  \"data\" is so generic.  What is this?  It's a RDBMS indexing tool implemented as a request handler.  I'd pick a better, more specific name both for the config and the handler itself - DataImportHandler - does it import from a file?  A BDB?  RDBMS?  Another search engine?  Can't tell from a generic name.\n\nReally well documented, good job, and I'm looking forward to seeing this in Solr! "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12570591",
            "date": "2008-02-20T06:40:44+0000",
            "content": "Hi Otis,\n\nThanks for showing interest in this issue and your feedback.\n\nOriginally we started developing this to be a pure DB Import tool. But our own requirements led to us to keep this general enough to be used with other kinds of data sources. For example, we're using this internally for reading from REST API's (including RSS/ATOM feeds). Therefore, we kept the name as DataImportHandler on purpose. Previously, our data source was JdbcDataSource and EntityProcessor was called SqlEntityProcessor. We later extracted interfaces out of them as DataSource and EntityProcessor to make them as generic as possible. Also note that the DataImportHandler does not care about the name of data-config.xml. It could be called anything, all we need is that it should be specified in solrconfig.xml\n\nWe're developing our generic REST datasources and entity processors and plan to contribute them as well. We too are looking forward to see this in Solr and we're committed to do whatever it takes to make sure it becomes a part of Solr. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12570594",
            "date": "2008-02-20T06:49:57+0000",
            "content": "I see, I just read to the bottom of the Wiki page - I spoke too soon.  It would be great, then, to include another non-SQL/RDBMS example in there.\n\nWhat site are you using this on by the way?  Oh, AOL?  It looks like AOL is jumping on Solr, Hadoop, and friends and contributing - bravo!\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12570596",
            "date": "2008-02-20T06:59:06+0000",
            "content": "The DB example was an easy one because we could get a schema out of the sample data.\n An RSS/ATOM example is in the works.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12579879",
            "date": "2008-03-18T14:52:03+0000",
            "content": "This is the biggest ever feature release for the patch . This contains almost all the planned features for DataImportHandler include:\n\n\n\tsupport for xml/http datasources\n\tJavascript for transformer (requires java 6)\n\tNumerous performance enhancements and bug fixes\n\tBetter logging and error handling\n\tAn improved command interface\n\tcommand to reload config\n\tstatistics integrated with solr statistics\n\tCan accessrequest parameters\n\tExtra configurable parameters can be passed from solrconfig.xml\n\tMultiple transformers possible (chaining)\n\tCan put in the handler without a data-config.xml and datasource\n*Can make an arbitrary entity a root entity\n\n\n\nMore documentation in the wiki "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12579919",
            "date": "2008-03-18T16:47:24+0000",
            "content": "The last patch started from the wrong root. This applies properly "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12583363",
            "date": "2008-03-29T20:39:59+0000",
            "content": "Fixes a bug with html handling in XPathRecordReader "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12583459",
            "date": "2008-03-30T16:21:16+0000",
            "content": "A change in behavior for XPathEntityProcessor. It now makes ''pk'' optional for entities having XPathEntityProcessor. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12586676",
            "date": "2008-04-08T06:36:09+0000",
            "content": "The scope has been changed from consuming just DB data. It is designed to consume any type of structured data "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12586680",
            "date": "2008-04-08T06:42:34+0000",
            "content": "The priority is changed to major "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12588384",
            "date": "2008-04-13T12:01:17+0000",
            "content": "A new patch consisting of a few bug fixes and some major new features. The changes include:\n\n\n\tNo need to write fields in data-config if the field name from DB/XML and field-name in schema.xml are the same. This removes a lot of useless verbosity from data-config.xml\n\tA cool new interactive development page, in which you write/change data-config.xml and see results immeadiately making interations extremely fast! Use http://host:port/solr/admin/dataimport.jsp or if using multi-core http://host:port/solr/core-name/admin/dataimport.jsp\n\tYou can start using the interactive mode without specifying data-config file in solrconfig.xml, however, specifying the data sources is necessary in solrconfig.xml\n\tInteractive development uses a new debug mode in DataImportHandler, add debug=on to the full-import command to see the actual documents which are created by DataImportHandler. This shows the first 10 documents created by DataImportHandler using the existing config without committing them to solr. It supports the start and rows parameter (just like query params) which you can use to see any document. This comes in very useful when suppose the 1000th document failed during indexing and you want to see the reason. If there are exceptions, the stacktrace is shown with the response.\n\tVerbose mode with verbose=on as a request parameter (used in conjunction with debug=on) which shows exactly how DataImportHandler created each document.\n\t\n\t\tWhat query was executed?\n\t\tHow much time it took?\n\t\tWhat rows it gave back?\n\t\tWhat transformers were applied and what was the result?\n\t\tAnother advantage is that you can see the fields which are indexed but not stored\n\t\n\t\n\tA show-config command has been added which gives the data-config.xml as a raw response (uses RawResponseWriter)\n\tA new interface called Evaluator has been added which makes it possible to plugin new expression evaluators (for resolving variable names)\n\tUsing the same Evaluator interface, a few new evaluators have been added\n\t\n\t\tformatDate - use as ${dataimporter.functions.formatDate('NOW',yyyy-MM-dd HH:mm)}, this will format NOW as per the given format and return a string which can be used in queries or urls. It supports the full DateMathParser syntax. You can also format fields e.g. ${dataimporter.functions.formatDate(A.purchase_date,dd-MM-yyyy)}\n\t\tencodeUrl - useful for URL-encoding parameters when making a HTTP call. Use as ${dataimport.functions.encodeUrl(emp.name)}\n\t\tescapeSql - useful for escaping parameters supplied in sql statements. This can replace quotes with two quotes to avoid sql syntax errors. Use as ${dataimporter.functions.escapeSql(emp.name)}\n\t\n\t\n\tCustom Evaluators can be specified in data-config.xml (more details and example will be added to the wiki)\n\tHttpDataSource now reads the content encoding from the response by default. Previously it assumed the default encoding to be UTF-8. This behavior can be overriden by explicitly specifying an encoding in solrconfig.xml\n\tA FileDataSource has been added which can read content from local files (e.g. XML feed files on local disk).\n\tTransformers can signal skipping a document by adding a key \"$skipDoc\" with value \"true\" in the returned map.\n\tNumberFormatTransformer is a new transformer which can be used to extract/convert numbers from strings. It uses the java.text.NumberFormat class in Java to provide its features.\n\tThe Context interface has been enhanced to add new methods for getting/setting session variables which can be used by Transformers to share data. Also a new method called getParentContext can enable a Transformer/EntityProcessor to get the parent entity's context in full imports.\n\n\n\nPlease let us know your comments and feedback. More details and examples will soon be added to the wiki page at http://wiki.apache.org/solr/DataImportHandler "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12591257",
            "date": "2008-04-22T10:59:27+0000",
            "content": "This patch contains the following changes\n\n\n\tDataSource definitions can now be added inside data-config.xml so there is no need to maintain configuration in two files. It also comes in handy with the interactive development mode.\n\tXSLT support in XPathEntityProcessor can apply a given XSL on the XML document before processing it. For example: <entity name=\"e\" processor=\"XPathEntityProcessor\" xsl=\"/home/user/my.xsl\">\n\tXPathEntityProcessor now knows how to process Solr Add XMLs. This is handy when using XSLT to change fetched XML directly into Solr Add XML format. Add an extra attribute useSolrAddSchema=\"true\" to enable this. If useSolrAddSchema=\"true\" is specified, then there is no need to put fields in the entity.\n\tA new EntityProcessor called FileListEntityProcessor has been added which can operate over a filesystem (directory) and can be used to get files by name (using a regex), size (in bytes) and can also exclude files matching a regex. Recursively operating over a directory is also supported.\n\tA TemplateTransformer which lets you put multiple fields into one field according the the given template. For example <field column=\"name\" template=\"${e.lastName}, ${e.firstName} ${e.middleName}\" />\n\tIn-built transformers are now enhanced to operate on multi-valued fields also.\n\tA Test harness has been created to make it easier to test DataImportHandler features. It is called AbstractDataImportHandlerTest and extends from AbstractSolrTestCase. Look at TestDataConfig and TestDocBuilder2 for examples\n\n\n\nWe shall write documentation and examples on these changes on the wiki at http://wiki.apache.org/solr/DataImportHandler "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12591273",
            "date": "2008-04-22T12:21:29+0000",
            "content": "This is some really cool stuff and should be added at some point soon.\n\nSome high level questions/comments, as I haven't looked in depth into the patch yet:\n\nIs it possible to just pass in SQL statements, etc. via a request?  Or do they have to be configured ahead of time?  What about connections?  On the one hand, having to configure it ahead of time can lock things down and be a little more secure, on the other hand, having to configure it ahead of time can lock things down and take away flexibility.  I hope to combine some of the stuff I've written to do this, with your patch.\n\nNot sure how to say it, but all the configuration starts to have the feel of Hibernate and/or the other ORMs.  Would there be a way to leverage something that already exists?  Although I do see from the comment the other day that you have reduced some of the verbosity\n\nHow is scheduling handled?  \n\nFinally, I'm not totally sure where this should live.  Solr doesn't currently have a \"contrib\" area, but this feels like a (major) contrib and may warrant adding it under a contrib area.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12591293",
            "date": "2008-04-22T13:18:04+0000",
            "content": "Giving a single SQL may limit the utility, because you may need to join more than one table in most of the usecases. \n\nBut it is possible to pass on the whole dataconfig itself as a request parameter. .We currently use that in the interactive development mode. \n\nWe have tried hard to cut down the verbosity of the configuration patch after patch . Now the 'metadata' i.e the extra information other than the queries itself is minimal. We leverage on the data such as schema etc to achieve it.\n\nThe connections are created once and consumed throughout one import. We take in the details for creating connections in the configuration (see documentation)\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12591300",
            "date": "2008-04-22T13:42:30+0000",
            "content": "\n\n\nI was just thinking as an option.  How does it limit?  A single SQL  \ncan join across tables.  I will try to update the patch w/ my merges  \nwhen I get a breather.  I have the case where I can send in something  \nlike:\n\nselect col1 as field1, col2 as field2, ... from table1, ... where ...;\n\nand it goes and runs that SQL against the specified connection.   \nBasically, any valid SQL select statement can be sent in.\n\nActually, I can send in multiple SQL statements as well, or just  \nspecify the table, or the table and certain columns.\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12591310",
            "date": "2008-04-22T14:20:25+0000",
            "content": "Grant, the limitation comes from multi-valued fields. When you join tables, it is most probably because you have a 1-to-many relationship. However, in that case a single row in the result does not contain all the information needed to create the Solr document. You'd need to combine many rows using the primary/foreign key to get all the data required in the Solr document. Btw, SOLR-103 is similiar to the functionality you have in mind. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12591320",
            "date": "2008-04-22T14:58:27+0000",
            "content": "hi Grant,\nwe started of with something like that and very soon realized that it cannot scale beyond the very basic usecases. \nWe need the ability to apply transformations like, splitting, merging fields etc etc.\nsometimes we need to put in a totally different piece of data .\neg: if a value is 1-5 put in the string 'low' , 5-10 put in 'medium' etc etc. \n\nAll these are really driven by the business requirements\n\nAnd there is the need for joining one table with another from the values in one table or merging one table with many tables. \n\nThen we had use cases where data comes from a Db and using a key we have to fetch data from an xml/http datasource etc etc. \n\nSo , the fundamental design or the 'kernel' of the system is supposed to be totally agnostic of the use cases and we let the users plug in the  implemenations in java/JS etc so that they can do what they actually want. And we want to share some of the components which can be common for others.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12591327",
            "date": "2008-04-22T15:13:45+0000",
            "content": "The best example of the simple usecase can be seen here http://wiki.apache.org/solr/DataImportHandler#shortconfig. \nHere we have joined 4 different tables with so little configuration "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12594553",
            "date": "2008-05-06T11:37:19+0000",
            "content": "This patch adds DataImportHandler as a contrib project into Solr. It uses standard Maven directory structure and a build.xml file. No changes have been made to the codebase.\n\nNote - I've opened SOLR-563 to track contrib area creation in Solr. Using this patch with the SOLR-563 patch lets you compile, test and package DataImportHandler with Solr war file. "
        },
        {
            "author": "Chris Moser",
            "id": "comment-12595928",
            "date": "2008-05-11T17:46:25+0000",
            "content": "Hi, \n\nThanks for all of your work with the dataimporter.  It's made working with Solr much easier.\n\nI think I found a small bug in SqlEntityProcessory.java starting on line 120:\n\nSqlEntityProcessor.java\n120:    boolean first = true;\n121:    String[] primaryKeys = context.getEntityAttribute(\"pk\").split(\",\");\n122:    for (int i = 0; i < primaryKeys.length; i++) {\n123:      if (!first) {\n124:        sb.append(\" and \");\n125:        first = false;\n126:      }\n\n\n\nThis causes problems in a generated SQL statement because it doesn't add the \"and\" string into the SQL statement when more than one field is provided in the pk entity value.  End result being a SQL syntax error.\n\nGiven first initialized as true, the if statement on line 123 will never happen (and first will never be set to false).  It looks like it would be more appropriate to have line 125 happen after the if statement on line 123.\n\nThis leads me to another issue, and that is the question of how to specify the table of the primary key when the primary key is ambiguous?  If there's a join condition in the SQL statment of a deltaQuery, and the any of the primary key columns are present in the joined table, the key is ambiguous and will cause a SQL error.\n\nIs there a way to specify the table for the primary key?  Perhaps an attribute \"pkTable\" can be added as an option for the entity declaration, i.e. in SqlEntityProcessor.java:\n\nSqlEntityProcessor.java\n127:      Object val = resolver.resolve(primaryKeys[i]);\n-->\t  if (context.getEntityAttribute(\"pkTable\").length()>0)\n-->\t\tsb.append(context.getEntityAttribute(\"pkTable\")+\".\");\n128:\t  sb.append(primaryKeys[i]).append(\" = \");\n\n\n\nThis removes any potential ambiguity issues with joins when pkTable is specified. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12595931",
            "date": "2008-05-11T18:31:39+0000",
            "content": "Thanks Chris, nice catch! The first one is definitely a bug.  I'll fix that, add a test and upload a new patch. I'm not sure if I understand your second point completely, can you please give an example? "
        },
        {
            "author": "Chris Moser",
            "id": "comment-12596237",
            "date": "2008-05-13T00:05:23+0000",
            "content": "Hi Shalin, \n\nI'm indexing forums with Solr and have tables with a structure similar to this:\n\n\nposts\n------\nforumid int\nmessageid int\ndeleted boolean\nmessage text\n\nforums\n------\nforumid int\nname text\ndeleted boolean\n\n\n\n\nThe simplified data query I'm running goes like this:\n\n\nSELECT \n   p.forumid,\n   p.messageid,\n   IF (p.deleted OR f.deleted,true,false) as deleted,\n   p.message\n  \nFROM \n   posts p, forums f\n\nWHERE\n   f.forumid = p.forumid\n\n\n\nThe query checks to see if the post or the forum is deleted, and marks it in the index as deleted in either case (which is why I'm doing the join).  The problem I'm running into is that the importer is running the WHERE clause like this:\n\n\nWHERE \n   f.forumid = p.forumid and forumid=123 and messageid=123456789\n\n\n\nIn this case, the forumid=123 part is ambiguous (forumid being in the posts and the forums table) so this causes a SQL error.  So I added an additional attribute to the entity defintion (pkTable) which prepends the forumid=123 with the pkTable value so it generates pkTable.forumid=123.\n\nNot sure if this is the best way to do it but it fixed the problem  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12597541",
            "date": "2008-05-16T16:35:36+0000",
            "content": "Moser :I guess I now understand your requirement . The solution you have proposed is indeed a good one. \nHow about this one:\n\nThe pk is used only for this purpose . So you must be able to keep it as pk=\"forum.forumId\" and when the query is generated I can use it as it is and when I fetch the value , I can just use the part after the period (.)\n\nIn addition I can make the getQuery() method in SqlEntityProcessor public so that you can implement your custom logic very easily "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12598820",
            "date": "2008-05-21T22:12:51+0000",
            "content": "creating a contrib structure and making the DataImportHandler a contrib definitely seems like the smart way to go ... particularly since it doesn't require any \"core\" changes. "
        },
        {
            "author": "Olivier Poitrey",
            "id": "comment-12602089",
            "date": "2008-06-03T21:58:25+0000",
            "content": "Paul,\n\nThe current version of the code seems not to allow the construction pk=\"forum.forumId\" you're talking about. I did a small patch to make it possible. I don't know if it's the correct way to do it but it worked well for me.\n\nHere is the patch:\n\n\n--- a/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/SqlEntityProcessor.java\n+++ b/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/SqlEntityProcessor.java\n@@ -124,7 +124,9 @@ public class SqlEntityProcessor extends EntityProcessorBase {\n         sb.append(\" and \");\n         first = false;\n       }\n-      Object val = resolver.resolve(primaryKeys[i]);\n+      // Only send the field part of the pk when pk includes the table ref\n+      String[] pkParts = primaryKeys[i].split(\"\\\\.\");\n+      Object val = resolver.resolve(pkParts[pkParts.length - 1]);\n       sb.append(primaryKeys[i]).append(\" = \");\n       if (val instanceof Number) {\n         sb.append(val.toString());\n\n\n\nHope that helps. "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12603662",
            "date": "2008-06-09T19:51:16+0000",
            "content": "There's a slight problem using Connector/J for mysql, in that it doesn't fully implement the jdbc spec for \nsetFetchSize, resulting in all rows in mysql being selected into memory.\n\nConnector/J states that you must pass ?useCursorFetch=true in the connect string, but it exposes another mysql bug with server-side parsed queries throwing an error of \"incorrect key file\" on the temp tables generated by the cursor, \nas yet there isn't a fix in mysql that I know of.\n\nSomething that seems to work is to set the batchSize to Integer.MIN_VALUE:\n\nJdbcDataSource.java\n\n if (bsz != null) {\n      try {\n        batchSize = Integer.parseInt(bsz);\n        if (batchSize < 0)\n            batchSize = Integer.MIN_VALUE;  // pjaol : setting batchSize to <0 in dataSource forces connector / j to use Integer.MIN_VALUE\n      } catch (NumberFormatException e) {\n        LOG.log(Level.WARNING, \"Invalid batch size: \" + bsz);\n      }\n    }\n\n\n\nThis basically puts the result set size at 1 row, a little slow, but if you can't set your JVM memory settings high enough\nit gives you a option.\n\nAlso suggest null-ing the row hashmap in DocBuilder immediately after use to allow GC to clean up\nthe reference faster within eden space.\n\nDocBuilder.java\n\n    if (entity.isDocRoot) {\n            if (stop.get())\n              return;\n            boolean result = writer.upload(doc);\n            doc = null;\n            if (result)\n              importStatistics.docCount.incrementAndGet();\n          }\n          \n       arow = null; // pjaol : set reference to hashmap to null to eliminate strong reference                                                   \n       \n\n       } catch (DataImportHandlerException e)\n..........\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12603769",
            "date": "2008-06-10T04:41:58+0000",
            "content": "Thanks for the suggestions . \nJdbcDataSource is a generic implementation for all jdbc drivers . so batchSize itself is a configurable parameter for JdbcdataSource. set the value and it should be fine. Anyway we will incorporate the changes you have suggested because it is convenient for users.\n\nThe var arow goes out of scope immedietly because the method terminates after this . I'm not sure it make a any difference if I explicitly set it to null "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12604257",
            "date": "2008-06-11T15:41:59+0000",
            "content": "A new patch file (SOLR-469.patch) consisting of some important bug fixes and minor enhancements. The changes and the corresponding classes are given below\n\nChanges\n\n\tSet fetch size to Integer.MIN_VALUE if batchSize in configuration is -1 as per Patrick's suggestion \u2013 JdbcDataSource\n\tTransformers can add a boost to a document by adding a key/value pair row.put(\"$docBoost\", 2.0f) from any entity \u2013 DocBuilder,SolrWriter and DataImportHandler\n\tFixes for infinite loop in SqlEntityProcessor when delta query fails for some reason and NullPointerException is thrown in EntityProcessorBase \u2013 EntityProcessorBase\n\tFix for NullPointerException in TemplateTransformer and corresponding test \u2013 TemplateTransformer, TestTemplateTransformer\n\tEnhancement for specifying table.column syntax for pk attribute in entity as per issue reported by Chris Moser and Olivier Poitrey \u2013 SqlEntityProcessor,TestSqlEntityProcessor2\n\tFix for NullPointerException in XPathRecordReader when attribute specified through xpath is null as per issue reported by Nicolas Pastorino in solr-user \u2013 XPathRecordReader, TestXPathRecordReader\n\tEnhancement to DataSource interface to provide a close method \u2013 DataSource, FileDataSource, HttpDataSource, MockDataSource\n\tContext interface has a new method getDataSource(String name) for getting a new DataSource instance as per the name specified in solrconfig.xml or data-config.xml \u2013 Context, ContextImpl, DataImporter, DocBuilder\n\tFileListEntityProcessor implements olderThan and newerThan filtering parameters \u2013 FileListEntityProcessor, TestFileListEntityProcessor\n\tDebug Mode can be disabled from solrconfig.xml by enableDebug=false \u2013 DataImporter, DataImportHandler\n\tRunning statistics are exposed on the Solr Statistics page in addition to cumulative statictics \u2013 DataImportHandler, DocBuilder\n\tThe dataSource attribute can be null when using certain EntityProcessors such as FileListEntityProcessor which does not need a dataSource. So when dataSource=\"null\", no attempt is made to create a DataSource instance \u2013 DataImporter\n\n\n\nUpdated as per Noble's comment below. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12604259",
            "date": "2008-06-11T15:51:21+0000",
            "content": "small correction\nContext interface has a new method getDataSource(String entityName) for getting a new DataSource instance for the given entity - Context, ContextImpl, DataImporter, DocBuilder\n\n/**\n   * Gets a new DataSource instance with a name.\n   * @return\n   * @param name Name of the dataSource as defined in the dataSource tag\n   */\n  public DataSource getDataSource(String name);\n\n "
        },
        {
            "author": "Olivier Poitrey",
            "id": "comment-12604261",
            "date": "2008-06-11T15:55:54+0000",
            "content": "No -contrib version this time?  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12604263",
            "date": "2008-06-11T15:59:33+0000",
            "content": "A lot of people were using the older patch. I'm generating the contrib one too  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12604269",
            "date": "2008-06-11T16:47:39+0000",
            "content": "Copying changes in codebase from SOLR-469.patch to SOLR-469-contrib.patch "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12604276",
            "date": "2008-06-11T17:39:28+0000",
            "content": "With the arow, I noticed by nulling it, that CMS GC was cleaning items up faster in eden space.\nWithout it, Full GC kicked in more frequently. This was with indexing about 250~MB from mysql.\nIf you've not got that much data then there isn't much of a worry, it's just a little optimization that reduces the need\nto increase your jvm's mx and newsize settings.\n\nAnother thing I was looking at is the SolrWriter, instead of calling an updateHandler directly, I think you should call\nthe UpdateRequestProcessorFactory and allow the UpdateRequestProcessor chain handle the\n*processAdd\n*processDelete\n*processCommit\n*finish\n\nIt allows for custom ChainedUpdateProcessor'Factory's which is a fantastic little known about item. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12605047",
            "date": "2008-06-14T08:06:09+0000",
            "content": "This patch contains\n\n\tintegration with SOLR-505 ( disable cache headers)\n\tTests inTestSCriptTransformer Ignored. (it requires java 6)\n\tNew feature CachedSqlEntityProcessor. It can dramatically speed up indexing if there are sub-entities. It can cache the rows and avoid subsequent database calls. Consumes a lot of RAM. See wiki\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12605285",
            "date": "2008-06-16T13:44:56+0000",
            "content": "Changes\n\n\tUpdated the build.xml to compile Solr before building DataImportHandler and place DataImportHandler's javadoc jar to solr/dist folder so that the javadocs are available in Solr nightly builds\n\tRemoved @author Javadoc tags from all source files in accordance with Solr coding conventions\n\tImproved Javadocs for a lot of classes especially the public interfaces\n\tFormatted code using the Eclipse codestyle xml given at HowToContribute wiki page\n\tAdded @since solr 1.3 to all source files\n\tI've verified that the Apache license text is present in all the source files\n\n\n\nNo changes have been made to the code (in terms of functionality)\n\nNote \u2013 The SOLR-563 patch must be applied before this patch to build Solr with DataImportHandler as a contrib project.\n\nA lot of people are using this patch and it would be easier for them if DataImportHandler is available in the nightly builds. Also, this patch has become huge and enhancements and bug fixes would also be easier if it were committed. Grant \u2013 We feel that this is ready to be committed now whenever you can take a look. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12607171",
            "date": "2008-06-23T09:48:03+0000",
            "content": "Changes \n\n\tclassloading is done using SolrresourceLoader . adding jars to solrhome/lib must work\n\tThe request parameter can add optimize=false to disable optimize\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12607659",
            "date": "2008-06-24T15:49:32+0000",
            "content": "The last patch wasn't generated correctly. This one fixes it. No changes in the code since the last patch. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12607661",
            "date": "2008-06-24T15:51:28+0000",
            "content": "This time with the correct name SOLR-469-contrib.patch "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12608421",
            "date": "2008-06-26T13:25:24+0000",
            "content": "Patch applies cleanly, tests pass, although I notice several @ignore in there.   Docs look good in my preliminary perusing.  I've only started looking at things, and have a lot of reading to catch up on, so these first comments, please take with a grain of salt, as the English saying goes... \n\nI'd suggest,that instead of relying on MySQL in TestJdbcDataSource, we instead use and embedded Derby or some sort of JDBC mock.  I suggest Derby mainly b/c it's already ASF and I don't want to bother looking up licenses for HSQL or any of the others that might work.  \n\nAlso, I notice several interfaces that have a number of methods on them.  Have you thought about abstract base classes instead?  I know, there is a whole big debate over it, and people will argue that if you get the interface exactly correct, you should use interfaces.  Nice in theory, but Lucene/Solr experience suggests that rarely happens.  Of course, I think the correct way is to actually do both, as one can easily decorate an abstract base class with more interfaces as needed.  Just food for thought, b/c what's going to quickly happen after release is someone is going to need a new method on the DataSource or something and then we are going to be stuck doing all kinds of workarounds due to back compatibility reasons.  The alternative is to clearly mark all Interfaces as being experimental at this point and clearly note that we expect them to change.  We may even want to consider both!  The other point, though, is contrib packages need not be held to the same standard as core when it comes to back compat.\n\nWhat relation does the Context have to the HttpDataSource?  In other words, the DataSource init method takes a Context, meaning the HttpDataSource needs a Context, yet in my first glance at the Context, it seems to be DB related.\n\nWhat if I wanted to slurp from a table on the fly?  That is, I want to send in a select statement in my request and I let the columns line up where they may Field wise (i.e. via dynamic fields or I rely on something like select id, colA as fieldA, colB as fieldB from MyTable;   )\nIs that possible?  \n\nInteractive mode has a bit of a chicken and the egg problem when it comes to JDBC, right, in that the Driver needs to be present in Solr/lib right?  So, one can currently only interactively configure a JDBC DataSource if the driver is already in lib and loaded by the ClassLoader.   If you haven't already, it might actually be useful to show what drivers are present by using the DriverManager.\n\nIn the JDBCDataSource, not sure I follow the connection stuff.  Can you explain a bit?  Also, what if I wanted to plug in my own Connection Pooling library, as I may already have one setup for other things (if using Solr embedded)?\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12608429",
            "date": "2008-06-26T13:46:34+0000",
            "content": "I'd suggest,that instead of relying on MySQL in TestJdbcDataSource, we instead use and embedded Derby or some sort of JDBC mock. I suggest Derby mainly b/c it's already ASF and I don't want to bother looking up licenses for HSQL or any of the others that might work.\n\nWe must remove the TestJdbcDataSource if we cannot integrate derby in the dev dependencies. \nAlso, I notice several interfaces that have a number of methods on them. Have you thought about abstract base classes instead?\n\nYes/No A lot of interfaces are never implemented by users like Context, VariableResolver They are kept as interfaces to make API's simple\nThe interfaces people need to implement are \n\n\tEntityProcessor: We  expect users to extend EntityProcessorBase\n\tTransformer : The most commonly implemented interface. I am ambivalent regarding this. I'm do  not know if it will change\n\tDataSource : This may be made abstract class\n\n\n\nWhat relation does the Context have to the HttpDataSource? \n\nDataSource is always created for an entity. The Context is the easiest  way to get info about the entity. The current DataSources do not use that info . But because we have the info readily available just pass it over.\n\nWhat if I wanted to slurp from a table on the fly?\n\nCachedSqlEntityProcessor already does that. It slurps the table and caches the info\n\nInteractive mode has a bit of a chicken and the egg problem when it comes to JDBC, right, in that the Driver needs to be present in Solr/lib right?\n\nNot sure If I got the question . Interactive dev mode does not need the drivers\n\nIn the JDBCDataSource, not sure I follow the connection stuff. Can you explain a bit? \nWe create connections using Drivermanager.getConnection(). No pooling because, the same connection is used throughout the indexing. one conn is created per entity. So no pooling implemented.\n\nA  PooledJdbcDataSource impl?\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12608447",
            "date": "2008-06-26T14:16:03+0000",
            "content": "Patch applies cleanly, tests pass, although I notice several @ignore in there.\nThe @ignore are present in TestJdbcDataSource (for lack of mysql to test with) and in TestScriptTransformer (script tests can only be run with Java 6 which has a JS ScriptEngine present by default). We can rewrite the test with Derby if needed.\n\nAlso, I notice several interfaces that have a number of methods on them. Have you thought about abstract base classes instead?\nApart from the ones Noble pointed out, there's Evaluator which users can use to extend the power of VariableResolver. The EvaluatorBag provides some generally useful implementations. Probably the context can be passed to Evaluator as well. Apart from that, I'm not sure if/how they would change in the future. An AbstractDataSource can be added \u2013 maybe we can templatize the query as well in addition to the return type.\n\nWhat relation does the Context have to the HttpDataSource? \nThe Context is independent of a data source. It's just extra information which is passed along if someone needs to use. Most of the implementation do not actually use it.\n\nWhat if I wanted to slurp from a table on the fly?\nIf you mean passing an SQL query on the fly as a request parameter then no, it is not supported. We haven't seen a use-case for it yet \u2013 since schema and indexing are well defined in advance and there is no harm in putting the query in the configuration. However, if someone really wants to do something like that, he/she can pass a full data-config as a request parameter (debug mode) which can be executed. The interactive mode uses this approach. An alternate approach can be to extend SqlEntityProcessor and override the getQuery method to use the Context#getRequestParameters and if sql param is present, use that as the query instead of the sql in configuration.\n\nInteractive mode has a bit of a chicken and the egg problem when it comes to JDBC, right, in that the Driver needs to be present in Solr/lib right?\nYes, to play interactively while using a JdbcDataSource, one would need to have the driver jar present in the class-path before hand. The interactive mode is however independent \u2013 HttpDataSource does not have this limitation (slashdot example on the wiki)\n\nIn the JDBCDataSource, not sure I follow the connection stuff. Can you explain a bit? \nThe connection is acquired once and used throught the import process. It is closed if not used for 10 seconds. The idea behind the time-out was to avoid the connection getting closed by the server due to the inactivity. Apart from that scenario, there's very less probability of a connection error happening \u2013 and even if it did, we may not have a way to deal with it.\n "
        },
        {
            "author": "Jeremy Hinegardner",
            "id": "comment-12610418",
            "date": "2008-07-04T02:11:08+0000",
            "content": "I think there is a bug in the -contrib patch.  setting optimize=false as a request parameter appears to turn off clean instead of turning off optimize.\n\n\nDataImporter.java line 496\nif (requestParams.containsKey(\"clean\"))\n    clean = Boolean.parseBoolean((String) requestParams.get(\"clean\"));\nif (requestParams.containsKey(\"optimize\"))\n    clean = Boolean.parseBoolean((String) requestParams.get(\"optimize\"));\n\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614245",
            "date": "2008-07-17T06:42:04+0000",
            "content": "\n\tAll interfaces are marked as experimental\n\tThe bug optimize=true fixed\n\tadded a new variable to dataimporter namespace ${dataimporter.index_start_time}\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615516",
            "date": "2008-07-22T05:12:46+0000",
            "content": "\n\tAdded a method destroy() to EntityProcessor . This coupled with init() can be used for pre/post actions\n\tJdbcDataSource uses Statement#execute()  instead of Statement#executeQuery() . So users can execute DDL/DML using JdbcDataSource\n\tContext has a new method getSolrCore() which returns the SolrCore instance\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615518",
            "date": "2008-07-22T05:17:14+0000",
            "content": "it was a bad patch  "
        },
        {
            "author": "Jonathan Lee",
            "id": "comment-12615952",
            "date": "2008-07-23T10:56:57+0000",
            "content": "This patch has been a wonderful addition to solr - thanks for all the work!\n\nI believe that there is a bug in CachedSqlEntityProcessor that causes transformers to be ignored.  Here is a patch that worked for me, but I am not sure it is entirely correct:\n\n\n--- contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/CachedSqlEntityProcessor.java\n+++ contrib.new/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/CachedSqlEntityProcessor.java\n@@ -43,18 +43,23 @@\n   }\n \n   public Map<String, Object> nextRow() {\n-    if (rowcache != null)\n-      return getFromRowCache();\n-    if (!isFirst)\n+    Map<String, Object> r;\n+    if (rowcache != null) {\n+      r = getFromRowCache();\n+    } else if (!isFirst) {\n       return null;\n-    String query = resolver.replaceTokens(context.getEntityAttribute(\"query\"));\n-    isFirst = false;\n-    if (simpleCache != null) {\n-      return getSimplCacheData(query);\n     } else {\n-      return getIdCacheData(query);\n+      String query = resolver.replaceTokens(context.getEntityAttribute(\"query\"));\n+      isFirst = false;\n+      if (simpleCache != null) {\n+        r = getSimplCacheData(query);\n+      } else {\n+        r = getIdCacheData(query);\n+      }\n     }\n-\n+    if (r == null)\n+      return null;\n+    return applyTransformer(r);\n   }\n \n   protected List<Map<String, Object>> getAllNonCachedRows() {\n \n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615960",
            "date": "2008-07-23T11:26:42+0000",
            "content": "bug fix in CachedSqlEntityProcessor "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615982",
            "date": "2008-07-23T12:06:37+0000",
            "content": "ignore the previous patch "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12616203",
            "date": "2008-07-23T18:20:26+0000",
            "content": "The previous patch did not take care of multirow-transformers for CachedSqlEntityProcessor. Added a testcase and fixed that "
        },
        {
            "author": "Jonathan Lee",
            "id": "comment-12617472",
            "date": "2008-07-28T17:00:06+0000",
            "content": "When using CachedSqlEntityProcessor, an NPE is thrown (EntityProcessorBase.java:367) if a key value doesn't exist in the cached row set.   This change to EntityProcessorBase.java should fix that, or let me know if I've missed something here!\n\n\n--- contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java\t2008-07-28 12:49:21.000000000 -0400\n+++ contrib.new/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java\t2008-07-28 12:40:17.000000000 -0400\n@@ -348,7 +348 @@\n-    if (rowIdVsRows != null) {\n-      rows = rowIdVsRows.get(key);\n-      if (rows == null)\n-        return null;\n-      dataSourceRowCache = new ArrayList<Map<String, Object>>(rows);\n-      return getFromRowCacheTransformed();\n-    } else {\n+    if (rowIdVsRows == null) {\n@@ -367,6 +360,0 @@\n-        dataSourceRowCache = new ArrayList<Map<String, Object>>(rowIdVsRows.get(key));\n-        if (dataSourceRowCache.isEmpty()) {\n-          dataSourceRowCache = null;\n-          return null;\n-        }\n-        return getFromRowCacheTransformed();\n@@ -374,0 +363,5 @@\n+    rows = rowIdVsRows.get(key);\n+    if (rows == null)\n+      return null;\n+    dataSourceRowCache = new ArrayList<Map<String, Object>>(rows);\n+    return getFromRowCacheTransformed();\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12617483",
            "date": "2008-07-28T17:35:36+0000",
            "content": "Nice catch! We shall incorporate the fix into the next patch.\n\nYes indeed it can throw NullPointerException when key value does not exist in cached row set. However, I am wondering what can cause such a cache miss. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12618372",
            "date": "2008-07-30T14:54:17+0000",
            "content": "Sorry for the spam due to my (multiple) mistakes. I think this one is the one \n\nA new patch containing the following changes:\n\n\n\tOn further thinking about Interface vs. Abstract classes, we have decided to replace all interfaces with abstract classes. Transformer, Context, EntityProcessor, Evaluator, DataSource and VariableResolver are now abstract classes.\n\tThe bug reported by Jonathan has been fixed and the TestCachedEntityProcessor has been updated to catch it. This exception used to be thrown only if the first request to CachedEntityProcessor needs a row which is not in cache. Subsequent requests were not affected.\n\tJavadoc improvements. In particular, all the API related classes are marked as experimental and subject to change.\n\tPropset Id in all classes.\n\n\n\nUsers who have written their own custom transformers using the API will need to change their code. Sorry for the inconvenience.\n\nGrant - Is there anything else we need to do to get it committed? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12618388",
            "date": "2008-07-30T15:31:18+0000",
            "content": "Shalin, \n\nI don't have the time at the moment on this so feel free to use your new powers.  I think putting it into contrib and marking it as experimental is good (such that it is bound as strictly by back compat rules).\n\nI have some needs that I would like to see worked in, but I haven't had the time and I don't think they should hold back others, as it is obviously in significant use already.  They are also nothing earth-shattering\n\nSo, it's all yours.  Enjoy. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12618395",
            "date": "2008-07-30T15:43:12+0000",
            "content": "Thanks Grant \n\nI shall go over the javadocs once more and then commit it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12618483",
            "date": "2008-07-30T19:45:47+0000",
            "content": "Committed revision 681182.\n\nA big thanks to Noble for designing these cool features and to Grant for his reviews, feedback and support! Thanks to everybody who helped us by using the patch, giving suggestions and pointing out bugs!\n\nSolr rocks! "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618938",
            "date": "2008-08-01T06:12:08+0000",
            "content": "xpath entity processor can stream rows one by one (for huge xml files) my making stream=\"true\" "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12619512",
            "date": "2008-08-04T13:38:22+0000",
            "content": "Committed revision 682383. "
        },
        {
            "author": "ms",
            "id": "comment-12633046",
            "date": "2008-09-21T05:30:36+0000",
            "content": "Great patch. I am trying to use this with Firebird. The root Entity makes it's way into the index - but not the sub entities. On debugging with dataimport.jsp, the sub entities seem to be correctly processed. I can submit a test case with embedded firebird if necessary. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12633048",
            "date": "2008-09-21T07:01:56+0000",
            "content": "Great patch. I am trying to use this with Firebird. The root Entity makes it's way into the index - but not the sub entities. On debugging with dataimport.jsp, the sub entities seem to be correctly processed. I can submit a test case with embedded firebird if necessary.\n\nYou don't need to use the patch anymore. DataImportHandler has been released with Solr 1.3\n\nA test case to reproduce your problem will be nice. Just showing us the debug output will also help. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12633074",
            "date": "2008-09-21T16:02:53+0000",
            "content": "This issue has served its purpose. Any new requirements/bugs can be raised on separate issue "
        },
        {
            "author": "ms",
            "id": "comment-12633075",
            "date": "2008-09-21T16:11:20+0000",
            "content": "Shalin\nSent a test case for Firebird just now to your email ID. If it is a bug with firebird, please let me know how I may report it. thanks! "
        },
        {
            "author": "Mis Tigi",
            "id": "comment-12849093",
            "date": "2010-03-24T07:51:05+0000",
            "content": "Thanks for everyone involved for this wonderful contribution. I find it extremely useful, saves lots of time and allows has an added benefit of rapid prototyping.\n\nOne of the initial goals for this project was to be able to schedule automatic imports at regular intervals. I cannot find any reference to it beyond the initial goals. Has this been implemented ? If not any suggestions what is best way to accomplish that ?\n\nI also have a question if documents deletes is something that this handler can do or it should be done outside of it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12849121",
            "date": "2010-03-24T09:27:57+0000",
            "content": "Thanks!\n\nScheduling is not implemented inside Solr. You can use a cron job for scheduling automatic imports. For example, you can call \"wget http://solr.host:port/solr/dataimport?command=full-import\". "
        },
        {
            "author": "David Smiley",
            "id": "comment-12849299",
            "date": "2010-03-24T17:29:35+0000",
            "content": "However doing so is a protocol crime \u2013 HTTP GET verb should be read-only.  Use HTTP POST instead. "
        }
    ]
}