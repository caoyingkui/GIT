{
    "id": "LUCENE-1926",
    "title": "Back compat break with old next() consumer API",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "There is a bug that causes tokenstreams to return different results, depending upon whether they are consumed with the incrementToken() api or the next() api.\n\nI found this because the Solr analysis tool in the admin page uses the next() api, and i was seeing strange results.\n\nI've created a test case to show the problem. when calling captureState(),  the current state is erased, but only when consuming with the next() api.\nIf I consume with incrementToken(), things work. \n\n\nState tempState = captureState(); // after we capture state here, things get strange.\nString right = termAtt.term(); // when using old consumer API, this value is wrong!!!!",
    "attachments": {
        "CaptureStateTestcase.java": "https://issues.apache.org/jira/secure/attachment/12420559/CaptureStateTestcase.java",
        "LUCENE-1926.patch": "https://issues.apache.org/jira/secure/attachment/12420562/LUCENE-1926.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-09-25T14:54:43+0000",
            "content": "i looked into this some, and it appears the problem isn't due to captureState(), but instead what is happening is my termAttribute is getting erased even before then.\n\nI suspect this might be linked to the changes in LUCENE-1919 ",
            "author": "Robert Muir",
            "id": "comment-12759553"
        },
        {
            "date": "2009-09-25T15:03:23+0000",
            "content": "That's exactly the case. You should also capture the state in \"case 1:\". The attributes API does not guarantee, that the attributes are preserved between calls to incrementToken (the same like the reusable TokenAPI is not forced to always use the same reusable token). If you do not reuse tokens, this is exactly the case (The Token instance in the wrapper is replaced), so the attribute contents gets lost (empty token instance). One could fix this ba an extra token cloning, but even with the old API (next(Token) it would never have been worked. Because of this, all Tokenizer should call clearAttributes() first to have a new start.\n\nI am not sure, if it worked correctly before LUCENE-1919.\n\nADDENDUM:\nYou should never rely on attributes preserved between calls. If you plug another TokenFilter on top of your filter, this filter could change the tokens. The Tokens are currently only preserved 100% if you only use incrementToken() and your filter/Tokenizer is the only one modifying the tokens. You can never guarantee that.\n\nThis issue is won't fix, as exspected behaviour. Ok with that? ",
            "author": "Uwe Schindler",
            "id": "comment-12759557"
        },
        {
            "date": "2009-09-25T15:11:40+0000",
            "content": "\nThe attributes API does not guarantee, that the attributes are preserved between calls to incrementToken\n\nUwe, perhaps this is my understanding then. Its not obvious from the documentation that incrementToken will erase my attributes.\n\nTokenStream now extends AttributeSource, which provides\n access to all of the token Attributes for the TokenStream.\n Note that only one instance per AttributeImpl is created and reused\n for every token. This approach reduces object creation and allows local\n caching of references to the AttributeImpls.\n\nWhat else is \"local caching of references to the AttributeImpls\" supposed to mean?\n\nbtw, my TestCase uses StandardTokenizer, which does call clearAttributes(). ",
            "author": "Robert Muir",
            "id": "comment-12759563"
        },
        {
            "date": "2009-09-25T15:16:01+0000",
            "content": "Yes, I think calling captureState() before incrementToken() doesn't make sense (as case:2 does) since the state would seem to be undefined at that point? ",
            "author": "Yonik Seeley",
            "id": "comment-12759566"
        },
        {
            "date": "2009-09-25T15:16:23+0000",
            "content": "This issue is won't fix, as exspected behaviour. Ok with that?\n\nUwe, based upon what you said (additional filters could modify the tokens), I tend to agree with you, but its wierd it only happens with next() consumer api.\nIt does work with next(Token)\n\nI still think its also not very obvious, and wierd to see inconsistencies depending upon how things are consumed.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12759567"
        },
        {
            "date": "2009-09-25T15:18:16+0000",
            "content": "btw, my TestCase uses StandardTokenizer, which does call clearAttributes().\n\nI have seen this, because of that the attrs between step 0 and 1 are cleared. As you do not call incrementToken in the underlying filter in step 2, it seems to be preserved (in fact, you are the source of tokens and should call clearAttributes() for this step).\n\nThe problem with preserving the attribute state between calls to incrementToken is e.g. the following even with incrementToken():\n\nJust put an ReverseTokenFilter on top of this TokenFilter. This tokenfilter reverses the term. If you only consume with incrementToken() and rely on the fact that the tokens from the last call are preserved, you fail: The Token is reversed by the reverse filter and then step 2 would then see the reversed term text and not the forward one exspected from step 1.\n\nIf you want to preserve states between incrementToken calls, you have to capture the state. Maybe the Javadocs should be extended, to clearly note, that attribute contents (may) not preserved between calls to incrementToken(). ",
            "author": "Uwe Schindler",
            "id": "comment-12759568"
        },
        {
            "date": "2009-09-25T15:21:10+0000",
            "content": "Yes, I think calling captureState() before incrementToken() doesn't make sense (as case:2 does) since the state would seem to be undefined at that point?\n\nThis is because its out of context (I had to narrow the test down).\nThe idea was that in case 2, i wanted to capture the unchanged TermAttribute from case 1 (since i felt if i didnt call incrementToken, it would not be changed) ",
            "author": "Robert Muir",
            "id": "comment-12759570"
        },
        {
            "date": "2009-09-25T15:22:59+0000",
            "content": "Maybe the Javadocs should be extended, to clearly note, that attribute contents (may) not preserved between calls to incrementToken().\n\nUwe, yes. I expect that if I added a stemmer, or reversetokenfilter, or something it would modify my termAttribute.\nWhat i didnt expect is that the back compat layer would modify my termAttribute. ",
            "author": "Robert Muir",
            "id": "comment-12759571"
        },
        {
            "date": "2009-09-25T15:23:13+0000",
            "content": "I think this info from next(Token) javadocs also applies to incrementToken():\n\n   * Also, the producer must make no assumptions about a {@link Token} after it\n   * has been returned: the caller may arbitrarily change it. If the producer\n   * needs to hold onto the {@link Token} for subsequent calls, it must clone()\n   * it before storing it. Note that a {@link TokenFilter} is considered a\n   * consumer.\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12759572"
        },
        {
            "date": "2009-09-25T15:26:00+0000",
            "content": "Uwe, yes. I expect that if I added a stemmer, or reversetokenfilter, or something it would modify my termAttribute. What i didnt expect is that the back compat layer would modify my termAttribute.\n\nOK, but this was the same with next(Token) (see above). You could not rely on the fact that the reusableToken is preserved, it could even be changed by the consumer or whatever.\n\nYou can implement you TokenFilter with next(reusableToken) and will have the same problems if you rely on the reusableToken is preserved from the last call. ",
            "author": "Uwe Schindler",
            "id": "comment-12759573"
        },
        {
            "date": "2009-09-25T15:32:11+0000",
            "content": "Uwe, what you are saying is true (its really a bug in my filter and I agree you should cancel this as won't fix, any javadoc clarification might prevent someone else from doing this).\n\nOne side note I worry about a bit now, is the possibility of similar bugs might exist or crop up somewhere like shingle... but the tests might pass and they appear to be working ",
            "author": "Robert Muir",
            "id": "comment-12759575"
        },
        {
            "date": "2009-09-25T15:35:31+0000",
            "content": "this behavior really was not guaranteed as explained by Uwe... sorry to waste your time with this  ",
            "author": "Robert Muir",
            "id": "comment-12759577"
        },
        {
            "date": "2009-09-25T15:40:17+0000",
            "content": "One side note I worry about a bit now, is the possibility of similar bugs might exist or crop up somewhere like shingle... but the tests might pass and they appear to be working\n\nShingle was reviewed and changed by me, I think this one is OK. The problem you described could have been catched by backwards tests, but these are only running for core, not contrib. ",
            "author": "Uwe Schindler",
            "id": "comment-12759582"
        },
        {
            "date": "2009-09-25T15:43:43+0000",
            "content": "This is an addition to javadocs (just copied from next(Token)). ",
            "author": "Uwe Schindler",
            "id": "comment-12759584"
        },
        {
            "date": "2009-09-25T15:44:10+0000",
            "content": "\nShingle was reviewed and changed by me, I think this one is OK. The problem you described could have been catched by backwards tests, but these are only running for core, not contrib.\n\nUwe, again I apologize, thanks for explaining it to me. I thought I had found something when i saw different results from incrementToken versus next, but clearly all I found was a bug in my code  ",
            "author": "Robert Muir",
            "id": "comment-12759585"
        },
        {
            "date": "2009-09-25T15:45:34+0000",
            "content": "this behavior really was not guaranteed as explained by Uwe... sorry to waste your time with this  \n\nYou did not waste my time, it was more my health. I got a heart attack when I read \"Back compat break in old next()...\"  ",
            "author": "Uwe Schindler",
            "id": "comment-12759586"
        },
        {
            "date": "2009-09-25T15:49:00+0000",
            "content": "You did not waste my time, it was more my health. I got a heart attack when I read \"Back compat break in old next()...\" \n\nI had to grab your attention since I couldn't figure it out  \nIf you want, you should change the title to \"Bug in Robert Muir's naive code\"... I deserve it for giving you heart failures.\nThe javadoc patch you uploaded might help to prevent someone from creating similar bugs in the future. ",
            "author": "Robert Muir",
            "id": "comment-12759588"
        },
        {
            "date": "2009-09-25T15:50:28+0000",
            "content": "You did not waste my time, it was more my health. I got a heart attack when I read \"Back compat break in old next()...\" \n\nYou weren't the only one  ",
            "author": "Mark Miller",
            "id": "comment-12759589"
        },
        {
            "date": "2009-09-25T15:57:12+0000",
            "content": "One side note I worry about a bit now, is the possibility of similar bugs might exist or crop up somewhere like shingle... but the tests might pass and they appear to be working\n\nWe could check the filters again by changing assertAnalyzesTo to consume the stream three times with all three APIs  ",
            "author": "Uwe Schindler",
            "id": "comment-12759594"
        },
        {
            "date": "2009-09-25T16:01:41+0000",
            "content": "\nWe could check the filters again by changing assertAnalyzesTo to consume the stream three times with all three APIs \n\nRight but maybe you could have implemented back compat differently, where it would appear to work with next() also.\nOr maybe at some point next() will go away?\nStill as you said, there's a bug because something else could modify these attributes.\nMaybe instead, in assertTokenStreamContents, after asserting the value is correct, it could do something like \"zero out\" the values?\n\nThis would probably detect bugs like this. ",
            "author": "Robert Muir",
            "id": "comment-12759599"
        },
        {
            "date": "2009-09-25T16:10:00+0000",
            "content": "This is also a good isea, even better. It should simply call clearAttributes() before each incrementToken(). A real consumer would not do this for speed reasons, but the test.\n\nRight but maybe you could have implemented back compat differently, where it would appear to work with next() also. Or maybe at some point next() will go away?\n\nAs you said before, somebody else could also modify the attributes, not only the backwards layer. For speed reasons: Preventing this would add an extra clone or somehow other copy of the attribute.\n\nnext() and next(Token) will go away the next weeks... ",
            "author": "Uwe Schindler",
            "id": "comment-12759602"
        },
        {
            "date": "2009-09-25T16:17:32+0000",
            "content": "I checked: All TokenStreams in core/contrib pass the tests with a separate clearAttributes() before each call to incrementToken(). ",
            "author": "Uwe Schindler",
            "id": "comment-12759605"
        },
        {
            "date": "2009-09-25T16:17:43+0000",
            "content": "It should simply call clearAttributes() before each incrementToken(). \n\nmy thoughts too, this causes my test to fail with incrementToken, exposing the bug.\n\nI will update your patch with this one-liner once i let ant test finish, just to make sure it doesnt break the build and there arent any similar bugs somewhere in contrib.\n\nedit: nevermind, your computer is much faster than mine. ",
            "author": "Robert Muir",
            "id": "comment-12759606"
        },
        {
            "date": "2009-09-25T16:25:09+0000",
            "content": "edit: nevermind, your computer is much faster than mine.\n\nNot really, I used \"ant test -Dtestpackage=analysis\"\n\nI will commit this addition to assertTokenStreamContents, soon (the javadoc fix is already committed) ",
            "author": "Uwe Schindler",
            "id": "comment-12759609"
        },
        {
            "date": "2009-09-25T16:28:13+0000",
            "content": "Uwe, I think its a great idea to prevent future problems.\n\nThe only thing i could add, maybe overkill, would be to actually zero out the term buffer in addition to clearAttributes() in the base test case.\nThis might seem absurd, but I could have cached .termLength(), clearAttributes() only sets the length to zero, and a few analyzer tests only test for term text...\nIn that case it might have still slipped by... ",
            "author": "Robert Muir",
            "id": "comment-12759610"
        },
        {
            "date": "2009-09-25T16:32:39+0000",
            "content": "I do not think this is needed. clearAttributes() should be enough. ",
            "author": "Uwe Schindler",
            "id": "comment-12759613"
        },
        {
            "date": "2009-09-25T17:42:56+0000",
            "content": "Committed improved test, rev 818920 ",
            "author": "Uwe Schindler",
            "id": "comment-12759647"
        }
    ]
}