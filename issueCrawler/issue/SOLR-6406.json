{
    "id": "SOLR-6406",
    "title": "ConcurrentUpdateSolrServer hang in blockUntilFinished.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.5"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Not sure what is causing this, but SOLR-6136 may have taken us a step back here. I see this problem occasionally pop up in ChaosMonkeyNothingIsSafeTest now - test fails because of a thread leak, thread leak is due to a ConcurrentUpdateSolrServer hang in blockUntilFinished. Only started popping up recently.",
    "attachments": {
        "CPU Sampling.png": "https://issues.apache.org/jira/secure/attachment/12751050/CPU%20Sampling.png",
        "SOLR-6406.patch": "https://issues.apache.org/jira/secure/attachment/12768553/SOLR-6406.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-14106906",
            "date": "2014-08-22T14:37:33+0000",
            "content": "\n   1) Thread[id=55, name=qtp823025155-55, state=WAITING, group=TGRP-ChaosMonkeyNothingIsSafeTest]\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:503)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.blockUntilFinished(ConcurrentUpdateSolrServer.java:374)\n        at org.apache.solr.update.StreamingSolrServers.blockUntilFinished(StreamingSolrServers.java:103)\n        at org.apache.solr.update.SolrCmdDistributor.blockAndDoRetries(SolrCmdDistributor.java:228)\n        at org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:89)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.doFinish(DistributedUpdateProcessor.java:766)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.finish(DistributedUpdateProcessor.java:1662)\n        at org.apache.solr.update.processor.LogUpdateProcessor.finish(LogUpdateProcessorFactory.java:179)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:83)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1967)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:777)\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14106909",
            "date": "2014-08-22T14:40:36+0000",
            "content": "This was on a nightly of ChaosMonkeyNothingIsSafeTest. It's fairly rare. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14297214",
            "date": "2015-01-29T17:44:52+0000",
            "content": "I still see this happen in tests. This hangs at  runners.wait(); and no notify or anything comes and it's just an ugly hang. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14297238",
            "date": "2015-01-29T17:54:41+0000",
            "content": "Would it make sense to change the code to wait(timeoutMs) and we can recheck the state of things and going back to waiting if it makes sense vs. the indefinite way you're seeing? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14297242",
            "date": "2015-01-29T17:56:54+0000",
            "content": "Mabye. I've been trying to spot how it can happen (without a runner also still going, which I don't see). So far, I cannot spot how it happens. "
        },
        {
            "author": "Stephan Lagraulet",
            "id": "comment-14701450",
            "date": "2015-08-18T15:43:01+0000",
            "content": "I have attached a cpu sample of a solr cloud server which has very poor update performance since a few hours.\nI guess it could be related to this problem. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14972813",
            "date": "2015-10-24T18:59:11+0000",
            "content": "A more recent set of stack traces:\n\n\n   [junit4] ERROR   0.00s | HdfsChaosMonkeyNothingIsSafeTest (suite) <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: ERROR: SolrIndexSearcher opens=39 closes=38\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([71608A03B4692CB]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.endTrackingSearchers(SolrTestCaseJ4.java:468)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.afterClass(SolrTestCaseJ4.java:234)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)Throwable #2: com.carrotsearch.randomizedtesting.ThreadLeakError: 2 threads leaked from SUITE scope at org.apache.solr.cloud.hdfs.HdfsChaosMonkeyNothingIsSafeTest: \n   [junit4]    >    1) Thread[id=243, name=qtp487431535-243, state=WAITING, group=TGRP-HdfsChaosMonkeyNothingIsSafeTest]\n   [junit4]    >         at java.lang.Object.wait(Native Method)\n   [junit4]    >         at java.lang.Object.wait(Object.java:502)\n   [junit4]    >         at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient.blockUntilFinished(ConcurrentUpdateSolrClient.java:404)\n   [junit4]    >         at org.apache.solr.update.StreamingSolrClients.blockUntilFinished(StreamingSolrClients.java:103)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.blockAndDoRetries(SolrCmdDistributor.java:231)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:89)\n   [junit4]    >         at org.apache.solr.update.processor.DistributedUpdateProcessor.doFinish(DistributedUpdateProcessor.java:778)\n   [junit4]    >         at org.apache.solr.update.processor.DistributedUpdateProcessor.finish(DistributedUpdateProcessor.java:1622)\n   [junit4]    >         at org.apache.solr.update.processor.LogUpdateProcessor.finish(LogUpdateProcessorFactory.java:183)\n   [junit4]    >         at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:83)\n   [junit4]    >         at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:151)\n   [junit4]    >         at org.apache.solr.core.SolrCore.execute(SolrCore.java:2079)\n   [junit4]    >         at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:667)\n   [junit4]    >         at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:460)\n   [junit4]    >         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:220)\n   [junit4]    >         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:179)\n   [junit4]    >         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n   [junit4]    >         at org.apache.solr.client.solrj.embedded.JettySolrRunner$DebugFilter.doFilter(JettySolrRunner.java:109)\n   [junit4]    >         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n   [junit4]    >         at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:83)\n   [junit4]    >         at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:300)\n   [junit4]    >         at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n   [junit4]    >         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n   [junit4]    >         at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)\n   [junit4]    >         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)\n   [junit4]    >         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n   [junit4]    >         at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n   [junit4]    >         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)\n   [junit4]    >         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n   [junit4]    >         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n   [junit4]    >         at org.eclipse.jetty.server.Server.handle(Server.java:499)\n   [junit4]    >         at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n   [junit4]    >         at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n   [junit4]    >         at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n   [junit4]    >         at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n   [junit4]    >         at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n   [junit4]    >         at java.lang.Thread.run(Thread.java:745)\n   [junit4]    >    2) Thread[id=266, name=searcherExecutor-30-thread-1, state=WAITING, group=TGRP-HdfsChaosMonkeyNothingIsSafeTest]\n   [junit4]    >         at sun.misc.Unsafe.park(Native Method)\n   [junit4]    >         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n   [junit4]    >         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n   [junit4]    >         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n   [junit4]    >         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n   [junit4]    >         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n   [junit4]    >         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n   [junit4]    >         at java.lang.Thread.run(Thread.java:745)\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([71608A03B4692CB]:0)\n\n\n\nIt would seem that runners.wait() can race with a final runners.notifyAll() or something. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14972821",
            "date": "2015-10-24T19:22:13+0000",
            "content": "OK, here's one theory after a quick look:\n\n\n      } finally {\n        synchronized (runners) {\n          if (runners.size() == 1 && !queue.isEmpty()) {\n            // keep this runner alive\n            scheduler.execute(this);\n          } else {\n            runners.remove(this);\n            if (runners.isEmpty())\n              runners.notifyAll();\n          }\n        }\n\n\n\nWhat if the queue isn't empty, so we try to do \"scheduler.execute\", but the scheduler has been shut down?  That will throw an exception and the else block containing notifyAll() will never be executed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14972840",
            "date": "2015-10-24T20:25:05+0000",
            "content": "OK, how about this patch?\n\nIt's probably hard to read as an actual patch file, so basically what I did was:\n\n\tpull out the \"while (!queue.isEmpty())\" loop into sendUpdateStream() method.\n\tadded a for(; loop around the call to sendUpdateStream() and the associated error handling.\n\tstayed in the outer loop if we're the last runner and something else was added to the queue.\n\tremoved runnerLock and the code to resubmit ourselves to the executor.\n\n\n\nThis shouldn't change any behavior except for the hypothetical buggy behavior, but it's completely untested at this point. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14972844",
            "date": "2015-10-24T20:45:38+0000",
            "content": "Yeah, it seems like we need to be sure we remove a  Runner when it is rejected by the scheduler. Patch attached - just a quick stab. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14972845",
            "date": "2015-10-24T20:46:25+0000",
            "content": "Ha - hadn't refreshed my browser.\n\nI'll review this approach. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14973243",
            "date": "2015-10-25T13:40:58+0000",
            "content": "I'm testing Yonik's approach today and will see if it resolves this. My quick patch does not FWIW. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14973270",
            "date": "2015-10-25T14:47:10+0000",
            "content": "Got a hang at the same spot - 2 threads stuck on it this time rather than the usual 1:\n\n\n   [junit4]    >    2) Thread[id=1071, name=qtp612828486-1071, state=WAITING, group=TGRP-HdfsChaosMonkeyNothingIsSafeTest]\n   [junit4]    >         at java.lang.Object.wait(Native Method)\n   [junit4]    >         at java.lang.Object.wait(Object.java:502)\n   [junit4]    >         at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient.blockUntilFinished(ConcurrentUpdateSolrClient.java:418)\n   [junit4]    >         at org.apache.solr.update.StreamingSolrClients.blockUntilFinished(StreamingSolrClients.java:106)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.blockAndDoRetries(SolrCmdDistributor.java:231)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:89)\n   [junit4]    >         at org.apache.solr.update.processor.DistributedUpdateProcessor.doFinish(DistributedUpdateProcessor.java:778)\n   [junit4]    >         at org.apache.solr.update.processor.DistributedUpdateProcessor.finish(DistributedUpdateProcessor.java:1622)\n   [junit4]    >         at org.apache.solr.update.processor.LogUpdateProcessor.finish(LogUpdateProcessorFactory.java:183)\n   [junit4]    >         at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:83)\n   [junit4]    >         at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:151)\n   [junit4]    >         at org.apache.solr.core.SolrCore.execute(SolrCore.java:2079)\n   [junit4]    >         at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:667)\n   [junit4]    >         at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:460)\n   [junit4]    >         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:220)\n   [junit4]    >         at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:179)\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14974518",
            "date": "2015-10-26T16:42:22+0000",
            "content": "Got a hang at the same spot - 2 threads stuck on it this time rather than the usual 1:\n\nHmmm, yeah, I only fixed one spot where runners are submitted. I'll try taking another crack at it...\nAlthough having 2 threads stuck at the same place in blockUntilFinished should be impossible... it's a synchronized method. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14980772",
            "date": "2015-10-29T16:50:49+0000",
            "content": "Just two threads stuck - not necessarily from the same client. Previously I had only ever seen 1 thread stuck. Just noting it, may not mean much. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14980812",
            "date": "2015-10-29T17:05:12+0000",
            "content": "OK, I was able to reproduce... Interestingly, this is pretty easy to hit (and I also saw 2 threads stuck at the same point... which as you say must be 2 different client objects).  There must be something more here than a subtle/little race condition. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14982724",
            "date": "2015-10-30T15:45:41+0000",
            "content": "Here's another patch (currently untested, but I'm going to start looping the chaos tests... they seemed to be good at hitting this).\n\nI added this comment to explain why I think we were hanging:\n        // NOTE: if the executor is shut down, runners may never become empty (a scheduled task may never be run,\n        // which means it would never remove itself from the runners list.  This is why we don't wait forever\n        // and periodically check if the scheduler is shutting down.\n\nSo instead of waiting forever now, we periodically exit the wait and check if the scheduler is still running.\n\nI changed scheduler.isTerminated() to scheduler.isShutdown()... the latter should be true when the executor is starting to try and shut down.  I think the former is only true when it actually succeeds.\n\nI also refactored the \"add another runner\" logic out to addRunner() "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14983761",
            "date": "2015-10-31T02:44:58+0000",
            "content": "Ha, that's actually close to the first hack fix I made. Occasionally waking up in the wait and checking if empty again.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14984535",
            "date": "2015-11-01T20:18:15+0000",
            "content": "OK, so I haven't hit any hangs with this latest patch and shutdownNow() on the associated executor.  Interestingly enough though, this still results in inconsistent shard failures.  My guess is that the shutdown of the executor is done as one of the last steps in CoreContainer.shutdown(), which still gives time for streaming update requests to continue streaming. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14985389",
            "date": "2015-11-02T15:29:00+0000",
            "content": "Commit 1712045 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1712045 ]\n\nSOLR-6406: fix race/hang in ConcurrentUpdateSolrClient.blockUntilFinished when executor service is shut down "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14985392",
            "date": "2015-11-02T15:31:01+0000",
            "content": "Commit 1712047 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1712047 ]\n\nSOLR-6406: fix race/hang in ConcurrentUpdateSolrClient.blockUntilFinished when executor service is shut down "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14990254",
            "date": "2015-11-04T19:41:22+0000",
            "content": "Reopening... something is wrong.\n\nOverview of what happened:\n\n\tI tested my update to DistributedUpdateProcessor in SOLR-8203 alone, and verified that there were no shard inconsistency failures\n\tMark tested his change to use shutdownNow on the updateExecutor alone (w/o my change), and reported no shard inconsistency failures, but he did hit hangs, which led to me to tackle this issue\n\tI tested this issue w/o my fix in SOLR-8203, to more easily reproduce the hang, and to verify it had been fixed - I was not looking for shard inconsistency failures\n\tNow that both patches are committed, I'm seeing shard inconsistency failures again!\n\n\n\nEither:\n\n\tI messed up this patch somehow, causing updates to be further reordered\n\tThis idea of this patch is somehow incompatible with SOLR-8203 (unlikely)\n\tSomething else in trunk has changed (unlikely)\n\n\n\nFirst, I'm going to go back to trunk w/o both of these patches and start with just the check in the DistributedUpdateProcessor, and move on from there until I find out what reintroduced the problem. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14991961",
            "date": "2015-11-05T16:50:53+0000",
            "content": "Update: I looped 3 tests overnight...\n1) trunk with shutdownNow on the update executor reverted\n2) trunk with shutdownNow and the client changes in this patch reverted\n3) plain trunk\n\nOnly #3 resulted in inconsistent shards.  I'm setting up some new variants to test now... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14995263",
            "date": "2015-11-07T16:03:20+0000",
            "content": "The other variants:\n 4) trunk with only client changes reverted (i.e. DUH check enabled, shutdownNow used)\n 5) trunk with client + DUH changes reverted (i.e. only shutdownNow enabled)\n 6) trunk with alternate client changes (only changes to blockUntilFinished)\n 7) trunk with alternate client changes 2 (only changes to blockUntilFinished, but using former isTerminated instead of isShutdown) \n\nI managed to get inconsistent shard runs with all of these.\nThe common element is shutdownNow being used on the shard update executor. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14997248",
            "date": "2015-11-09T19:55:31+0000",
            "content": "Strange. I got over 300 runs without an out of sync with it originally. I have not tried on recent trunk or recent changes though. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15001429",
            "date": "2015-11-12T00:56:24+0000",
            "content": "I was analyzing another \"shards-out-of-sync\" failure on trunk.\nIt looks like that certain update are just not being forwarded from the leader to a certain replica.\n\nWorking theory: the max connections per host of the HttpClient is being hit, starving updates from certain update threads.\nThis could account for why shutdownNow on the update executor service is having such an impact.  In an orderly shutdown, all scheduled jobs will still be run (I think), which means that connections will be released, and the updates that were being starved will get to proceed.  But it's for exactly this reason that we should probably keep the shutdownNow... it mimics much better what will happen in real world situations when a node goes down.\n\nFrom this, it looks like max connections per host is 20:\n\n\n13404 INFO  (TEST-HdfsChaosMonkeyNothingIsSafeTest.test-seed#[A22375CC545D2B82]) [    ] o.a.s.h.c.HttpShardHandlerFactory created with socketTimeout : 90000,urlScheme : ,connTimeout : 15000,maxConnectionsPerHost : 20,maxConnections : 10000,corePoolSize : 0,maximumPoolSize : 2147483647,maxThreadIdleTime : 5,sizeOfQueue : -1,fairnessPolicy : false,useRetries : false,\n\n\n\nedit: oops the above is for search not updates.  The default for updates looks like it's 100, so harder to hit.  Although if we have a mix of streaming and non-streaming, and connections are not reused immediately, perhaps still possible.  Still digging along this line of logic.\n\nThe test used 12 nodes (and 2 shards)... increasing the chance of hitting the max connections (since all nodes run on the same host). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15009016",
            "date": "2015-11-17T16:58:46+0000",
            "content": "So increasing maxConnectionsPerHost didn't fix the problem.\nI instrumented the ConcurrentUpdateSolrServer to try and understand what is happening when, and am analyzing some of those fails now.\nThey are all beyond the max size that can be uploaded to JIRA though, so I'lll just put up a summary based on what I find. "
        },
        {
            "author": "Stephan Lagraulet",
            "id": "comment-15261732",
            "date": "2016-04-28T08:17:00+0000",
            "content": "Hi Yonik Seeley did you make any progress on this issue? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15262332",
            "date": "2016-04-28T15:26:25+0000",
            "content": "Actually, since this issue is more about the hang in blockUntilFinished (and that's been fixed), this can be closed. "
        }
    ]
}