{
    "id": "SOLR-5611",
    "title": "When documents are uniformly distributed over shards, enable returning approximated results in distributed query",
    "details": {
        "affect_versions": "None",
        "status": "Reopened",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Query with rows=1000, which sent to a collection of 100 shards (shard key behaviour is default - based on hash of the unique key), will generate 100 requests of rows=1000, on each shard.\nThis results to total number of rows*numShards unique keys to be retrieved. This behaviour is getting worst as numShards grows.\n\nIf the documents are uniformly distributed over the shards, the expected number of document should be ~ rows/numShards. Obviously, there might be extreme cases, when all of the top X documents are in a specific shard.\n\nI suggest adding an optional parameter, say approxResults=true, which decides whether we should limit the rows in the shard requests to rows/numShardsor not. Moreover, we can add a numeric parameter which increases the limit, to be more accurate.\nFor example, the query approxResults=true&approxResults.factor=1.5 will retrieve 1.5*rows/numShards from each shard. In the case of 100 shards and rows=1000, each shard will return 15 documents.\n\nFurthermore, this can reduce the problem of deep paging, because the same thing can be applied there. when requested start=100000, Solr creating shard request with start=0 and rows=START+ROWS. In the approximated approach, start parameter (in the shard requests) can be set to 100000/numShards. The idea of the approxResults.factor creates some difficulties here, though.",
    "attachments": {
        "lec5-distributedIndexing.pdf": "https://issues.apache.org/jira/secure/attachment/12682164/lec5-distributedIndexing.pdf"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Isaac Hebsh",
            "id": "comment-13864519",
            "date": "2014-01-07T18:42:49+0000",
            "content": "Oops. I missed the shards.rows parameter. "
        },
        {
            "author": "Isaac Hebsh",
            "id": "comment-13869726",
            "date": "2014-01-13T17:37:56+0000",
            "content": "shards.rows is excellent, but we should add an optional param that would implicitly assign the shards.rows param, according to:\n\n\trows requested\n\tnum of shards in collection\n\tprobability of getting the highest topRows results.\n\n\n\nThere's a dynamic programming function that shows that you can significantly reduce the rows return per shard (shards.rows) for high values of shards and rows.\n\nExample I calculated:\nrows = 1000, shards = 36, probability = 0.99 => shards.rows=49 (instead of 1000!)\n\nwhere above probability is the probability of not missing any document that would have returned in the regular scenario (shards.rows = 1000) "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971160",
            "date": "2014-04-16T12:57:19+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Manuel Lenormand",
            "id": "comment-14216167",
            "date": "2014-11-18T13:31:39+0000",
            "content": "The equation is on the 20th slide. \n\nNeed to write an approximation for this or calculating offline for main values and making a 3d map out of it (#shards, rows, confidence level) that outputs shards.rows for each request "
        },
        {
            "author": "Justin Sarma",
            "id": "comment-16048223",
            "date": "2017-06-13T18:37:19+0000",
            "content": "I've created an open source tool which you can use to approximate appropriate shards.rows values through a Monte Carlo simulation. You can specify what percentage of the time you want the results to be 100% accurate, for different search depths, and different shard counts. We used this tool at Shutterstock to improve Solr latency and reduce heap pressure:\n\nhttps://github.com/jsarma/solr-shards-rows-optimizer\n\nThere's also a blog entry here which explains it in more detail, along with some perf tests of the results:\n\nhttps://tech.shutterstock.com/2017/05/09/efficiently-handling-deep-pagination-in-a-distributed-search-engine/ "
        }
    ]
}