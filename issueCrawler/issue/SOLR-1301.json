{
    "id": "SOLR-1301",
    "title": "Add a Solr contrib that allows for building Solr indexes via Hadoop's Map-Reduce.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.7",
            "6.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "This patch contains  a contrib module that provides distributed indexing (using Hadoop) to Solr EmbeddedSolrServer. The idea behind this module is twofold:\n\n\n\tprovide an API that is familiar to Hadoop developers, i.e. that of OutputFormat\n\tavoid unnecessary export and (de)serialization of data maintained on HDFS. SolrOutputFormat consumes data produced by reduce tasks directly, without storing it in intermediate files. Furthermore, by using an EmbeddedSolrServer, the indexing task is split into as many parts as there are reducers, and the data to be indexed is not sent over the network.\n\n\n\nDesign\n----------\n\nKey/value pairs produced by reduce tasks are passed to SolrOutputFormat, which in turn uses SolrRecordWriter to write this data. SolrRecordWriter instantiates an EmbeddedSolrServer, and it also instantiates an implementation of SolrDocumentConverter, which is responsible for turning Hadoop (key, value) into a SolrInputDocument. This data is then added to a batch, which is periodically submitted to EmbeddedSolrServer. When reduce task completes, and the OutputFormat is closed, SolrRecordWriter calls commit() and optimize() on the EmbeddedSolrServer.\n\nThe API provides facilities to specify an arbitrary existing solr.home directory, from which the conf/ and lib/ files will be taken.\n\nThis process results in the creation of as many partial Solr home directories as there were reduce tasks. The output shards are placed in the output directory on the default filesystem (e.g. HDFS). Such part-NNNNN directories can be used to run N shard servers. Additionally, users can specify the number of reduce tasks, in particular 1 reduce task, in which case the output will consist of a single shard.\n\nAn example application is provided that processes large CSV files and uses this API. It uses a custom CSV processing to avoid (de)serialization overhead.\n\nThis patch relies on hadoop-core-0.19.1.jar - I attached the jar to this issue, you should put it in contrib/hadoop/lib.\n\nNote: the development of this patch was sponsored by an anonymous contributor and approved for release under Apache License.",
    "attachments": {
        "SolrRecordWriter.java": "https://issues.apache.org/jira/secure/attachment/12419405/SolrRecordWriter.java",
        "hadoop.patch": "https://issues.apache.org/jira/secure/attachment/12414208/hadoop.patch",
        "SOLR-1301-hadoop-0-20.patch": "https://issues.apache.org/jira/secure/attachment/12443024/SOLR-1301-hadoop-0-20.patch",
        "commons-logging-1.0.4.jar": "https://issues.apache.org/jira/secure/attachment/12420428/commons-logging-1.0.4.jar",
        "commons-logging-api-1.0.4.jar": "https://issues.apache.org/jira/secure/attachment/12420429/commons-logging-api-1.0.4.jar",
        "README.txt": "https://issues.apache.org/jira/secure/attachment/12419406/README.txt",
        "hadoop-0.19.1-core.jar": "https://issues.apache.org/jira/secure/attachment/12414209/hadoop-0.19.1-core.jar",
        "SOLR-1301.patch": "https://issues.apache.org/jira/secure/attachment/12419196/SOLR-1301.patch",
        "log4j-1.2.15.jar": "https://issues.apache.org/jira/secure/attachment/12420471/log4j-1.2.15.jar",
        "hadoop-core-0.20.2-cdh3u3.jar": "https://issues.apache.org/jira/secure/attachment/12515663/hadoop-core-0.20.2-cdh3u3.jar",
        "SOLR-1301-maven-intellij.patch": "https://issues.apache.org/jira/secure/attachment/12615951/SOLR-1301-maven-intellij.patch",
        "hadoop-0.20.1-core.jar": "https://issues.apache.org/jira/secure/attachment/12443799/hadoop-0.20.1-core.jar"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Jason Rutherglen",
            "id": "comment-12734376",
            "date": "2009-07-22T23:12:41+0000",
            "content": "I downloaded the patch.  I'd like to be able to execute this as an ant target and integrate (if possible) test cases?   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12736944",
            "date": "2009-07-30T00:46:17+0000",
            "content": "I think we'll want to integrate this patch with Katta which\nconveniently creates many shards and merges them in Hadoop. \n\nThe merging in Hadoop is a bit tricky as if we create numerous\nshards using the current patch, merging the shards into the\nexisting index with MergeIndexesCommand would likely create too\nmuch IO and CPU overhead on what would be the search server and\npossibly degrade search performance. \n\nInstead of maintaining separate write servers, I would rather\nallocate all Solr servers as read only, and rely on Hadoop (with\nEC2) for quickly reindexing all documents (i.e. for a schema\nchange) or incremental indexing. I'm not sure how documents\nupdates should be handled. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12737120",
            "date": "2009-07-30T14:45:04+0000",
            "content": "Thought I haven't tested it, in browsing the Katta code it looks\nlike it isn't merging shards in Hadoop as I couldn't find a call\nto IW.addIndexes (which is used to merge indexes in different\ndirectories). I'm not sure how expensive it is to copy two\nshards out of HDFS, merge them, then copy the newly merged shard\nback to to HDFS, delete the old shards, then notify Zookeeper of\nthe changes.  Maybe we can expand on this patch to add those\ncapabilities, or add the functionality to Katta. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12737253",
            "date": "2009-07-30T19:46:51+0000",
            "content": "This patch is intended to work with Solr as it is now, and the idea is to use Hadoop to buld shards (in the Solr sense) so that they could be used by the current Solr distributed search. I have no idea how / whether Katta/Zookeeper fits in this picture - if you want to pursue this integration I feel it would be best to do it in a separate issue. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12737299",
            "date": "2009-07-30T21:25:27+0000",
            "content": "Andrzej,\n\n\n\tAre you going to add a way to automatically add an index to a Solr core?\n\n\n\n\n\tAre you planning on adding test cases for this patch?\n\n\n\n\n\tHow does one set the maximum size of a generated shard?\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12737306",
            "date": "2009-07-30T21:43:23+0000",
            "content": "Are you going to add a way to automatically add an index to a Solr core?\n\nThis way already exists by (ab)using the forced replication to a slave from a temporary master.\n\nAre you planning on adding test cases for this patch?\n\nThis functionality requires a running Hadoop cluster. I'm not sure how to write functional tests without bringing more Hadoop dependencies. I could add unit tests that test some aspects of the patch, but they would be trivial.\n\nHow does one set the maximum size of a generated shard?\n\nOne doesn't, at the moment  The size of each shard (in # of documents) is a function of the total number of records divided by the number of reduce tasks. "
        },
        {
            "author": "Ken Krugler",
            "id": "comment-12737568",
            "date": "2009-07-31T15:25:10+0000",
            "content": "Hi Jason,\n\nRe Katta, you're right that it doesn't support merging indexes. In a way, it does run-time merging by searching across multiple shards, though you can't add new shards to a deployed index. Some people work around this by searching in all indexes, which is a different level of run-time merging that Katta supports.\n\nIn general I think everybody agrees that merging indexes on the search servers is a bad idea, due to potentially high loads impacting end-user search performance. The most common approach I've seen is to use a general map-reduce job in Hadoop to generate N shards, controlled via number of reducers, and then deploy this as a branch new Katta index.\n\n\u2013 Ken "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12747492",
            "date": "2009-08-25T17:19:03+0000",
            "content": "Anyone using this patch set?\n\nWhat are people using for the reduce piece shard size, that will be merged, and a final shard for searching?\n\nI am going to give it a try, I have one change I am going to hack in, which is to allow an existing zip file to hold the solr data to avoid having to rebuild and push it out for each job run. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12747525",
            "date": "2009-08-25T18:05:29+0000",
            "content": "Jv,\n\nI've used the patch.  It works, though I'm sure there are changes and additions that can be made.  Why would you want to put the data (i.e. the index files) into the zip?\n\n-J "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12747542",
            "date": "2009-08-25T18:31:56+0000",
            "content": "currently you pass the directory of your solr conf/lib to the job, which makes a zip file, and loads it into hdfs. my mod would be to simply allow pointing at an existing config zip file in hdfs, to minimize the job start time. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12748935",
            "date": "2009-08-28T18:41:30+0000",
            "content": "I have used this at a decent scale, and will be adding a few patches, to allow mutliple tasks per machine to build.\n\nThe code currently uses the same directory in /tmp for the solr config, and if multipel tasks are running, the directory may be removed by earlier tasks that finish. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12752006",
            "date": "2009-09-07T03:55:38+0000",
            "content": "Updated patch with a little clearer documentation and safe to use when more than one instance of the output format is running on a single machine.\nAlso attempts to resolve the task timeouts during index updates and the index optimize phases of index building. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753317",
            "date": "2009-09-09T22:55:27+0000",
            "content": "I think we can parallelize the indexing in SolrRecordWriter?\nMeaning multiple threads add docs to Solr? Most machines will\nhave multiple cores and only one task is running per machine at\na time? "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12753332",
            "date": "2009-09-09T23:51:59+0000",
            "content": "In my case, I have 6 tasks per machine, but only 4 disks, so I should\nactually throttle.\n\nIt is an interesting trade of question of whether to make the writes run in\nthe background or to just block the job.\n\nIn operation on my cluster, the write time for each batch increases slowly\nfrom nothing to 20 minutes as the job runs, and the system buffer cache and\nthe disk arms get saturated.\n\nI also was lazy and didn't feel like holding exceptions from a background\nwriting thread and delivering them back to the writing thread, on the next\nwrite or close call.\nThere are some operational hadoop api features that make the assumption that\nan error is associated with the key, value that is being output. On the flip\nside the buffered batch is already hiding potential errors associated with a\nspecific record...\n\nThe case you are referring to would be the MultiThreadedMapper case which is\nnot used widely, the only change for that, which I should have done, is make\nthe write and close thread safe, which they are explicitly not right now.\n\n\n\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753717",
            "date": "2009-09-10T17:30:32+0000",
            "content": "Here's jv ning's patch as a regular patch file to more easily see the changes.  JV, can you add notes on about what's changed and why? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753731",
            "date": "2009-09-10T17:56:20+0000",
            "content": "Should we add ThreadedIndexWriter (from Lucene in Action\nsource http://www.manning.com/hatcher3/) type of functionality\nwhere we add documents in parallel using a thread pool? This\ncould increase performance on multicore machines. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12753739",
            "date": "2009-09-10T18:07:44+0000",
            "content": "I don't know anything about ThreadedIndexWriter, but the SolrJ StreamingUpdateSolrServer uses multiple threads on the client side (and thus causes multiple threads to be used on the server side) to increase concurrency.  It's quite speedy. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12753754",
            "date": "2009-09-10T18:30:12+0000",
            "content": "Within a Map/Reduce task, there is usually a significant constraint on available ram, cpu and disk bandwidth.\n\nFor my current use case, if we turn on the various background write threads, I will need to make these configurable to help me manage the task resource consumption.\n\nIn the ideal world, the Map/Reduce framework, via cluster and job configuration, is taking care of running the tasks in parallel, and the tuning to optimize throughput is happening at that level. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12753756",
            "date": "2009-09-10T18:32:41+0000",
            "content": "My notes on the patchupdate were in a README.txt that didn't make it into the .patch for some reason.\nBasically, the per task configuration uses the unpacked data that the framework creates, rather than creating a new configuration directory.\nAll paths used are task specific, so that multiple tasks can run concurrently without colliding.\nThe write method has a heartbeat thread so that the task does not get killed if the batch write takes more than 600 seconds. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753763",
            "date": "2009-09-10T18:46:21+0000",
            "content": "\n\tI implemented a thread pool version, which eliminates the need\nfor adding in batches (which I'm not sure was necessary?)\n\n\n\n\n\tA numthreads property may be set\n\n\n\n\n\tA maxqueuesize may be set, which acts fulfills the same\nfunction as batching\n\n\n\n\n\tIt's untested yet\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753765",
            "date": "2009-09-10T18:49:55+0000",
            "content": "Yonik,\n\nIt looks like StreamingUpdateSolrServer can't be used with EmbeddedSolrServer because of the requirement for a URL. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12753769",
            "date": "2009-09-10T18:57:51+0000",
            "content": " In the ideal world, the Map/Reduce framework, via\ncluster and job configuration, is taking care of running the\ntasks in parallel, and the tuning to optimize throughput is\nhappening at that level. \n\nTrue, Hadoop should probably manage calling\nSolrRecordWriter.write from multiple threads, or maybe it\nalready is? In which case there wouldn't be a need for thread\npooling or batching/queuing.  "
        },
        {
            "author": "Kris Jirapinyo",
            "id": "comment-12754268",
            "date": "2009-09-11T18:23:50+0000",
            "content": "Because we are using MultipleOutputFormat, we can't have the data directory be the task_id, as when the indexes were building, the directories were conflicting.  This patch uses a random UUID instead as the data directory so that if there are more than one shards being created under a reducer, the directories will not conflict. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12754276",
            "date": "2009-09-11T18:48:01+0000",
            "content": "I have an updated version that uses a sequence number, to ensure uniqueness\nin the multiple output format case.\n\nSame concept, shorter path.\n\n\n\n "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12754647",
            "date": "2009-09-13T01:43:36+0000",
            "content": "Updated SolrRecordWriter that uses a static AtomicLong to create a unique sequence number for each index instance created.\nThis allows safe use of MultipleOutputFormat as well as the original patch which allowed multiple task instances per machine.\n\nThe only potential issue is that the writer will block during index updates, which may cause the task to run slower than it could. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12754648",
            "date": "2009-09-13T01:44:41+0000",
            "content": "Readme for the patch. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12758989",
            "date": "2009-09-24T03:25:39+0000",
            "content": "Here's a new patch, with log jar dependencies.  \n\nThe heartbeat is improved, and there's a queuing mechanism that can result in faster execution times. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12759189",
            "date": "2009-09-24T17:44:51+0000",
            "content": "An additional required library for the latest patch. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12760309",
            "date": "2009-09-28T17:49:52+0000",
            "content": "We need to include the schema.xml in the shard stored in HDFS as otherwise we could get confused about which schema the index was built with.  \n\nWe don't need include solrconfig in HDFS because it's used to define parameters related to how the index is built. "
        },
        {
            "author": "Jason Venner (www.prohadoop.com)",
            "id": "comment-12764717",
            "date": "2009-10-12T15:29:34+0000",
            "content": "I need to update this patch, there is an error in the close method that\ncause timeouts to occur.\n\nBasically, the two lines in the close method need to be changed form, there\nis still the possibility of close happening too early in the rare use cases,\nwhich is why I haven't updated the patch.\nBasically close can't proceed until the thread pool is done AND\nbatchWriter.executingBatches.get() == 0\n\nFrom:\n          batchWriter.close(reporter, core);\n          heartBeater.needHeartBeat();\n\n\nTo:\n          heartBeater.needHeartBeat();\n          batchWriter.close(reporter, core);\n\n\n\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12764745",
            "date": "2009-10-12T16:56:47+0000",
            "content": "Thanks for the update Jason.  It runs great, I've generated over a terabyte of indexes using the patch.  Now I'm trying to deploy them, and that's harder! "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12767608",
            "date": "2009-10-19T23:19:02+0000",
            "content": "Here's an update that includes the change Jason mentioned above\n(needHeartBeat in SRW.close). I've run this patch in production,\nhowever I was unable to turn off logging due to complexities\nwith SLF4J layering Hadoop where I could not turn off the Solr\nupdate logs. I had to comment out the logging lines in Solr to\ninsure the Hadoop logs did not fill up.  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12800720",
            "date": "2010-01-15T15:15:20+0000",
            "content": "Seems like this would make the most sense as a contrib module to Solr. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12800723",
            "date": "2010-01-15T15:25:20+0000",
            "content": "Furthermore, by using an EmbeddedSolrServer, the indexing task is split into as many parts as there are reducers, and the data to be indexed is not sent over the network.\n\nI'm curious about the not sending over the network.  Have you tried the Streaming Server or even just the regular one?  How would this work with someone who already has a separate Solr cluster setup?\n\nAlso, I haven't looked closely at the patch, but if I understand correctly, it is writing out the indexes to the local disks on the Hadoop cluster?   "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12800746",
            "date": "2010-01-15T16:33:35+0000",
            "content": "I'm curious about the not sending over the network. Have you tried the Streaming Server or even just the regular one?\n\nHmm, I don't think this would make sense - the whole point of this patch is to distribute the load by indexing into multiple Solr instances that use the same config - and this can be an existing user's config including the components from ${solr.home}/lib .\n\nHow would this work with someone who already has a separate Solr cluster setup?\n\nIt wouldn't - partly because there is no canonical Solr cluster setup against which to code this ... Would that be the same cluster (1:1 mapping) as the Hadoop cluster?\n\nAlso, I haven't looked closely at the patch, but if I understand correctly, it is writing out the indexes to the local disks on the Hadoop cluster?\n\nHDFS doesn't support enough POSIX to support writing Lucene indexes directly to HDFS - for this reason indexes are always created on local storage of each node, and then after closing they are copied to HDFS. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12800748",
            "date": "2010-01-15T16:40:18+0000",
            "content": "Hmm, I don't think this would make sense - the whole point of this patch is to distribute the load by indexing into multiple Solr instances that use the same config - and this can be an existing user's config including the components from ${solr.home}/lib .\n\nObviously, you would need to have a configuration of Solr indexing servers.  This could easily be obtained from ZK per the other work being done.  Then, the reduce steps just create their SolrServer based on those values and can index directly.  (I realize the ZK stuff didn't exist when you put up this patch.)\n\nHDFS doesn't support enough POSIX to support writing Lucene indexes directly to HDFS - for this reason indexes are always created on local storage of each node, and then after closing they are copied to HDFS.\n\nRight, and then copied down from HDFS and installed in Solr, correct?  You still have the issue of knowing which Solr instances get which shards off of HDFS, right?  Just seems like a little more configuration knowledge could alleviate all that extra copying/installing, etc. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12800756",
            "date": "2010-01-15T17:01:12+0000",
            "content": "Andrzej's model works great in production. We have both 1)\nmaster -> slave for incremental updates, and 2) index in Hadoop\nwith this patch, we then deploy each new core/shard in a\nbalanced fashion to many servers. They're two separate\nmodalities. The ZK stuff (as it's modeled today) isn't useful\nhere, because I want the schema I indexed with as a part of the\nzip file stored in HDFS (or S3, or wherever). \n\nAny sort of ZK thingy is good for managing the core/shards\nacross many servers, however Katta does this already (so we're\neither reinventing the same thing, not necessarily a bad thing\nif we also have a clear path for incremental indexing, as\ndiscussed above). Ultimately, the Solr server can be viewed as\nsimply a container for cores, and the cloud + ZK branch as a\nmanager of cores/shards. Anything more ambitious will probably\nbe overkill, and this is what I believe Ted has been trying to get at. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12800758",
            "date": "2010-01-15T17:07:24+0000",
            "content": "Iff we somehow could get a mapping between a mapred task on node X  to a particular target Solr server (beyond the two obvious choices, ie. single URL for one Solr, or localhost for per-node Solr-s) then sure why not. And you are right that we wouldn't use the embedded Solr in that case. But this patch solves a different problem, and it solves it within the facilities of the current config \n\nRight, and then copied down from HDFS and installed in Solr, correct? You still have the issue of knowing which Solr instances get which shards off of HDFS, right? Just seems like a little more configuration knowledge could alleviate all that extra copying/installing, etc.\n\nYes. But that would be a completely different scenario - we could wrap it in a Hadoop OutputFormat as well, but the implementation would be totally different from this patch. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12800760",
            "date": "2010-01-15T17:08:48+0000",
            "content": "Don't confuse the ZK stuff for search w/ the indexing side.  Using ZK was just an example of a way to get the list of Solr indexing nodes.  What I meant was the Hadoop job could simply know what the set of master indexers are and send the documents directly to them.  Then the slaves simply pull the replications from there.  It all works with existing capabilities instead of the need for scritps, etc. to pull shards down, etc. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12800775",
            "date": "2010-01-15T17:36:48+0000",
            "content": "What I meant was the Hadoop job could simply know what\nthe set of master indexers are and send the documents directly\nto them\n\nOne can use Hadoop for this purpose, we have implemented the\nsystem in this way for the incremental indexes, however it\ndoesn't require a separate patch or contrib module. The problem\nwith the Hadoop streaming model is it doesn't scale well, if for\nexample, we need to reindex using the CJKAnalyzer, or using\nBasis' analyzer etc. We use SOLR-1301 for reindexing loads of\ndata, as fast as possible by parallelizing the indexing. There\nare lots of little things I'd like to add to the functionality,\nthough, implementing ZK based core management takes a higher\npriority, as I spend a lot of time doing this manually today. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12800785",
            "date": "2010-01-15T17:43:43+0000",
            "content": "I don't follow how sending docs to a suite of master indexers prevents incremental (re)indexing or any of the analyzers.  Those are all on the Solr side, not Hadoop.  BTW, I'm not talking about \"Hadoop Streaming\", just the notion of Hadoop streaming the output of the reduce tasks to the Solr indexing servers. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12800802",
            "date": "2010-01-15T18:11:19+0000",
            "content": "Hadoop streaming the output of the reduce tasks to the Solr\nindexing servers. \n\nYes, this is what we've implemented, it's just normal Solr HTTP\nbased indexing, right? It works well to a limited degree, and\nfor the particular implementation details, there are reasons why\nthis can be less than ideal. The balanced, distributed\nshards/cores system works far better and enables us to use less\nhardware (but I'm not going into all the details here). \n\nOne issue I can mention, is the switch over to a new set of\nincremental servers (which happens then the old servers fill\nup), I'm looking to automate this, and will likely focus on it\nand the core management in the cloud branch.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12802526",
            "date": "2010-01-19T23:08:49+0000",
            "content": "I started on the Solr wiki page for this guy...\n\nhttp://wiki.apache.org/solr/HadoopIndexing\n "
        },
        {
            "author": "Kevin Peterson",
            "id": "comment-12806538",
            "date": "2010-01-29T22:26:16+0000",
            "content": "As written, this constructs a path relative to the local directories, which is not automatically cleaned up by Hadoop, line 942 in the patch, the constructor for SolrRecordWriter. If this is changed to something like\n\ntemp = new Path(job.getWorkingDirectory(), \"solr/_\" + job.get(\"mapred.task.id\") + '.' + sequence.incrementAndGet() + '.' + perm.getName());\n\nthis will be in the tasks working directory and automatically cleaned up on exit. I've checked with #hadoop and the consensus seems to be that this will be cleaned up unless the TT dies. "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12806547",
            "date": "2010-01-29T22:59:05+0000",
            "content": "\nIt is critical to put indexes in the task local area on both local and hdfs storage areas not just because of task cleanup, but also because a task may be run more than once.  Hadoop handles all the race conditions that would otherwise happen as a result. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12827997",
            "date": "2010-02-01T03:41:21+0000",
            "content": "This update include's Kevin's recommended path change.... "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12828172",
            "date": "2010-02-01T16:47:23+0000",
            "content": "There's a bug caused by the latest change:\n\njava.io.IOException: java.lang.IllegalArgumentException: Wrong FS: hdfs://mi-prod-app01.ec2.biz360.com:9000/user/hadoop/solr/_attempt_201001212110_2841_r_000001_0.1.index-a, expected: file:///\nat org.apache.solr.hadoop.SolrRecordWriter.close(SolrRecordWriter.java:371)\nat com.biz360.mi.index.hadoop.HadoopIndexer$ArticleReducer.reduce(HadoopIndexer.java:147)\nat com.biz360.mi.index.hadoop.HadoopIndexer$ArticleReducer.reduce(HadoopIndexer.java:103)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\nat org.apache.hadoop.mapred.Child.main(Child.java:170)\nCaused by: java.lang.IllegalArgumentException: Wrong FS: hdfs://mi-prod-app01.ec2.biz360.com:9000/user/hadoop/solr/_attempt_201001212110_2841_r_000001_0.1.index-a, expected: file:///\nat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:305)\nat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:47)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:357)\nat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)\nat org.apache.solr.hadoop.SolrRecordWriter.zipDirectory(SolrRecordWriter.java:459)\nat org.apache.solr.hadoop.SolrRecordWriter.packZipFile(SolrRecordWriter.java:390)\nat org.apache.solr.hadoop.SolrRecordWriter.close(SolrRecordWriter.java:362)\n... 5 more  "
        },
        {
            "author": "Karthik K",
            "id": "comment-12828232",
            "date": "2010-02-01T19:25:29+0000",
            "content": "Did the latest patch involve an upgrade of the hdfs / patched hdfs running ? If there were a change - was the fs migration script run to upgrade the file system being referenced ? \n\nAlso - what version of hdfs is being used ?Would that be 0.19.1 . \n\nIn the roadmap is to release 0.20.2 of hadoop very soon. Of particular interest in that would be HDFS-127  , to recover (gracefully) from failed reads. "
        },
        {
            "author": "Kevin Peterson",
            "id": "comment-12828356",
            "date": "2010-02-02T00:11:16+0000",
            "content": "I pointed you in the wrong direction. It isn't getWorkingDirectory. I'm trying to find the standard way to get to ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid or construct a Path using the current working directory, but I'm having trouble making sense of which directories refer to local and which to HDFS. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12828368",
            "date": "2010-02-02T00:26:51+0000",
            "content": "I'm testing deleting the temp dir in SRW.close finally... "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12828667",
            "date": "2010-02-02T16:57:43+0000",
            "content": "I added the following to the SRW.close method's finally clause:\n\n\nFileUtils.forceDelete(new File(temp.toString()));\n\n "
        },
        {
            "author": "shyjuThomas",
            "id": "comment-12828915",
            "date": "2010-02-03T02:57:18+0000",
            "content": "I have a need to perform Solr indexing in MapReduce task, to achive parallelism. I have noticed 2 Jira issues related to that: SOLR-1045 & SOLR-1301. \n\nI have tried out the patches available with both the issues, and my observation is given below:\n1. The SOLR-1301 patch, performs  input-record to key-value conversion in Map phase; Hadoop (key, value) to SolrInputDocument conversion and the actual indexing will happen in the Reduce phase.\nMeanwhile, SOLR-1045 patch performs the record-to-Doc conversion and the actual indexing in the Map phase; User can make use of the Reducer to perform merging of multiple indices (if required). In another way we can configure the number of reducers as same as the number of Shards. \n2. The SOLR-1301 patch doesn't supports merging of the indices, while SOLR-1045 patch supports.\n3. As per SOLR-1301 patch, no big activity happens in the Map phase (only input-record to key-value conversion). Most of the heavy jobs (esp. the indexing) are happening in the Reduce phase. If we need the final output as a single index, we can use only one reducer, which means bottleneck at Reducer & almost the whole operation happens non-paralelly. \n                       But the case is different with SOLR-1045 patch. It achieves better parallelism when the number of map tasks is greater than the number of reduce tasks, which is usually the case.\n\nBased on these observation, I have few questions. (I am a beginner to the Hadoop & Solr world. So, please forgive me if my questions are silly):\n1. As per above observation, SOLR-1045 patch is functionally better (performance I have not verified yet ). Can anyone tell me, whats the actual advantage SOLR-1301 patch offers over SOLR-1045 patch?\n2. If both the jira issues are trying to solve the same problem, do we really need 2 separate issues?\n\nNOTE : I felt this Jira issue is more active than SOLR-1045. Thats why I posted my comment here. "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12828961",
            "date": "2010-02-03T05:45:52+0000",
            "content": "\nBased on these observation, I have few questions. (I am a beginner to the Hadoop & Solr world. So, please forgive me if my questions are silly):\n1. As per above observation, SOLR-1045 patch is functionally better (performance I have not verified yet ). Can anyone tell me, whats the actual advantage SOLR-1301 patch offers over SOLR-1045 patch?\n2. If both the jira issues are trying to solve the same problem, do we really need 2 separate issues?\n\nIn the katta community, the recommended practice started with SOLR-1045 (what I call map-side indexing) behavior, but I think that the consensus now is that SOLR-1301 behavior (what I call reduce side indexing) is much, much better.  This is not necessarily the obvious result given your observations.  There are some operational differences between katta and SOLR that might make the conclusions different, but what I have observed is the following:\n\na) index merging is a really bad idea that seems very attractive to begin with because it is actually pretty expensive and doesn't solve the real problems of bad document distribution across shards.  It is much better to simply have lots of shards per machine (aka micro-sharding) and use one reducer per shard.  For large indexes, this gives entirely acceptable performance.  On a pretty small cluster, we can index 50-100 million large documents in multiple ways in 2-3 hours.  Index merging gives you no benefit compared to reduce side indexing and just increases code complexity.\n\nb) map-side indexing leaves you with indexes that are heavily skewed by being composed of of documents from a single input split.  At retrieval time, this means that different shards have very different term frequency profiles and very different numbers of relevant documents.  This makes lots of statistics very difficult including term frequency computation, term weighting and determining the number of documents to retrieve.  Map-side merge virtually guarantees that you have to do two cluster queries, one to gather term frequency statistics and another to do the actual query.  With reduce side indexing, you can provide strong probabilistic bounds on how different the statistics in each shard can be so you can use local term statistics and you can depend on the score distribution being this same which radically decreases the number of documents you need to retrieve from each shard.\n\nc) reduce-side indexing improves the balance of computation during retrieval.  If (as is the rule) some document subset is hotter than other document subset due, say to data-source boosting or recency boosting, you will have very bad cluster utilization with skewed shards from map-side indexing while all shards will cost about the same for any query leading to good cluster utilization and faster queries with reduce-side indexing.\n\nd) with reduce-side indexing has properties that can be mathematically stated and proved.  Map-side indexing only has comparable properties if you make unrealistic assumptions about your original data.\n\ne) micro-sharding allows very simple and very effective use of multiple cores on multiple machines in a search cluster.  This can be very difficult to do with large shards or a single index.\n\nNow, as you say, these advantages may evaporate if you are looking to produce a single output index.  That seems, however, to contradict the whole point of scaling.   If you need to scale indexing, presumably you also need to scale search speed and throughput.  As such you probably want to have many shards rather than few.  Conversely, if you can stand to search a single index, then you probably can stand to index on a single machine. \n\nAnother thing to think about is the fact SOLR doesn't yet do micro-sharding or clustering very well and, in particular, doesn't handle multiple shards per core.  That will be changing before long, however, and it is very dangerous to design for the past rather than the future.\n\nIn case, you didn't notice, I strongly suggest you stick with reduce-side indexing. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12829200",
            "date": "2010-02-03T19:10:06+0000",
            "content": "In production the latest patch does not leave temporary files behind... Though before we had failed tasks, so perhaps there's still a bug, we won't know until we run out of disk space again. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12833108",
            "date": "2010-02-12T18:38:02+0000",
            "content": "There still seems to be a bug where the temporary directory index isn't deleted on job completion. "
        },
        {
            "author": "Matt Revelle",
            "id": "comment-12859838",
            "date": "2010-04-22T15:14:47+0000",
            "content": "Hi, Jason, I noticed a few problems with the latest patch and am working on the fixes. On SolrRecordWriter:L373, FileUtils.forceDelete is called but prior to this the path dir to delete may have been moved. The move happens when the output isn't a zip file, at line 364 with a call to FileSystem#completeLocalOutput. \n\nLess important, the ls process, which runs when logging is set to the debug level (SolrRecordWriter:L278, appears to not always exit properly and throws an exception. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12859853",
            "date": "2010-04-22T15:49:20+0000",
            "content": "Matt, interesting.  I'm most concerned about the left over files, which is still an issue.  In production I use a script that deletes the left overs, which isn't ideal but works.  I'm not sure if the SolrRecordWriter:L373 bug is related to that, I'm always zipping the indexes into HDFS.   "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12861621",
            "date": "2010-04-27T23:53:51+0000",
            "content": "This is the version of the patch rewritten to use the new mapreduce API in hadoop 0.20. I did a quick port of the patch from 2010-02-02 11:57 AM without any optimizations, just to make it work with a new syntax. There are some slight changes around local filesystem temp file name generation etc. The CSVReducer class added just to pass a proper context to use counters in BatchWriter, if you know the better way to do this, please let me know. Tested with hadoop 0.20.2 on csv data, with both compressed and non-compressed output; seems to be OK, but no extensive regression testing performed. Code review and suggestions/corrections are welcome.  "
        },
        {
            "author": "Matt Revelle",
            "id": "comment-12861641",
            "date": "2010-04-28T01:23:21+0000",
            "content": "Updated the latest patch to include a check for the temp file before calling FileUtils.forceDelete. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12864593",
            "date": "2010-05-06T00:06:07+0000",
            "content": "Matt, Can you post a patch including the contrib directory structure (and build.xml)?   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12864604",
            "date": "2010-05-06T00:39:16+0000",
            "content": "Matt, nevermind, I'm just using the patch as is (ie, as a part of Solr core). "
        },
        {
            "author": "Matt Revelle",
            "id": "comment-12864606",
            "date": "2010-05-06T00:50:48+0000",
            "content": "Jason, Ok.  =) "
        },
        {
            "author": "Viktors Rotanovs",
            "id": "comment-12869962",
            "date": "2010-05-21T12:22:44+0000",
            "content": "It looks like when converter returns only 1 document, which is the most common case, number of batches will be equal to number of documents. In earlier version of this patch documents were accumulated and then sent as a batch, and this is what a comment in SolrRecordWriter still says. "
        },
        {
            "author": "Matt Revelle",
            "id": "comment-12869977",
            "date": "2010-05-21T13:15:04+0000",
            "content": "Viktors: That must have been a regression from Alexander's patch to support the newer Hadoop API.  I may have a chance to investigate today.  If anyone else takes it on, please leave a comment. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872607",
            "date": "2010-05-27T22:08:43+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12872882",
            "date": "2010-05-28T07:03:15+0000",
            "content": "I see comments and patches adding support for newer versions of Hadoop.  But has anyone used these patches with Elastic Map Reduce (EMR) on EC2? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12873577",
            "date": "2010-05-31T08:16:21+0000",
            "content": "We are using this patch (Andrzej version + custom code) for our several projects. It works great but sometimes we get OOM when indexing new input data and the data include unexpected large records. I think it is worthy if SolrRecordWriter could have bufferSizeMB (like IndexWriter) to flush buffered docs rather than batchSize basis. Thought? "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12875819",
            "date": "2010-06-05T01:08:47+0000",
            "content": "Matt, I think Viktors mentioned the original batching logic eliminated by Jason (see his comment from 10/Sep/09 02:46 PM) "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12875949",
            "date": "2010-06-05T21:48:58+0000",
            "content": "Matt, I think Viktors mentioned the original batching logic eliminated by Jason (see his comment from 10/Sep/09 02:46 PM) \n\nRight, I don't think we need batching because no efficiency will be gained (ie, there's no network overhead being eliminated). "
        },
        {
            "author": "Mathias Walter",
            "id": "comment-12896514",
            "date": "2010-08-09T12:09:14+0000",
            "content": "I tried this patch with Hadoop 0.20.2. It works pretty well, except if speculative execution is enabled (at least for the reducer). If that is the case, some jobs are running twice. The first job is creating the zip file. The second tries this too and failed. Unfortunately, the first job also fails. I've added a fs.exists(perm) to the SolrRecordWriter.packZipFile method, but the first job still fails with the following exception right after the last write and at nearly the same time the other job tests the existence of the zip file:\n\n2010-08-06 15:35:33,883 INFO com.excerbt.mapreduce.solrindexing.SolrRecordWriter: RawPath /hadoop/hdfs5/tmp/solr_attempt_201007231114_0068_r_000001_0.1/data/index/_4.frq, baseName part-00001, root /hadoop/hdfs5/tmp/solr_attempt_201007231114_0068_r_000001_0.1, inZip 2 part-00001/data/index/_4.frq\n2010-08-06 15:35:36,164 ERROR com.excerbt.mapreduce.solrindexing.SolrRecordWriter: packZipFile exception {}\njava.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:230)\n\tat org.apache.hadoop.hdfs.DFSClient.access$600(DFSClient.java:65)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:3058)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:100)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:161)\n\tat java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:118)\n\tat java.util.zip.ZipOutputStream.write(ZipOutputStream.java:272)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:51)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:87)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.zipDirectory(SolrRecordWriter.java:493)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.zipDirectory(SolrRecordWriter.java:469)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.zipDirectory(SolrRecordWriter.java:469)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.zipDirectory(SolrRecordWriter.java:469)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.packZipFile(SolrRecordWriter.java:385)\n\tat com.excerbt.mapreduce.solrindexing.SolrRecordWriter.close(SolrRecordWriter.java:349)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n\nThat's really strange. I checked this for many jobs. Also, the incomplete zip file is not removed after this exception. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12897507",
            "date": "2010-08-11T23:09:28+0000",
            "content": "Mathias, I did not test the patch for 0.20 with speculative execution for the reducers; but it is probably failing because the other task attempt deletes the perm file  (see SolrRecordWriter constructor, there is a cleanup fs.delete(perm, true) call after constructing the perm Path). \n\nIf you really need the speculative execution for the reducers, you could try to use the Reducer context to construct the perm file using the getWorkOutputPath instead of getOutputPath() (in this case, if the particular attempt was successful then its perm file should be promoted to the task work dir automatically - see that side effect explanation here: http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.html#getWorkOutputPath%28org.apache.hadoop.mapreduce.TaskInputOutputContext%29), \n\ni.e. instead of \n\nperm = new Path(FileOutputFormat.getOutputPath(context), getOutFileName(context, \"part\"));\n\ntry to use something like this:\n\nReducer.Context rContext = contextMap.get(context.getTaskAttemptID().getTaskID());\nperm = new Path(FileOutputFormat.getWorkOutputPath(rContext), getOutFileName(context, \"part\")); "
        },
        {
            "author": "Daniel Ivan Pizarro",
            "id": "comment-12902645",
            "date": "2010-08-25T21:40:13+0000",
            "content": "I'm getting the following error:\n\njava.lang.IllegalStateException: Failed to initialize record writer for , attempt_local_0001_r_000000_0\n\n\nWhere can I find instructions to run the CVSUploader?\n\n(readme file says \"Please read the original patch readme for details on the CSV bulk uploader.\", and I can't find that readme file) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12907267",
            "date": "2010-09-08T15:44:11+0000",
            "content": "I think this should be a contrib module.  Alexander, would you be willing to update it to trunk and make it a Solr contrib? "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12907347",
            "date": "2010-09-08T18:21:20+0000",
            "content": "Grant, sure. Will do this in a next couple of days.  "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12912393",
            "date": "2010-09-20T08:40:26+0000",
            "content": "The latest 0.20 patch is repackaged to be placed under the contrib, as it was initially (build.xml is included). and tested against the current trunk. As usual, after applying the patch put the 4 lib jars (hadoop, log4j, and two commons-logging) to the contrib/hadoop/lib. No unit tests as for now  but I hope to add some soon. Here is the big question: as Andrzej once mentioned, the unit tests require a running Hadoop cluster. One approach is to make the patch and unit tests working with the Hadoop mini--cluster (ClusterMapReduceTestCase), however this will bring some extra dependencies needed to run the cluster (like jetty). Another idea is to use \"your own\" cluster and just configure access to this cluster in untt tests; this approach seems to be logical but potentially may give different test results on different clusters, and also may not give some low-level access to the execution, needed for tests. So what is your opinion on how the tests for solr-hadoop should be run? I am not really happy with the idea of starting and running the Hadoop cluster while performing the Solr unit tests, but this still could be the better option than no unit tests at all.   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12912497",
            "date": "2010-09-20T15:25:33+0000",
            "content": "Alexander,\n\nI think we'll need to use Hadoop's Mini Cluster in order to have a proper unit test.  Adding Jetty as a dependency shouldn't be too much of a problem as Solr already includes a small version of Jetty?  That being said, it doesn't mean it's fun to write the unit test.  I can assist if needed. "
        },
        {
            "author": "Dhruv Bansal",
            "id": "comment-12920402",
            "date": "2010-10-12T23:57:56+0000",
            "content": "I am unable to compile SOLR 1.4.1 after patching with the latest (2010-09-20 04:40 AM) SOLR-1301.patch.\n\n\n$ wget http://mirror.cloudera.com/apache//lucene/solr/1.4.1/apache-solr-1.4.1.tgz\n...\n$ tar -xzf apache-solr-1.4.1.tgz\n$ cd apache-solr-1.4.1/contrib\napache-solr-1.4.1/contrib$ wget https://issues.apache.org/jira/secure/attachment/12455023/SOLR-1301.patch\napache-solr-1.4.1/contrib$ patch -p2 -i SOLR-1301.patch\n...\napache-solr-1.4.1/contrib$ mkdir lib\napache-solr-1.4.1/contrib$ cd lib\napache-solr-1.4.1/contrib/lib$ wget .. # download hadoop, log4j, commons-logging, commons-logging-api jars from top of this page\n...\napache-solr-1.4.1/contrib/lib$ cd ../..\napache-solr-1.4.1$ ant dist -k\n\n...\n\ncompile:\n    [javac] Compiling 9 source files to /home/dhruv/projects/infochimps/search/apache-solr-1.4.1/contrib/hadoop/build/classes\nTarget 'compile' failed with message 'The following error occurred while executing this line:\n/home/dhruv/projects/infochimps/search/apache-solr-1.4.1/common-build.xml:159: Reference lucene.classpath not found.'.\nCannot execute 'build' - 'compile' failed or was not executed.\nCannot execute 'dist' - 'build' failed or was not executed.\n   [subant] File '/home/dhruv/projects/infochimps/search/apache-solr-1.4.1/contrib/hadoop/build.xml' failed with message 'The following error occurred whil\\\ne executing this line:\n   [subant] /home/dhruv/projects/infochimps/search/apache-solr-1.4.1/contrib/hadoop/build.xml:65: The following error occurred while executing this line:\n   [subant] /home/dhruv/projects/infochimps/search/apache-solr-1.4.1/common-build.xml:159: Reference lucene.classpath not found.'.\n\n....\n\n \n\nAm I following the procedure properly?  I'm able to build SOLR just fine out of the box as well as after applying SOLR-1395. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12922329",
            "date": "2010-10-18T22:28:08+0000",
            "content": "Dhruv, thank you, I overlooked this reference. To fix the issue, please go to the build.xml in contrib/hadoop folder and delete the line \"<path refid=\"lucene.classpath\"/>\" - or just download the new version of the patch (attached). You followed the procedure properly except that the hadoop and logging jars are supposed to go to the contrib/hadoop/lib, not the contrib/lib. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-12981930",
            "date": "2011-01-14T21:25:53+0000",
            "content": "Note for the Hadoop 0.21 users: the current patch can be used \"as is\" with 0.21, but you will need to make sure to compile it with appropriate jars (hadoop-common-0.21.0.jar and hadoop-mapred-0.21.0.jar instead of hadoop-0.20.x-core.jar). Also, as a workaround, I had to put all the relevant jars (solr, solrj etc.) to the lib folder of the job's jar file (i.e.  apache-solr-hadoop-xxx-dev.jar) to avoid InvocationTargetException/ClassNotFound exceptions I did not have with Hadoop 0.20. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13030586",
            "date": "2011-05-09T02:39:44+0000",
            "content": "Hadoop contains something called MR, a unit test framework. Is it possible to use that for this purpose? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043689",
            "date": "2011-06-03T16:46:32+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Mark Johnson",
            "id": "comment-13088829",
            "date": "2011-08-22T17:14:39+0000",
            "content": "It appears that this issue has fallen by the wayside.  Is there still a plan to roll this into contrib?  Why has all activity stopped on it? "
        },
        {
            "author": "Mark Johnson",
            "id": "comment-13088860",
            "date": "2011-08-22T18:02:53+0000",
            "content": "Also does anyone have the json converter listed in the readme?   "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13090429",
            "date": "2011-08-24T19:37:15+0000",
            "content": "Mark, I planned to add some unit tests and the packaging for hadoop 0.21.x but unfortunately had no time for this. The problem with unit tests is that you need to either to use your own external hadoop cluster or to run mini-cluster, both ways do not work well for a Solr contrib module in my opinion. I tried to use MRUnit approach while ago with 0.20.x, without success. Maybe will get back to this and try again with 0.21 but I do not anticipate this until mid of September.   "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106294",
            "date": "2011-09-16T14:50:26+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Viktors Rotanovs",
            "id": "comment-13106900",
            "date": "2011-09-16T23:14:24+0000",
            "content": "Beware: with ZIP option enabled, this patch probably has 4 GiB limit on entries inside the zip file, because of file format limitation. I was able to generate 6.8 GB zip file with this patch, but unzip -t fails when encountering >10 GB file inside it:\n\n$ unzip -t /tmp/test.zip\nArchive:  /tmp/test.zip\nwarning [/tmp/test.zip]:  4294967296 extra bytes at beginning or within zipfile\n  (attempting to process anyway)\nfile #1:  bad zipfile offset (local header sig):  4294967296\n  (attempting to re-compensate)\n    testing: part-00000/              OK\n    testing: part-00000/conf/         OK\n    testing: part-00000/conf/schema.xml   OK\n    testing: part-00000/data/         OK\n    testing: part-00000/data/spellchecker/   OK\n    testing: part-00000/data/spellchecker/segments_1   OK\n    testing: part-00000/data/spellchecker/segments.gen   OK\n    testing: part-00000/data/index/   OK\n    testing: part-00000/data/index/_10i.nrm   OK\n    testing: part-00000/data/index/_10i.tii   OK\n    testing: part-00000/data/index/_10i.tis   OK\n    testing: part-00000/data/index/_10i.fnm   OK\n    testing: part-00000/data/index/segments_2   OK\n    testing: part-00000/data/index/_10i.fdx   OK\n    testing: part-00000/data/index/_10i.prx   OK\n    testing: part-00000/data/index/_10i.fdt  \n  error:  invalid compressed data to inflate\nfile #17:  bad zipfile offset (local header sig):  1528156471\n  (attempting to re-compensate)\n    testing: part-00000/data/index/_10i.frq "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13109190",
            "date": "2011-09-21T01:36:10+0000",
            "content": "Viktors, can you increase the number of reducers to avoid big output files? As for the zip format, the Java 7 seems to support zip64 extensions, alternatively we can add an option to generate jtar'ed output or something similar. "
        },
        {
            "author": "Mark Johnson",
            "id": "comment-13155291",
            "date": "2011-11-22T17:43:12+0000",
            "content": "Has anyone updated this contrib to work with the new ant tasks in solr 3.4? "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13214100",
            "date": "2012-02-22T23:19:57+0000",
            "content": "SOLR-1301 patch modified to work with Solr 3.x ant build. tested with Solr 3.5.0 and Cloudera CDH3u3 (also attached)  "
        },
        {
            "author": "Randy Prager",
            "id": "comment-13214101",
            "date": "2012-02-22T23:23:48+0000",
            "content": "I will be out of the office from 2/21 to 2/27 with limited email access.\n\nFor immediate inquiries please contact support@fixflyer.com or 888-349-3593. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13214102",
            "date": "2012-02-22T23:24:04+0000",
            "content": "Note, the hadoop-core-0.20.2-cdh3u3.jar is a part of Cloudera's CDH3 Hadoop distribution and is licensed under Apache License v. 2.0. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13214152",
            "date": "2012-02-23T01:06:26+0000",
            "content": "OK, so I changed the patch to work with 3.5 ant build and re-tested it with Solr 3.5 and Cloudera's CDH3u3 (both the build and csv test run in pseudo-distributed mode). Still no unit tests but I am working on this \n\nNo changes compared to previous version except that I had to comment out the code that sets the debug level dynamically in SolrRecordWriter - because of the conflics with slf4j parts in current Solr; I think it is minor but if not please feel free to resolve this and update the patch. With this done, no need to put the log4j and commons-logging jars in the hadoop/lib at a compile time anymore, only the hadoop jar. I provided the hadoop-core-0.20.2-cdh3u3.jar used for testing as a part of the patch but you can use the other versions of 0.20.x if you'd like; it also should work with hadoop 0.21.x. Note that you still need to make the other related jars (solr, solrj, lucene, commons etc) available while you running your job; one way to do this is to put all the needed jars into the lib subfolder of apache-solr-hadoop jar, another ways are described here: http://www.cloudera.com/blog/2011/01/how-to-include-third-party-libraries-in-your-map-reduce-job/. \n\nFinally, the quick steps to get the patch compiled (on linux):\n1.  get the solr source tarball (apache-solr-3.5.0-src.tgz in this example), put it into some folder, cd there\n2.  tar -xzf apache-solr-3.5.0-src.tgz\n3.  cd apache-solr-3.5.0/solr\n4.  wget https://issues.apache.org/jira/secure/attachment/12515662/SOLR-1301.patch\n5.  patch -p0 -i SOLR-1301.patch\n6.  mkdir contrib/hadoop/lib\n7.  cd contrib/hadoop/lib\n8.  wget https://issues.apache.org/jira/secure/attachment/12515663/hadoop-core-0.20.2-cdh3u3.jar\n9.  cd ../../..\n10. ant dist\n\nand you should have the apache-solr-hadoop-3.5-SNAPSHOT.jar in solr/dist folder. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234660",
            "date": "2012-03-21T18:08:46+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Greg Bowyer",
            "id": "comment-13237738",
            "date": "2012-03-25T00:17:00+0000",
            "content": "Updated patch that deals with a minor change post SOLR-3204 "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13678948",
            "date": "2013-06-09T05:45:57+0000",
            "content": "Noticed this is issue #11 in terms of votes and has 50 watchers.\nFrom the description, this is only about MapReduce-based indexing.\n\nQs:\n\n\tAre there plans to make it possible to use Solr as Input for MapReduce?\n\tMark Miller, do you know if anything related will be contributed to Solr by Cloudera?\n\tIs SOLR-1045 a subset of this issue and thus closable?\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13679070",
            "date": "2013-06-09T14:17:15+0000",
            "content": "Yeah, we have taken this issue as a starting point and extended and polished it quite a bit. Wolfgang will provide an initial 'dump' of the work that was done shortly, but there will still be some work integrating it into our build system and trunk. "
        },
        {
            "author": "Alexander Kanarsky",
            "id": "comment-13688330",
            "date": "2013-06-19T19:10:06+0000",
            "content": "Otis Gospodnetic, do you mean to use the Solr query result as an MapReduce job input?\nAlso, regarding the SOLR-1045, it is a different approach (in Map phase vs. Reduce phase- great explanation by Ted is up here: https://issues.apache.org/jira/browse/SOLR-1301#comment-12828961) "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13693019",
            "date": "2013-06-25T13:24:58+0000",
            "content": "Alexander Kanarsky - yes, take Solr results and use them for MR input, as well as run a MR job and index into Solr (SOLR-1045).\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13716621",
            "date": "2013-07-23T17:16:06+0000",
            "content": "As I mentioned above, Cloudera has a done a lot with moving this issue forward. I've been working on converting the build system from maven to ivy+ant and will post my current progress before long. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13755612",
            "date": "2013-08-31T21:32:25+0000",
            "content": "Here is a patch with my current progress.\n\nThis is a Solr contrib module that can build Solr indexes in HDFS via MapReduce. It builds upon the Solr support for reading and writing to HDFS.\n\nIt supports a GoLive feature that allows merging into a running cluster as the final step of the MapReduce job.\n\nThere is fairly comprehensive help documentation as part of the MapReduceIndexerTool.\n\nFor ETL, Morphlines from the open source Cloudera CDK is used: https://github.com/cloudera/cdk/tree/master/cdk-morphlines This is the same ETL library that the Solr integration with Apache Flume uses.\n\nWhat I have recently done: updated to latest code, fixed 5x requires solr.xml now, converted maven to ivy+ant, updated license files, fixed validation errors, integrated tests fully into test framework, got tests passing.\n\nAll tests are passing with this patch for me, but there are still a variety of issues to address:\n\n\n\trun yarn and mr1 - the maven build would run the unit tests against yarn or mr1 depending on the profile chosen on the command line - this patch runs against yarn.\n\n\n\n\n\tThe MiniYarnCluster used for unit tests is hard coded to use the 'current-working-dir'/target path. This is a bad and illegal location. For the moment, I've relaxed the Lucene tests policy file to allow read/writes anywhere - this needs to be addressed before committing.\n\n\n\n\n\tWe depend on some Morphline commands that depend on Solr - this could cause us problems in the future, and we want to own the code for this commands in Solr I think.\n\n\n\n\n\tThere are thread leaks in the tests that should be looked into - some might not be avoidable as in other Hadoop tests (as we wait for fixes from the Hadoop project).\n\n\n\n\n\tWe need to sync up with the latest code from the maven version - there have been some changes since this code was extracted.\n\n\n\nThere are a number of new contributors to this issue that I will be sure to enumerate in CHANGES.\n\nI'll add whatever I'm forgetting in a later comment. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13755613",
            "date": "2013-08-31T21:38:50+0000",
            "content": "Another thing I have not looked at: The final jar that is created in the dist for the MapReduceIndexerTool - it likely still needs tweaking. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13755642",
            "date": "2013-09-01T03:10:00+0000",
            "content": "And another note: need to add support for the skip hadoop tests system property as well. "
        },
        {
            "author": "Phani Chaitanya Vempaty",
            "id": "comment-13763527",
            "date": "2013-09-10T21:22:37+0000",
            "content": "I wanted to look at the code but after I downloaded solr-4.4.0 and applied the patch, I'm not able to create the eclipse project. It says that there is no ivy.xml in solr-mr directory and it is missing indeed. I created one and now everything is fine. Is ivy.xml missed as it is an initial-cut or am I doing something wrong. Below is my ivy.xml.\n\n\n<ivy-module version=\"2.0\">\n    <info organisation=\"org.apache.solr\" module=\"solr-mr\"/>\n    <dependencies>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-common\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-core\" rev=\"1.2.1\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-hdfs\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-mapreduce\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-mapreduce-client\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-mapreduce-client-core\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-mapred\" rev=\"0.22.0\" transitive=\"false\"/>\n      <dependency org=\"org.apache.hadoop\" name=\"hadoop-yarn\" rev=\"2.0.5-alpha\" transitive=\"false\"/>\n      <dependency org=\"com.codahale.metrics\" name=\"metrics-core\" rev=\"3.0.1\" transitive=\"false\"/>\n      <dependency org=\"com.cloudera.cdk\" name=\"cdk-morphlines-core\" rev=\"0.7.0\" transitive=\"false\"/>\n      <dependency org=\"com.cloudera.cdk\" name=\"cdk-morphlines-solr-core\" rev=\"0.7.0\" transitive=\"false\"/>\n      <dependency org=\"org.skife.com.typesafe.config\" name=\"typesafe-config\" rev=\"0.3.0\" transitive=\"false\"/>\n      <dependency org=\"net.sourceforge.argparse4j\" name=\"argparse4j\" rev=\"0.4.1\" transitive=\"false\"/>\n      <exclude org=\"*\" ext=\"*\" matcher=\"regexp\" type=\"${ivy.exclude.types}\"/>\n    </dependencies>\n</ivy-module>\n\n\n\nThough at least now I'm able to get the eclipse project to view the code, I still have some compile errors in the project which I guess is mainly due to the hadoop versions that I have in the above ivy.xml file w.r.t hadoop-core and others (I'm not able to find 2.0.5-alpha from http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/ - I did not look into CDH jars though). I'm also not able to compile the code base after applying this patch due to this very reason. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13763602",
            "date": "2013-09-10T22:12:37+0000",
            "content": "Is ivy.xml missed as it is an initial-cut or am I doing something wrong. \n\nNo, it should be there - not sure why it wouldn't have made it into the patch. I'll post another before long. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13763618",
            "date": "2013-09-10T22:23:23+0000",
            "content": "FYI, One things that's definitely off in that adhoc ivy.xml above is that it should use com.typesafe rather than org.skife.com.typesafe.config. Use version 1.0.2 of it. See http://search.maven.org/#search%7Cga%7C1%7Ctypesafe-config\n\nMaybe best to wait for Mark to post our full ivy.xml, though. \n\n(Moving all our solr-mr dependencies from Cloudera Search maven to ivy was a bit of a beast).  "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13763636",
            "date": "2013-09-10T22:33:36+0000",
            "content": "By the way, docs and the downstream code for our solr-mr contrib submission is here: https://github.com/cloudera/search/tree/master/search-mr\n "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13763644",
            "date": "2013-09-10T22:46:40+0000",
            "content": "This new solr-mr contrib uses morphlines for ETL from MapReduce into Solr. To get started, here are some pointers for morphlines background material and code:\n\ncode:\n\n\thttps://github.com/cloudera/cdk/tree/master/cdk-morphlines\n\nblog post:\n\n\thttp://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/\n\nreference guide:\n\n\thttp://cloudera.github.io/cdk/docs/current/cdk-morphlines/morphlinesReferenceGuide.html\n\nslides:\n\n\thttp://www.slideshare.net/cloudera/using-morphlines-for-onthefly-etl\n\ntalk recording:\n\n\thttp://www.youtube.com/watch?v=iR48cRSbW6A "
        },
        {
            "author": "Phani Chaitanya Vempaty",
            "id": "comment-13763690",
            "date": "2013-09-10T23:22:11+0000",
            "content": "Thanks Wolfgang. I corrected it in my xml, though will be waiting for Mark to give the full version of the xml file. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13763715",
            "date": "2013-09-10T23:39:31+0000",
            "content": "Here is another patch. No major changes. I've confirmed it has ivy.xml in it.\n\nI made some minor tweaks to fix issues 'precommit' brought up, but it's currently failing on 3 javadoc warnings due to some new hadoop test dependencies:\n\n\n  [javadoc] /ssd/workspace3/lucene-solr-5x-mr/solr/contrib/solr-mr/lib/hadoop-mapreduce-client-jobclient-2.0.5-alpha-tests.jar(org/apache/hadoop/mapreduce/TestLocalRunner.class): warning: Cannot find annotation method 'timeout()' in type 'Test': class file for org.junit.Test not found\n  [javadoc] /ssd/workspace3/lucene-solr-5x-mr/solr/contrib/solr-mr/lib/hadoop-common-2.0.5-alpha-tests.jar(org/apache/hadoop/util/TestClassUtil.class): warning: Cannot find annotation method 'timeout()' in type 'Test'\n  [javadoc] /ssd/workspace3/lucene-solr-5x-mr/solr/contrib/solr-mr/lib/hadoop-common-2.0.5-alpha-tests.jar(org/apache/hadoop/io/TestSortedMapWritable.class): warning: Cannot find annotation method 'timeout()' in type 'Test'\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13763718",
            "date": "2013-09-10T23:41:36+0000",
            "content": "Given that my first patch was 963k and the latest is 2.33MB, it seems the first was missing a bunch of things due to me not yet adding them to svn - sorry about that - the latest patch should be complete. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13763743",
            "date": "2013-09-11T00:04:39+0000",
            "content": "FYI - wolfgang hoschek and Patrick Hunt are the primary authors of the code - it was based on the initial patches in this issue, but has been heavily processed and expanded. Full unit tests have also been written. Gregory Chanan, Roman Shaposhnik, I, and Eric Wong also contributed thoughts, code, and features. I have not yet added a CHANGES entry to the patch, but this is the current list of authors that would go in, along with the authors of the previous work in this JIRA. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13765785",
            "date": "2013-09-12T19:13:01+0000",
            "content": "Note so I do not forget: Wolfgang just mentioned that we are missing runtime libs that our tests don't require - such as Tika libs - our tests of course don't exercise all of them.\n\nI'll need to make sure we pull all of those in. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13766220",
            "date": "2013-09-13T03:57:53+0000",
            "content": "I'm working on making a new patch with some changes:\n\n\n\n\tI updated to Hadoop 2.1.0 beta\n\n\n\n\n\tI updated the versions of some of the dependencies\n\n\n\n\n\tI added some run time dependencies that the tests don't require as well as their license files\n\n\n\n\n\tI started working around the issue where the Yarn cluster is hard coded to write to the CWD/target illegal location. This involved copying and modding some Hadoop test files until we can get Hadoop to make things more flexible. Unfortunately there is still an issue - the mkdirs used by hdfs requires write permissions all the way up the tree it seems, whether the directory you are making already exists or not - this keeps the yarn mini test cluster from being able to run with our test policy. It fails in init when it does this mkdir. Don't know of a good workaround at the moment.\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13766252",
            "date": "2013-09-13T05:33:31+0000",
            "content": "Latest patch attached. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13766448",
            "date": "2013-09-13T12:49:47+0000",
            "content": "Fix version still says 4.5. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13766494",
            "date": "2013-09-13T14:09:55+0000",
            "content": "I've worked out the javadoc warnings - that has led to some new issue(s) with jtidy. It's failing on SolrReducer.html. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13766512",
            "date": "2013-09-13T14:47:07+0000",
            "content": "Got it - the jtidy output was very generic (failed, returned 1), but I worked out that the problem was some '<' and '>' in the javadoc of SolrReducer and another class or two. After addressing that and adding a couple package.html files,  the precommit ant task now passes.\n\nI have a variety of items still on the TODO list, but I think the critical path to an initial commit is:\n\n\n\tMove the Solr Morphline commands in.\n\n\n\n\n\tGet the tests to run without a hacked test.policy file - see my comment above about FileSystem#mkDirs.\n\n\n\n\n\tLook at the final jar we produce and how it works with the dependencies (eg it's currently going to the extraction contrib for tika, etc).\n\n\n\nThe other outstanding issues are not blocking an initial commit I don't think.\n\nAlso, FYI, since I did not mention, the previous patch will run the mini cluster tests based on the tests.disableHdfs sys prop now, so that is checked off. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13767529",
            "date": "2013-09-14T17:13:11+0000",
            "content": "I've moved in the Solr Morphlines code from cdk-morphlines-solr-core and cdk-morphlines-solr-cell. I've made them compliant with the test framework and got the tests passing. There are still some ant precommit and license issues to handle, but otherwise this should be mostly done. I'm still unclear on what will be required for packaging, but I am tackling packaging last.\n\nI've also updated the Tika parser dependencies to include a couple Solr did not have.\n\nOnce I wrap up the loose ends on this I'll attach my latest patch, and then, only two issues on the critical path:\n\n\n\tGet the tests to run without a hacked test.policy file.\n\tDist packaging.\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13767583",
            "date": "2013-09-14T21:15:57+0000",
            "content": "Latest patch with Solr Morphlines and the other items I mentioned above.\n\nNew issues though.\n\nSomething is still writing to {CWD}/target that I need to track down.\n\nPrecommit is not yet passing with Solr morphlines code - need to resolve use of forbidden apis:\n[forbidden-apis] Forbidden class/interface use: com.sun.org.apache.xml.internal.serialize.OutputFormat [non-public internal runtime class]\n[forbidden-apis]   in org.apache.solr.hadoop.morphline.solrcell.SolrCellBuilder$SolrCell (SolrCellBuilder.java:242)\n[forbidden-apis] Forbidden class/interface use: com.sun.org.apache.xml.internal.serialize.XMLSerializer [non-public internal runtime class]\n[forbidden-apis]   in org.apache.solr.hadoop.morphline.solrcell.SolrCellBuilder$SolrCell (SolrCellBuilder.java:242)\n\nAlso, I'm not sure where exactly the Solr morphlines should end up - as their own module or where I put them, but this is where they are for now. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13768629",
            "date": "2013-09-16T19:03:59+0000",
            "content": "cdk-morphlines-solr-core and cdk-morphlines-solr-cell should remain separate and be available through separate maven modules so that clients such as Flume Solr Sink and Hbase Indexer can continue to choose to depend (or not depend) on them. For example, not everyone wants tika and it's dependency chain. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13768662",
            "date": "2013-09-16T19:30:45+0000",
            "content": "Seems like the patch still misses tika-xmp. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13768772",
            "date": "2013-09-16T21:15:53+0000",
            "content": "This is likely the last patch I'll put up for a bit - I'm on vacation from Wed-Mon.\n\nPatch Notes:\n\nant precommit passes again. I've fixed the forbidden api calls and a couple minor javadoc issues in the new morphlines code. Also fixed a more problematic javadocs issue due to broken links from the morphlines code to extraction code due to extending extraction classes.\n\nI've added tika-xmp to the extraction dependencies.\n\nI don't like that tests can pass when some necessary run-time jars are missing - we will likely need to look into adding simple tests that cause each necessary jar to be used - or even just have hack tests that try and create a class in the offending jars or something. I'll save that for a follow up issue though - the solr cell morphlines tests actually upped the number of dependencies tests hit quite a bit at least.\n\nThere is also a test speed issue that is not on the critical path - on my fast machine that does 8 tests in parallel, this adds about 4-5 minutes to the tests. It would be good to try and minimize some of the longer tests for std runs, and keep them as is for @nightly runs. That can wait post commit though.\n\nThat leaves the following 2 critical path items to deal with:\n\n\n\tGet the tests to run without a hacked test.policy file.\n\tDist packaging. This includes things like creation of the final MapReduceIndexerTool jar file and dealing with it's dependencies, as well as the location of the morphlines code and how it is distributed.\n\n\n\nOther than that we are looking pretty good - all tests passing and precommit passing.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13790411",
            "date": "2013-10-09T14:47:21+0000",
            "content": "I have a new patch I'm cleaning up that tackles some of the packaging:\n\n\n\n\tSplit out solr-morphlines-core and solr-morphlines-cell into their own modules.\n\n\n\n\n\tUpdated to trunk and the new modules are now using the new dependency version tracking system.\n\n\n\n\n\tFixed an issue in the code around the TokenStream contract being violated - the latest code detected this and failed a test - end and close now called.\n\n\n\n\n\tUpdated to use Morphlines from CDK 0.8.\n\n\n\n\n\tSetup the main class in the solr-mr jar manifest.\n\n\n\n\n\tI enabled an ignored test which exposed a few bugs because of the required solr.xml in Solr 5.0 - I addressed those bugs.\n\n\n\n\n\tAdded a missing metrics health-check dependency that somehow popped up.\n\n\n\n\n\tI played around with naming the solr-mr artifact MapReduceIndexTool.jar, but the system really want's us to follow the rules of the artifacts and have something like solr-solr-mr-5.0.jar. Anything else has some random issues, such as with javadoc, and if your name does not start with solr-, it will be changed to start with lucene-. I'm not yet sure if it's worth the trouble to expand the system or use a different name, so for now it's still just using the default jar name based on the contrib module name (solr-mr).\n\n\n\nBesides the naming issue, there are a couple other things to button up:\n\n\n\tHow we are going to set up the classpath - script, in the manifest, leave it up to the user and doc, etc.\n\n\n\n\n\tAll dependencies are currently in solr-morphlines-core - this was a simple way to split out the modules since solr-mr and solr-morphlines-cell depend on solr-morphlines-core.\n\n\n\nFinally, we will probably need some help from Steve Rowe to get the Maven build setup correctly.\n\nI spent a bunch of time trying to use asm to work around the hacked test policy issue. There are multiple problems I ran into. One is that another module uses asm 4.1, but Hadoop brings in asm 3.1 - if you are doing some asm coding, this can cause compile issues with your ide (at least eclipse). It also ends up being really hard to get an injection in the right place because of how the yarn code is structured. After spending a bunch of time trying to get this to work, I'm backing out and considering other options. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13793868",
            "date": "2013-10-14T01:15:32+0000",
            "content": "Here is the patch I referred to in the above comment.\n\nPrecommit still passing and tests passing with the tests policy hack. "
        },
        {
            "author": "Rafa\u0142 Ku\u0107",
            "id": "comment-13824426",
            "date": "2013-11-16T09:22:21+0000",
            "content": "Mark, is the version attached to this issue a newest patch or maybe you have something newer? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13825363",
            "date": "2013-11-18T14:47:18+0000",
            "content": "Hey Rafal - that is the latest at the moment - gotten side tracked with other things. Shortly, I'll upload a new patch that changes around the dependencies between modules a bit. Beyond that there is figuring out the strategy for the classpath, some manual testing, and finally working around that darn test policy issue.\n\nHowever, things should be in a useable state regardless of those remaining issues. "
        },
        {
            "author": "Rafa\u0142 Ku\u0107",
            "id": "comment-13825365",
            "date": "2013-11-18T14:51:08+0000",
            "content": "Thanks Mark  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13829678",
            "date": "2013-11-22T05:52:48+0000",
            "content": "New Patch.\n\n\n\tUpdated to trunk\n\n\n\n\n\tA pass at putting dependencies in the correct modules\n\n\n\n\n\tA script for running the MapReduceIndexTool - classpath in the manifest doesn't seem very nice.\n\n\n\n\n\tUpdated to CDK 0.8.1\n\n\n\nI'm sure there are a variety of other things to polish, fix, decide and finalize, as well as code to sync up - but nothing that needs to be done before this is committed. I need to get this in asap as it's a large burden to maintain over time.\n\nExcept for the test policy issue. That is the only blocker I know of for committing remaining.\n\nAlso have to do a bit of manual testing.\n\nYou can run the tool by running Solr's 'ant package' and then expand one of the release zip/tgz files. Try something like:\n\ncd solr/example/scripts/solr-mr\nsh solr-mr.sh --help "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13832006",
            "date": "2013-11-25T22:41:34+0000",
            "content": "I have a plan for the test policy issue.\n\n\n\tdisable the couple large integration tests that have a problem by default\n\tuse a 'no.test.policy' sys prop of some kind to allow those tests to run in a local jenkins setup with no test policy running\n\tfile a jira with yarn requesting that they take pity on us and make it so that yarn will run with our test policy (eg, if this dir already exists, don't mkdirs up it's parents to the root)\n\tonce that makes it into a release, we can reenable these couple tests by default\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13832151",
            "date": "2013-11-26T01:04:56+0000",
            "content": "Steve Rowe sir - I'm going to need your assistance I think  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13832153",
            "date": "2013-11-26T01:08:08+0000",
            "content": "Steve Rowe sir - I'm going to need your assistance I think \n\nI'll take a look. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13832481",
            "date": "2013-11-26T10:55:48+0000",
            "content": "FYI, when I apply the latest patch against trunk using svn patch SOLR-1301.patch, svn says:\n\n\nSkipped missing target: 'solr/example/scripts/cloud-scripts/zkcli.sh'\nSkipped missing target: 'solr/example/scripts/cloud-scripts/zkcli.bat'\n\n\n\nThe patch assumes these two files already exist, but solr/example/scripts/ doesn't exist on trunk.\n\nWhen I run the following before applying the patch, svn no longer complains about those scripts:\n\n\nsvn mkdir solr/example/scripts\nsvn mv solr/example/cloud-scripts solr/example/scripts/\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13832711",
            "date": "2013-11-26T16:02:42+0000",
            "content": "Sorry about that - eclipse gets confused sometimes when you do some local refactoring.\n\nI'll commit to 5x and it will be easier to work on this. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13832721",
            "date": "2013-11-26T16:14:49+0000",
            "content": "bq, I'll commit to 5x and it will be easier to work on this.\n\nI'm working on the Maven build, and am almost there - do you want to wait for a revised patch?  I also made some minor modifications to the Ant build: removed solr-mr dependency from solr-morphlines-cell; resolving solr-morphlines-core test deps to test-lib/ instead of lib/.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13832730",
            "date": "2013-11-26T16:23:59+0000",
            "content": "I was a little scared of committing from a patch and not the svn checkout I've been working from. It's such a huge patch \n\nHowever, whatever is easiest for you. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13832732",
            "date": "2013-11-26T16:24:35+0000",
            "content": "removed solr-mr dependency from solr-morphlines-cell; resolving solr-morphlines-core test deps to test-lib/ instead of lib/.\n\nNice, thanks. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13832734",
            "date": "2013-11-26T16:27:31+0000",
            "content": "It's such a huge patch\n\nYeah, it is... \n\nI guess you should commit, then I'll make a patch of the diff between my modified patched dir and what you commit.  That should be the simplest/least scary. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13832738",
            "date": "2013-11-26T16:31:18+0000",
            "content": "Mark, BTW, some tests are failing for me in solr-morphlines-cell and in solr-morphlines-core, and test(s?) are hanging in solr-mr (OS X, Oracle JDK 1.7.0_25), both with and without my modifications.  Hopefully it's just a local thing. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13833269",
            "date": "2013-11-27T00:27:52+0000",
            "content": "Patch that extends the Maven and IntelliJ builds to include the three new modules.  Minor Ant build modifications included as well.\n\nThis patch was created with dev-tools/scripts/diffSources.py, comparing two trunk directories patched with Mark's latest patch, the second of which has my modifications as well.  So it has to be applied after Mark's latest patch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13833422",
            "date": "2013-11-27T03:45:37+0000",
            "content": "Thanks Steve - took me a bit, but I'm about ready to commit to 5x. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13836768",
            "date": "2013-12-02T18:42:26+0000",
            "content": "Commit 1547139 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547139 ]\n\nSOLR-1301: Add a Solr contrib that allows for building Solr indexes via Hadoop's MapReduce. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13836909",
            "date": "2013-12-02T20:51:21+0000",
            "content": "Commit 1547187 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547187 ]\n\nSOLR-1301: Ivy likes to act funny if you don't declare compile and test resources in the same dependency. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13836934",
            "date": "2013-12-02T21:18:36+0000",
            "content": "I've setup a local jenkins job to run the two tests that have a problem with the test policy/manager. Next I'll file a JIRA issue for Yarn. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13836945",
            "date": "2013-12-02T21:34:40+0000",
            "content": "One issue that I had to work around will be solved with https://issues.apache.org/jira/browse/YARN-1442 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13836988",
            "date": "2013-12-02T22:17:59+0000",
            "content": "Hi,\nit seems to resolve correctly now. There is one inconsistency: the folder names. The new contribs have all \"solr-\" in the folder name, which is inconsistent to the others. I would prefer to rename the folder names with svn mv and maybe fix some paths in dependencies and maven. The build.xml files use the correct name already, so JAR files are named correctly.\nUwe "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13837001",
            "date": "2013-12-02T22:26:46+0000",
            "content": "Removing solr from the module names sounds good to me. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13837032",
            "date": "2013-12-02T22:45:35+0000",
            "content": "Commit 1547232 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1547232 ]\n\nSOLR-1301: Fix compilation for Java 8 (the Java 8 compiler is more picky, but it's not a Java 8 regression: the code was just wrong) "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13837036",
            "date": "2013-12-02T22:50:04+0000",
            "content": "I found out that some tests don't work on Windows, for the same reason why the MiniDFS tests don't work in Solr-Core: Some crazy command line tools are missing. I would mark all those tests with the same assume like HdfsDirectory tests?\n\nShould I start doing this? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13837049",
            "date": "2013-12-02T23:02:53+0000",
            "content": "Hmm...yeah, you might as well. I'll investigate on my VM. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13837068",
            "date": "2013-12-02T23:24:39+0000",
            "content": "There is also a known issue in that Morphlines don't work on Windows because the Guava Classpath utility doesn't work with windows path conventions. For example, see http://mail-archives.apache.org/mod_mbox/flume-dev/201310.mbox/%3C5ACFFCD9-4AD7-4E6E-8365-CEADFAC78B1A@cloudera.com%3E "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13837070",
            "date": "2013-12-02T23:30:56+0000",
            "content": "Commit 1547239 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1547239 ]\n\nSOLR-1301: Fix windows problem with escaping of folder name (see crazy https://github.com/typesafehub/config/blob/master/HOCON.md for correct format: string must be quoted and escaped like javascript) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13837090",
            "date": "2013-12-02T23:49:32+0000",
            "content": "Commit 1547242 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1547242 ]\n\nSOLR-1301: Ignore windows tests that cannot work because they use UNIX semantics. Also remove a never-executed test which tests nothing "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13837110",
            "date": "2013-12-03T00:01:13+0000",
            "content": "OK, I fixed the test suite to pass on Windows. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13837775",
            "date": "2013-12-03T15:05:45+0000",
            "content": "For posterity, there is a thread on the dev list where we are working through an issue with Saxon on java 8 and ibm's j9. Wolfgang filed https://saxonica.plan.io/issues/1944 upstream. (Saxon is pulled in via cdk-morphlines-saxon). "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13837808",
            "date": "2013-12-03T15:42:17+0000",
            "content": "Commit 1547442 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547442 ]\n\nSOLR-1301: Ignore these tests on java 8 and j9 for now. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13837830",
            "date": "2013-12-03T15:58:39+0000",
            "content": "Removing Solr from the module names would give:\n\nmorphlines-cell Perhaps should be morphlines-extraction? We have always made cell / extraction confusing. The module folder is extraction though, so I see that as the name. We really should standardize on one name.\n\nmorphlines-core Removing Solr is a bit confusing - morphlines-core is a module in the morphlines project - this is a morphlines module with stuff for interacting with Solr - perhaps we just call it morphlines?\n\nmr Seems we should rename this. Steve suggested map-reduce-indexer in IRC, which seems good to me. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13837953",
            "date": "2013-12-03T17:54:58+0000",
            "content": "Commit 1547498 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1547498 ]\n\nSOLR-1301: remove unnecessary (POM-only) dependency org.apache.hadoop:hadoop-yarn-server "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13837976",
            "date": "2013-12-03T18:12:41+0000",
            "content": "module/dir names\n\nI propose morphlines-solr-core and morphlines-solr-cell as names. This avoids confusion by fitting nicely with the existing naming pattern, which is cdk-morphlines-solr-core and cdk-morphlines-solr-cell. (https://github.com/cloudera/cdk/tree/master/cdk-morphlines). Thoughts? "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13837979",
            "date": "2013-12-03T18:14:03+0000",
            "content": "+1 to \"map-reduce-indexer\" module name/dir. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13838038",
            "date": "2013-12-03T19:02:23+0000",
            "content": "I propose morphlines-solr-core and morphlines-solr-cell as names. This avoids confusion by fitting nicely with the existing naming pattern, which is cdk-morphlines-solr-core and cdk-morphlines-solr-cell. (https://github.com/cloudera/cdk/tree/master/cdk-morphlines). Thoughts?\n\nThe problem with these two names is that the artifact names will have \"solr-\" prepended, and then \"solr\" will occur twice in their names: solr-morphlines-solr-core-4.7.0.jar, solr-morphlines-solr-cell-4.7.0.jar.  Yuck. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13838039",
            "date": "2013-12-03T19:02:40+0000",
            "content": "That sounds fine to me. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13838040",
            "date": "2013-12-03T19:03:24+0000",
            "content": "Yuck.\n\nWhoops - cross posted. Yeah, didn't realize that - not ideal. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13838054",
            "date": "2013-12-03T19:15:59+0000",
            "content": "The problem with these two names is that the artifact names will have \"solr-\" prepended, and then \"solr\" will occur twice in their names: solr-morphlines-solr-core-4.7.0.jar, solr-morphlines-solr-cell-4.7.0.jar. Yuck.\n\nAh, argh. In this light, what Mark suggested seems good to me as well. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13838060",
            "date": "2013-12-03T19:19:12+0000",
            "content": "In this light, what Mark suggested seems good to me as well.\n\n+1 to:\n\n\n\n\ncontrib name\nartifact name\n\n\nmorphlines-core\nsolr-morphlines-core\n\n\nmorphlines-cell\nsolr-morphlines-cell\n\n\nmap-reduce-indexer\nsolr-map-reduce-indexer\n\n\n\n "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13838064",
            "date": "2013-12-03T19:23:44+0000",
            "content": "+1 on  Steve's suggestion as well. Thanks for helping out! "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13838305",
            "date": "2013-12-03T23:11:04+0000",
            "content": "Upon a bit more reflection might be better to call the contrib \"map-reduce\" and the artifact \"solr-map-reduce\". This keeps the door open to potentially later add things like a Hadoop SolrInputFormat, i.e. read from solr via MR, rather than just write to solr via MR. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13838968",
            "date": "2013-12-04T15:15:25+0000",
            "content": "Commit 1547819 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547819 ]\n\nSOLR-1301: Straighten out module names so that they match current convention "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13839173",
            "date": "2013-12-04T18:28:46+0000",
            "content": "Commit 1547871 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547871 ]\n\nSOLR-1301: Merge in latest solr-map-reduce updates. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13839228",
            "date": "2013-12-04T19:11:41+0000",
            "content": "Commit 1547879 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547879 ]\n\nSOLR-1301: Merge in latest morphlines module updates. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13839237",
            "date": "2013-12-04T19:16:29+0000",
            "content": "MorphlineGoLiveMiniMRTest, which is ignored while the test policy issue gets straightened out, is now too slow for the standard test run. Before re-enabling it, we will have to tone it down for non nightly runs. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13839308",
            "date": "2013-12-04T20:21:05+0000",
            "content": "There are also some fixes downstream in cdk-morphlines-core and cdk-morphlines-solr-cell that would be good to push upstream. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13839311",
            "date": "2013-12-04T20:24:36+0000",
            "content": "Minor nit: could remove jobConf.setBoolean(ExtractingParams.IGNORE_TIKA_EXCEPTION, false) in MorphlineBasicMiniMRTest + MorphlineGoLiveMiniMRTest because such a flag is nomore needed, and it removes an unnecessary dependency on tika.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13839459",
            "date": "2013-12-04T23:05:10+0000",
            "content": "it removes an unnecessary dependency on tika.\n\nWhoops - that is why I had changed to just using the string param and I accidentally just reverted that in the merge. I'll remove the params entirely. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13839478",
            "date": "2013-12-04T23:28:52+0000",
            "content": "Commit 1547962 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1547962 ]\n\nSOLR-1301: Clean up. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13839556",
            "date": "2013-12-05T00:54:01+0000",
            "content": "FWIW, a current printout of --help showing the CLI options is here: https://github.com/cloudera/search/tree/master_1.1.0/search-mr "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13840633",
            "date": "2013-12-05T22:16:18+0000",
            "content": "Commit 1548319 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1548319 ]\n\nSOLR-1301: ignore '.iml' in new Solr contribs' directories; put new Solr contribs' lib/ and test-lib/ directories under Subversion control; ignore '.jar' in these directories "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13841058",
            "date": "2013-12-06T07:23:27+0000",
            "content": "Getting started at the moment might be a bit daunting - to help people get started, to help with testing, and to help with figuring out what we need to provide to improve usability, I've started the following GitHub project: https://github.com/markrmiller/solr-map-reduce-example\n\nIt's a script that downloads Hadoop and a nightly build of Solr and then builds an index via map-reduce and deploys that index to Solr.\n\nFor now, it's just for looking - it won't actually work until I make a couple commits so that the standard example config files will correctly work with the map-reduce module.\n\nThis should lower the barrier to entry for anyone that wants to play with things and serve as a nice guide for those looking to try this out on a real cluster.\n\nI'll make the commit(s) I referenced above sometime today later when I wake up. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13841429",
            "date": "2013-12-06T17:04:48+0000",
            "content": "Commit 1548600 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1548600 ]\n\nSOLR-1301: Fix a couple of bugs around setting up the embedded Solr instance. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13841434",
            "date": "2013-12-06T17:08:54+0000",
            "content": "My plan is to merge this back to 4X before long - I do think we should mark it as an experimental module though and avoid promising strong back compat for a couple of releases. 4X releases frequently and we want to gather some feedback before locking in too much. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13841453",
            "date": "2013-12-06T17:34:56+0000",
            "content": "Commit 1548605 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1548605 ]\n\nSOLR-1301: Update to Morphlines 0.9.0 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13841915",
            "date": "2013-12-06T23:59:56+0000",
            "content": "If you want to try this out, this example repo script should now be working for everyone: https://github.com/markrmiller/solr-map-reduce-example\n\nIt works with Linux and I just updated it to work with OSX (at least my copies). "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13842034",
            "date": "2013-12-07T02:57:00+0000",
            "content": "There are also some important fixes downstream in 0.9.0 of cdk-morphlines-solr-core and cdk-morphlines-solr-cell that would be good to merge upstream (solr locator race, solr cell bug, etc). Also there are new morphline modules jars to add with 0.9.0 and jars to update (plus upstream is also missing some morphline modules from 0.8 as well) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13842093",
            "date": "2013-12-07T06:17:32+0000",
            "content": "Commit 1548795 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1548795 ]\n\nSOLR-1301: Update jar checksums for Morphlines 0.9.0 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13842555",
            "date": "2013-12-08T17:05:59+0000",
            "content": "The Maven Jenkins build on trunk has been failing for a while because com.sun.jersey:jersey-bundle:1.8, a morphlines-core dependency, causes ant validate-maven-dependencies to fail - here's a log excerpt from the most recent failure https://builds.apache.org/job/Lucene-Solr-Maven-trunk/1046/console:\n\n\n     [echo] Building solr-map-reduce...\n\n-validate-maven-dependencies.init:\n\n-validate-maven-dependencies:\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-cell:5.0-SNAPSHOT: checking for updates from maven-restlet\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-cell:5.0-SNAPSHOT: checking for updates from releases.cloudera.com\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-morphlines-cell:5.0-SNAPSHOT: checking for updates from maven-restlet\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-morphlines-cell:5.0-SNAPSHOT: checking for updates from releases.cloudera.com\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-morphlines-core:5.0-SNAPSHOT: checking for updates from maven-restlet\n[artifact:dependencies] [INFO] snapshot org.apache.solr:solr-morphlines-core:5.0-SNAPSHOT: checking for updates from releases.cloudera.com\n[artifact:dependencies] An error has occurred while processing the Maven artifact tasks.\n[artifact:dependencies]  Diagnosis:\n[artifact:dependencies] \n[artifact:dependencies] Unable to resolve artifact: Unable to get dependency information: Unable to read the metadata file for artifact 'com.sun.jersey:jersey-bundle:jar': Cannot find parent: com.sun.jersey:jersey-project for project: null:jersey-bundle:jar:null for project null:jersey-bundle:jar:null\n[artifact:dependencies]   com.sun.jersey:jersey-bundle:jar:1.8\n[artifact:dependencies] \n[artifact:dependencies] from the specified remote repositories:\n[artifact:dependencies]   central (http://repo1.maven.org/maven2),\n[artifact:dependencies]   releases.cloudera.com (https://repository.cloudera.com/artifactory/libs-release),\n[artifact:dependencies]   maven-restlet (http://maven.restlet.org),\n[artifact:dependencies]   Nexus (http://repository.apache.org/snapshots)\n[artifact:dependencies] \n[artifact:dependencies] Path to dependency: \n[artifact:dependencies] \t1) org.apache.solr:solr-map-reduce:jar:5.0-SNAPSHOT\n[artifact:dependencies] \n[artifact:dependencies] \n[artifact:dependencies] Not a v4.0.0 POM. for project com.sun.jersey:jersey-project at /home/hudson/.m2/repository/com/sun/jersey/jersey-project/1.8/jersey-project-1.8.pom\n\n\n\nI couldn't reproduce locally.\n\nTurns out the parent POM in question, at /home/hudson/.m2/repository/com/sun/jersey/jersey-project/1.8/jersey-project-1.8.pom, has the wrong contents:\n\n\n<html>\n<head><title>301 Moved Permanently</title></head>\n<body bgcolor=\"white\">\n<center><h1>301 Moved Permanently</h1></center>\n<hr><center>nginx/0.6.39</center>\n</body>\n</html>\n\n\n\nI replaced this by manually downloading the correct POM and it's checksum file from Maven Central and putting them in the hudson user's local Maven repository.\n\nMark Miller: While investigating this failure, I tried dropping the triggering Ivy dependency com.sun.jersey:jersey-bundle, and all enabled tests succeed.  Okay with you to drop this dependency?  The description from the POM says:\n\n\n<description>\nA bundle containing code of all jar-based modules that provide JAX-RS and Jersey-related features. Such a bundle is *only intended* for developers that do not use Maven's dependency system. The bundle does not include code for contributes, tests and samples.\n</description>\n\n\n\nSounds like it's a sneaky replacement for transitive dependencies?  IMHO, if we need some of the classes this jar provides, we should declare direct dependencies on the appropriate artifacts. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13843343",
            "date": "2013-12-09T17:37:09+0000",
            "content": "if we need some of the classes this jar provides, we should declare direct dependencies on the appropriate artifacts.\n\nRight - Wolfgang likely knows best when it comes to Morphlines.. At a minimum we should pull the necessary jars in explicitly I think. I've got to take a look at what they are. "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13843443",
            "date": "2013-12-09T19:27:06+0000",
            "content": "I'm not aware of anything needing jersey except perhaps hadoop pulls that in.\n\nThe combined dependencies of all morphline modules is here: http://cloudera.github.io/cdk/docs/current/cdk-morphlines/cdk-morphlines-all/dependencies.html\n\nThe dependencies of each individual morphline modules is here: http://cloudera.github.io/cdk/docs/current/dependencies.html\n\nThe source and POMs are here, as usual: https://github.com/cloudera/cdk/tree/master/cdk-morphlines\n\nBy the way, a somewhat separate issue is that it seems to me that the ivy dependences for solr-morphlines-core and solr-morphlines-cell and solr-map-reduce are a bit backwards upstream in that currently solr-morphlines-core pulls in a ton of dependencies that it doesn't need, and those deps should rather be pulled in by the solr-map-reduce (which is a essentially an out-of-the-box app that bundles user level deps). Correspondingly, would be good to organize ivy and mvn upstream in such a way that \n\n\n\tsolr-map-reduce should depend on solr-morphlines-cell plus cdk-morphlines-all minus cdk-morphlines-solr-cell (now upstream) minus cdk-morphlines-solr-core (now upstream) plus xyz\n\tsolr-morphlines-cell should depend on solr-morphlines-core plus xyz\n\tsolr-morphlines-core should depend on cdk-morphlines-core plus xyz\n\n\n\nMore concretely, FWIW, to see how the deps look like in production releases downstream review the following POMs: \n\nhttps://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-core/pom.xml\n\nand\n\nhttps://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-solr-cell/pom.xml\n\nand\n\nhttps://github.com/cloudera/search/blob/master_1.1.0/search-mr/pom.xml "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13843496",
            "date": "2013-12-09T20:18:10+0000",
            "content": "wolfgang hoschek, I'm lost: what do you mean by \"upstream\"/\"downstream\"?  In my experience, \"upstream\" refers to a parent project, i.e. one from which the project in question is derived, and \"downstream\" is the child/derived project.  I don't know the history here, but you seem to be referring to the solr contribs when you say \"upstream\"?  If that's true, then my understanding of these terms is the opposite of how you're using them.  Maybe the question I should be asking is: what is/are the relationship(s) between/among cdk-morphlines-solr-* and solr-morphlines-*?\n\nAnd (I assume) relatedly, how how does cdk-morphlines-all relate to cdk-morphlines-solr-core/-cell? "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13843523",
            "date": "2013-12-09T20:31:36+0000",
            "content": "Apologies for the confusion. We are upstreaming cdk-morphlines-solr-cell into the solr contrib solr-morphlines-cell as well as cdk-morphlines-solr-core into the solr contrib solr-morphlines-core as well as search-mr into the solr contrib solr-map-reduce. Once the upstreaming is done these old modules will go away. Next, \"downstream\" will be made identical to \"upstream\" plus perhaps some critical fixes as necessary, and the upstream/downstream terms will apply in the way folks usually think about them, but we are not quite yet there today, but getting there...\n\ncdk-morphlines-all is simply a convenience pom that includes all the other morphline poms so there's less to type for users who like a bit more auto magic. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13843524",
            "date": "2013-12-09T20:32:07+0000",
            "content": "And (I assume) relatedly, how how does cdk-morphlines-all relate to cdk-morphlines-solr-core/-cell?\n\nI can answer this one myself from https://github.com/cloudera/cdk/blob/master/cdk-morphlines/cdk-morphlines-all/pom.xml: it's an aggregation-only module that depends on all of the cdk-morphlines-* modules. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13843827",
            "date": "2013-12-10T01:53:43+0000",
            "content": "I'm not aware of anything needing jersey except perhaps hadoop pulls that in.\n\nYeah, tests use this for running hadoop. "
        },
        {
            "author": "Gary Schulte",
            "id": "comment-13846975",
            "date": "2013-12-13T00:39:24+0000",
            "content": "FYI, a colleague and I just spent the better part of a week trying to get the latest 1301 patch against 4.6 working in our cdh 4.1.2 dev environment, and/or a local cdh 4.3 cluster.    \n\nWe discovered that while the indexing process itself worked and we could see the docs and index merges in the reducer output logs, we never actually ended up with anything in the data directories in hdfs for our shards.  \n\nPresumably, hadoop 2.0 is silently fails to do a distributed write when solr is using hdfs for a core's data directory.  After reverting SolrRecordWriter to the prior behavior of generating a local index and copying it to hdfs on completion, we were able to get MR indexing to work. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13846986",
            "date": "2013-12-13T00:46:22+0000",
            "content": "Sorry - latest patch is no good due to a bug. It was writing the data to the local filesystem. A lot has been committed beyond the last patch.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13846989",
            "date": "2013-12-13T00:47:52+0000",
            "content": "You need at least the commit above that talks about fixing where we set system properties in the solr record writer.  "
        },
        {
            "author": "Gary Schulte",
            "id": "comment-13846998",
            "date": "2013-12-13T00:58:20+0000",
            "content": "I am getting the same behavior from solr/contrib/map-reduce in http://svn.apache.org/repos/asf/lucene/dev/trunk \n\nI just verified I am able to reproduce this behavior on cdh 4.1.2 even after https://svn.apache.org/r1548600\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13847007",
            "date": "2013-12-13T01:06:36+0000",
            "content": "Strange - should work fine. If I run the github project above, it has the index in hdfs and they are merged to solr. It uses 5x from a couple days ago.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13847011",
            "date": "2013-12-13T01:11:11+0000",
            "content": "It's the \"fix a couple bugs around setting up embeddedsolrserver\" commit. Keep in mind your solrconfig will need to have the directoryFactory setup to be subbed by sys prop currently - as it is by default..  "
        },
        {
            "author": "Gary Schulte",
            "id": "comment-13847016",
            "date": "2013-12-13T01:15:58+0000",
            "content": "The example works fine for us also.  The reality is that we are still on java 1.6 for the most part and therefore can't use Solr 5.x.  All of our testing is with java 1.6 and lucene_solr_4_6. \n\nWe've tried using solr-mr with the 1301 patch against 4.6, as well as 'transplanting' contrib/map-reduce from trunk into the 4.6 branch.  Both yield the same behavior.  Indexing works, but the indexes never 'arrive' in hdfs.   \n\nPerhaps there is an issue with solr-core and hdfs that was addressed in trunk that we haven't picked up?  (due to our java 1.6 source restriction) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13847020",
            "date": "2013-12-13T01:25:25+0000",
            "content": "I'd bet the hdfs directory is not being set for some reason. I was seeing the same thing until that commit. Look around for an errant folder being created on the local fs that starts with hdfs.  "
        },
        {
            "author": "Gary Schulte",
            "id": "comment-13847024",
            "date": "2013-12-13T01:30:50+0000",
            "content": "The directoryFactory appears to have been the root of the issue.  We were adapting our local solrconfig for use in the embedded solr server and did not have :\n\n  <directoryFactory name=\"DirectoryFactory\"\n                    class=\"${solr.directoryFactory:solr.NRTCachingDirectoryFactory}\"/>\n\nin our setup.  In light of that, we can confirm it works on cdh 4.1.2.  Thx "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13847025",
            "date": "2013-12-13T01:34:27+0000",
            "content": "Thanks for closing the loop on that. That part is fragile - will be improved.  "
        },
        {
            "author": "Gary Schulte",
            "id": "comment-13848029",
            "date": "2013-12-13T22:56:14+0000",
            "content": "Some additional feedback, it would be convenient if we could ignore the underscore (\"_\") hidden files in hdfs as well as the \".\" hidden files when reading input files from hdfs.  \n\nWhen trying to index an AvroStorage directory created by Pig, we are having to send each part name individually because the job will fail if we pass the directory.  Passing the directory, we end up picking up \"_logs/*\", \"_SUCCESS\", etc - the corresponding avro morphlines map jobs fail.    "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13848097",
            "date": "2013-12-14T00:24:20+0000",
            "content": "Might be best to write a program that generates the list of files and then explicitly provide that file list to the MR job, e.g. via the --input-list option. For example you could use the HDFS version of the Linux file system 'find' command for that (HdfsFindTool doc and code here: https://github.com/cloudera/search/tree/master_1.1.0/search-mr#hdfsfindtool)\n "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13848775",
            "date": "2013-12-16T02:48:51+0000",
            "content": "it would be convenient if we could ignore the underscore (\"_\") hidden files in hdfs as well as the \".\" hidden files when reading input files from hdfs.\n\n+1 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13853071",
            "date": "2013-12-19T17:51:42+0000",
            "content": "Commit 1552381 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1552381 ]\n\nSOLR-1301: Update to Kite 0.10 from CDK 0.9 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13853163",
            "date": "2013-12-19T19:13:48+0000",
            "content": "Commit 1552398 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1552398 ]\n\nSOLR-1301: Merge Morphlines modules up to Kite 0.10 and CDK 0.9 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13855859",
            "date": "2013-12-23T19:11:22+0000",
            "content": "Commit 1553184 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1553184 ]\n\nSOLR-1301: Ignore this test on Windows - there is a problem with Windows paths and Morphlines. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13856340",
            "date": "2013-12-24T14:45:43+0000",
            "content": "Commit 1553281 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1553281 ]\n\nSOLR-1301: maven config: fix map-reduce test compilation problem by adding dependency on morphline-core's test jar "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-13856657",
            "date": "2013-12-25T20:38:27+0000",
            "content": "Also see https://issues.cloudera.org/browse/CDK-262 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13862719",
            "date": "2014-01-06T02:03:34+0000",
            "content": "Commit 1555647 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1555647 ]\n\nSOLR-1301: make debugging these tests a whole lot easier by sending map reduce job logging to std out "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13862737",
            "date": "2014-01-06T03:08:52+0000",
            "content": "Bah - I think the above commit only works on map reduce one as far as sending the logs to std out. Tried to write some code to do it for map reduce 2, but I have not been able to figure out how to programmaticly get the secret key to hash the request url for the http logs API. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13866723",
            "date": "2014-01-09T15:46:06+0000",
            "content": "Commit 1556846 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1556846 ]\n\nSOLR-1301: IntelliJ config: morphlines-cell Solr contrib needs lucene-core test-scope dependency "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872435",
            "date": "2014-01-15T19:06:51+0000",
            "content": "Commit 1558520 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558520 ]\n\nSOLR-1301: Add a Solr contrib that allows for building Solr indexes via Hadoop's MapReduce. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872441",
            "date": "2014-01-15T19:08:48+0000",
            "content": "Commit 1558522 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558522 ]\n\nSOLR-1301: Ivy likes to act funny if you don't declare compile and test resources in the same dependency. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872443",
            "date": "2014-01-15T19:10:35+0000",
            "content": "Commit 1558523 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558523 ]\n\nSOLR-1301: Fix compilation for Java 8 (the Java 8 compiler is more picky, but it's not a Java 8 regression: the code was just wrong) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872448",
            "date": "2014-01-15T19:13:24+0000",
            "content": "Commit 1558524 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558524 ]\n\nSOLR-1301: Fix windows problem with escaping of folder name (see crazy https://github.com/typesafehub/config/blob/master/HOCON.md for correct format: string must be quoted and escaped like javascript) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872450",
            "date": "2014-01-15T19:15:18+0000",
            "content": "Commit 1558525 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558525 ]\n\nSOLR-1301: Ignore windows tests that cannot work because they use UNIX semantics. Also remove a never-executed test which tests nothing "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872455",
            "date": "2014-01-15T19:19:34+0000",
            "content": "Commit 1558529 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558529 ]\n\nSOLR-1301: Ignore these tests on java 8 and j9 for now. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872460",
            "date": "2014-01-15T19:21:48+0000",
            "content": "Commit 1558533 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558533 ]\n\nSOLR-1301: remove unnecessary (POM-only) dependency org.apache.hadoop:hadoop-yarn-server "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872481",
            "date": "2014-01-15T19:31:16+0000",
            "content": "Commit 1558540 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558540 ]\n\nSOLR-1301: Straighten out module names so that they match current convention "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872485",
            "date": "2014-01-15T19:33:22+0000",
            "content": "Commit 1558541 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558541 ]\n\nSOLR-1301: Merge in latest solr-map-reduce updates. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872489",
            "date": "2014-01-15T19:35:47+0000",
            "content": "Commit 1558544 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558544 ]\n\nSOLR-1301: Merge in latest morphlines module updates. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872492",
            "date": "2014-01-15T19:37:20+0000",
            "content": "Commit 1558545 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558545 ]\n\nSOLR-1301: Clean up. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872497",
            "date": "2014-01-15T19:39:08+0000",
            "content": "Commit 1558547 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558547 ]\n\nSOLR-1301: ignore '.iml' in new Solr contribs' directories; put new Solr contribs' lib/ and test-lib/ directories under Subversion control; ignore '.jar' in these directories "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872500",
            "date": "2014-01-15T19:41:20+0000",
            "content": "Commit 1558548 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558548 ]\n\nSOLR-1301: Fix a couple of bugs around setting up the embedded Solr instance. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872505",
            "date": "2014-01-15T19:43:06+0000",
            "content": "Commit 1558551 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558551 ]\n\nSOLR-1301: Update to Morphlines 0.9.0 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872512",
            "date": "2014-01-15T19:45:34+0000",
            "content": "Commit 1558553 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558553 ]\n\nSOLR-1301: Update jar checksums for Morphlines 0.9.0 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872636",
            "date": "2014-01-15T21:39:06+0000",
            "content": "Commit 1558572 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558572 ]\n\nSOLR-1301: Update to Kite 0.10 from CDK 0.9 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872652",
            "date": "2014-01-15T21:57:58+0000",
            "content": "Commit 1558580 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558580 ]\n\nSOLR-1301: Merge Morphlines modules up to Kite 0.10 and CDK 0.9 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872654",
            "date": "2014-01-15T22:00:19+0000",
            "content": "Commit 1558582 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558582 ]\n\nSOLR-1301: Ignore this test on Windows - there is a problem with Windows paths and Morphlines. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872655",
            "date": "2014-01-15T22:02:07+0000",
            "content": "Commit 1558584 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558584 ]\n\nSOLR-1301: maven config: fix map-reduce test compilation problem by adding dependency on morphline-core's test jar "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872662",
            "date": "2014-01-15T22:04:22+0000",
            "content": "Commit 1558586 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558586 ]\n\nSOLR-1301: make debugging these tests a whole lot easier by sending map reduce job logging to std out "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872665",
            "date": "2014-01-15T22:05:38+0000",
            "content": "Commit 1558588 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558588 ]\n\nSOLR-1301: IntelliJ config: morphlines-cell Solr contrib needs lucene-core test-scope dependency "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872916",
            "date": "2014-01-16T01:29:33+0000",
            "content": "Commit 1558647 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558647 ]\n\nSOLR-1301: Move CHANGES entry to 4.7 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872957",
            "date": "2014-01-16T01:59:02+0000",
            "content": "Commit 1558670 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558670 ]\n\nSOLR-1301: Throw an error if HdfsDirectoryFactory is not configured for now. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13872959",
            "date": "2014-01-16T02:00:08+0000",
            "content": "Commit 1558671 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558671 ]\n\nSOLR-1301: Throw an error if HdfsDirectoryFactory is not configured for now. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13882522",
            "date": "2014-01-27T02:32:33+0000",
            "content": "Major performance issue that relates to this:\n\nSOLR-5667 Performance problem when not using hdfs block cache. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13898232",
            "date": "2014-02-11T19:52:47+0000",
            "content": "Commit 1567337 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1567337 ]\n\nSOLR-1301: Implement the set-map-reduce-classpath.sh script. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13898238",
            "date": "2014-02-11T19:57:12+0000",
            "content": "Commit 1567340 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1567340 ]\n\nSOLR-1301: Implement the set-map-reduce-classpath.sh script. "
        },
        {
            "author": "Christian Moen",
            "id": "comment-13901180",
            "date": "2014-02-14T07:31:12+0000",
            "content": "I've been reading through (pretty much all) the comments on this JIRA and I'd like to thank you all for the great effort you have put into this. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13901471",
            "date": "2014-02-14T14:39:01+0000",
            "content": "Commit 1568317 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1568317 ]\n\nSOLR-1301: Add some readme files. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13901474",
            "date": "2014-02-14T14:40:14+0000",
            "content": "Commit 1568318 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1568318 ]\n\nSOLR-1301: Add some readme files. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13901475",
            "date": "2014-02-14T14:41:09+0000",
            "content": "Plenty of comments to go through  Time to finally close this issue out. I'll file a new issue for some remaining work. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13901481",
            "date": "2014-02-14T14:44:58+0000",
            "content": "I filed SOLR-5729 for some additional work post 4.7. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13901516",
            "date": "2014-02-14T14:57:09+0000",
            "content": "Commit 1568328 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1568328 ]\n\nSOLR-1301: Fix spelling. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13901517",
            "date": "2014-02-14T14:57:57+0000",
            "content": "Commit 1568329 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1568329 ]\n\nSOLR-1301: Fix spelling. "
        },
        {
            "author": "rulinma",
            "id": "comment-13942720",
            "date": "2014-03-21T03:12:56+0000",
            "content": "mark. "
        }
    ]
}