{
    "id": "SOLR-6193",
    "title": "using facet.* parameters as local params inside of facet.field causes problems in distributed search",
    "details": {
        "affect_versions": "4.8.1",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "The distributed request logic for faceting (which has to clone&modify requests to individual shards for dealing with things like facet.mincount, facet.sort, facet.limit, & facet.offset so that the distributed aggregation is correct) doesn't properly take into account localparams contained in each of the facet params and how they should affect the initial shard requests and the subsequent refinement requests.\n\nInitial problem example reported by user\nWhen a distributed search contains multiselect faceting the per-field faceting options are not honored for alternate selections of the field. For example with a query like:\n\nfacet.field=blah&facet.field={!key myblah facet.offset=10}blah&f.blah.facet.offset=20\n\n\nThe returned facet results for both blah and myblah will use an offset of 20 as opposed to a standard search returning myblah with an offset of 10.",
    "attachments": {
        "bad_facet_offset_test_4_8_x.patch": "https://issues.apache.org/jira/secure/attachment/12652013/bad_facet_offset_test_4_8_x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "John Gibson",
            "id": "comment-14041050",
            "date": "2014-06-23T18:03:13+0000",
            "content": "Here's a patch for an existing unit test on the lucene_solr_4_8 branch that illustrates the issue.\n\nAlso note that while working on this I noticed that the facet.limit local parameter is ignored for both the regular and distributed versions of the search. Is that intentional? Or a bug? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14059391",
            "date": "2014-07-11T21:32:36+0000",
            "content": "The crux of the problem is that the distributed facet logic, and the the shard sub-requests are generated, pre-dates the support for using local params in facet.field and is built upon the previous work of using \"per-field\" overrides (ie: f.myFieldName.facet.mincount=5).  \n\nAs a result, from my quick review of the code, there seem to be 2 different types of mistakes that can pop up in logic for building the shard requests:\n\n\n\tnot recognizing at the local params when computing shard request values (ie: ignoring localparam facet.limit to decide what the overrequest values should be for a given field)\n\tpropogating the localparam values to the shards in addition to the syntheticly generated f.foo.param equivilent for the shard (ie: sending the original localparams which might include facet.mincount even when the distributed logic is trying to force a mincount of 0 for the initial \"top-N\" computation.\n\n\n\nAdding to the complications, is that off the top of my head, i can't remember what sort of decisions were made when the localparam support was added regarding the precidence between a general local param vs a top level per-field param \u2013 ie: what should the effective limit be here: f.foo.facet.limit=99&facet.field={!facet.limit=44}foo\n\n\u2014\n\nI think in general we should overhaul the way the distributed requests modify the per-field facet params to instead put all of those per field modifications directly in the local params of the shard requests \u2013 among other things, this will help eliminate collision in some of the computed facet params when faceting on the same field multiple ways.\n\nbefore we tackle this though, we need a lot more comprehensive tests for some of these more complex situations \u2013 beyond just the minimal distrib test that compares with the control collection.  We need to assert that we get the specific expect responses, otherwise we could break both the existing single-node behavior in a lot of cases and never notice as long as the distrib behavior breaks in the same way. "
        }
    ]
}