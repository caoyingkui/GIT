{
    "id": "SOLR-11450",
    "title": "ComplexPhraseQParserPlugin not running charfilter for some multiterm queries in 6.x",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "6.6.1",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "On the user list, Bjarke Mortensen reported that the charfilter is not being applied in PrefixQueries in the ComplexPhraseQParserPlugin in 6.x.  Bjarke fixed my proposed unit tests to prove this failure. All appears to work in 7.x and trunk. If there are plans to release a 6.6.2, let's fold this in.",
    "attachments": {
        "SOLR-11450-unit-test.patch": "https://issues.apache.org/jira/secure/attachment/12891065/SOLR-11450-unit-test.patch",
        "SOLR-11450.patch": "https://issues.apache.org/jira/secure/attachment/12891308/SOLR-11450.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-10-09T14:33:14+0000",
            "content": "This demonstrates the failure in fuzzy, prefix and range, but not in wildcard or reverse wildcard.\n\nIf you correct the expected to `numFound=1`, this test passes in 7.x and trunk. ",
            "author": "Tim Allison",
            "id": "comment-16197061"
        },
        {
            "date": "2017-10-09T20:12:51+0000",
            "content": "To confirm behavior at the Lucene level in 6.6.1, where \"mapping.txt\" contains:\n\n\n\"e\" => \"E\"\n\"i\" => \"I\"\n\"u\" => \"U\"\n\"o\" => \"O\"\n\n\n\n\n  @Test\n  public void testMultiterm() throws Exception {\n\n    Map<String, String> map = new HashMap<>();\n    map.put(\"mapping\", \"mapping.txt\");\n    Analyzer ucVowelAnalyzer = CustomAnalyzer.builder()\n            .addCharFilter(MappingCharFilterFactory.class, map)\n            .withTokenizer(\"whitespace\")\n            .build();\n\n    Directory rd = new RAMDirectory();\n    IndexWriter w = new IndexWriter(rd, newIndexWriterConfig(ucVowelAnalyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(defaultFieldName, \"quick brown\", Field.Store.YES));\n    w.addDocument(doc);\n    w.close();\n    IndexReader localReader = DirectoryReader.open(rd);\n    IndexSearcher localSearcher = newSearcher(localReader);\n\n    try {\n      assertHits(1, \"quick\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"quik~1\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"quick*\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"qui*k\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"[quicj TO quicl]\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"quick brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"\\\"quik~1 brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"\\\"quick* brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"\\\"qui*k brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(0, \"\\\"[quicj TO quicl] brown\\\"\", ucVowelAnalyzer, localSearcher);\n    } finally {\n      localReader.close();\n      rd.close();\n    }\n  }\n\n  public Query getQuery(String qString, Analyzer analyzer) throws Exception {\n    QueryParser qp = new ComplexPhraseQueryParser(defaultFieldName, analyzer);\n    return qp.parse(qString);\n\n  }\n  private void assertHits(int expectedDocHits, String queryString, Analyzer analyzer, IndexSearcher searcher) throws Exception {\n    Query q = getQuery(queryString, analyzer);\n    System.out.println(q + \" : \"+q.getClass());\n    TopDocs topDocs = searcher.search(q, expectedDocHits+10);\n    assertEquals(queryString, expectedDocHits, topDocs.scoreDocs.length);\n  }\n\n\n\nwhereas in 7.0.1, all tests pass:\n\n\n  @Test\n  public void testMultiterm() throws Exception {\n\n    Map<String, String> map = new HashMap<>();\n    map.put(\"mapping\", \"mapping.txt\");\n    Analyzer ucVowelAnalyzer = CustomAnalyzer.builder()\n            .addCharFilter(MappingCharFilterFactory.class, map)\n            .withTokenizer(\"whitespace\")\n            .build();\n\n    Directory rd = new RAMDirectory();\n    IndexWriter w = new IndexWriter(rd, newIndexWriterConfig(ucVowelAnalyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(defaultFieldName, \"quick brown\", Field.Store.YES));\n    w.addDocument(doc);\n    w.close();\n    IndexReader localReader = DirectoryReader.open(rd);\n    IndexSearcher localSearcher = newSearcher(localReader);\n\n    try {\n      assertHits(1, \"quick\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"quik~1\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"quick*\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"qui*k\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"[quicj TO quicl]\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"quick brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"quik~1 brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"quick* brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"qui*k brown\\\"\", ucVowelAnalyzer, localSearcher);\n      assertHits(1, \"\\\"[quicj TO quicl] brown\\\"\", ucVowelAnalyzer, localSearcher);\n    } finally {\n      localReader.close();\n      rd.close();\n    }\n  }\n\n ",
            "author": "Tim Allison",
            "id": "comment-16197629"
        },
        {
            "date": "2017-10-10T17:31:27+0000",
            "content": "This patch fixes the problem in 6.x.  I have no idea if there are any plans to release 6.6.2. ",
            "author": "Tim Allison",
            "id": "comment-16199035"
        },
        {
            "date": "2017-10-11T07:24:50+0000",
            "content": "Thanks Tim Allison, I have just confirmed that this indeed fixes the bug \n\nGreat work. I have already made a pull request for another issue (SOLR-10078) that we are also depending on for our Solr, so I would really like to see an official 6.6.2 release. Right now, we need to run our own Solr build. ",
            "author": "Bjarke Mortensen",
            "id": "comment-16199906"
        },
        {
            "date": "2017-10-12T20:25:50+0000",
            "content": "Michael McCandless Adrien Grand, any chance you'd be willing to review and push this into 6.6.2? ",
            "author": "Tim Allison",
            "id": "comment-16202573"
        },
        {
            "date": "2017-10-12T20:36:54+0000",
            "content": "To get the directionality right...sorry.  The issue I opened is a duplicate of LUCENE-7687...my error. ",
            "author": "Tim Allison",
            "id": "comment-16202585"
        },
        {
            "date": "2017-10-13T13:10:52+0000",
            "content": "Mikhail Khludnev or any other committer willing to review and push for 6.6.2?   ",
            "author": "Tim Allison",
            "id": "comment-16203541"
        },
        {
            "date": "2017-10-13T13:26:09+0000",
            "content": "The bug that you are describing was not fixed in 6.x on the classic query parser on purpose because it would be a breaking change to some users so I'm wondering whether we should fix this query parser in a bugfix release? 7.x doesn't have this issue because we replaced the lowercaseExpandedTerms option with the new Analyzer.normalize API which allows to selectively apply only the normalization components of the analysis chain (for instance lowercasing and accent folding but not stemming of tokenizing) which has the advantage of not only working with lowercasing but also accent folding, etc. ",
            "author": "Adrien Grand",
            "id": "comment-16203564"
        },
        {
            "date": "2017-10-13T13:40:02+0000",
            "content": "Adrien Grand, thank you for your response!\n\nY, the changes in 7.x are fantastic.\n\nAm I misunderstanding 6.x, though?  This test passes, which suggests that normalization was working correctly for the classic queryparser in 6.x, but not the cpqp.  Or am I misunderstanding?\n\nIf your point is that this would be a breaking change for some users of cpqp and it therefore doesn't belong in a bugfix release, I'm willing to accept that.\n\n\n  @Test\n  public void testCharFilter() {\n    assertU(adoc(\"iso-latin1\", \"craezy traen\", \"id\", \"1\"));\n    assertU(commit());\n    assertU(optimize());\n\n    assertQ(req(\"q\",  \"iso-latin1:cr\\u00E6zy\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:tr\\u00E6n\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:c\\u00E6zy~1\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:cr\\u00E6z*\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:*\\u00E6zy\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:cr\\u00E6*y\")\n            , \"//result[@numFound='1']\"\n            , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:/cr\\u00E6[a-z]y/\")\n        , \"//result[@numFound='1']\"\n        , \"//doc[./str[@name='id']='1']\"\n    );\n\n    assertQ(req(\"q\", \"iso-latin1:[cr\\u00E6zx TO cr\\u00E6zz]\")\n        , \"//result[@numFound='1']\"\n        , \"//doc[./str[@name='id']='1']\"\n    );\n}\n\n ",
            "author": "Tim Allison",
            "id": "comment-16203573"
        },
        {
            "date": "2017-10-13T14:13:48+0000",
            "content": "Maybe Solr's classic analyzer is doing more than Lucene's. When I run the following code:\n\n\nAnalyzer analyzer = CustomAnalyzer.builder()\n    .withTokenizer(StandardTokenizerFactory.class)\n    .addTokenFilter(LowerCaseFilterFactory.class)\n    .addTokenFilter(ASCIIFoldingFilterFactory.class)\n    .build();\nQueryParser qp = new QueryParser(\"field\", analyzer);\nQuery query = qp.parse(\"K\u00f6nig*\");\nSystem.out.println(query);\n\n\n\nI get field:k\u00f6nig*, so the ascii folding filter is definitely not applied. Is Solr maybe using AnalyzingQueryParser? ",
            "author": "Adrien Grand",
            "id": "comment-16203615"
        },
        {
            "date": "2017-10-13T16:14:36+0000",
            "content": "Ha.  Right.  Solr does do its own thing.  FieldTypePluginLoader generates a multiterm analyzer in the TextField by subsetting the TokenizerChain's components that are MultitermAware and/or swapping in a KeywordAnalyzer --here ...almost like Analyzer.normalize() in 7.x \n\nThen SolrQueryParserBase has an analyzeIfMultiTermText here, which in turn calls TextField's analyzeMultiTerm with TextField's multitermanalyzer that was built back in the FieldTypePluginLoader above.\n\nSo, in Solr 6.x, the basic QueryParser relies on the SolrQueryParserBase and all is good.  However, the CPQP doesn't extend the SolrQueryParserBase.  \n\nTwo things make this feel like a bug and not a feature in Solr 6.x:\n\n1) multiterm analysis works for the classic query parser but not fully for the CPQP in Solr 6.x\n2) multiterm analysis works for CPQP for some multiterms (wildcard/reverse wildcard) and range, but not in the other multiterms: prefix, regex and fuzzy. ",
            "author": "Tim Allison",
            "id": "comment-16203778"
        },
        {
            "date": "2017-10-13T16:42:57+0000",
            "content": "I see. Yes so it looks like something that can get fixed in 6.6.2 in the end. Thanks for explaining. I'm not familiar enough with Solr query parsers to do a proper review but if we can find someone who understands these things better than me, I have no objection to getting it committed. Jan H\u00f8ydahl Maybe you can review it? ",
            "author": "Adrien Grand",
            "id": "comment-16203821"
        },
        {
            "date": "2017-10-13T16:50:12+0000",
            "content": "I'm not familiar enough with Solr query parsers\n\nY, I've been away from this for too long and got the first couple of answers to Bjarke Mortensen wrong on the user list because of the diff btwn Lucene and Solr.  It is good to be back.\n\nThank you! ",
            "author": "Tim Allison",
            "id": "comment-16203840"
        },
        {
            "date": "2017-10-13T21:38:57+0000",
            "content": "I'll pass on this I'm afraid. ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16204233"
        }
    ]
}