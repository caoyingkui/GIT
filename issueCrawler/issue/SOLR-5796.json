{
    "id": "SOLR-5796",
    "title": "With many collections, leader re-election takes too long when a node dies or is rebooted, leading to some shards getting into a \"conflicting\" state about who is the leader.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.7.1",
            "4.8",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I'm doing some testing with a 4-node SolrCloud cluster against the latest rev in branch_4x having many collections, 150 to be exact, each having 4 shards with rf=3, so 450 cores per node. Nodes are decent in terms of resources: -Xmx6g with 4 CPU - m3.xlarge's in EC2.\n\nThe problem occurs when rebooting one of the nodes, say as part of a rolling restart of the cluster. If I kill one node and then wait for an extended period of time, such as 3 minutes, then all of the leaders on the downed node (roughly 150) have time to failover to another node in the cluster. When I restart the downed node, since leaders have all failed over successfully, the new node starts up and all cores assume the replica role in their respective shards. This is goodness and expected.\n\nHowever, if I don't wait long enough for the leader failover process to complete on the other nodes before restarting the downed node, \nthen some bad things happen. Specifically, when the dust settles, many of the previous leaders on the node I restarted get stuck in the \"conflicting\" state seen in the ZkController, starting around line 852 in branch_4x:\n\n\n852       while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n853         if (tries == 60) \nUnknown macro: {854           throw new SolrException(ErrorCode.SERVER_ERROR,855               \"There is conflicting information about the leader of shard} \n859         Thread.sleep(1000);\n860         tries++;\n861         clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n862             timeoutms);\n863         leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n864             .getCoreUrl();\n865       }\n\nAs you can see, the code is trying to give a little time for this problem to work itself out, 1 minute to be exact. Unfortunately, that doesn't seem to be long enough for a busy cluster that has many collections. Now, one might argue that 450 cores per node is asking too much of Solr, however I think this points to a bigger issue of the fact that a node coming up isn't aware that it went down and leader election is running on other nodes and is just being slow. Moreover, once this problem occurs, it's not clear how to fix it besides shutting the node down again and waiting for leader failover to complete.\n\nIt's also interesting to me that /clusterstate.json was updated by the healthy node taking over the leader role but the /collections/<coll>leaders/shard# was not updated? I added some debugging and it seems like the overseer queue is extremely backed up with work.\n\nMaybe the solution here is to just wait longer but I also want to get some feedback from the community on other options? I know there are some plans to help scale the Overseer (i.e. SOLR-5476) so maybe that helps and I'm trying to add more debug to see if this is really due to overseer backlog (which I suspect it is).\n\nIn general, I'm a little confused by the keeping of leader state in multiple places in ZK. Is there any background information on why we have leader state in /clusterstate.json and in the leader path znode?\n\nAlso, here are some interesting side observations:\n\na. If I use rf=2, then this problem doesn't occur as leader failover happens more quickly and there's less overseer work? \nMay be a red herring here, but I can consistently reproduce it with RF=3, but not with RF=2 ... suppose that is because there are only 300 cores per node versus 450 and that's just enough less work to make this issue work itself out.\n\nb. To support that many cores, I had to set -Xss256k to reduce the stack size as Solr uses a lot of threads during startup (high point was 800'ish)              \nMight be something we should recommend on the mailing list / wiki somewhere.",
    "attachments": {
        "SOLR-5796.patch": "https://issues.apache.org/jira/secure/attachment/12632300/SOLR-5796.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Timothy Potter",
            "id": "comment-13916521",
            "date": "2014-02-28T22:43:51+0000",
            "content": "Little more to this for today ...\n\nI tried this on more powerful nodes (m3.2xlarge) and changed the wait to tries == 180 (instead of 60) and viola, the restarted node came back as expected. This begs the question whether we should make that wait period configurable for those installations that have many collections in a cluster? To be clear, I'm referring to the wait period in ZkController, while loop starting around line 852 (see above). I'd prefer to have something more deterministic vs. an upper limit on waiting as that seems like a ticking time bomb in a busy cluster. I'm going to try a few more ideas out over the weekend. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13916575",
            "date": "2014-02-28T23:35:07+0000",
            "content": "Cool - recently saw a user post a problem with this conflicting state - glad to see you already have a jump on it  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13916579",
            "date": "2014-02-28T23:38:12+0000",
            "content": "\nThis begs the question whether we should make that wait period configurable for those installations that have many collections in a cluster? To be clear, I'm referring to the wait period in ZkController, while loop starting around line 852 (see above).\n\n\n\nI have no qualms with making any timeouts configurable, but it seems the defaults should be fairly high as well - we want to work out of the box with most reasonable setups if possible.\n\n+1 to making it configurable, but let's also crank it up. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13918184",
            "date": "2014-03-03T15:52:30+0000",
            "content": "Ok, sounds good. Also, let's assume that the timeout resolves this for most cores, but a couple stragglers on the end still end up in this state. I assume the solution there would be to send a core reload to those specific cores, which should re-run all the startup recovery logic. If so, I think a simple utility app that uses CloudSolrServer to find all the angry cores and reload them would be useful, which I'll also work on. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13918249",
            "date": "2014-03-03T16:47:57+0000",
            "content": "Quick and dirty patch (for trunk) that introduces this as a ConfigSolrXml property (sibling to leaderVoteWait). However, getting this property passed along to where it's eventually needed in ZkController is a bit ugly. Specifically,\n\na. default value (DEFAULT_LEADER_CONFLICT_RESOLVE_WAIT) in ConfigSolr is private (did this to match the default for leaderVoteWait). This makes it hard to not break signatures on public methods, such as ZkContainer#initZooKeeper, because this class doesn't have access to the default value.\n\nb. ZkControllerTest doesn't have access to the default value either for constructing a ZkController; so I've done something ugly and used a scalar value of 60000 in the test directly\n\nIt seems like other config properties may be needed in the future, so I'm wondering if it's better to just make the ConfigSolr object available from the CoreContainer, which means adding: \n\npublic ConfigSolr getConfig() {\n  return cfg;\n}\n\nto the CoreContainer object. Is this a bad idea? If not, then ZkController can just pull this setting straight from the ConfigSolr object via the ref to CoreContainer.\n\nOr, maybe adding props like this is a rare-enough activity that I'm over-thinking this and introducing a new property into the public method / ctor signatures is OK? "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13919529",
            "date": "2014-03-04T15:39:30+0000",
            "content": "Hi Mark,\n\nSo another thing I'm curious about is why does leader failover take so long here? Even without increasing the wait period, 60 seconds seems like a long time for a new leader to be elected esp when it's failing over to a healthy node. I get technically why it takes so long (propagating the /clusterstate.json change around the cluster), so I guess my question is what we can do to speed that up? Is SOLR-5476 (scaling the overseer) the solution here? Just wanted to get your thoughts on that as well.\n\nThanks. Tim "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13919537",
            "date": "2014-03-04T15:45:12+0000",
            "content": "Honestly, I don't know why it's taking so long. We should get to the bottom of it. \n\nPropagating the clusterstate.json on a fast network should not take long at all.\n\nZooKeeper should be apple to handle a very high rate of updates and reads - I'm not even sure why the Overseer is so slow at publishing state.\n\nWe can find out though  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13919540",
            "date": "2014-03-04T15:47:29+0000",
            "content": "When a leader tries to take over, it first tries to sync with the rest of the shard - we should look at how long this is taking. There are a variety of places we might be able to start logging time intervals to help get an idea how long various things take. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-13920645",
            "date": "2014-03-05T08:19:01+0000",
            "content": "Doesn't Overseer throttle updates? It used to be a needless 1.5s before, but even now it does sleep 100ms between batches of updates I think (have to confirm with code). Maybe that's enough to slow things down enough in this case? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13920995",
            "date": "2014-03-05T16:06:42+0000",
            "content": "AFAIK, Noble and I changed that so that if the queue has the items, the overseer will plow through them. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13921286",
            "date": "2014-03-05T19:37:03+0000",
            "content": "Let's spin off the performance issue into a new JIRA issue - I'll wrap this one up. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13921295",
            "date": "2014-03-05T19:41:52+0000",
            "content": "Commit 1574638 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1574638 ]\n\nSOLR-5796: Increase how long we are willing to wait for a core to see the ZK advertised leader in it's local state.\nSOLR-5796: Make how long we are willing to wait for a core to see the ZK advertised leader in it's local state configurable. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13921321",
            "date": "2014-03-05T19:58:34+0000",
            "content": "Commit 1574641 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1574641 ]\n\nSOLR-5796: Increase how long we are willing to wait for a core to see the ZK advertised leader in it's local state.\nSOLR-5796: Make how long we are willing to wait for a core to see the ZK advertised leader in it's local state configurable. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13921411",
            "date": "2014-03-05T21:12:42+0000",
            "content": "Commit 1574664 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1574664 ]\n\nSOLR-5796: Fix illegal API call to format. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13921470",
            "date": "2014-03-05T21:50:33+0000",
            "content": "Commit 1574682 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1574682 ]\n\nSOLR-5796: Fix illegal API call to format. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13922711",
            "date": "2014-03-06T16:28:44+0000",
            "content": "I'm still a little concern about a couple of things:\n\n1) why is the leader \"state\" stored in two places in ZooKeeper (/clusterstate.json and /collections/<coll>/leaders/shard#)? I'm sure there is a good reason for this but can't see why \n\n2) if the timeout still occurs (as we don't want to wait forever), can't the node with the conflict just favor what's in the leader path assuming that replica is active and agrees? In other words, instead of throwing an exception and then just ending up a \"down\" state, why can't the replica seeing the conflict just go with what ZooKeeper says?\n\nI'm digging into leader failover timing / error handling today.\n\nThanks. Tim "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13923209",
            "date": "2014-03-06T22:55:15+0000",
            "content": "\n\t1: At one time, the leader was not actually in the cluster state I think...anyway, we do make use of it in one case in recovery where we make sure we have an update cloudstate that includes the latest leader for the shard. The ZK node is always up to date, clusterstate info can be stale. That is the main difference.\n\n\n\n\n\t2: Perhaps - if the clusterstate won't update though, seems like something is probably fairly wrong and we may want to favor not claiming we are active.\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13924889",
            "date": "2014-03-08T14:59:15+0000",
            "content": "Lets roll out a perf investigation and other concerns into new issues. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13934039",
            "date": "2014-03-13T20:53:38+0000",
            "content": "Just wanted to add a final comment here that it helps immensely to restart the node that is hosting the overseer last as part of a rolling restart. It's very subtle but there's a chance that each restart requires the overseer to need to be failed-over to a node each time you do a rolling restart. Consider 4 nodes: node1, node2, node3, node4. If the overseer is on node1, if I started my rolling restart using sequence: 2,3,4,1, then the overseer only needs to migrate once. This seems to really help stability during a rolling restart of the cluster (for obvious reasons). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13937957",
            "date": "2014-03-17T16:06:06+0000",
            "content": "Do we have a JIRA issue for the instability you mention in the failover? I can guess what it is... but we should track it and harden it. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13945261",
            "date": "2014-03-24T15:53:38+0000",
            "content": "Mark Miller, Timothy Potter, any reason not to backport this to 4.7.1? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13946064",
            "date": "2014-03-25T02:59:23+0000",
            "content": "Commit 1581195 from Steve Rowe in branch 'dev/branches/lucene_solr_4_7'\n[ https://svn.apache.org/r1581195 ]\n\nSOLR-5796: Increase how long we are willing to wait for a core to see the ZK advertised leader in it's local state.\nSOLR-5796: Make how long we are willing to wait for a core to see the ZK advertised leader in it's local state configurable. \nSOLR-5796: Fix illegal API call to format. (merged branch_4x revisions r1574641 and r1574682) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13946068",
            "date": "2014-03-25T03:00:07+0000",
            "content": "Commit 1581196 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1581196 ]\n\nSOLR-5796: move CHANGES.txt entries to 4.7.1 section "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13946069",
            "date": "2014-03-25T03:01:25+0000",
            "content": "Commit 1581198 from Steve Rowe in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1581198 ]\n\nSOLR-5796: move CHANGES.txt entries to 4.7.1 section (merged trunk r1581196) "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13957731",
            "date": "2014-04-02T15:03:36+0000",
            "content": "Bulk close 4.7.1 issues "
        },
        {
            "author": "Rich Mayfield",
            "id": "comment-13963658",
            "date": "2014-04-09T00:32:27+0000",
            "content": "I ran into this problem also (we have upwards of 1000 cores in each replica).\n\nI greatly appreciate the fix and the advice about rolling the overseer last. Can you point me to any other recommendations and caveats for doing a rolling restart?\n "
        }
    ]
}