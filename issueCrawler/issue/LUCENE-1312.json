{
    "id": "LUCENE-1312",
    "title": "InstantiatedIndexReader does not implement getFieldNames properly",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/other"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.4"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Causes error in org.apache.lucene.index.SegmentMerger.mergeFields",
    "attachments": {
        "lucene-1312.patch": "https://issues.apache.org/jira/secure/attachment/12384442/lucene-1312.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-06-22T01:16:14+0000",
            "content": "lucene-1312.patch\n\nFixed this bug and one related to termenum with no term.  These made SegmentMerger fail.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12607056"
        },
        {
            "date": "2008-06-22T06:18:49+0000",
            "content": "lucene-1312.patch\n\nA few additional updates related to deleted docs in InstantiatedIndexReader ",
            "author": "Jason Rutherglen",
            "id": "comment-12607064"
        },
        {
            "date": "2008-06-22T20:44:58+0000",
            "content": "Hi Jason!\n\nFixed this bug and one related to termenum with no term. These made SegmentMerger fail.\n\nCan you please supply a test case that demonstrate SegmentMerger failing? Your next() in InstantiatedTermEnum() changes the behaviour InstantiatedIndexReader#terms() compared to IndexReader#terms() and makes the index comparation test to to fail:\n\n\njunit.framework.AssertionFailedError: expected:<a:0> but was:<a:1>\n\tat org.apache.lucene.store.instantiated.TestIndicesEquals.testEquals(TestIndicesEquals.java:244)\n\n\n\nInstantiatedIndex#fieldSettingsByFieldName that getFieldNames(FieldOption) seem to only be updated by InstantiatedIndexWriter and not when populated by InstantiatedIndex(IndexReader).\n\nCan you please supply test cases that demonstrate getFieldNames(FieldOption) works with both index population strategies? \n\nI think you can factor out the FieldSetting class from InstantiatedIndexWriter as it now is used by InstantiatedIndex and InstantiatedIndexReader too.\n\nA few additional updates related to deleted docs in InstantiatedIndexReader\n\nThis looks good. I noticed that TestIndicesEquals does not actually delete any documents and make sure the indices still equals. I can fix that.\n\n\nAlso, please try not to reformat the code, it makes it harder to see the important changes. \n\nThanks! ",
            "author": "Karl Wettin",
            "id": "comment-12607117"
        },
        {
            "date": "2008-06-23T15:37:11+0000",
            "content": "lucene-1312.patch\n\nThe problem with TermEnum is term() needed: \n\nif (term == null) return null;\n\nThat fixed the issue and next has been removed from InstantiatedTermEnum(InstantiatedIndexReader reader).\n\nWill work on the rest when I have time. ",
            "author": "Jason Rutherglen",
            "id": "comment-12607263"
        },
        {
            "date": "2008-06-23T15:54:34+0000",
            "content": "lucene-1312.patch\n\nAdded the code to InstantiatedIndex(IndexReader sourceIndexReader, Set<String> fields) to create the fieldsetting map.  Separated FieldSetting into it's own class.  TestIndicesEquals worked. ",
            "author": "Jason Rutherglen",
            "id": "comment-12607272"
        },
        {
            "date": "2008-06-24T01:49:38+0000",
            "content": "Thanks for the updated patch Jason!\n\nI worked a bit on it:\n\n\n\tFactored out the writer specific field settings (omitnorms, binary, etc)  and introduced an extention in the writer.\n\tAdded test cases for deleting documents and comparing field options in TestIndiciesEquals.\n\tFixed bug in index writer that did not create field settings for non indexed non stored fields, identified by the new test in previous point.\n\tIntroduced new class FieldNames that actually merges the field options. The previous patch just copied the most recent field setting from the writer, thus changing a setting  from true to false in some cases.\n\n\n\nI think that was all. I'll be committing this soon pending any comments. ",
            "author": "Karl Wettin",
            "id": "comment-12607441"
        },
        {
            "date": "2008-06-24T02:14:32+0000",
            "content": "Will be interesting to try out your new patch.  The previous patch (may not be related to InstantiatedIndex though) is still yielding some errors in SegmentMerger.  It would be good to have a test using IndexWriter.addIndexes(IndexReader[] readers) simply passing in one InstantiatedIndexReader.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12607445"
        },
        {
            "date": "2008-06-24T16:57:26+0000",
            "content": "Will be interesting to try out your new patch.\n\nIt's right there in the issue, as the top patch. ; )\n\nThe previous patch (may not be related to InstantiatedIndex though) is still yielding some errors in SegmentMerger. It would be good to have a test using IndexWriter.addIndexes(IndexReader[] readers) simply passing in one InstantiatedIndexReader.\n\nFeel free to add such a test to the patch. If not I'll look in to it next time I sit down with it.\n ",
            "author": "Karl Wettin",
            "id": "comment-12607683"
        },
        {
            "date": "2008-06-24T17:16:25+0000",
            "content": "The error I was seeing in SegmenMerger was related to how InstantiatedIndex keeps Document in memory, so any editting of the Document after the Document is returned by InstantiatedIndexReader later messes up SegmentMerger.  Tried out the patch and it worked for me. ",
            "author": "Jason Rutherglen",
            "id": "comment-12607686"
        },
        {
            "date": "2008-06-25T00:10:16+0000",
            "content": "The error I was seeing in SegmenMerger was related to how InstantiatedIndex keeps Document in memory, so any editting of the Document after the Document is returned by InstantiatedIndexReader later messes up SegmentMerger.\n\nDo you still have the exception? Can you paste it here?\n\nIt's a bit dangerous to go modifying the Document instances returned by InstantiatedIndexReader unless you are really sure of what you are doing. The documents are the actual index document instances and not a deserialized clones as when using the Directory implementations. \n\nIf you delete a field of such a document it's gone from the index. If you add a new field not previously known in the index, it will not be in sync with field options and you'll probably see strange side effects when merging with other indices, et c.  \n\nThis could also be seen as a great feature that was lost in documentation. One of my favorites is to store the domain object(s) as Fieldable in the document(s) representing them.\n\nI'll add an appropriate comment about this in the javadocs.\n ",
            "author": "Karl Wettin",
            "id": "comment-12607813"
        },
        {
            "date": "2008-06-25T01:18:35+0000",
            "content": "I have an exception but it's different and not sure what it's related to.  I need more debug code to see the details, if I can reproduce it.  I am assuming it is related to InstantiatedIndexReader, given it would be difficult to make happen with the regular IndexReader code.  Fails on the last line of the code given.  Probably something to do with InstantiatedIndexReader and deleted docs differing somehow with other data SegmentMerger is obtaining.\n\nException:\n\norg.apache.lucene.index.CorruptIndexException: doc counts differ for segment _0: fieldsReader shows 5 but segmentInfo shows 20\n\tat org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:322)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:267)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:235)\n\tat org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:90)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)\n\tat org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:97)\n\tat org.apache.lucene.index.IndexReader.open(IndexReader.java:213)\n\tat org.apache.lucene.index.IndexReader.open(IndexReader.java:209)\n\nCode:\n\nRAMDirectory ramDirectory = new RAMDirectory();\nIndexWriter indexWriter = new IndexWriter(ramDirectory, false, system.getDefaultAnalyzer(), true);\nindexWriter.setMergeScheduler(new SerialMergeScheduler());\nindexWriter.setUseCompoundFile(true);\nindexWriter.addIndexes(indexReaders);\nindexWriter.close();\nDirectory.copy(ramDirectory, directory, true);\ninitialIndexReader = IndexReader.open(directory, indexDeletionPolicy);\n\n\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12607828"
        },
        {
            "date": "2008-06-25T01:29:11+0000",
            "content": "In order to simulate a different IndexReader per update using InstantiatedIndexReader  I wrote the following code.  There must be some flaws in it as it keeps on causing errors in SegmentMerger.  I am overriding deleted docs and max doc.  Also the latest error, which I am sure is probably somehow fixable.\n\n\npublic class OceanInstantiatedIndexReader extends InstantiatedIndexReader {\n  private int maxDoc;\n  private Set<Integer> deletedDocs;\n  \n  public OceanInstantiatedIndexReader(int maxDoc, InstantiatedIndex index, Set<Integer> deletedDocs) {\n    super(index);\n    this.maxDoc = maxDoc;\n    this.deletedDocs = deletedDocs;\n  }\n  \n  public int maxDoc() {\n    return maxDoc;\n  }\n  \n  public int numDocs() {\n    return maxDoc() - deletedDocs.size();\n  }\n  \n  public boolean isDeleted(int n) {\n    if (n >= maxDoc) return true;\n    if (deletedDocs != null && deletedDocs.contains(n)) return true;\n    return false;\n  }\n  \n  public boolean hasDeletions() {\n    return true;\n  }\n}\n\n\n\n\njava.lang.ArrayIndexOutOfBoundsException\nat java.lang.System.arraycopy(Native Method)\nat org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:276)\nat org.apache.lucene.index.SegmentMerger.mergeNorms(SegmentMerger.java:693)\nat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:136)\nat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:111)\nat org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3045)\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12607833"
        },
        {
            "date": "2008-06-25T01:58:57+0000",
            "content": "Because the patch looks like a mess with the various changes I copied and pasted the alterations to InstantiatedIndexReader to allow the following code which works.  Basically saving a copy of normsByFieldNameAndDocumentNumber into the OceanInstantiatedIndexReader fixes the problem.  \n\n\nprotected Map<String,List<NormUpdate>> updatedNormsByFieldNameAndDocumentNumber = null;\n\n  public static class NormUpdate {\n    public int doc;\n    public byte value;\n\n    public NormUpdate(int doc, byte value) {\n      this.doc = doc;\n      this.value = value;\n    }\n  }\n\n\n\n\npublic class OceanInstantiatedIndexReader extends InstantiatedIndexReader {\n  private int maxDoc;\n  private Set<Integer> deletedDocs;\n  private Map<String,byte[]> normsByFieldNameAndDocumentNumber;\n\n  public OceanInstantiatedIndexReader(int maxDoc, InstantiatedIndex index, Set<Integer> deletedDocs) {\n    super(index);\n    this.maxDoc = maxDoc;\n    this.deletedDocs = deletedDocs;\n    normsByFieldNameAndDocumentNumber = new HashMap<String,byte[]>(index.getNormsByFieldNameAndDocumentNumber());\n  }\n\n  public int maxDoc() {\n    return maxDoc;\n  }\n\n  protected void doSetNorm(int doc, String field, byte value) throws IOException {\n    if (updatedNormsByFieldNameAndDocumentNumber == null) {\n      updatedNormsByFieldNameAndDocumentNumber = new HashMap<String,List<NormUpdate>>(normsByFieldNameAndDocumentNumber.size());\n    }\n    List<NormUpdate> list = updatedNormsByFieldNameAndDocumentNumber.get(field);\n    if (list == null) {\n      list = new LinkedList<NormUpdate>();\n      updatedNormsByFieldNameAndDocumentNumber.put(field, list);\n    }\n    list.add(new NormUpdate(doc, value));\n  }\n\n  public byte[] norms(String field) throws IOException {\n    byte[] norms = normsByFieldNameAndDocumentNumber.get(field);\n    if (updatedNormsByFieldNameAndDocumentNumber != null) {\n      norms = norms.clone();\n      List<NormUpdate> updated = updatedNormsByFieldNameAndDocumentNumber.get(field);\n      if (updated != null) {\n        for (NormUpdate normUpdate : updated) {\n          norms[normUpdate.doc] = normUpdate.value;\n        }\n      }\n    }\n    return norms;\n  }\n\n  public void norms(String field, byte[] bytes, int offset) throws IOException {\n    byte[] norms = normsByFieldNameAndDocumentNumber.get(field);\n    System.arraycopy(norms, offset, bytes, 0, norms.length);\n  }\n\n  public int numDocs() {\n    return maxDoc() - deletedDocs.size();\n  }\n\n  public boolean isDeleted(int n) {\n    if (n >= maxDoc)\n      return true;\n    if (deletedDocs != null && deletedDocs.contains(n))\n      return true;\n    return false;\n  }\n\n  public boolean hasDeletions() {\n    return true;\n  }\n}\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12607838"
        },
        {
            "date": "2008-06-26T15:17:10+0000",
            "content": "Because the patch looks like a mess with the various changes I copied and pasted the alterations to InstantiatedIndexReader to allow the following code which works. Basically saving a copy of normsByFieldNameAndDocumentNumber into the OceanInstantiatedIndexReader fixes the problem.\n\nI haven't had time to look in to why you need to extend the code, but notice that InstantiatedIndexWriter creates a new byte[] on commit so your copy of the norms will be out of sync with the index unless you also update it at that point. There might be more to this than I can think of right now.\n\nAny other problems with the patch? I'm ready to commit it. ",
            "author": "Karl Wettin",
            "id": "comment-12608466"
        },
        {
            "date": "2008-06-26T16:03:53+0000",
            "content": "The byte[] stuff seems ok.  \n\nIs there an easy way to add a InstantiatedIndexWriter.addIndexes(IndexReader[] readers) method?  Seems doable with the InstantiatedIndex(IndexReader sourceIndexReader) constructor however I want to me able to merge another IndexReader in.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12608477"
        },
        {
            "date": "2008-06-26T16:15:36+0000",
            "content": "Is there an easy way to add a InstantiatedIndexWriter.addIndexes(IndexReader[] readers) method? Seems doable with the InstantiatedIndex(IndexReader sourceIndexReader) constructor however I want to me able to merge another IndexReader in.\n\nIt's doable. The simplest solution I can think of is to reconstruct all the documents in one single enumeration of the source index and then add them to the writer. I'm however not certain this is the best way nor if InstantiatedIndexWriter is the place for the code. \n\nI think it should be discussed in a new issue. ",
            "author": "Karl Wettin",
            "id": "comment-12608479"
        },
        {
            "date": "2008-06-28T17:26:53+0000",
            "content": "Committed in revision 672556.\n\nThanks Jason! ",
            "author": "Karl Wettin",
            "id": "comment-12609028"
        }
    ]
}