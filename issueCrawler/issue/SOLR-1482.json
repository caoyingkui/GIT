{
    "id": "SOLR-1482",
    "title": "Solr master and slave freeze after query",
    "details": {
        "affect_versions": "1.4",
        "status": "Resolved",
        "fix_versions": [],
        "components": [],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Cannot Reproduce"
    },
    "description": "We're having issues with the deployment of 2 master-slave setups.\n\nOne of the master-slave setups is OK (so far) but on the other both the master and the slave keep freezing, but only after I send a query to them. And by freezing I mean indefinite hanging, with almost no output to log, no errors, nothing. It's as if there's some sort of a deadlock. The hanging servers need to be killed with -9, otherwise they keep hanging.\n\nThe query I send queries all instances at the same time using the ?shards= syntax.\n\nOn the slave, the logs just stop - nothing shows up anymore after the query is issued. On the master, they're a bit more descriptive. This information seeps through very-very slowly, as you can see from the timestamps:\n\nSEVERE: java.lang.OutOfMemoryError: PermGen space\n\nOct 1, 2009 2:16:00 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.OutOfMemoryError: PermGen space\n\nOct 1, 2009 2:19:37 PM org.apache.catalina.connector.CoyoteAdapter service\nSEVERE: An exception or error occurred in the container during the request processing\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:19:37 PM org.apache.coyote.http11.Http11Processor process\nSEVERE: Error processing request\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:19:39 PM org.apache.catalina.connector.CoyoteAdapter service\nSEVERE: An exception or error occurred in the container during the request processing\njava.lang.OutOfMemoryError: PermGen space\nException in thread \"ContainerBackException in thread \"pool-29-threadOct 1, 2009 2:21:47 PM org.apache.catalina.connector.CoyoteAdapter service\nSEVERE: An exception or error occurred in the container during the request processing\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\n-22\" java.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\nException in thread \"http-8080-42\" Oct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\nOct 1, 2009 2:21:47 PM org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler process\nSEVERE: Error reading request, ignored\njava.lang.OutOfMemoryError: PermGen space\nException in thread \"http-8080-26\" Exception in thread \"http-8080-32\" Exception in thread \"http-8080-25\" Exception in thread \"http-8080-22\" Exception in thread \"http-8080-15\" Exception in thread \"http-8080-45\" Exception in thread \"http-8080-13\" Exception in thread \"http-8080-48\" Exception in thread \"http-8080-7\" Exception in thread \"http-8080-38\" Exception in thread \"http-8080-39\" Exception in thread \"http-8080-28\" Exception in thread \"http-8080-1\" Exception in thread \"http-8080-2\" Exception in thread \"http-8080-12\" Exception in thread \"http-8080-44\" Exception in thread \"http-8080-47\" Exception in thread \"http-8080-29\" Exception in thread \"http-8080-33\" Exception in thread \"http-8080-27\" Exception in thread \"http-8080-36\" Exception in thread \"http-8080-113\" Exception in thread \"http-8080-112\" Exception in thread \"http-8080-37\" Exception in thread \"http-8080-18\" java.lang.OutOfMemoryError: PermGen space\njava.lang.OutOfMemoryError: PermGen space\njava.lang.OutOfMemoryError: PermGen space\njava.lang.OutOfMemoryError: PermGen space\njava.lang.OutOfMemoryError: PermGen space\nException in thread \"http-8080-34\" java.lang.OutOfMemoryError: PermGen space\njava.lang.OutOfMemoryError: PermGen space\nException in thread \"http-8080-103\"\n\nSo the problem seems to be related to PermGen space. I found http://www.nabble.com/Number-of-webapps-td22198080.html and tried -XX:MaxPermSize=256m, but it didn't fix the problem. The current CATALINA_OPTS looks like this:\n\nexport CATALINA_OPTS=\"-XX:MaxPermSize=256m -Xmx6500m -XX:+UseConcMarkSweepGC\"\n\nIs the only solution at this point going multicore, as Noble suggested (is Noble your first name? I always assumed it was Paul and Noble was part of the nickname)? Will multicore get rid of the problem, before we spend time looking at it? For multicore, will the existing data dirs be compatible or would a complete reindex be needed? \n\nI'm willing to provide any information to you guys, just not sure what at the moment. I'm also open to communicating outside of JIRA, at artem [_aT_] plaxo \n{dot}\n com.\n\nThanks.",
    "attachments": {
        "catalina.out": "https://issues.apache.org/jira/secure/attachment/12421140/catalina.out",
        "catalina2.out": "https://issues.apache.org/jira/secure/attachment/12421141/catalina2.out"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Artem Russakovskii",
            "id": "comment-12761449",
            "date": "2009-10-02T01:49:52+0000",
            "content": "I'm getting an error even just trying to access a single shard's admin interface, even after adjusting -XX:MaxPermSize=512m\n\n\n==> catalina.out <==\nOct 1, 2009 6:47:06 PM org.apache.coyote.http11.Http11Processor process\nSEVERE: Error processing request\njava.lang.OutOfMemoryError: PermGen space\n        at java.lang.Throwable.getStackTraceElement(Native Method)\n        at java.lang.Throwable.getOurStackTrace(Throwable.java:591)\n        at java.lang.Throwable.printStackTrace(Throwable.java:510)\n        at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:72)\n        at org.apache.juli.FileHandler.publish(FileHandler.java:129)\n        at java.util.logging.Logger.log(Logger.java:458)\n        at java.util.logging.Logger.doLog(Logger.java:480)\n        at java.util.logging.Logger.logp(Logger.java:680)\n        at org.apache.juli.logging.DirectJDKLog.log(DirectJDKLog.java:167)\n        at org.apache.juli.logging.DirectJDKLog.error(DirectJDKLog.java:135)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:324)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:849)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:454)\n        at java.lang.Thread.run(Thread.java:619)\n\n:-/ "
        },
        {
            "author": "Bill Au",
            "id": "comment-12761654",
            "date": "2009-10-02T17:37:32+0000",
            "content": "You probably want to take a JVM thread dump (kill -3) while the JVM is hung to find out what's going on.\n\nIs your webapp app being reloaded?  You can check the appserver log file to see if that's happening.  One common way of running out of PermGen space is a classloader link which occurs when a webapp is reloaded. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12761675",
            "date": "2009-10-02T19:12:22+0000",
            "content": "One thing I forgot to mention - when the hang occurs on the slave, 1 out of 8 CPUs on the machine starts using 100%, which might point in a direction of a bug rather than a Java memory issue. Remember - the slave never throws those Java errors to the log, only the master does. The slave just hangs. Using htop, I can see one of the children java processes use that 100% CPU.\n\nBill, the appserver log is catalina.out, right? In any case, I'm tailing every file in the tomcat log dir and that's the log I've been pasting and talking about.\n\nI've attached 2 full thread dumps after kill -3 (it's quite verbose) on both slaves (both slaves are affected now).\n\nThe first one catalina.out is from the slave that had the Perm limit raised to 512MB, the 2nd one catalina2.out is from the server without any changes to Perm limits. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12761677",
            "date": "2009-10-02T19:13:47+0000",
            "content": "Also, just saw this on the first slave:\n\n\nINFO: Closing Searcher@3efceb09 main\n        fieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nOct 2, 2009 11:43:27 AM org.apache.solr.handler.SnapPuller doCommit\nINFO: Force open index writer to make sure older index files get deleted\nOct 2, 2009 11:43:35 AM org.apache.solr.update.SolrIndexWriter finalize\nSEVERE: SolrIndexWriter was not closed prior to finalize(), indicates a bug -- POSSIBLE RESOURCE LEAK!!!\n\n "
        },
        {
            "author": "Gus Heck",
            "id": "comment-13117599",
            "date": "2011-09-29T20:47:40+0000",
            "content": "I've seen a lock up similar to this with just a single stand-alone instance, no master-slave relationship, so that may be a red herring.\n\n\nSep 29, 2011 7:14:41 AM org.apache.solr.update.SolrIndexWriter finalize\nSEVERE: SolrIndexWriter was not closed prior to finalize(), indicates a bug -- POSSIBLE RESOURCE LEAK!!!\nSep 29, 2011 9:22:16 AM org.apache.catalina.core.AprLifecycleListener init\nINFO: Loaded APR based Apache Tomcat Native library 1.1.20.\n\n\n\n(solr 1.4.1, tomcat 7, windows in my case on JDK 1.6)\n\nThe server was completely unresponsive and the tomcat service wasn't even responding to a restart request. The machine had to be rebooted to get tomcat going again. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13836115",
            "date": "2013-12-01T20:51:03+0000",
            "content": "This is a very old issue. I haven't heard of similar hangs happening recently. Please let us know if anyone thinks it should be re-opened. "
        }
    ]
}