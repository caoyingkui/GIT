{
    "id": "LUCENE-2498",
    "title": "add sentence boundary charfilter",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "From the discussion of LUCENE-2167:\n\nIt would be nice to have a CharFilter? to mark sentence boundaries.\nSuch functionality would be useful for:\n\n\tprevent phrase queries with 0 slop from matching across sentences\n\tinhibiting multiword synonyms, or shingles, etc.\n\n\n\nFor sentence boundary detection we could use Jflex's support for the Unicode Sentence_Break property etc,\nand the UAX#29 definition as a default grammar.\n\nOne idea is to just mark the boundaries with a user-provided String.\n\nAs a simple use-case, a user could then add this string to a stopfilter, and it would introduce a position increment.\nThis would inhibit phrase queries, etc.\n\na user could use the sentence-markers to do more advanced processing downstream.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-06-12T22:36:17+0000",
            "content": "I wonder if it would be possible/make sense to make this a tokenizer instead of a charfilter: one token per sentence.  Then token production would be in a filter stage. ",
            "author": "Steve Rowe",
            "id": "comment-12878324"
        },
        {
            "date": "2010-06-13T06:22:47+0000",
            "content": "FWIW, I've implemented sentence boundary support by returning a special token (Type.EOF) from the Tokenizer and created a EOSFilter which increments the posIncr attribute (set it to 100). I haven't been following UAX#29 issue, but I'm sure it's a great thing .\n\nI've used the following http://www.fileformat.info/info/unicode/category/Po/list.htm to detect EOS + \\u0085 (Next Line (NEL). I'm sure though that there are other markers as well. ",
            "author": "Shai Erera",
            "id": "comment-12878354"
        },
        {
            "date": "2010-06-13T23:29:07+0000",
            "content": "I wonder if it would be possible/make sense to make this a tokenizer instead of a charfilter: one token per sentence. Then token production would be in a filter stage.\n\nwell maybe we should reword the issue, it doesnt have to be a charfilter or even use a special string to mark the sentence boundaries. But I thought as a charfilter it would allow you to use your own tokenizer, such as StandardTokenizer along with sentence boundaries.\n\nFWIW, I've implemented sentence boundary support by returning a special token (Type.EOF) from the Tokenizer and created a EOSFilter which increments the posIncr attribute (set it to 100).\n\nThis sounds similar to what we are proposing here... did you integrate this into your tokenizer yourself? \n\nI've used the following http://www.fileformat.info/info/unicode/category/Po/list.htm to detect EOS + \\u0085 (Next Line (NEL). I'm sure though that there are other markers as well.\n\nWell, there is nothing wrong with this approach. The advantage of using the unicode segmentation standard for sentences is that it can give some better handling for corner cases, since it has a grammar.\n\nsome examples quoted directly from the spec: http://unicode.org/reports/tr29/#Sentence_Boundaries\n\nRules SB6-8 are designed to forbid breaks within strings such as\n\n\n\n\nc.d\n\n\n3.4\n\n\nU.S.\n\n\n... the resp. leaders are ...\n\n\n... etc.)' '(the ...\n\n\n\n\n\nThey permit breaks in strings such as\n\n\n\n\nShe said \"See spot run.\"\nJohn shook his head. ...\n\n\n... etc.\n\u5b83\u4eec\u6307...\n\n\n...\u7406\u6570\u5b57.\n\u5b83\u4eec\u6307...\n\n\n\n\n\nThey cannot detect cases such as \"...Mr. Jones...\"; more sophisticated tailoring would be required to detect such cases. ",
            "author": "Robert Muir",
            "id": "comment-12878450"
        },
        {
            "date": "2010-06-14T03:13:03+0000",
            "content": "I also have an abbreviations filter, which once it faces an EOS token it checks its table for the previous word - if it's a match then it does not consider this a true EOS token. So cases like \"mr.\" are covered.\n\nThere are false negatives too though, if the abbreviation does end a sentence, but you've got to make some trade-offs ...\n\nlike I said, I'm sure the standard defines things better than I did... ",
            "author": "Shai Erera",
            "id": "comment-12878461"
        },
        {
            "date": "2010-06-15T12:49:35+0000",
            "content": "\nI wonder if it would be possible/make sense to make this a tokenizer instead of a charfilter: one token per sentence. Then token production would be in a filter stage.\n\nwell maybe we should reword the issue, it doesnt have to be a charfilter or even use a special string to mark the sentence boundaries. But I thought as a charfilter it would allow you to use your own tokenizer, such as StandardTokenizer along with sentence boundaries.\n\nUsing \"special\" tokens to carry token stream metadata feels like a fragile hack to me.\n\nMaybe we need a new kind of analysis component to chunk input and then call a tokenizer for each chunk.  It could be called a segmenter.  So e.g. SentenceSegmenter would detect sentence boundaries, then send each sentence to the user's choice of tokenizer, fixing up offsets and maybe also adding in an attribute for beginning/end of sentence.  Analysis chains could use a segmenter+tokenizer in the same way that a tokenizer is now used.  I think segmenters could be nested, too, e.g. a ParagraphSegmenter could take in a SentenceSegmenter as its tokenizer. ",
            "author": "Steve Rowe",
            "id": "comment-12878963"
        }
    ]
}