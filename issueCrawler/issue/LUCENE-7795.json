{
    "id": "LUCENE-7795",
    "title": "WordDelimiterFilter produces invalid offsets in single word case",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "6.5,                                            7.0",
        "status": "Open",
        "type": "Bug",
        "components": [],
        "fix_versions": []
    },
    "description": "This problem is not present in WordDelimiterGraphFilter, but it is present in WordDelimiterFilter's interaction with HTMLStripCharFilter.\n\nTest code:\n\n\npublic class TestTokenizationIssue2 {\n    public static void main(String... args) throws IOException {\n        HTMLStripCharFilter charFilter = new HTMLStripCharFilter(getText());\n        WhitespaceTokenizer whitespaceTokenizer = new WhitespaceTokenizer();\n        whitespaceTokenizer.setReader(charFilter);\n       // WordDelimiterGraphFilter wdgf = new WordDelimiterGraphFilter(whitespaceTokenizer,\n        //       WordDelimiterGraphFilter.GENERATE_WORD_PARTS, CharArraySet.EMPTY_SET);\n\n        WordDelimiterFilter wdgf = new WordDelimiterFilter(whitespaceTokenizer,\n               WordDelimiterFilter.GENERATE_WORD_PARTS, CharArraySet.EMPTY_SET);\n        wdgf.reset();\n\n        while (wdgf.incrementToken()) {\n            CharTermAttribute charTermAttribute = wdgf.getAttribute(CharTermAttribute.class);\n            OffsetAttribute offsetAttribute = wdgf.getAttribute(OffsetAttribute.class);\n\n            System.out.println(charTermAttribute.toString() + \" - \" + offsetAttribute.startOffset() + ',' + offsetAttribute.endOffset());\n        }\n    }\n\n    private static Reader getText() {\n        return new StringReader(\"&#x93;Risk\");\n    }\n}\n\n\n\n\nThe offsets produced by the WordDelimiterFilter are 1,10. With WordDelimiterGraphFilter the offsets produced are 0,10. It should be 0,10 as this is the original text:    \n\n&#x93;Risk\n\n   - and 1 is between the ampersand and hash.\n\nInside WordDelimiterFilter, I believe the conditional branch from \"if (isSingleWord && startOffset <= savedEndOffset) \"   is invalid and it should always use the saved start and end offsets because it can't make the assertion that the iterator's current and end are reliable markers.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15977585",
            "date": "2017-04-20T21:36:23+0000",
            "content": "GitHub user michaelbraun opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/194\n\n    LUCENE-7795 - illegal offsets in WordDelimiterFilter should prevent advancing start offset\n\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/michaelbraun/lucene-solr lucene-7795\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/194.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #194\n\n\ncommit 4461cbd92769700126bab45c0ce583459d2fb8b7\nAuthor: Michael Braun <mbraun38@bloomberg.net>\nDate:   2017-04-20T21:35:02Z\n\n    LUCENE-7795 - illegal offsets in WordDelimiterFilter should prevent advancing start offset\n\n ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15977586",
            "date": "2017-04-20T21:37:37+0000",
            "content": "Added what the simple fix looks like - modified an existing test which is this case in which hasIllegalOffsets is true. ",
            "author": "Michael Braun"
        }
    ]
}