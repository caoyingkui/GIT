{
    "id": "LUCENE-1859",
    "title": "TermAttributeImpl's buffer will never \"shrink\" if it grows too big",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "This was also an issue with Token previously as well\n\nIf a TermAttributeImpl is populated with a very long buffer, it will never be able to reclaim this memory\n\nObviously, it can be argued that Tokenizer's should never emit \"large\" tokens, however it seems that the TermAttributeImpl should have a reasonable static \"MAX_BUFFER_SIZE\" such that if the term buffer grows bigger than this, it will shrink back down to this size once the next token smaller than MAX_BUFFER_SIZE is set\n\nI don't think i have actually encountered issues with this yet, however it seems like if you have multiple indexing threads, you could end up with a char[Integer.MAX_VALUE] per thread (in the very worst case scenario)\n\n\nperhaps growTermBuffer should have the logic to shrink if the buffer is currently larger than MAX_BUFFER_SIZE and it needs less than MAX_BUFFER_SIZE",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-08-26T13:45:58+0000",
            "content": "This also applies to Token. If we fix that, we should also fix it in Token. ",
            "author": "Uwe Schindler",
            "id": "comment-12747942"
        },
        {
            "date": "2009-08-26T13:47:45+0000",
            "content": "it seems like the new TokenStream API may aggravate this issue a bit as it encourages even more reuse of the underlaying term char[] buffer (if i'm not mistaken) ",
            "author": "Tim Smith",
            "id": "comment-12747943"
        },
        {
            "date": "2009-08-26T18:22:14+0000",
            "content": "The worst-case scenario seems kind of theoretical, since there are so many\nreasons that huge tokens are impractical. (Is a priority of \"major\"\njustified?) If there's a significant benefit to shrinking the allocation, it's\nminimizing average memory usage over time.  But even that assumes a nearly\npathological distribution in field size \u2013 it would have to be large for early\ndocuments, then consistently small for subsequent documents.  If it's\nscattered, you have to plan for worst case RAM usage as an app developer,\nanyway.  Which generally means limiting token size.\n\nI assume that, based on this report, TermAttributeImpl never gets reset or\ndiscarded/recreated over the course of an indexing session?\n\n-0 if the reallocation happens no more often than once per document.\n\n-1 if it the reallocation has be performed in an inner loop. ",
            "author": "Marvin Humphrey",
            "id": "comment-12748064"
        },
        {
            "date": "2009-08-26T18:31:43+0000",
            "content": "The worst-case scenario seems kind of theoretical\n100% agree, but even if one extremely large token gets added to the stream (and possibly dropped prior to indexing), the char[] grows without ever shrinking back (so it can result in memory usage growing if \"bad\" content is thrown in (and people have no shortage of bad content)\n\nIs a priority of \"major\" justified?\n\nmajor is just the default priority (feel free to change)\n\nI assume that, based on this report, TermAttributeImpl never gets reset or discarded/recreated over the course of an indexing session?\nusing reusable TokenStream will never cause the buffer to be nulled (as far as i can tell) for the lifetime of the thread (please correct me if i'm wrong on this)\n\n\ni would argue for a semi-large value for MAX_BUFFER_SIZE (potentially allowing this to be statically updated), just as a means to bound the max memory used here\ncurrently, the memory use is bounded by Integer.MAX_VALUE (which is really big)\nIf someone feeds a large text document with no spaces or other delimiting characters, a \"non-intelligent\" tokenizer would treat this a 1 big token (and grow the char[] accordingly) ",
            "author": "Tim Smith",
            "id": "comment-12748071"
        },
        {
            "date": "2009-08-26T18:33:53+0000",
            "content": "The problem is, that it may be possible to shrink the buffer once per document, when TokenStream's reset() is called (which is done before each new document). To achieve this, all TokenStreams must notify the termattribute in reset() to shrink its size, which is impractical.\n\nOn the other hand, the reallocation would always be for each token (you call that inner loop).\n\nI agree, that normally, the tokens will not grow very large (if they do, you do something wrong during tokenization). Even things like KeywordTokenizer that only creates one token has an upper limit of the term size (as far as I know).\n\nI would set this to minor and would not take care before 2.9. The problem of maybe large buffers was there even in older versions with Token as attribute implementation. It is the same problem like preserving an ArrayList for very long time, it also only grows but never automatically shrinks. ",
            "author": "Uwe Schindler",
            "id": "comment-12748072"
        },
        {
            "date": "2009-08-26T18:38:29+0000",
            "content": "I would set this to minor and would not take care before 2.9.\n\ni would agree with this\n\njust reported the issue as it has the potential to cause memory issues (and would think something should be done about it (in the long term at least))\nalso, the AttributeSource stuff does result in TermAttributeImpl being held onto pretty much forever if using a reusableTokenStream (correct?)\nwas't a new Token() by the indexer for each doc/field in 2.4?, so the unbounding would only last at most for the duration of indexing that one document?\nwith Attribute caching in the TokenStream, the bounding lasts the duration of the TokenStream now (or its underlaying AttributeSource), which could remain until shutdown ",
            "author": "Tim Smith",
            "id": "comment-12748077"
        },
        {
            "date": "2009-08-26T18:41:16+0000",
            "content": "If someone feeds a large text document with no spaces or other delimiting characters, a \"non-intelligent\" tokenizer would treat this a 1 big token (and grow the char[] accordingly)\n\nwhich non-intelligent tokenizers are you referring to? nearly all the lucene tokenizers have 255 as a limit.  ",
            "author": "Robert Muir",
            "id": "comment-12748079"
        },
        {
            "date": "2009-08-26T18:46:25+0000",
            "content": "which non-intelligent tokenizers are you referring to? nearly all the lucene tokenizers have 255 as a limit.\n\nperhaps this is a non-issue with regards to \"lucene tokenizers\"\nhowever, Tokenizers can be implemented by anyone (not sure if there are adequate warnings about keeping tokens short)\nit also may not be possible to keep tokens short, i may need to index a rather long \"id\" string in a TokenStream fashion which will grow the buffer without reclaiming this\n\nperhaps it should be the responsibility of the Tokenizer to shrink the TermBuffer if it adds long tokens (but this will probably require some helper methods) ",
            "author": "Tim Smith",
            "id": "comment-12748082"
        },
        {
            "date": "2009-08-26T18:51:07+0000",
            "content": "perhaps it should be the responsibility of the Tokenizer to shrink the TermBuffer if it adds long tokens (but this will probably require some helper methods)\n\nI like this idea better than having any resizing behavior that I might not be able to control. ",
            "author": "Robert Muir",
            "id": "comment-12748083"
        },
        {
            "date": "2009-08-26T19:05:14+0000",
            "content": "IMO, the benefit of adding these theoretical helper methods to lower average \u2013 but not peak \u2013 memory usage by non-core Tokenizers which are probably doing something impractical anyway... does not justify the complexity cost. ",
            "author": "Marvin Humphrey",
            "id": "comment-12748089"
        },
        {
            "date": "2009-08-26T19:17:06+0000",
            "content": "i fail to see the complexity of adding one method to TermAttribute:\n\npublic void shrinkBuffer(int maxSize) {\n  if ((maxSize > termLength) && (buffer.length > maxSize)) {\n    termBuffer = java.util.Arrays.copyOf(termBuffer, maxSize);\n  } \n}\n\n\n\nNot having this is fine as long as its well documented that emitting large tokens can and will result in memory growing uncontrolled (especially if using many indexing threads) ",
            "author": "Tim Smith",
            "id": "comment-12748091"
        },
        {
            "date": "2009-08-26T19:31:13+0000",
            "content": "> i fail to see the complexity of adding one method to TermAttribute:\n\nDeath by a thousand cuts.  This is one cut.\n\nI wouldn't even add the note to the documentation.  If you emit large tokens,\nyou have to plan for obscene peak memory usage anyway, and if you're not\nprepared for that, you deserve what you get.  Keeping the average down \ndoesn't help that.\n\nThe only reason to do this is to keep average memory usage down for\nthe hell of it, and if it goes in, it should be an implementation detail. ",
            "author": "Marvin Humphrey",
            "id": "comment-12748102"
        },
        {
            "date": "2009-08-26T19:40:49+0000",
            "content": "Death by a thousand cuts. This is one cut.\n\nby this logic, nothing new can ever be added. \nThe thing that brought this to my attention was the new TokenStream API (one cut (rather big, but i like the new API so i'm happy with the blood loss (makes me dizzy and happy)))\nThe new TokenStream API holds onto theses char[] much longer (if not forever), so this results in memory growing unbounded unless there is some facility to truncate/null out the char[]\n\nI wouldn't even add the note to the documentation.\n\nI don't believe there is ever any valid argument against adding documentation.\nIf someone can shoot themselves in the foot with the gun you gave them, at least tell them not to point the gun at their foot with the safety off.\n\nThe only reason to do this is to keep average memory usage down for the hell of it.\nkeeping average memory usage down prevents those wonderful OutOfMemory Exceptions (which are difficult at best to recover from) ",
            "author": "Tim Smith",
            "id": "comment-12748103"
        },
        {
            "date": "2009-08-26T19:54:06+0000",
            "content": "> I don't believe there is ever any valid argument against adding\n> documentation.\n\nThe more that documentation grows, the harder it is to absorb.  The more\nbells and whistles on an API, the harder it is to grok and to use effectively.\nThe more a code base bloats, the harder it is to maintain or to evolve.\n\n> keeping average memory usage down prevents those wonderful OutOfMemory\n> Exceptions\n\nNo, it won't.  If someone is emitting large tokens regularly, it is likely\nthat several threads will require large RAM footprints simultaneously, and an\nOOM will occur.  That would be the common case.\n\nIf someone is emmitting large tokens periodically, well, this doesn't prevent\nthe OOM, it just makes it less likely.  That's not worthless, but it's not\nsomething anybody should count on when assessing required RAM usage.\n\nKeeping average memory usage down is good for the system at large.  If this is\nimplemented, that should be the justification. ",
            "author": "Marvin Humphrey",
            "id": "comment-12748109"
        },
        {
            "date": "2009-08-26T20:15:11+0000",
            "content": "On documentation:\nany warnings/precautions should always be called out (calling out the external link (wiki/etc) for in depth details)\nin depth descriptions of the details can be pushed off to wiki pages or external references, as long as a link is provided for the curious, but i would still argue that they should exist\n\nthis doesn't prevent the OOM, it just makes it less likely\n\nall you can ever do for OOM issues is make them less likely (short of just fixing a bug that holds onto memory like mad). \nIf accepting arbitrary content, there will always be a possibility of the content forcing OOM issues. In general, everything possible should be done to \nreduce the likelyhood of such OOM issues where possible (IMO). ",
            "author": "Tim Smith",
            "id": "comment-12748122"
        },
        {
            "date": "2009-12-06T19:47:20+0000",
            "content": "without a proposed patch from someone, I'm tempted to close this issue... ",
            "author": "Mark Miller",
            "id": "comment-12786680"
        },
        {
            "date": "2009-12-06T20:54:42+0000",
            "content": "+1, In my opinion this is useless, as terms do not grow uncontrolled. ",
            "author": "Uwe Schindler",
            "id": "comment-12786700"
        },
        {
            "date": "2009-12-07T14:17:53+0000",
            "content": "close if you like\n\napplication writers can add guards for this if they like/need to as a custom TokenFilter\n\nmainly created this ticket as this can result in an unbound buffer should people use the token stream api incorrectly (or against suggestions of lucene core developers) ",
            "author": "Tim Smith",
            "id": "comment-12786921"
        }
    ]
}