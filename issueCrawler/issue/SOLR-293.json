{
    "id": "SOLR-293",
    "title": "Add \"minPartLength\" to WordDelimiterFilter",
    "details": {
        "affect_versions": "1.3",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "WDF is handy but over-tokenizes when faced with short word parts:\n\nA9\nR2D2\nmp3\n\nThis creates one- or two- character tokens which are extremely slow to query as the doc freq is so high (this is contributing to a significant portion of our slowest queries).\n\nThis patch adds a \"minPartLength\" option that disables generation of parts below a certain length.  It is recommended to use it with catenateAll, so as to not lose tokens.\n\nI'll add factory options and tests if we decide to include this (and are happy with the parameter name).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12511248",
            "date": "2007-07-09T20:42:01+0000",
            "content": "Would it be useful to be able to configure this separately for words and numbers?\nminWordLength\nminNumberLength\n\nOn the indexing side, it makes sense to index \"A9\" and not \"A\" or \"9\"\n\n> It is recommended to use it with catenateAll\n\nIs there anything that can be done along the same lines, when not catenating for the query analyzer, so \"foo-bar\" will still become \"foo bar\", but \"A9\" would stay as \"A9\"?\n "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12511306",
            "date": "2007-07-10T01:36:25+0000",
            "content": "> Would it be useful to be able to configure this separately for words and numbers? \n\nI think it would, but I wasn't sure.  Trivial to implement in either case.\n\n>Is there anything that can be done along the same lines, when not catenating for the query analyzer, so \"foo-bar\" will still become \"foo bar\", but \"A9\" would stay as \"A9\"? \n\nThere are a couple ways to approach this (though I'm not exactly sure what your question is):\n\n\n\tinstead of minimum part length, restrict analysis to tokens with length < some value.  with N=3, this would let \"HiFi/hi-fi\" -> \"hi fi\" but \"hi8\" -> \"hi8\".  This makes the setting dependent on separator characters.\n\n\n\n\n\tensure character inclusion.  If any letter/number character was not included in any generated subpart, ensure that a larger containing token is generated.\n\n\n\n\"high-figh-888\" -> \"high figh 888\" (and not \"highfigh888\")\n\"hi-fi-8\" -> \"hifi8\"\n\n\n\tapproach the delimiter question differently.  Currenly, parts are delimited on case change, alpha->num (and v.v.), and delimiter chars.  The last is much, much stronger as a lexical delimiter, and it would be nice to recognize the difference between \"java5\", \"mp3\", \"4x4\" and \"99-bottle\" \"20-cent-piece\", etc.\n\n\n\nSave for the first, I can't think of easy, efficient implementations.  Perhaps WDF shouldn't get too sophisticated. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13580220",
            "date": "2013-02-17T17:07:09+0000",
            "content": "Cleaning up old JIRAs, re-open if necessary. "
        }
    ]
}