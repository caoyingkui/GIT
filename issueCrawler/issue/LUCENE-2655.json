{
    "id": "LUCENE-2655",
    "title": "Get deletes working in the realtime branch",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "Realtime Branch"
        ],
        "affect_versions": "Realtime Branch",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Deletes don't work anymore, a patch here will fix this.",
    "attachments": {
        "LUCENE-2655.patch": "https://issues.apache.org/jira/secure/attachment/12455969/LUCENE-2655.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-09-20T02:59:07+0000",
            "content": "Here's a relevant comment from LUCENE-2324:\n\nhttps://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=12891256&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12891256 ",
            "author": "Jason Rutherglen",
            "id": "comment-12912315"
        },
        {
            "date": "2010-09-20T03:20:55+0000",
            "content": "Maybe we could reuse (factor out) TermsHashPerField's\ncustom hash here, for the buffered Terms? It efficiently maps a\nBytesRef --> int.\n\nI'm trying to get a feel for what we kind of deletes we want\nworking in the flush by DWPT merge viz-a-viz the realtime branch\n(ie, the release where we have the new realtime search/indexing\nfunctionality). \n\nFactoring out BytesHash and storing the terms in a byte block\npool will allow replacing the current hash map of terms and\nlikely conserve more RAM. Will we need to replace doc id upto\nand instead use sequence ids? \n\nWe can additionally, for now, implement flushing deletes on\nmerges, like today, for the flush by DWPT merge to trunk. Then\nimplement live, aka foreground deletes, for the realtime search\nbranch merge to trunk.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12912319"
        },
        {
            "date": "2010-09-20T04:14:41+0000",
            "content": "A couple more things... \n\nThe BytesHash or some other aptly named class can implement the\nquick sort of terms, again from TermsHashField, which will\nreplace the sorted terms map used in the RT branch for deletes.\nThe RT branch isn't yet calculating the RAM usage of deletes. By\nusing the byte block pool, calculating the RAM usage will be\ntrivial (as the the BBP automatically records the num bytes used). \n\nThe RT branch has an implementation of delete using the min/max\nsequence ids for a given segment. What else is needed?  ",
            "author": "Jason Rutherglen",
            "id": "comment-12912339"
        },
        {
            "date": "2010-09-30T03:40:21+0000",
            "content": "Here's a basic patch with a test case showing delete by term not working.  It's simply not finding the docs in the reader in apply deletes, I'm guessing it's something basic that's wrong. ",
            "author": "Jason Rutherglen",
            "id": "comment-12916362"
        },
        {
            "date": "2010-09-30T03:59:35+0000",
            "content": "Maybe I was using the new flex API wrongly, when I pass in the deleted docs to MultiFields.getTermDocsEnum, the test case passes. ",
            "author": "Jason Rutherglen",
            "id": "comment-12916366"
        },
        {
            "date": "2010-09-30T04:03:46+0000",
            "content": "I'm only seeing this test from TestIndexWriterDelete fail:\n\n[junit] junit.framework.AssertionFailedError: expected:<3> but was:<0>\n    [junit] \tat org.apache.lucene.index.TestIndexWriterDelete.testMaxBufferedDeletes(TestIndexWriterDelete.java:118)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328) ",
            "author": "Jason Rutherglen",
            "id": "comment-12916367"
        },
        {
            "date": "2010-09-30T04:10:53+0000",
            "content": "There's this one as well, which I'll focus on, though it's an error in IR.isCurrent, it doesn't immediately appear to be related to deletes.\n\n[junit] Testsuite: org.apache.lucene.index.TestIndexWriterReader\n[junit] Testcase: testUpdateDocument(org.apache.lucene.index.TestIndexWriterReader):\tFAILED\n[junit] null\n[junit] junit.framework.AssertionFailedError: null\n[junit] \tat org.apache.lucene.index.TestIndexWriterReader.testUpdateDocument(TestIndexWriterReader.java:104)\n[junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12916370"
        },
        {
            "date": "2010-10-01T10:20:04+0000",
            "content": "Stepping back here...\n\nThere are two different goals mixed into the RT branch effort, I\nthink:\n\n\n\tMake thread states fully independent so flushing is no longer sync'd (plus it's a nice simplification, eg no more *PerThread in the indexing chain)\n\tEnable direct searching on the thread states RAM buffer, for awesome\n  NRT performance\n\n\n\nIt seems to me like the first one is not so far off?  Ie we nearly\nhave it already (LUCENE-2324)... it's just that we don't have the\ndeletes working?\n\nWhereas the 2nd one is a much bigger change, and still iterating under\nLUCENE-2475.\n\nIs it possible to decouple #1 from #2?  Ie, bring it to a committable\nstate and land it on trunk and let it bake some?\n\nEg, on deletes, what if we simply have each thread state buffer its own\ndelete term -> thread's docID, privately?  We know this approach\nwill \"work\" (it does today), right?  It's just wasteful of RAM (though,\ncutover to BytesRefHash should help alot here), and, makes\ndeletes somewhat slower since you must now enroll the del term\ninto each thread state....\n\nIt wouldn't actually be that wasteful of RAM, since the BytesRef instance\nwould be shared across all the maps.  Also, if we wanted (separately)\nwe could make a more efficient buffer when the app deletes many terms\nat once, or, many calls to delete-by-single-term with no adds in between\n(much like how Amazon optimizes N one-click purchases in a row...).\n\nI really want to re-run my 6-thread indexing test on beast and see the\nindexing throughput double!! ",
            "author": "Michael McCandless",
            "id": "comment-12916851"
        },
        {
            "date": "2010-10-01T16:10:35+0000",
            "content": "Are you saying we should simply make deletes work (as is, no BytesRefHash conversion) then cleanup the RT branch as a merge to trunk of the DWPT changes?  I was thinking along those lines.  I can spend time making the rest of the unit tests work on the existing RT revision, though should this should probably happen in conjunction with a merge from trunk.  \n\nOr simply make the tests pass, and merge RT -> trunk afterwards?\n\nAlso, from what I've seen, deletes seem to work, I'm not sure what exactly Michael is referring to.  I'll run the full 'suite' of unit tests, and just make each work? ",
            "author": "Jason Rutherglen",
            "id": "comment-12916933"
        },
        {
            "date": "2010-10-01T16:42:31+0000",
            "content": "Are you saying we should simply make deletes work (as is, no BytesRefHash conversion) then cleanup the RT branch as a merge to trunk of the DWPT changes?\n\nYes!  Just keep using the Map we now use, but now it must be per-DWPT.  And when IW.deleteDocs(Term/Query) is called, we must go to each DWPT, grab its current docID, and enroll Term/Query -> docID into that DWPT's pending deletes map.\n\nAlso, from what I've seen, deletes seem to work, I'm not sure what exactly Michael is referring to.\n\nBut this is using the long sequenceID right?  (adds 8 bytes per buffered docID).  I think we wanted to explore ways to reduce that?  Or, if we can make it modal (you only spend these 8 bytes if IW is open in NRT mode), then that's OK?\n\nI was hoping to cleanly separate the DWPT cutover (solves the nasty flush bottleneck) from the NRT improvements. ",
            "author": "Michael McCandless",
            "id": "comment-12916946"
        },
        {
            "date": "2010-10-01T17:57:37+0000",
            "content": "when IW.deleteDocs(Term/Query) is called, we must go to each DWPT, grab its current docID, and enroll Term/Query -> docID into that DWPT's pending deletes map.\n\nOk, that's the change you're referring to. In the current RT revision, the deletes are held in one map in DW, guess we need to change that. However if we do, why do we need to keep the seq id or docid as the value in the map? When the delete arrives into the DWPT, we know that any buffered docs with that term/query need to be deleted on flush? (ie, lets not worry about the RT search use case, yet). ie2, we can simply add the terms/queries to a set, and apply them on flush, ala LUCENE-2679?\n\nNRT improvements\n\nWe're referring to LUCENE-1516 as NRT and LUCENE-2312 as 'RT'. I'm guessing you mean RT?  ",
            "author": "Jason Rutherglen",
            "id": "comment-12916979"
        },
        {
            "date": "2010-10-01T19:09:05+0000",
            "content": "I'm guessing you mean RT?\n\nDuh sorry yes... I'll try to use the right name \n\nIn the current RT revision, the deletes are held in one map in DW, guess we need to change that.\n\nRight, one map that maps the del term to the long sequence ID right?  I'm thinking we revert just this part, and go back to how trunk how buffers deletes (maps to docID), but per-DWPT.\n\nHowever if we do, why do we need to keep the seq id or docid as the value in the map? When the delete arrives into the DWPT, we know that any buffered docs with that term/query need to be deleted on flush?  (ie, lets not worry about the RT search use case, yet). ie2, we can simply add the terms/queries to a set, and apply them on flush, ala LUCENE-2679?\n\nFor the interleaved case.  Ie, LUCENE-2679 is optional \u2013 we still must handle the interleaved case correctly (and, I think, by default).  But if an app uses the opto in LUCENE-2679 then we only need a single Set.\n\nBasically, the DWPT change, alone, is hugely valuable and (I think) decouple-able from the trickier/riskier/RAM-consuminger RT changes. ",
            "author": "Michael McCandless",
            "id": "comment-12917004"
        },
        {
            "date": "2010-10-01T20:11:37+0000",
            "content": "go back to how trunk how buffers deletes (maps to docID), but per-DWPT.\n\nI don't think we need seq ids for the flush-by-DWPT merge to trunk.  I was getting confused about the docid being a start from, rather than a stop at.  I'll implement a map of query/term -> docid-upto per DWPT.\n\nBasically, the DWPT change, alone, is hugely valuable and (I think) decouple-able from the trickier/riskier/RAM-consuminger RT changes.\n\nYes indeed!\n\nI'll try to use the right name\n\nNRT -> RT.  The next one will be need to be either R or T, shall we decide now or later?\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12917018"
        },
        {
            "date": "2010-10-01T21:34:26+0000",
            "content": "We've implied an additional change to the way deletes are flushed in that today, they're flushed in applyDeletes when segments are merged, however with flush-DWPT we're applying deletes after flushing the DWPT segment.\n\nAlso we'll have a globalesque buffered deletes presumably located in IW that buffers deletes for the existing segments, and these should [as today] be applied only when segments are merged or getreader is called? ",
            "author": "Jason Rutherglen",
            "id": "comment-12917046"
        },
        {
            "date": "2010-10-01T22:02:10+0000",
            "content": "We've implied an additional change to the way deletes are flushed in that today, they're flushed in applyDeletes when segments are merged, however with flush-DWPT we're applying deletes after flushing the DWPT segment.\n\nWhy is that change needed again?  Deferring until merge kickoff is a win, eg for a non N/R/T (heh) app, it means 1/10th the reader open cost (w/ default mergeFactor=10)?  Opening/closing readers can be costly for a large index.\n\nReally, some day, we ought to only apply deletes to those segments about to be merged (and keep the buffer for the rest of the segments). Eg most merges are small... yet we'll pay huge cost opening that massive grandaddy segment every time these small merges kick off.  But that's another issue...\n\nWhy can't we just do what we do today?  Ie push the DWPT buffered deletes into the flushed deletes? ",
            "author": "Michael McCandless",
            "id": "comment-12917055"
        },
        {
            "date": "2010-10-01T22:27:31+0000",
            "content": "Ok, I have been stuck/excited about not having to use/understand the remap-docids method, because it's hard to debug.  However I see what you're saying, and why remap-docids exists.  I'll push the DWP buffered deletes to the flushed deletes.  \n\nwe'll pay huge cost opening that massive grandaddy segment \n\nThis large cost is from loading the terms index and deleted docs?   When those large segments are merged though, the IO cost is so substantial that loading tii or del into RAM probably doesn't account for much of the aggregate IO, they're probably in the noise?  Or are you referring to the NRT apply deletes flush, however that is on a presumably pooled reader?  Or you're just saying that today we're applying deletes across the board to all segments prior to a merge, regardless of whether or not they're even involved in the merge?  It seems like that is changeable? ",
            "author": "Jason Rutherglen",
            "id": "comment-12917076"
        },
        {
            "date": "2010-10-02T09:44:54+0000",
            "content": "Ok, I have been stuck/excited about not having to use/understand the remap-docids method, because it's hard to debug. However I see what you're saying, and why remap-docids exists. I'll push the DWP buffered deletes to the flushed deletes.\n\nI think we still must remap, at least on the pushed (deletesFlushed) deletes?\n\nOn the buffered deletes for the DWPT (deletesInRAM), I think we can make these relative to the DWPT (ie start from 0), but on pushing them into flushed deletes we re-base them?\n\nThis large cost is from loading the terms index and deleted docs?\n\nYes.  We don't (hopefully) load norms, field cache, etc.\n\nWhen those large segments are merged though, the IO cost is so substantial that loading tii or del into RAM probably doesn't account for much of the aggregate IO, they're probably in the noise?\n\nWell, the applyDeletes method is sync'd, vs merging which is fully concurrent.  (Also, merging doesn't load the tii).\n\nOr are you referring to the NRT apply deletes flush, however that is on a presumably pooled reader?\n\nRight, it would be pooled for the NRT case, so this is only a (sizable) perf problem for the non-nrt case.\n\nOr you're just saying that today we're applying deletes across the board to all segments prior to a merge, regardless of whether or not they're even involved in the merge? It seems like that is changeable?\n\nRight!  That's what we do today (apply deletes to all segs) whereas it's really only necessary to apply them to the segments being merged.  I opened LUCENE-2680 to track this. ",
            "author": "Michael McCandless",
            "id": "comment-12917173"
        },
        {
            "date": "2010-10-10T00:38:18+0000",
            "content": "With per DWPT deletes, we're maintaining a docidupto/limit per term/query.  When the DWPT deletes are transferred to an IW deletes flushed, there isn't a way to globalize the docupto/limit value that's been maintained per DWPT.  Today we globalize the docidupto/limit value because we're only flushing one segment at a time.  \n\nWe need to implement per segment deletes queuing to make this patch work, basically implement LUCENE-2680.  I'll start in this direction.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12919548"
        },
        {
            "date": "2010-10-11T10:03:44+0000",
            "content": "\nWith per DWPT deletes, we're maintaining a docidupto/limit per term/query. When the DWPT deletes are transferred to an IW deletes flushed, there isn't a way to globalize the docupto/limit value that's been maintained per DWPT. Today we globalize the docidupto/limit value because we're only flushing one segment at a time.\n\nI think we can still globalize?\n\nThe trick is it'd require doing the remapping in the same sync block that appends the new SegmentInfo into the SegmentInfos, right?  Because it's a that moment that the newly flushed segment is assigned its position w/ the rest of the segments.  And so, at that moment, it knows the sum of the maxDoc() of all prior segments, and it can globalize all of the docIDs mapped to the pending delete Term/Query? ",
            "author": "Michael McCandless",
            "id": "comment-12919778"
        },
        {
            "date": "2010-10-11T14:51:45+0000",
            "content": "When a new segment is flushed, then the docid-upto is reset to what's in the latest segment?  Aren't we then losing point-in-timeness of the docid-uptos per DWPT by simply resetting dut to the highest value of the newest segment? ",
            "author": "Jason Rutherglen",
            "id": "comment-12919853"
        },
        {
            "date": "2010-10-12T17:44:57+0000",
            "content": "When a new segment is flushed, then the docid-upto is reset to what's in the latest segment?\n\nDo you mean the flushedDocCount (in DocumentsWriter), by \"docid-upto\"?  Ie, this is what we use to \"globalize\" the docid-upto stored for each delete Term/Query.\n\nAren't we then losing point-in-timeness of the docid-uptos per DWPT by simply resetting dut to the highest value of the newest segment?\n\nI'm confused... that flushedDocCount is just an int, tracking the total doc count of all already-flushed segments.  This shouldn't affect point-in-timeness....? ",
            "author": "Michael McCandless",
            "id": "comment-12920276"
        },
        {
            "date": "2010-10-13T15:24:47+0000",
            "content": "If there are 2 DWPTs, and each has a the following pending deletes:\n\nDWPT #1 maxdoc: 50; term id:1 limit:25\n\nDWPT #2 maxdoc: 45; term id:1 limit:30\n\nWhere limit is the docid-upto stored as an int value in the pending deletes map.  If we flush one of these DWPTs without first applying the deletes to a bitvector, and we flush the DWPTs in order, then DWPT #1 would flush the deletes, the term doc limit would be reset to segmentinfos maxdoc + 25.  However there' a 50 - 25 = 25 gap.  When DWPT #2 is flushed, the limit for term id:1 is set to segmentinfos maxdoc + 50 + 30.  eg, we're effectively losing DWPT #1s limit (26 - 50)? ",
            "author": "Jason Rutherglen",
            "id": "comment-12920606"
        },
        {
            "date": "2010-10-13T17:13:11+0000",
            "content": "Just to confirm: when a delete is done, we go and buffer that delete into each DWPT, right?  (Mapped to the then-current docid-upto for that DWPT).\n\nOK I see the problem.  It's because we now have a single pool for deletesFlushed, right?  Ie, DWPT #2 will overwrite the term id:1 entry.\n\nBut, I think the switch to generations of pending deletes (LUCENE-2680) would fix this?  Maybe we should go do that one first...\n\nIe, DWPT #1's flush would enter a new gen delete pool (maps term -> global docid-upto).  Then DWPT #2's flush would also enter a new gen delete pool.  Hmm, but not quite... the generations can't simply stack on top of one another.  I think there's a graph structure somehow?  Ie every DWPT that's flushed must record the segments that existed (were already flushed) when it was first created, because it's only those segments that should get the deleted term.  Segments in the future obviously shouldn't get it.  And segments from parallel DWPTs (ie that existed at the same time) should also not get it since they will separately track the deleted term.\n\nBTW, I think this makes LUCENE-2679 all the more important (the ability to delete such that delete will only apply to already-committed segments), since this'd mean we only store the pending delete in a single map instead of map per DWPT. ",
            "author": "Michael McCandless",
            "id": "comment-12920665"
        },
        {
            "date": "2010-10-13T18:09:05+0000",
            "content": "I think the switch to generations of pending deletes (LUCENE-2680) would fix this? Maybe we should go do that one first.\n\nYes, lets work on that one as it directly relates to solving this issue.  The generational system won't work for DWPT pending flushed deletes for the reasons described (ie, the docidupto/limit will be inappropriately globalized).  \n\nI think there's a graph structure somehow? Ie every DWPT that's flushed must record the segments that existed (were already flushed) when it was first created, because it's only those segments that should get the deleted term\n\nYou're saying record a list of segments that existed at the time of flushing a DWPT's deletes?  Lets get that data structure mapped out to start on LUCENE-2680?\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12920698"
        },
        {
            "date": "2010-10-20T09:03:56+0000",
            "content": "You're saying record a list of segments that existed at the time of flushing a DWPT's deletes?\n\nActually, I think it's simpler!  I think the DWPT just records the index of the last segment in the index, as of when it is created (or re-inited after it's been flushed).\n\nOn flush of a given DWPT, its buffered deletes are recorded against that segment, and still carry over the lastSegmentIndex.  This way, when we finally do resolve these deletes to docIDs, we 1) always apply the delete if segment <= lastSegmentIndex, or 2) the doc is in that segment and is <= the docID upto.  I think this'd mean we can keep the docid-upto as local docIDs, which is nice (no globalizing/shifting-on-merge/flush needed).\n\nSo, segments flushed in the current IW session will carry this private pool of pending deletes.  But, pre-existing segments in the index don't need their own pool.  Instead, when it's time to resolve the buffered deletes against them (because they are about to be merged), they must walk all of the per-segment pools, resolving the deletes from that pool if its segment index is <= the lastSegmentIndex of that pool.  We should take care to efficiently handle dup'd terms, ie where the same del term is present in multiple pools.  The most recent one \"wins\", and we should do only one delete (per segment) for that term.\n\nThese per-segment delete pools must be updated on merge.  EG if the lastSegmentIndex of a pool gets merged, that's fine, but then on merge commit we must move that lastSegmentIndex \"backwards\" to the last segment before the merge, because any deletes necessary within the segment will have been resolved already.\n\nWhen segments with del pools are merged, we obviously apply the deletes to the segments being merged, but, then, we have to coalesce those pools and move them into a single pool on the segment just before the merge.  We could actually use a Set at this point since there is no more docid-upto for this pool (ie, it applies to all docs on that segment and in segments prior to it).\n\nSo I think this is much simpler than I first thought!\n\nLets get that data structure mapped out to start on LUCENE-2680?\n\n+1 ",
            "author": "Michael McCandless",
            "id": "comment-12922894"
        },
        {
            "date": "2010-10-20T21:43:51+0000",
            "content": "Can you explain what you mean by \"per-segment pools\" compared with the flushed deletes data structure in there today, ie, why are they pools?\n\nI can start on the simplified version of this for the non-DWPT use case, eg, LUCENE-2680.  That version'll implement the delete pool per segment only for those segments created after the last flush/commit? ",
            "author": "Jason Rutherglen",
            "id": "comment-12923186"
        },
        {
            "date": "2010-10-20T21:52:49+0000",
            "content": "Can you explain what you mean by \"per-segment pools\" compared with the flushed deletes data structure in there today, ie, why are they pools?\n\nSorry, by pool I mean what we have today.  Ie a map from del term/query -> global docid-upto.\n\nExcept, with this change it will be per-segment, only for those segments flushed since IW was opened or since last commit.\n\nI can start on the simplified version of this for the non-DWPT use case, eg, LUCENE-2680.\n\nYes, please!\n\nThis approach works fine for the single-DWPT case (ie what we now have).  It'll be good to get it first working on this simpler case, then port to the RT branch to get it working for multiple DWPTs.\n\nThat version'll implement the delete pool per segment only for those segments created after the last flush/commit?\n\nCreated after IW was opened or after last commit, yes.\n\nI think we'll need an applyAllDeletes (called by commit) vs an applyDeletesToSegment (called by merge). ",
            "author": "Michael McCandless",
            "id": "comment-12923189"
        },
        {
            "date": "2010-12-09T18:12:51+0000",
            "content": "Per-segment deletes were implemented as a part of LUCENE-2680. ",
            "author": "Jason Rutherglen",
            "id": "comment-12969837"
        }
    ]
}