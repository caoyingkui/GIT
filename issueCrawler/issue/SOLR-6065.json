{
    "id": "SOLR-6065",
    "title": "Solr should give you clear error if you try to add too many docs",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "LUCENE-5843 recently fixed IndexWriter to throw a specific exception in the event that an index grows too big, preventing people from creating indexes they can't later search.  That issue also added a new test-only setter that can be used to lower the limit for hte purposes of testing.\n\nFor the Solr side of things, we should primarily focus on making sure we have some cloud based tests that set the lower limit and then verify that users get clean error messages when they exceed it for a single shard.\n\noriginal bug report\nyamazaki reported an error on solr-user where, on opening a new searcher, he got an IAE from BaseCompositeReader because the numDocs was greater then Integer.MAX_VALUE.\n\nI'm surprised that in a straight forward setup (ie: no \"AddIndex\" merging) IndexWriter will even let you add more docs then max int.  We should investigate if this makes sense and either add logic in IndexWriter to prevent this from happening, or add logic to Solr's UpdateHandler to prevent things from getting that far.\n\nie: we should be failing to \"add\" too many documents, and leaving the index usable \u2013 not accepting the add and leaving hte index in an unusable state.\n\nstack trace reported by user...\n\n\nERROR org.apache.solr.core.CoreContainer  \u2013 Unable to create core: collection1\norg.apache.solr.common.SolrException: Error opening new searcher\n    at org.apache.solr.core.SolrCore.<init>(SolrCore.java:821)\n    at org.apache.solr.core.SolrCore.<init>(SolrCore.java:618)\n    at org.apache.solr.core.CoreContainer.createFromLocal(CoreContainer.java:949)\n    at org.apache.solr.core.CoreContainer.create(CoreContainer.java:984)\n    at org.apache.solr.core.CoreContainer$2.call(CoreContainer.java:597)\n    at org.apache.solr.core.CoreContainer$2.call(CoreContainer.java:592)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)\n    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n    at java.lang.Thread.run(Thread.java:662)\nCaused by: org.apache.solr.common.SolrException: Error opening new searcher\n    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1438)\n    at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1550)\n    at org.apache.solr.core.SolrCore.<init>(SolrCore.java:796)\n    ... 13 more\nCaused by: org.apache.solr.common.SolrException: Error opening Reader\n    at org.apache.solr.search.SolrIndexSearcher.getReader(SolrIndexSearcher.java:172)\n    at org.apache.solr.search.SolrIndexSearcher.<init>(SolrIndexSearcher.java:183)\n    at org.apache.solr.search.SolrIndexSearcher.<init>(SolrIndexSearcher.java:179)\n    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1414)\n    ... 15 more\nCaused by: java.lang.IllegalArgumentException: Too many documents,\ncomposite IndexReaders cannot exceed 2147483647\n    at org.apache.lucene.index.BaseCompositeReader.<init>(BaseCompositeReader.java:77)\n    at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:368)\n    at org.apache.lucene.index.StandardDirectoryReader.<init>(StandardDirectoryReader.java:42)\n    at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:71)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:783)\n    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:52)\n    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:88)\n    at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:34)\n    at org.apache.solr.search.SolrIndexSearcher.getReader(SolrIndexSearcher.java:169)\n    ... 18 more",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Jack Krupansky",
            "id": "comment-13996549",
            "date": "2014-05-13T16:16:04+0000",
            "content": "As a historical note, I had filed LUCENE-4104 and LUCENE-4105, as well as SOLR-3504 and SOLR-3505 to both document and check against the per-index document limit in both Lucene and Solr.\n\nI think Lucene should check against the limit, and then Solr should respond to that condition.\n\nTwo interesting use cases:\n\n1. Deleted documents exist, so Solr should tell the user that \"optimize\" can resolve the problem.\n2. No deleted documents exist, Solr can only report that the document limit has been reached.\n\nAs an afterthought, maybe we should have a configurable Solr parameter for \"maximum documents per shard\" since anybody adding 2 billion documents to a shard is very likely to run into performance issues long before they get near the absolute maximum limit. I'd suggest a Solr configurable limit of like 250 million. Alternatively, this configurable limit could simply be a (noisy) warning, or maybe it could be configurable as either a hard error or a soft warning. "
        }
    ]
}