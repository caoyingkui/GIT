{
    "id": "LUCENE-8563",
    "title": "Remove k1+1 from the numerator of  BM25Similarity",
    "details": {
        "components": [],
        "status": "Resolved",
        "resolution": "Fixed",
        "fix_versions": [
            "master (8.0)"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Improvement"
    },
    "description": "Our current implementation of BM25 does\n\nboost * IDF * (k1+1) * tf / (tf + norm)\n\n\nAs (k1+1) is a constant, it is the same for every term and doesn't modify ordering. It is often omitted and I found out that the \"The Probabilistic Relevance Framework: BM25 and Beyond\" paper by Robertson (BM25's author) and Zaragova even describes adding (k1+1) to the numerator as a variant whose benefit is to be more comparable with Robertson/Sparck-Jones weighting, which we don't care about.\nA common variant is to add a (k1 + 1) component to the\n numerator of the saturation function. This is the same for all\n terms, and therefore does not affect the ranking produced.\n The reason for including it was to make the final formula\n more compatible with the RSJ weight used on its own\nShould we remove it from BM25Similarity as well?\n\nA side-effect that I'm interested in is that integrating other score contributions (eg. via oal.document.FeatureField) would be a bit easier to reason about. For instance a weight of 3 in FeatureField#newSaturationQuery would have a similar impact as a term whose IDF is 3 (and thus docFreq ~= 5%) rather than a term whose IDF is 3/(k1 + 1).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16683815",
            "author": "Elizabeth Haubert",
            "content": "Mathematically, it changes the ratio of \n\n\n \ntf * idf / ( tf + norm) \n{/code}\n\nwhich determines the relative importance of the norms parameter.   It seems like that should affect ranking, at least for low values of tf.   Why not just set the parameter to 0 for the cases you are looking at?\n\n ",
            "date": "2018-11-12T13:52:25+0000"
        },
        {
            "id": "comment-16683858",
            "author": "Adrien Grand",
            "content": "Elizabeth Haubert The change I'm suggesting would divide every BM25 score by (k1+1), which doesn't affect ranking. Setting k1 to 0 would have the undesirable side-effect of disabling the impact of term frequency and document length: the formula that I wrote was a bit simplified as norm actually depends on k1, it looks like below when expanding norm:\n\n\nboost * IDF * (k1+1) * tf / (tf + k1 * (1 - b + b * len / avgLen))\n\n ",
            "date": "2018-11-12T14:28:54+0000"
        },
        {
            "id": "comment-16683868",
            "author": "Robert Muir",
            "content": "+1 to nuke it. Currently the explain() goes out of its way to try to separate out this scaling factor to make it easier to see. Its unnecessary. ",
            "date": "2018-11-12T14:34:58+0000"
        },
        {
            "id": "comment-16683976",
            "author": "Elizabeth Haubert",
            "content": "The boost*IDF is not particularly important, this is about the handling of the TF component relative to the norms. \n\nPull that out as \n\n(tf + k1*tf) / (tf + k1*length_norms)\n\n\n\nRemoving it only from the numerator produces \n\n tf / (tf +k1* length norms) \n\n\n\nAt a minimum, that will need a new empirical default for k1. \n\nChanging k1 in the numerator is the knob to adjust the ratio of tf and norms.   In the case where document length does not follow standard models, it can be helpful to damp down b.  This is not the standard use case, but is not unusual, either.  At the extreme,  b=0 then this component reduces to \n\n(tf * (k1 +1)) / (tf + k1)\n\n\n\nRemoving the (k1 +1) from the numerator only produces \n\ntf / (tf + k1)\n\n\n\nThere will be cases where this affects relative scoring and ranking, and I don't understand the statement that it doesn't modify ordering.\n\nIf there is a need to remove it in the normal case, then perhaps the numerator and denominator should be split into two distinct constants.\n\n\n\n\n ",
            "date": "2018-11-12T15:54:04+0000"
        },
        {
            "id": "comment-16684051",
            "author": "Adrien Grand",
            "content": "There will be cases where this affects relative scoring and ranking\n\nI don't think this is correct. All scores would be divided by the same constant, so ordering would be preserved. ",
            "date": "2018-11-12T16:37:44+0000"
        },
        {
            "id": "comment-16684080",
            "author": "Doug Turnbull",
            "content": "It would modify ordering when dealing with multiple fields. Consider\u00a0one field\u00a0with a different k1 than another because the impact of term frequency is calibrated differently. If one calibrates one field to saturate term freq faster, and another slower, then ordering would be impacted ",
            "date": "2018-11-12T16:56:37+0000"
        },
        {
            "id": "comment-16684091",
            "author": "Doug Turnbull",
            "content": "For the sake of this discussion, here's a desmos graph with BM25 with/without k1 in the numerator\u00a0\n\nhttps://www.desmos.com/calculator/cklb27fcn9\u00a0 ",
            "date": "2018-11-12T17:05:31+0000"
        },
        {
            "id": "comment-16684107",
            "author": "Adrien Grand",
            "content": "Agreed Doug Turnbull I was assuming a single similarity. This would also change ordering if other fields use different similarities. ",
            "date": "2018-11-12T17:13:34+0000"
        },
        {
            "id": "comment-16686370",
            "author": "Luca Cavanna",
            "content": "Hi folks, I would like to work on this issue. ",
            "date": "2018-11-14T11:03:22+0000"
        },
        {
            "id": "comment-16686633",
            "author": "Adrien Grand",
            "content": "That would be great Luca Cavanna. I suspect most of the work is going to be about fixing tests that rely on absolute score values. ",
            "date": "2018-11-14T14:57:25+0000"
        },
        {
            "id": "comment-16686877",
            "author": "Michael Gibney",
            "content": "\"assuming a single similarity\" \u2013 is this something that we want to assume? If every field similarity uses the same k1 param, then sure, relative ordering among fields is maintained. But if we're using these scores outside of the context of single-similarity, and intend to preserve the ability to adjust the k1 param, it's worth noting that this change fundamentally alters the effect of the k1 param on absolute scores (and thus also on relative scores across similarities).\n\nNamely, removing k1 from the numerator places a hard cap on the score, regardless of TF or k1 setting. The concept of saturation is preserved, but with no numerator k1, saturation is implemented strictly by depressing scores (with respect to the hard cap, by varying amounts according to TF) as k1 increases. The model with k1 in the numerator strikes me as being more flexible, both depressing scores for lower TF and increasing scores for higher TF, around an inflection point determined by length norms and the value of b.\n\nI'm sure this change would be appropriate for some scenarios, but it's a fundamental change that could in some cases have significant downstream consequences, with no easy way (as far as I can tell) to maintain existing behavior. ",
            "date": "2018-11-14T17:11:24+0000"
        },
        {
            "id": "comment-16687260",
            "author": "Adrien Grand",
            "content": "\"assuming a single similarity\" \u2013 is this something that we want to assume?\n\nWe can't indeed, even though this is the most common case. That said if you are searching multiple fields at once today, the I'm afraid that relevance isn't very good anyway as we don't support something like BM25F (LUCENE-8216) to merge index and document statistics (BlendedTermQuery merges index statistics, but not norms and term frequencies). By the way BM25F doesn't allow to configure the value of k1 on a per-field basis, only b may have different per-field values.\n\nI'm sure this change would be appropriate for some scenarios, but it's a fundamental change that could in some cases have significant downstream consequences, with no easy way (as far as I can tell) to maintain existing behavior.\n\nUsers could multiply their per-field boosts by (k1+1)? ",
            "date": "2018-11-14T22:59:57+0000"
        },
        {
            "id": "comment-16687268",
            "author": "Doug Turnbull",
            "content": "I feel perhaps\u00a0one way forward is to create a second (default?) similarity - FastBM25Similarity? ConstantCeilingBM25Similarity? and leave in place the current BM25 similarity as an optional similarity to configure. There may be existing practices around tuning BM25 similarity at many places where writing a similarity plugin is not an option ",
            "date": "2018-11-14T23:12:41+0000"
        },
        {
            "id": "comment-16687440",
            "author": "Michael Gibney",
            "content": "Adrien Grand, thanks for pointing out the work on BM25F. I'm interested to take a closer look at that.\n\"Users could multiply their per-field boosts by (k1+1)?\" ... thanks, yes! That should work in a pinch, though I was so focused on the Similarity that I missed the possibility of scaling it externally in this way.\n\nHaving k1's presence in the numerator be configurable (either as an extra boolean parameter to the (modified) existing BM25Similarity, or something along the lines of what Doug Turnbull suggests) would make sense to me, regardless of the benefits of the change (performance or otherwise). ",
            "date": "2018-11-15T03:22:10+0000"
        },
        {
            "id": "comment-16687922",
            "author": "Robert Muir",
            "content": "No, we shouldn't clutter up BM25Similarity because some users have bad behavior.\n\nIf they did the wrong thing and rely on the exact absolute magnitude of the old similarity, well that's why the mechanism is extensible. ",
            "date": "2018-11-15T12:01:10+0000"
        },
        {
            "id": "comment-16688082",
            "author": "Jan H\u00f8ydahl",
            "content": "+1 to Doug's suggestion. Add the new Similarity and keep the old for the lifetime of 8.x so people have a graceful way to migrate if needed. ",
            "date": "2018-11-15T13:47:15+0000"
        },
        {
            "id": "comment-16688101",
            "author": "Adrien Grand",
            "content": "If keeping a way to get the old scores is the main concern, we could add a similarity that looks like this to lucene/misc and mention it in the upgrade notes:\n\n\npublic class LegacyBM25Similarity extends Similarity {\n\n    private final BM25Similarity bm25Similarity;\n\n    public LegacyBM25Similarity() {\n        bm25Similarity = new BM25Similarity();\n    }\n\n    public LegacyBM25Similarity(float k1, float b) {\n        bm25Similarity = new BM25Similarity(k1, b);\n    }\n\n    @Override\n    public long computeNorm(FieldInvertState state) {\n        return bm25Similarity.computeNorm(state);\n    }\n\n    @Override\n    public SimScorer scorer(float boost, CollectionStatistics collectionStats, TermStatistics... termStats) {\n        return bm25Similarity.scorer(boost * (1 + bm25Similarity.getK1()), collectionStats, termStats);\n    }\n}\n\n ",
            "date": "2018-11-15T13:58:40+0000"
        },
        {
            "id": "comment-16688382",
            "author": "Doug Turnbull",
            "content": "Thanks Adrien Grand - My feeling is if Lucene has something called \"BM25 Similarity\" it should match to the traditional definition of BM25, and shouldn't be deprecated. But if we want to create a faster version, and make it default, I think that would be great.\n\nOr\u00a0if you want to call the\u00a0current (what you call legacy) \"ClassicBM25Similarity\" instead of legacy...\u00a0\n\nI just don't feel it should be deprecated.\u00a0As an IR person, I would be surprised if I was new to Lucene, looked up BM25 and it wasn't actually BM25... ",
            "date": "2018-11-15T17:11:05+0000"
        },
        {
            "id": "comment-16688388",
            "author": "Adrien Grand",
            "content": "My goal is not to make things faster, I don't think it would change anything since this multiplication is only done once for every document anyway. My goal is rather to simplify (one less factor in the furmula, one less factor in the explanation) and also align with recent descriptions of BM25 by its original author himself: if you look at\u00a0 the paper that I mentioned in the description from 2009, it doesn't put (k1+1) on the numerator and says that there is a \"common variant\" of BM25 that does it. ",
            "date": "2018-11-15T17:29:19+0000"
        },
        {
            "id": "comment-16688424",
            "author": "Doug Turnbull",
            "content": "Ah... I assumed \"Adrien has his performance hat on\" which probably colored my perception of the issue\n\nAh yeah my mistake I see that now, I think your strategy makes sense now and helps with scoring comparability across queries. :+1: to your approach with the LegacyBM25 implementation then! ",
            "date": "2018-11-15T17:45:19+0000"
        },
        {
            "id": "comment-16688455",
            "author": "Michael Gibney",
            "content": "I see; +1 as well.\n\nSeeing the main practical motivation for the change as being \"comparable scores across queries\", this would I think also improve (unboosted) score comparability (relevant for dismax) across different fields configured with different similarities and different k1 (TF saturation rate). So this might ultimately help significantly in cases that paradoxically have the bumpiest migration path ... ",
            "date": "2018-11-15T18:10:54+0000"
        },
        {
            "id": "comment-16688474",
            "author": "Elizabeth Haubert",
            "content": "+1 if this gets us closer to BM25F.  I saw the previous paper, but did not understand that the  BM25 with (K1+1) was the non-standard version.  \n\nWould it be worth adding a note here  referencing the in-use algorithm?\n\n\n ",
            "date": "2018-11-15T18:25:34+0000"
        },
        {
            "id": "comment-16702294",
            "author": "Luca Cavanna",
            "content": "I opened https://github.com/apache/lucene-solr/pull/511\u00a0.\u00a0 ",
            "date": "2018-11-28T19:09:01+0000"
        },
        {
            "id": "comment-16702856",
            "author": "Adrien Grand",
            "content": "Thanks Luca Cavanna this looks good to me. Doug Turnbull Jan H\u00f8ydahl Regarding Solr, would you rather like to always use this new BM25Similarity or only if the luceneMatchVersion is greater than or equal to 8? Given that Luca is adding a way to get the old scores as well, it should be easy to pick the right one depending on the luceneMatchVersion like Hoss did in SOLR-8261. ",
            "date": "2018-11-29T08:35:02+0000"
        },
        {
            "id": "comment-16702966",
            "author": "Jan H\u00f8ydahl",
            "content": "I think it would be a far better approach to create a new Similarity with a distinct name (NewBM25Similarity, CleanBM25Similarity, SimplifiedBM25Similarity or similar) for this, so Lucene users can explicitly make an informed choice, instead of changing the implementation of the existing class. Then this issue would not need to touch any Solr code whatsoever.\n\nIf for some reason that is not possible, I think this is a classic example of a usecase for luceneMatchVersion conditional for Solr.\u00a0If so,\u00a0please create a new 8.0\u00a0blocker\u00a0SOLR Jira issue about completing the Solr-side of things. ",
            "date": "2018-11-29T10:11:36+0000"
        },
        {
            "id": "comment-16703007",
            "author": "Adrien Grand",
            "content": "My gut feeling is that this change is going to be unnoticed by the vast majority of users as ordering is preserved. So I'd rather not require changes on their end to use this simpler implementation of BM25 and just document the change in runtime behavior in the release notes. I'm happy with keeping Solr on the current scoring formula and opening a follow-up issue to discuss how to handle the migration.\n\nLuca Cavanna Based on Jan's comments, then let's configure Solr's BM25SimilarityFactory and SchemaSimilarityFactory to use the LegacyBM25Similarity that you added? ",
            "date": "2018-11-29T10:46:05+0000"
        },
        {
            "id": "comment-16703055",
            "author": "Robert Muir",
            "content": "Please deprecate the crazy legacy one too, so it can be eventually removed. ",
            "date": "2018-11-29T11:26:08+0000"
        },
        {
            "id": "comment-16703382",
            "author": "Luca Cavanna",
            "content": "I updated the PR according to the latest comments, and deprecated the newly introduced similarity like Robert suggested. ",
            "date": "2018-11-29T15:52:08+0000"
        },
        {
            "id": "comment-16704438",
            "author": "ASF subversion and git services",
            "content": "Commit cf016f8987e804bcd858a2a414eacdf1b3c54cf5 in lucene-solr's branch refs/heads/master from javanna\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cf016f8 ]\n\nLUCENE-8563: Remove k1+1 constant factor from BM25 formula numerator.\n\nSigned-off-by: Adrien Grand <jpountz@gmail.com> ",
            "date": "2018-11-30T09:04:05+0000"
        },
        {
            "id": "comment-16704452",
            "author": "Adrien Grand",
            "content": "I created a Solr blocker issue as Jan suggested: SOLR-13025. ",
            "date": "2018-11-30T09:12:45+0000"
        },
        {
            "id": "comment-16704455",
            "author": "Adrien Grand",
            "content": "Merged, thanks Luca Cavanna! ",
            "date": "2018-11-30T09:13:03+0000"
        }
    ]
}