{
    "id": "LUCENE-5400",
    "title": "Long text matching email local-part rule in UAX29URLEmailTokenizer causes extremely slow tokenization",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "4.5",
        "status": "Closed",
        "fix_versions": [
            "4.9.1",
            "4.10",
            "6.0"
        ]
    },
    "description": "This is a pretty nasty bug, and causes the cluster to stop accepting updates. I'm not sure how to consistently reproduce it but I have done so numerous times. Switching to a whitespace tokenizer improved indexing speed, and I never got the issue again.\n\nI'm running a 4.6 Snapshot - I had issues with deadlocks with numerous versions of Solr, and have finally narrowed down the problem to this code, which affects many/all versions of Solr.\n\nWhen the thread hits this issue it uses 100% CPU, restarting the node which has the error allows indexing to continue until hit again. Here is thread dump:\n\n\n\nhttp-bio-8080-exec-45 (201)\n\n    org.apache.lucene.analysis.standard.UAX29URLEmailTokenizerImpl.getNextToken\u200b(UAX29URLEmailTokenizerImpl.java:4343)\n    org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer.incrementToken\u200b(UAX29URLEmailTokenizer.java:147)\n    org.apache.lucene.analysis.util.FilteringTokenFilter.incrementToken\u200b(FilteringTokenFilter.java:82)\n    org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken\u200b(LowerCaseFilter.java:54)\n    org.apache.lucene.index.DocInverterPerField.processFields\u200b(DocInverterPerField.java:174)\n    org.apache.lucene.index.DocFieldProcessor.processDocument\u200b(DocFieldProcessor.java:248)\n    org.apache.lucene.index.DocumentsWriterPerThread.updateDocument\u200b(DocumentsWriterPerThread.java:253)\n    org.apache.lucene.index.DocumentsWriter.updateDocument\u200b(DocumentsWriter.java:453)\n    org.apache.lucene.index.IndexWriter.updateDocument\u200b(IndexWriter.java:1517)\n    org.apache.solr.update.DirectUpdateHandler2.addDoc\u200b(DirectUpdateHandler2.java:217)\n    org.apache.solr.update.processor.RunUpdateProcessor.processAdd\u200b(RunUpdateProcessorFactory.java:69)\n    org.apache.solr.update.processor.UpdateRequestProcessor.processAdd\u200b(UpdateRequestProcessor.java:51)\n    org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd\u200b(DistributedUpdateProcessor.java:583)\n    org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd\u200b(DistributedUpdateProcessor.java:719)\n    org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd\u200b(DistributedUpdateProcessor.java:449)\n    org.apache.solr.handler.loader.JavabinLoader$1.update\u200b(JavabinLoader.java:89)\n    org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator\u200b(JavaBinUpdateRequestCodec.java:151)\n    org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator\u200b(JavaBinUpdateRequestCodec.java:131)\n    org.apache.solr.common.util.JavaBinCodec.readVal\u200b(JavaBinCodec.java:221)\n    org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList\u200b(JavaBinUpdateRequestCodec.java:116)\n    org.apache.solr.common.util.JavaBinCodec.readVal\u200b(JavaBinCodec.java:186)\n    org.apache.solr.common.util.JavaBinCodec.unmarshal\u200b(JavaBinCodec.java:112)\n    org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal\u200b(JavaBinUpdateRequestCodec.java:158)\n    org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs\u200b(JavabinLoader.java:99)\n    org.apache.solr.handler.loader.JavabinLoader.load\u200b(JavabinLoader.java:58)\n    org.apache.solr.handler.UpdateRequestHandler$1.load\u200b(UpdateRequestHandler.java:92)\n    org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody\u200b(ContentStreamHandlerBase.java:74)\n    org.apache.solr.handler.RequestHandlerBase.handleRequest\u200b(RequestHandlerBase.java:135)\n    org.apache.solr.core.SolrCore.execute\u200b(SolrCore.java:1859)\n    org.apache.solr.servlet.SolrDispatchFilter.execute\u200b(SolrDispatchFilter.java:703)\n    org.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:406)\n    org.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:195)\n    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter\u200b(ApplicationFilterChain.java:243)\n    org.apache.catalina.core.ApplicationFilterChain.doFilter\u200b(ApplicationFilterChain.java:210)\n    org.apache.catalina.core.StandardWrapperValve.invoke\u200b(StandardWrapperValve.java:222)\n    org.apache.catalina.core.StandardContextValve.invoke\u200b(StandardContextValve.java:123)\n    org.apache.catalina.core.StandardHostValve.invoke\u200b(StandardHostValve.java:171)\n    org.apache.catalina.valves.ErrorReportValve.invoke\u200b(ErrorReportValve.java:99)\n    org.apache.catalina.valves.AccessLogValve.invoke\u200b(AccessLogValve.java:953)\n    org.apache.catalina.core.StandardEngineValve.invoke\u200b(StandardEngineValve.java:118)\n    org.apache.catalina.connector.CoyoteAdapter.service\u200b(CoyoteAdapter.java:408)\n    org.apache.coyote.http11.AbstractHttp11Processor.process\u200b(AbstractHttp11Processor.java:1023)\n    org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process\u200b(AbstractProtocol.java:589)\n    org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run\u200b(JIoEndpoint.java:312)\n    java.util.concurrent.ThreadPoolExecutor.runWorker\u200b(Unknown Source)\n    java.util.concurrent.ThreadPoolExecutor$Worker.run\u200b(Unknown Source)\n    java.lang.Thread.run\u200b(Unknown Source)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13822268",
            "author": "Chris Geeringh",
            "content": "Googling I found someone hit the same issue with elasticsearch, https://gist.github.com/jeremy/2925923 ",
            "date": "2013-11-14T09:29:23+0000"
        },
        {
            "id": "comment-13823764",
            "author": "Hoss Man",
            "content": "Ouch!\n\nChris: You mentioned \"deadlocks\" but your stack trace doesn't have enough detail to be clear if this is truly a deadlocked thread situation or not \u2013 if so, your thread dump should show the locks in contention.\n\n I'm not very familiar with this code, but my gut guess based on your description (particularly the 100% CPU) is that there isn't actually a deadlock, but that in some input causes the tokenizer to go into an infinite loop.\n\nI don't suppose you could post any example input that you are feeding to this Tokenizer that causes the problem?  (I'm guessing not since it's likely to be a big pile of email addresses that shouldn't be posted publicly) ",
            "date": "2013-11-15T16:11:50+0000"
        },
        {
            "id": "comment-13823777",
            "author": "Hoss Man",
            "content": "I don't suppose you could post any example input that you are feeding to this Tokenizer that causes the problem? (I'm guessing not since it's likely to be a big pile of email addresses that shouldn't be posted publicly)\n\nFWIW: If you have a manageable chunk of sample data that semi-consistently reproduces the problem \u2013 but you can't share it publicly on the open internet, please let us know anyway: if you could privately share it with a coupld of the devs (i'm thinking rmuir & sarowe) they might be able to figure out the problem and create new test cases w/o using your actual data. ",
            "date": "2013-11-15T16:26:51+0000"
        },
        {
            "id": "comment-13823795",
            "author": "Robert Muir",
            "content": "and either way, also any configuration information could help:\n\n\tconfiguration information (what flags being passed to the tokenizer)\n\tapproximate size of documents (1KB or 22MB or whatever)\n\tjvm version\n\n ",
            "date": "2013-11-15T16:40:41+0000"
        },
        {
            "id": "comment-13844855",
            "author": "Chris Geeringh",
            "content": "Been busy with an international relocation, however back up and running and got back to this.\n\nThe infinite loop is hit within the loop on line 4305. I have found the offending text, and it is not email addresses, but rather the source code of an html page which has been URLEncoded. Should be relatively easy to reproduce (url encode this pages source code for example). If you need the exact text I am using, I can provide it privately.\n\nAs a stop gap, since this text would never be searched, I'm detecting it and not pushing it up to solr.\n\nTo answer above Q's, im running on Linux, JVM version 7 update 25, docs range in size from 10KB to 4MB, and not passing any flags to the tokenizer. ",
            "date": "2013-12-11T00:10:48+0000"
        },
        {
            "id": "comment-13870482",
            "author": "Steve Rowe",
            "content": "Chris Geeringh privately sent me a document that triggers this problem.  The document consists of an HTML snippet containing a <script> block, which contains a 3-megabyte-long URL-encoded string in single-quotes, given as a parameter to a javascript function defined elsewhere. (The purpose of the javascript function is to URL-decode the string.)\n\nWhen I run this text through UAX29URLEmailTokenizer, it doesn't actually hang - it just tokenizes extremely slowly, consuming less than 100 characters per second on my laptop.  I didn't wait long enough to find out, but I estimate the average scan rate over the entire text is on the order of 200 characters per second, so it would probably take about 4 hours to finish.  (I also sent the same text through StandardTokenizer, which fortunately does not exhibit the slow tokenization behavior.)  To convince myself that this is not an endless loop of some kind, I ran shorter runs (hundreds of chars) of URL-encoded text through UAX29URLEmailTokenizer, and they successfully finished.\n\nI guessed that the problem was with email addresses, so I commented out that part of the UAX29URLEmailTokenizer specification, and that caused the text to be scanned at the same speed as StandardTokenizer.\n\nThe email rule in UAX29URLEmailTokenizer is basically the sequence <local-part>, \"@\", <domain>. What's happening is that the entire 3-MB-long URL-encoded string matches <local-part> (the stuff before the \"@\" in an email address), so for each \"%XX\" URL-encoded byte, the scanner scans through most of the remaining text looking for a \"@\" character, then gives up when it reaches the end of the URL-encoded string without finding one, and finally falls back to tokenizing \"XX\" as <ALPHANUM>.  The scanner then starts again trying to match an email address over the remainder of the URL-encoded string, and so on.  So it's not much of a surprise that this is slow.\n\nRFC5321 says:\n\n\n4.5.3.1.  Size Limits and Minimums\n\n   There are several objects that have required minimum/maximum sizes.\n   Every implementation MUST be able to receive objects of at least\n   these sizes.  Objects larger than these sizes SHOULD be avoided when\n   possible.  However, some Internet mail constructs such as encoded\n   X.400 addresses (RFC 2156 [35]) will often require larger objects.\n   Clients MAY attempt to transmit these, but MUST be prepared for a\n   server to reject them if they cannot be handled by it.  To the\n   maximum extent possible, implementation techniques that impose no\n   limits on the length of these objects should be used.\n\n   Extensions to SMTP may involve the use of characters that occupy more\n   than a single octet each.  This section therefore specifies lengths\n   in octets where absolute lengths, rather than character counts, are\n   intended.\n\n4.5.3.1.1.  Local-part\n\n   The maximum total length of a user name or other local-part is 64\n   octets.\n\n\n\nSo local-parts of email addresses that are going to work everywhere are effectively limited to 64 bytes.  (Section 3 of RFC3696 says the same thing.)\n\nOne possible solution to this problem is to limit the allowable length of the local-part.  Currently the rule looks like:\n\n\nEMAILquotedString = [\\\"] ([\\u0001-\\u0008\\u000B\\u000C\\u000E-\\u0021\\u0023-\\u005B\\u005D-\\u007E] | [\\\\] [\\u0000-\\u007F])* [\\\"]\nEMAILatomText = [A-Za-z0-9!#$%&'*+-/=?\\^_`{|}~]\nEMAILlabel = {EMAILatomText}+ | {EMAILquotedString}\nEMAILlocalPart = {EMAILlabel} (\".\" {EMAILlabel})*\n\n\n\nWhen I try to limit EMAILlabel as follows, JFlex takes forever (minutes) trying to generate the scanner, but then eventually OOMs, even with env. var. ANT_OPT=-Xmx2g (I didn't try larger):\n\n\nEMAILlabel = {EMAILatomText}{1,64} | {EMAILquotedString}\n\n\n\n(Note that EMAILquotedString has the same unlimited length problem - really long quoted ASCII strings could result in the same extremely slow tokenization behavior.)\n\nI think a solution could include a rule matching a fixed-length longer-than-maximum local-part, the action for which sets a lexical state where email addresses aren't allowed, and then pushes back the matched text onto the input stream.  I haven't figured out exactly how to do this yet, though.\n\nI'd welcome other ideas  ",
            "date": "2014-01-14T07:40:15+0000"
        },
        {
            "id": "comment-13870517",
            "author": "Uwe Schindler",
            "content": "This is a Lucene issue, we should move it to Lucene. ",
            "date": "2014-01-14T08:22:00+0000"
        },
        {
            "id": "comment-13871314",
            "author": "Chris Geeringh",
            "content": "No doubt there is a Lucene issue here.\n\nI would have thought that as this can put an entire Solr Cloud out of action there is room for this being a Solr architecture issue too. ",
            "date": "2014-01-14T22:48:40+0000"
        },
        {
            "id": "comment-13871773",
            "author": "Steve Rowe",
            "content": "This is a Lucene issue, we should move it to Lucene.\n\nI agree, I'll move it and update the issue title to reflect this, and also the fact that it's not hanging, but rather tokenizing very slowly. ",
            "date": "2014-01-15T07:48:57+0000"
        },
        {
            "id": "comment-13897182",
            "author": "Edu Garcia",
            "content": "Hi.\n\nWe've hit this bug in Atlassian Confluence (https://jira.atlassian.com/browse/CONF-32566) and it's causing a bit of customer pain.\n\nIs Steve Rowe's solution a viable one, or is someone working on a better one?\n\nThank you! ",
            "date": "2014-02-10T23:13:56+0000"
        },
        {
            "id": "comment-14106685",
            "author": "ASF subversion and git services",
            "content": "Commit 1619730 from Use account \"steve_rowe\" instead in branch 'dev/trunk'\n[ https://svn.apache.org/r1619730 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. ",
            "date": "2014-08-22T10:19:11+0000"
        },
        {
            "id": "comment-14106759",
            "author": "ASF subversion and git services",
            "content": "Commit 1619773 from Use account \"steve_rowe\" instead in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1619773 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged trunk r1619730) ",
            "date": "2014-08-22T12:11:56+0000"
        },
        {
            "id": "comment-14106967",
            "author": "ASF subversion and git services",
            "content": "Commit 1619836 from Ryan Ernst in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1619836 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules. The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged branch_4x r1619773) ",
            "date": "2014-08-22T15:28:36+0000"
        },
        {
            "id": "comment-14106977",
            "author": "ASF subversion and git services",
            "content": "Commit 1619840 from Ryan Ernst in branch 'dev/trunk'\n[ https://svn.apache.org/r1619840 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:32:16+0000"
        },
        {
            "id": "comment-14106980",
            "author": "ASF subversion and git services",
            "content": "Commit 1619841 from Ryan Ernst in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1619841 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:33:03+0000"
        },
        {
            "id": "comment-14106985",
            "author": "ASF subversion and git services",
            "content": "Commit 1619842 from Ryan Ernst in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1619842 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:33:55+0000"
        },
        {
            "id": "comment-14136735",
            "author": "ASF subversion and git services",
            "content": "Commit 1625458 from Use account \"steve_rowe\" instead in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1625458 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged branch_4x r1619773) ",
            "date": "2014-09-17T03:58:23+0000"
        },
        {
            "id": "comment-14136911",
            "author": "Michael McCandless",
            "content": "Thanks for backporting Use account \"steve_rowe\" instead!\n\nHmm, now I'm hitting this test failure on 4.9.x:\n\n\nant test  -Dtestcase=TestStandardAnalyzer -Dtests.method=testRandomHugeStringsGraphAfter -Dtests.seed=65FB3AF41D805AF9 -Dtests.locale=mk_MK -Dtests.timezone=Etc/GMT+5 -Dtests.file.encoding=UTF-8\n\n   [junit4] FAILURE 0.41s | TestStandardAnalyzer.testRandomHugeStringsGraphAfter <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([65FB3AF41D805AF9:CA1B98C5DDF4A2CB]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:751)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:614)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:513)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:437)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestStandardAnalyzer.testRandomHugeStringsGraphAfter(TestStandardAnalyzer.java:402)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: test params are: codec=Lucene46, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {}, locale=mk_MK, timezone=Etc/GMT+5\n   [junit4]   2> NOTE: Linux 3.13.0-32-generic amd64/Oracle Corporation 1.7.0_55 (64-bit)/cpus=8,threads=1,free=378278472,total=503316480\n   [junit4]   2> NOTE: All tests run in this JVM: [TestStandardAnalyzer]\n\n\n\nI dug just a bit... looks like we are passing len=0 to MockReaderWrapper.read(char[], int, int), which it can't handle (it calls realLen = TestUtil.nextInt(random, 1, len);) ... I'm not sure why we don't hit this on 4.x/trunk... ",
            "date": "2014-09-17T07:57:04+0000"
        },
        {
            "id": "comment-14136942",
            "author": "Michael McCandless",
            "content": "I think this \"passing len=0\" was fixed in 4.x/trunk by one of the JFlex upgrades?  When I diff StandardTokenizerImpl.java from 4.9.x to 4.x I see this difference:\n\n\n1025,1027c523,532\n<     /* finally: fill the buffer with new input */\n<     int numRead = zzReader.read(zzBuffer, zzEndRead,\n<                                             zzBuffer.length-zzEndRead);\n---\n>     /* fill the buffer with new input */\n>     int requested = zzBuffer.length - zzEndRead - zzFinalHighSurrogate;           \n>     int totalRead = 0;\n>     while (totalRead < requested) {\n>       int numRead = zzReader.read(zzBuffer, zzEndRead + totalRead, requested - totalRead);\n>       if (numRead == -1) {\n>         break;\n>       }\n>       totalRead += numRead;\n>     }\n\n\n\nI could \"fix\" this by having MockReaderWrapper.read immediately return 0 if len is 0, but this seems scary .... i.e. is there a real bug in StandardTokenizerImpl... ",
            "date": "2014-09-17T08:24:53+0000"
        },
        {
            "id": "comment-14137218",
            "author": "Steve Rowe",
            "content": "Thanks for finding the bug, Michael McCandless.\n\nThis problem doesn't exist on trunk or branch_4x because JFlex 1.6's zzRefill() doesn't call Reader.read() with len=0.  It's only a problem on lucene_solr_4_9 because when I adjusted the generated scanner munging in analysis-common's run-jflex-and-disable-buffer-expansion macro to work with JFlex 1.5-generated code for the 4.9.1 backport, I didn't also modify the code to not call Reader.read() with len=0.\n\nI've changed the munging code locally and TestStandardAnalyzer.testRandomHugeStringsGraphAfter() now passes with the above-mentioned seed.    Here's what StandardTokenizerImpl.zzRefill() has now:\n\n\n/* finally: fill the buffer with new input */\nint numRead = 0, requested = zzBuffer.length - zzEndRead;\nif (requested > 0) numRead = zzReader.read(zzBuffer, zzEndRead, requested);\n\n\n\nI'm currently beasting TestStandardAnalyzer and TestUAX29URLEmailTokenizer (no failures yet after 100 and 50 runs, respectively).\n\nCommitting the fix shortly. ",
            "date": "2014-09-17T13:40:59+0000"
        },
        {
            "id": "comment-14137226",
            "author": "ASF subversion and git services",
            "content": "Commit 1625586 from Use account \"steve_rowe\" instead in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1625586 ]\n\nLUCENE-5897, LUCENE-5400: change JFlex-generated source munging so that zzRefill() doesn't call Reader.read(buffer,start,len) with len=0 ",
            "date": "2014-09-17T13:45:10+0000"
        },
        {
            "id": "comment-14137229",
            "author": "Steve Rowe",
            "content": "Committed backporting fix to lucene_solr_4_9. ",
            "date": "2014-09-17T13:46:42+0000"
        },
        {
            "id": "comment-14137341",
            "author": "Uwe Schindler",
            "content": "This fix is fine, because it spares one method call.\n\nBut in any case the MockReader impl is wrong. You can always call Reader.read() with len=0, this is not disallowed. And all other readers support this. SO MockReader may just need an condition like if (len==0) rreturn 0; ",
            "date": "2014-09-17T14:43:22+0000"
        },
        {
            "id": "comment-14137345",
            "author": "Michael McCandless",
            "content": "+1 to fix MockReaderWrapper. ",
            "date": "2014-09-17T14:48:37+0000"
        },
        {
            "id": "comment-14137350",
            "author": "Michael McCandless",
            "content": "But then again I sort of want to know when a Lucene tokenizer is passing len=0 ... that's ... a strange thing to be doing. ",
            "date": "2014-09-17T14:49:13+0000"
        },
        {
            "id": "comment-14137489",
            "author": "Uwe Schindler",
            "content": "Yeah, so we have to decide:\n\n\tIf MockReaderWrapper is standards conformant\n\tof If we want to detect bugs\nFor the latter we should keep it as it is. Maybe make it explicit and print a good message in the assert.\n\n ",
            "date": "2014-09-17T16:27:21+0000"
        },
        {
            "id": "comment-14137547",
            "author": "Michael McCandless",
            "content": "+1 to make MRW \"anal\" and throw an exc on len==0 explaining that it's actually OK but WTF is your tokenizer doing... ",
            "date": "2014-09-17T16:56:56+0000"
        },
        {
            "id": "comment-14151046",
            "author": "Michael McCandless",
            "content": "Bulk close for Lucene/Solr 4.9.1 release ",
            "date": "2014-09-28T09:05:49+0000"
        }
    ]
}