{
    "id": "LUCENE-5578",
    "title": "Stored fields might accumulate checksums on merges",
    "details": {
        "type": "Bug",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.8"
        ]
    },
    "description": "The bulk merge operation of our stored fields format is optimized in order to avoid decompressing data when not needed. In order to know the offset of the end of the current block, it either consults the stored fields index, or uses fieldsStream.length() for the last chunk.\n\nHowever, we just added checksums at the end of index files, so it might currently copy the current checksum in addition to the last chunk, and then write a new checksum.",
    "attachments": {
        "LUCENE-5578.patch": "https://issues.apache.org/jira/secure/attachment/12638758/LUCENE-5578.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13960399",
            "author": "Adrien Grand",
            "content": "Here is an untested fix, I'd like to write tests for all our index formats before committing it though... ",
            "date": "2014-04-04T20:40:59+0000"
        },
        {
            "id": "comment-13960443",
            "author": "Uwe Schindler",
            "content": "I think in your patch you have to check the format version of the input, otherwise you might copy too less, if it is a pre-4.8 index? Or do we never copy the data if the version is not exactly the same in source and target? ",
            "date": "2014-04-04T21:27:06+0000"
        },
        {
            "id": "comment-13960505",
            "author": "Adrien Grand",
            "content": "Indeed, bulk merge is disabled when the version of the reader is different from the current version. But I would like to write a better fix that doesn't rely on the length of the file to compute the end offset of the last chunk. ",
            "date": "2014-04-04T22:31:28+0000"
        },
        {
            "id": "comment-13960516",
            "author": "Michael McCandless",
            "content": "Wow, nice catch   Why doesn't this cause any test failures?  Won't the last stored doc in a segment be messed up (have the extra checksum bytes appended to it) after a bulk merge? ",
            "date": "2014-04-04T22:37:25+0000"
        },
        {
            "id": "comment-13960522",
            "author": "Adrien Grand",
            "content": "No it won't. Because the metadata of a block of compressed documents also stores the length of the compressed data, so it would stop decompressing at the right place. This problem is completely silent! ",
            "date": "2014-04-04T22:40:53+0000"
        },
        {
            "id": "comment-13960525",
            "author": "Adrien Grand",
            "content": "I quickly discussed with Robert about a way to check for such issues by checking that the stored field files are stable through merges (eg. you merge into 1 segment twice and check that you got the same output every time). We could run this test on all index formats for which such a property is expected (stored fields, term vectors, postings, ...). ",
            "date": "2014-04-04T22:43:27+0000"
        },
        {
            "id": "comment-13960527",
            "author": "Michael McCandless",
            "content": "Ahh, I see.  So those bytes just grow and grow with each bulk merge but are otherwise ignored / have no effect.  Wild.\n\nMaybe we can somehow assert that the length in the metadata \"matches\" the expected length on the last doc. ",
            "date": "2014-04-04T22:45:14+0000"
        },
        {
            "id": "comment-13960535",
            "author": "Adrien Grand",
            "content": "Indeed we should add such sanity checks! ",
            "date": "2014-04-04T22:51:50+0000"
        },
        {
            "id": "comment-13960784",
            "author": "Uwe Schindler",
            "content": "But I would like to write a better fix that doesn't rely on the length of the file to compute the end offset of the last chunk.\n\nWe already changed the stored fields index format because of the additional checkum. Maybe we can add another entry at position maxDoc in the index file pointing to the data block after the last document?\n\nPersonally I also don't like seeking to positions relative to the end of the file. ",
            "date": "2014-04-04T23:27:35+0000"
        },
        {
            "id": "comment-13962079",
            "author": "Adrien Grand",
            "content": "Here is a patch:\n\n\tas suggested by Uwe, I modified the stored fields index file to store the maximum file pointer that is used to store stored fields data instead of relying on the file length,\n\tI added tests to our main index formats to make sure that they don't accumulate stale data when doing bulk merges.\n\n\n\nOur term vectors format (that is quite similar to stored fields) didn't have this bug because it disables bulk merging on the last chunk of a segment. ",
            "date": "2014-04-07T18:07:37+0000"
        },
        {
            "id": "comment-13962131",
            "author": "Uwe Schindler",
            "content": "Shouldn't maxPointer be initialized with getFileLength() if the version is below VERSION_CHECKSUM? Currently it is initialized with -1, which looks wrong. This would never affect us, because we never merge accross versions, but we should maybe add a comment about this. ",
            "date": "2014-04-07T18:47:53+0000"
        },
        {
            "id": "comment-13962298",
            "author": "Robert Muir",
            "content": "Is there not an assert somewhere something like:\n\n\n  assert version == VERSION_CURRENT; // we don't bulk merge across versions\n\n\n\nMaybe this would make it more obvious. ",
            "date": "2014-04-07T21:44:45+0000"
        },
        {
            "id": "comment-13962333",
            "author": "Uwe Schindler",
            "content": "I was referring to the place where it is initialized with -1L. If we would add a comment why this value is good enough, I would be happy. ",
            "date": "2014-04-07T22:20:53+0000"
        },
        {
            "id": "comment-13962668",
            "author": "Adrien Grand",
            "content": "Thanks Uwe, I agree this can be confusing. I just modified the patch to initialize maxPointer with the fields file length for older versions. ",
            "date": "2014-04-08T07:32:46+0000"
        },
        {
            "id": "comment-13962746",
            "author": "Michael McCandless",
            "content": "I like the new BaseIndexFormatTestCase!\n\nThe javadocs for BaseIndexFormatTestCase.extensions says \"Return the\nfile name extensions used by this stored fields format\" but it should\nbe \"by this codec\" (it's not just stored fields).\n\nMaybe rename fileLengths to \"bytesUsedByExtension\"?  (It's summing up\nby extension).  And rename \"expectedChecksums\" to \"expectedBytesUsed\"\nor something?\n\nInstead of requiring the test case to report all extensions it writes\nto (seems error prone?), can't we just list the directory and collate\nby extension ourselves?  Or, if there are extensions (which are they?)\nthat won't be the same size after indexing in two different ways\n... maybe we invert the method and just return those ones? ",
            "date": "2014-04-08T10:00:21+0000"
        },
        {
            "id": "comment-13962756",
            "author": "Adrien Grand",
            "content": "Let me try to see how it goes.  ",
            "date": "2014-04-08T10:15:56+0000"
        },
        {
            "id": "comment-13963003",
            "author": "Adrien Grand",
            "content": "Hey Mike, I tried to apply the changes that you proposed and I think it looks good: the only things that I needed to exclude from the byte counts by extension are:\n\n\tthe segments info file, because of diagnostics (in particular the current timestamp and the source of the segment)\n\tthe MockRandom postings format, since it randomizes various things on the fly such as skip intervals.\n\n ",
            "date": "2014-04-08T14:01:16+0000"
        },
        {
            "id": "comment-13963032",
            "author": "Michael McCandless",
            "content": "+1, looks great!  Thanks Adrien. ",
            "date": "2014-04-08T14:28:20+0000"
        },
        {
            "id": "comment-13963070",
            "author": "Adrien Grand",
            "content": "I ran a few more iterations of the tests and it looks like I need to disable this test on TestPostingsFormat and TestTermVectorsFormat as well since they might pick the MockRandom PF. ",
            "date": "2014-04-08T15:18:24+0000"
        },
        {
            "id": "comment-13963915",
            "author": "ASF subversion and git services",
            "content": "Commit 1585900 from jpountz@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1585900 ]\n\nLUCENE-5578: Prevent stored fields from accumulating stale data on merge. ",
            "date": "2014-04-09T07:44:49+0000"
        },
        {
            "id": "comment-13963935",
            "author": "ASF subversion and git services",
            "content": "Commit 1585906 from jpountz@apache.org in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1585906 ]\n\nLUCENE-5578: Prevent stored fields from accumulating stale data on merge. ",
            "date": "2014-04-09T08:37:58+0000"
        },
        {
            "id": "comment-13982579",
            "author": "Uwe Schindler",
            "content": "Close issue after release of 4.8.0 ",
            "date": "2014-04-27T23:25:48+0000"
        }
    ]
}