{
    "id": "SOLR-3653",
    "title": "Custom bigramming filter for to handle Smart Chinese edge cases",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "The \"Smart\" Simplified Chinese toolkit in lucene/analysis/smartcn does not work in some edge cases. It fails to split certain words which were not part of the dictionary or training corpus. \n\nThis patch supplies a bigramming class to handle these occasional mistakes. The algorithm creates bigrams out of all \"words\" longer than two ideograms.",
    "attachments": {
        "translations_first_500.trigrams.txt": "https://issues.apache.org/jira/secure/attachment/12546235/translations_first_500.trigrams.txt",
        "translations_first_500.quad.txt": "https://issues.apache.org/jira/secure/attachment/12546234/translations_first_500.quad.txt",
        "SmartChineseType.pdf": "https://issues.apache.org/jira/secure/attachment/12537313/SmartChineseType.pdf",
        "translations_450.five2thirteen.txt": "https://issues.apache.org/jira/secure/attachment/12546236/translations_450.five2thirteen.txt",
        "SOLR-3653.patch": "https://issues.apache.org/jira/secure/attachment/12537311/SOLR-3653.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Lance Norskog",
            "id": "comment-13418997",
            "date": "2012-07-20T08:06:02+0000",
            "content": "The SmartChineseWordTokenFilter is a statistical algorithm (Hidden Markov Model to be exact) which was trained on a corpus of training text. It's purpose is to split text into \"words\", which are singles, bigrams and occasionally trigrams of Simplified Chinese ideograms (letters). It does a very good job, but since it is statistically based it is not perfect. When it fails, it emits \"words\" that are 4 or more ideograms. These are really phrases. These phrases contain real words which should be searchable.\n\nThe attached PDF of the Analysis page shows the problem. Chinese legal text proved a pathological case and created a 7-ideogram word. In order to make parts of this text searchable, the 7-letter phrase has to be broken into n-grams. Unigrams give more recall while bigrams give more precision. \n\nThis patch includes a new SmartChineseBigramFilter takes any words not split by the WordTokenFilter and creates bigrams from them. The bigrams only span the unsplit phrase. They do not overlap between two adjoining unsplit phrases. The attached PDF shows this effect as well between the first and second unsplit phrases.\n\nI am not an expert on the Chinese language or the HMM technology used in the Smart Chinese toolkit. I created the bigram filter after difficulties attempting to supply a high-quality search experience for Chinese legal documents. This is a straw-man solution to the problem. If you know better, please say so and we will iterate.\n\nThe patch includes a 'text_zh' field type which includes the bigram filter. The bigram filter is essential if 'text_zh' is to be the preferred recommendation. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13419096",
            "date": "2012-07-20T12:48:03+0000",
            "content": "\nThe \"Smart\" Simplified Chinese toolkit in lucene/analysis/smartcn has no Solr factories\n\nActually there are factories in contrib/analysis-extras.\n\n\nand includes a \"fixup\" class to handle the occasional mistake made by the Smart Chinese implementation.\n\nI am not sure on this: if someone wants to mix an n-gram technique with a word model, they can just\nuse two fields? If they want to limit the n-gram field to only longer terms, they should use LengthFilter.\n\nFurthermore, I don't really understand the problem here.\nThe word you are upset about (\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd) is in the smartcn dictionary. As I understand, this word basically means \"PRC\". \nThis is a single concept and makes sense as an indexing unit. Why do we care how long it is in characters? "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13419326",
            "date": "2012-07-20T16:34:23+0000",
            "content": "Actually there are factories in contrib/analysis-extras.\nYou're right, I was thinking of a previous project.\nI am not sure on this: if someone wants to mix an n-gram technique with a word model, they can just use two fields? If they want to limit the n-gram field to only longer terms, they should use LengthFilter.\n\nIs this the design?\n\nWord-based field: \n    SmartChineseWordTokenFilter -> LengthFilter accept 1-3 letters\nBigram-based field:\n    SmartChineseWordTokenFilter -> LengthFilter accept 4 or longer -> Chinese-only bigrams\n\n\nThis works if the user searches simple words, like on a consumer site. In the legal document site, people block-copy 60-word document titles and expect to find the matching title first on the list. This requires a phrase search where 0 variations in position gives the exact title. If the two classes of terms are in two different fields, will that work? I did not think parsers did \n\nAlso, this design needs to allow for mixed language text: year numbers, English words. Are the existing Lucene filters flexible enough to do this?\n\nThe word you are upset about (\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd) is in the smartcn dictionary. As I understand, this word basically means \"PRC\". This is a single concept and makes sense as an indexing unit. Why do we care how long it is in characters?\n\nBecause parts of it are also words, which should be searchable. Here are two more failed words: \"\u4e2a\u4eba\u6240\u5f97\u7a0e\" (personal/individual \"income tax\") and \"\u793e\u4f1a\u4fdd\u9669\" (National Congress, political body). I can imagine Congress would be in the dictionary, but \"personal income tax\"? If you search for income tax: \"\u6240\u5f97\u7a0e\" you will not find personal income tax. This points up a flaw: the bigram trick will not find this trigram.\n\nHow do you know what's in the dictionary? The files are in a .mem format. I can't find a main program for them.\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13419330",
            "date": "2012-07-20T16:40:09+0000",
            "content": "\nBecause parts of it are also words, which should be searchable.\n\nSays who? There is no real word boundaries in this language. \n\nIf you want to start indexing individual characters, just use StandardTokenizer.\n\nNone of your examples are \"failures\" of this tokenizer. This is what it has in its dictionary! "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13419726",
            "date": "2012-07-21T02:50:07+0000",
            "content": "I was wrong, the packaging problem was not that Solr factories for Smart Chinese toolkit were missing, but that they were not packaged well. This is being addressed in SOLR-3623. Changing the JIRA issue name to reflect this. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13419729",
            "date": "2012-07-21T03:13:28+0000",
            "content": "The toolkit cannot be perfect, and this patch solves one problem I have found with it. This example is a good example of the edge case: \"\u4e2a\u4eba\u6240\u5f97\u7a0e\" (personal/individual \"income tax\"). The first two letters are a common modifier \"individual or personal\" and the last three letters mean (roughly) \"income tax\". The latter should be an indexable unit. For my sanity I'll refer to the word as ABCDE. The letters A and AB are common modifiers. \n\nIn cases where CDE' is not in the dictionary, the toolkit should split the word between ABCDE' at AB. This would be the point of using a statistical model instead of merely relying on a dictionary, right?  \n\nI found the CJK bigram system clumsy and the unigram approach to be useless. If the user of the Smart Chinese toolkit has to assume that it is perfect, then it is only useable for the context of the training data. It has proved a nice general solution to the problem and it would be a shame to render it useless because of this one rough edge.  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13419791",
            "date": "2012-07-21T10:14:11+0000",
            "content": "Why not just use a synonymfilter for your special cases? I think this is the most intuitive solution. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13419963",
            "date": "2012-07-21T22:34:10+0000",
            "content": "Yeah, good point. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13461572",
            "date": "2012-09-24T02:46:38+0000",
            "content": "I ran some counts on a database of 300k Chinese legal documents. The index has a unigram field based on the StandardAnalyzer, a bigram field based on the CJK analyzer, and a Smart Chinese field. I pulled the terms for all of them and filtered for Chinese ideograms only. These are text unigrams, with \n\n\n\tThe unigram field had 55k terms.\n\tThe bigram field had 1.8 million terms.\n\tThe Smart Chinese field had 417k terms:\n\t\n\t\tunigrams: 9.6k\n\t\tbigrams: 40k\n\t\ttrigrams: 14.6k\n\t\tfour: 5.6k\n\t\tfive: 300\n\t\tsix: 70\n\t\tseven: 51\n\t\teight: 19\n\t\tnine: 7\n\t\tten: 2\n\t\televen: 3\n\t\ttwelve: 2\n\t\tthirteen: 3\n\t\n\t\n\n\n\nThe 4+ ngrams are essentially parsing failures by the Smart Chinese tokenizer. I have attached three Google Translate versions of the longer ngrams. 'translations_first_500.trigrams.txt' and 'translations_first_500.quad.txt' are the most common 3-ideogram and 4-ideogram terms. They have a lot of phrases which should have been split.  'translations_450.five2thirteen.txt' are 450 ngrams which are 5 ideograms or longer.  The longer ones have a lot of formal geographical names, government organization names and official propaganda phrases, more as the length increases. \n\nFor this corpus, based the above breakdown and on other experience:\n\n\tCJK is a waste of disk space. Bigrams introduce a ton of noise.\n\tUnigrams might work well if you only do strict phrase searches. But searching for A, B, and C separately when given ABC is useless.\n\tIf you search for raw country names, Smart Chinese lets you down when the document uses the formal name.\n\n\n\nSmart Chinese really does need to be split into bigrams. To cut bigram noise, I would take the database of bigrams that it generates, and then use these to guide splitting 3+ grams into bigrams. That is, if it ever generates AB, then the splitter turns ABCD into (AB CD). BC would be considered 'bigram noise'. Similarly, if Smart Chinese generates EF, then DEFG would become (D EF G).\n\nHowever, a good fallback would be to have two fields, Smart Chinese and unigrams, with Smart Chinese boosted upwards and unigrams only with strict phrase search. With a high term count, bigrams are not helpful. You might even want to search Smart Chinese first, and then do unigram loose phrase search only if the recall is too low or the user is unhappy with the Smart Chinese results. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13461573",
            "date": "2012-09-24T02:48:47+0000",
            "content": "Another note: one trigram is the number 15. There are several conventions for representing integers, including regional quirks. There is no 'number canonicalizer' in the Smart Chinese toolkit. This could be a problem with formal documents: historical, government docs, treaties and the like.\n\nhttp://en.wikipedia.org/wiki/Chinese_numerals#Whole_numbers "
        }
    ]
}