{
    "id": "SOLR-9618",
    "title": "Tests hang on a forked process (deadlock inside the process)",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Cannot Reproduce",
        "priority": "Major"
    },
    "description": "",
    "attachments": {
        "trace.log.bz2": "https://issues.apache.org/jira/secure/attachment/12832510/trace.log.bz2"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-10-10T17:51:34+0000",
            "author": "Uwe Schindler",
            "content": "Here is the stack trace ",
            "id": "comment-15562971"
        },
        {
            "date": "2016-10-10T19:10:11+0000",
            "author": "Dawid Weiss",
            "content": "Thanks Mikhail, but knowing what's stuck on what is one thing and knowing why it got there is another (I actually did lock ownership analysis too). Check out this stack trace in full:\n\n\"zkCallback-3166-thread-10\" #47841 prio=5 os_prio=0 tid=0x00007f5f800eb000 nid=0x5912 waiting for monitor entry [0x00007f5e94c1a000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n\tat java.util.logging.StreamHandler.publish(StreamHandler.java:206)\n\t- waiting to lock <0x00000000e0e6e2b8> (a java.util.logging.ConsoleHandler)\n\tat java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:116)\n\tat java.util.logging.Logger.log(Logger.java:738)\n\tat java.util.logging.Logger.doLog(Logger.java:765)\n\tat java.util.logging.Logger.log(Logger.java:876)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler.uncaughtException(RandomizedRunner.java:524)\n\tat java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057)\n\tat java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052)\n\tat java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052)\n\tat com.carrotsearch.randomizedtesting.RunnerThreadGroup.uncaughtException(RunnerThreadGroup.java:32)\n\tat java.lang.Thread.dispatchUncaughtException(Thread.java:1956)\n\n\n\nit originates from a private method that is invoked by the JVM... \n\nI see an immediate patch in removing this from RR:\n\n      Logger.getLogger(RunnerThreadGroup.class.getSimpleName()).log(\n          Level.WARNING,\n          \"Uncaught exception in thread: \" + t, e);\n\n\n\nbut I'd like to understand how this interaction can be reliably repeated; perhaps there is a deeper problem that needs resolving. ",
            "id": "comment-15563184"
        },
        {
            "date": "2016-10-10T19:16:10+0000",
            "author": "Uwe Schindler",
            "content": "BTW, it is interesting that on Solaris this type of hanging thread seems to occur much more often. On the Solaris Jenkins node on Policeman, many jobs, also outside of Solr hang in a similar way. If it happens again, I will catch a stack trace, too. ",
            "id": "comment-15563199"
        },
        {
            "date": "2016-10-10T19:35:13+0000",
            "author": "Dawid Weiss",
            "content": "Sure, please do! It'd be very helpful to get more stack traces from such situations. One very dirty solution to the immediate problem of builds hanging for days is to use the super-duper JVM option telling it to halt itself after a deadline... this could be passed to forked off junit subprocesses... Works on OpenJDK JVMs (didn't check 9).\n\n\n  product(intx, SelfDestructTimer, 0,                                       \\\n          \"Will cause VM to terminate after a given time (in minutes) \"     \\\n          \"(0 means off)\")                                                  \\\n\n ",
            "id": "comment-15563252"
        },
        {
            "date": "2016-10-11T18:55:25+0000",
            "author": "Dawid Weiss",
            "content": "I made some internal changes to the runner's code. I still don't understand the scenario in which this deadlock/ stacktrace is possible (it's quite insane!), but hopefully we can get more diagnostics on next hangup or maybe the problem will go away after these changes (he, he, he).\n\nI'll add a few more changes to the runner and release 2.4.0, then integrate it with master. ",
            "id": "comment-15566263"
        },
        {
            "date": "2016-10-25T07:52:40+0000",
            "author": "Dawid Weiss",
            "content": "Do we still have hanging processes after the upgrade to RR 2.4.0 (Uwe Schindler?). ",
            "id": "comment-15604547"
        },
        {
            "date": "2016-10-25T08:02:28+0000",
            "author": "Uwe Schindler",
            "content": "Hi,\n\non Solaris there was one hanging: https://jenkins.thetaphi.de/job/Lucene-Solr-master-Solaris/916\n\nWhen did you upgrade the runner - before or after that? ",
            "id": "comment-15604576"
        },
        {
            "date": "2016-10-25T08:08:33+0000",
            "author": "Dawid Weiss",
            "content": "Seems like after that, in a19ec194d25692. ",
            "id": "comment-15604592"
        },
        {
            "date": "2017-02-13T09:54:28+0000",
            "author": "Dawid Weiss",
            "content": "Closing this issue for now. The changes made to the runner seem to solve the issue (at least partially; I know it's not perfect).\n\nIf there are hung tests, reopen or file a new issue please. ",
            "id": "comment-15863407"
        }
    ]
}