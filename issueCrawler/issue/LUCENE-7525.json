{
    "id": "LUCENE-7525",
    "title": "ASCIIFoldingFilter.foldToASCII performance issue due to large compiled method size",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "6.2.1",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "The ASCIIFoldingFilter.foldToASCII method has an enormous switch statement and is too large for the HotSpot compiler to compile; causing a performance problem.\n\nThe method is about 13K compiled, versus the 8KB HotSpot limit. So splitting the method in half works around the problem.\n\nIn my tests splitting the method in half resulted in a 5X performance increase.\n\nIn the test code below you can see how slow the fold method is, even when it is using the shortcut when the character is less than 0x80, compared to an inline implementation of the same shortcut.\n\nSo a workaround is to split the method. I'm happy to provide a patch. It's a hack, of course. Perhaps using the MappingCharFilterFactory with an input file as per SOLR-2013 would be a better replacement for this method in this class?\n\n\npublic class ASCIIFoldingFilterPerformanceTest {\n\n\tprivate static final int ITERATIONS = 1_000_000;\n\n\t@Test\n\tpublic void testFoldShortString() {\n\t\tchar[] input = \"testing\".toCharArray();\n\t\tchar[] output = new char[input.length * 4];\n\n\t\tfor (int i = 0; i < ITERATIONS; i++) {\n\t\t\tASCIIFoldingFilter.foldToASCII(input, 0, output, 0, input.length);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testFoldShortAccentedString() {\n\t\tchar[] input = \"\u00e9\u00fa\u00e9\u00fa\u00f8\u00df\u00fc\u00e4\u00e9\u00fa\u00e9\u00fa\u00f8\u00df\u00fc\u00e4\".toCharArray();\n\t\tchar[] output = new char[input.length * 4];\n\n\t\tfor (int i = 0; i < ITERATIONS; i++) {\n\t\t\tASCIIFoldingFilter.foldToASCII(input, 0, output, 0, input.length);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testManualFoldTinyString() {\n\t\tchar[] input = \"t\".toCharArray();\n\t\tchar[] output = new char[input.length * 4];\n\n\t\tfor (int i = 0; i < ITERATIONS; i++) {\n\t\t\tint k = 0;\n\t\t\tfor (int j = 0; j < 1; ++j) {\n\t\t\t\tfinal char c = input[j];\n\t\t\t\tif (c < '\\u0080') {\n\t\t\t\t\toutput[k++] = c;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.assertTrue(false);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
    "attachments": {
        "ASCIIFolding.java": "https://issues.apache.org/jira/secure/attachment/12835698/ASCIIFolding.java",
        "ASCIIFoldingFilter.java": "https://issues.apache.org/jira/secure/attachment/12835703/ASCIIFoldingFilter.java",
        "TestASCIIFolding.java": "https://issues.apache.org/jira/secure/attachment/12835699/TestASCIIFolding.java",
        "LUCENE-7525.patch": "https://issues.apache.org/jira/secure/attachment/12849691/LUCENE-7525.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15612135",
            "author": "Adrien Grand",
            "date": "2016-10-27T15:01:01+0000",
            "content": "This is probably one of the most used token filters, so +1 to explore if/how we can make it faster. Maybe we should add an analyzer that has case folding enabled to http://people.apache.org/~mikemccand/lucenebench/analyzers.html. "
        },
        {
            "id": "comment-15612160",
            "author": "Michael Braun",
            "date": "2016-10-27T15:09:25+0000",
            "content": "Was just profiling indexing yesterday on our side and noticed the exact same thing -  would love to see the performance of this method improved!  "
        },
        {
            "id": "comment-15612234",
            "author": "Uwe Schindler",
            "date": "2016-10-27T15:34:49+0000",
            "content": "I'd make this huge switch statement a different lookup structure, e.g., a HashMap or similar (possibly/maybe with Java9 Lambdas as values). "
        },
        {
            "id": "comment-15613746",
            "author": "Karl von Randow",
            "date": "2016-10-28T00:11:22+0000",
            "content": "I have created a version that uses static arrays and a binary search. This performs similarly to splitting the existing code into two methods; splitting the switch cases between the methods, and delegating to the second method from the first's default: block. Not surprising, as the switch of that size (and breadth of values) is a binary search, but native.\n\nI'll try to attach the code examples of the two approaches. These two approaches are perhaps a simpler change than going to a new lookup structure, and I think will be more performant (as they're a binary search over an array)?\n\nThe simplest change is the switch-split. It will also be the most attractive in the Git history (as the method is literally split in two about half-way through the case statements). It will also be the easiest to prove that the behaviour is the same as before. "
        },
        {
            "id": "comment-15613753",
            "author": "Karl von Randow",
            "date": "2016-10-28T00:15:12+0000",
            "content": "An example of a binary search over an array data structure as an alternative for the large switch statement. "
        },
        {
            "id": "comment-15613774",
            "author": "Karl von Randow",
            "date": "2016-10-28T00:25:07+0000",
            "content": "Attached ASCIIFoldingFilter.java with an example of splitting the switch statement into two methods to come in within the compile limit. "
        },
        {
            "id": "comment-15616922",
            "author": "Ahmet Arslan",
            "date": "2016-10-28T23:24:21+0000",
            "content": "Can workings of ICUFoldingFilter give any insight here? "
        },
        {
            "id": "comment-15617679",
            "author": "Uwe Schindler",
            "date": "2016-10-29T07:55:10+0000",
            "content": "I'd suggest to use the simple binary search approach, but without generated code. I'd suggest to convert the large switch statement once to a simple text file and load it as resource in static initializer.\n\nThis allows to maybe further extend the folding filter so people can use their own mappings by pointing to an input stream! "
        },
        {
            "id": "comment-15618018",
            "author": "Michael McCandless",
            "date": "2016-10-29T12:04:42+0000",
            "content": "Maybe an FST, like MappingCharFilter? "
        },
        {
            "id": "comment-15618025",
            "author": "Uwe Schindler",
            "date": "2016-10-29T12:10:39+0000",
            "content": "I am not sure if an FST is not like to use a sledgehammer to crack a nut  We just need a lookup for single ints in a for-loop and replace those ints with a sequence of other ints.\n\nI will check the ICU source code to se what they are doing. "
        },
        {
            "id": "comment-15618031",
            "author": "Uwe Schindler",
            "date": "2016-10-29T12:13:50+0000",
            "content": "I think, we can for now replace the large switch statement with a resource file. I'd have 2 ideas:\n\n\n\tA UTF-8 encoded file with 2 columns: first column is a single char, 2nd column is a series of replacements. I don't really like this approach as it is very sensitive to corrumption by editors and hard to commit correct\n\tA simple file like int => int,int,int // comment, this is easy to parse and convert, but backside is that its harder to read the codepoints (for that we have a comment)\n\n\n\nThe actual code that parses the file and converts to the \"lookup table\" could be replaced easily afterwards. I'd start with a binary lookup as suggested (similar to the switch statement's internal impl). "
        },
        {
            "id": "comment-15618059",
            "author": "Alexandre Rafalovitch",
            "date": "2016-10-29T12:36:42+0000",
            "content": "I thought the ASCII Filter was basically a legacy filter and it grew into MappingCharFilterFactory. Now, we are talking about making ASCII Filter even more like Mapping Char Filter. Why would we need both then? I know one is Token Filter and another Char filter, but I am not sure it is sufficiently important for this discussion. "
        },
        {
            "id": "comment-15618179",
            "author": "Uwe Schindler",
            "date": "2016-10-29T14:14:51+0000",
            "content": "I thought the ASCII Filter was basically a legacy filter and it grew into MappingCharFilter\n\nI generally prefer ASCIIFoldingFilter because it allows you much more flexibility. The problem with MappingCharFilter is that you can only apply it before tokenization. And this is the major downside: If you have tokenizers that rely on language specific stuff (Asian like Japanese, Chinese,...) it is a bad idea to do such stuff like before the tokenization. Also if you do stemming: Stemming in France breaks if you remove accents before! So ASCIIFoldingFilter is in most analysis chains the very last filter! "
        },
        {
            "id": "comment-15620484",
            "author": "Karl von Randow",
            "date": "2016-10-30T19:58:18+0000",
            "content": "It seemed to me that we could try to generalise the FST creation from MappingCharFilter and use it in both places; therefore retaining the existing differences between the two classes. I haven't tried that route as the switch-split was simple, minimal, and addressed the issue  "
        },
        {
            "id": "comment-15830662",
            "author": "Michael Braun",
            "date": "2017-01-19T21:46:59+0000",
            "content": "We are still experiencing this - is there someone actively working on getting a solution committed?  "
        },
        {
            "id": "comment-15842889",
            "author": "Adrien Grand",
            "date": "2017-01-27T13:50:36+0000",
            "content": "I tried to work on a minimal patch that addresses the performance issue. It reuses the existing slow method to build a conversion map and then uses the conversion map at runtime. It seems to run an order of magnitude faster on my machine. I only see it as a short term solution, I think Uwe's plan is better for the long term. "
        },
        {
            "id": "comment-15842901",
            "author": "Steve Rowe",
            "date": "2017-01-27T14:05:57+0000",
            "content": "\nI think, we can for now replace the large switch statement with a resource file. I'd have 2 ideas:\n\n\tA UTF-8 encoded file with 2 columns: first column is a single char, 2nd column is a series of replacements. I don't really like this approach as it is very sensitive to corrumption by editors and hard to commit correct\n\tA simple file like int => int,int,int // comment, this is easy to parse and convert, but backside is that its harder to read the codepoints (for that we have a comment)\n\n\n\nI wrote a Perl script to create mapping-FoldToASCII.txt, which is usable with MappingCharFilter, from the ASCIIFoldingFilter code - the script is actually embedded in that file, which is included in several of Solr's example configsets, e.g. under solr/server/solr/configsets/sample_techproducts_configs/conf/.  Maybe this file could be used directly?  It's human friendly, so would allow for easy user customization. "
        },
        {
            "id": "comment-15842904",
            "author": "Adrien Grand",
            "date": "2017-01-27T14:08:24+0000",
            "content": "Same patch, but with comments. "
        },
        {
            "id": "comment-15842914",
            "author": "Adrien Grand",
            "date": "2017-01-27T14:24:14+0000",
            "content": "+1 Steve Rowe "
        },
        {
            "id": "comment-15942446",
            "author": "Michael Braun",
            "date": "2017-03-26T21:02:36+0000",
            "content": "Should a new utility Char-To-Int map be created in lucene-core utils to store the char->int (combined chars) map? "
        }
    ]
}