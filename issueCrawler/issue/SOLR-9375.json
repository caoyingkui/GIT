{
    "id": "SOLR-9375",
    "title": "Collections API BACKUP distribution of responsibilities is odd",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "6.1",
        "status": "Closed",
        "resolution": "Won't Fix",
        "priority": "Minor"
    },
    "description": "We are running a four node solr cloud setup in 6.1.0 and have been playing around with the new BACKUP command out of collections api.  We experienced an oddity that may be by design, but I thought I would raise it as a potential issue as it seems non-optimal.  \n\nMy understanding is that the backup command assumes a shared NFS mount that is available across all nodes, and in that configuration things work fine in our testing.  But we were looking at an alternate configuration where each solr node has a local mount (to a local drive).  Our hope was that the backup process would cause the collection leader to backup the collection to its local drive.  However, in practice this fails with a \"directory non-existent error\".  What appears to be happening is that overseer node is creating the backup directory on its local mount, and then forwarding the request to the collection leader (which happens to be a different node) for actual backup execution, which then fails because the created directory doesn't exist for it.  In the configuration where everyone shares a remote mount, this isn't problematic since the two nodes are operating on the same endpoint.  \n\nMy expectation would be that the collection leader would create the local directory and then backup to that directory, which would support this configuration.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-08-03T21:33:16+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Well, since the current backup feature requires shared file system, this is by design as you say yourself.\nIn 6.2 we got the support for custom backup repository types (SOLR-7374), so perhaps it would be possible to create a repository impl that does what you're looking for - have not checked the code. ",
            "id": "comment-15406656"
        },
        {
            "date": "2016-08-03T21:45:09+0000",
            "author": "Ronald Braun",
            "content": "Ok, cool, feel free to close this ticket then.  It was just my mistaken expectations about how it would work under the hood. ",
            "id": "comment-15406678"
        },
        {
            "date": "2016-08-04T05:53:58+0000",
            "author": "Varun Thacker",
            "content": "Hi Ronald,\n\nThe problem with not making local backups is - what if the replica has moved to another machine? how will we restore then?\n\nThe idea of local copy works well if we have a set number of servers and the replicas for a collection are assigned to a server and doesn't change . \n\nHence we went with the NFS mount option so it could cater to more use cases where the NFS mount stays , but machines can change over time etc. \n\nI don't think the repository concept helps much here. Repositories are designed to make the underlying storage pluggable i.e before SOLR-7374 only NFS mount worked. But now we have HDFS support as well. And one could write a S3 repository etc.\n\nFor your use case I would recommend to use the Standalone Mode backup/restore APIs - https://cwiki.apache.org/confluence/display/solr/Making+and+Restoring+Backups\nIt \n\nYou would need to change gettingstarted with collectionName_shardX_replicaY and call backup / restore individually on each replica of your collection ",
            "id": "comment-15407219"
        },
        {
            "date": "2016-08-04T16:26:01+0000",
            "author": "Ronald Braun",
            "content": "Hi Varun, thanks for the suggestions!  We were originally looking at the Standalone Mode methodology but were drawn to the new backup op since it seems to package together more useful stuff in the backup.  Your point about restoration of local backups is a good one, and we were going to manage that locally as part of our longer archiving process anyway, but I certainly agree the shared design makes sense.  I just didn't realize the shared endpoint was a hard requirement for using collections api backup methodology \u2013 the docs seems to make it sound like a suggestion more than not, but that was probably my misreading.  The one oddity I thought from a design perspective was having different nodes create the backup location and then use that location \u2013 the coordination between nodes seems to inject a failpoint into the process if something goes wrong in their worldviews.  But if you want to ensure the overseer has access to the actual restoration location for restore, I guess it kinda sorta makes sense that it is the one to create it.  Anyway, all good, thanks for your insights! ",
            "id": "comment-15408054"
        },
        {
            "date": "2016-08-04T16:35:40+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "NFS can cache directory listings locally so it is dangerous to rely on the fact that a node has created a folder that can be used immediately on a different node. I think that we should always attempt to create the backup folder on all nodes which are actually performing the backup. In the worst case you will get an exception saying that the directory already exists. It will also remove the hard requirement of a shared mount point which we have no way of enforcing anyway. ",
            "id": "comment-15408072"
        },
        {
            "date": "2016-08-04T16:36:02+0000",
            "author": "Varun Thacker",
            "content": "he docs seems to make it sound like a suggestion more than not, but that was probably my misreading. \n\nI'll go over the docs and see if we can make it more clear\n\nSo the way it works is - the backup command goes to the overseer . Say the location you specify is \"/my/nfs/mount/solr_backup\" and the backup name is \"backup1\"\n\nSo under \"/my/nfs/mount/solr_backup/backup1\" , you'll see every shards indexes , ZK data and a metadata file .  ",
            "id": "comment-15408073"
        }
    ]
}