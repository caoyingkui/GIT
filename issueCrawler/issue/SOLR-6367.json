{
    "id": "SOLR-6367",
    "title": "empty tlog on HDFS when hard crash - no docs to replay on recovery",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "Filing this bug based on an email to solr-user@lucene from Tom Chen (Fri, 18 Jul 2014)...\n\n\nReproduce steps:\n1) Setup Solr to run on HDFS like this:\n\n\njava -Dsolr.directoryFactory=HdfsDirectoryFactory\n     -Dsolr.lock.type=hdfs\n     -Dsolr.hdfs.home=hdfs://host:port/path\n\n\n\nFor the purpose of this testing, turn off the default auto commit in solrconfig.xml, i.e. comment out autoCommit like this:\n\n<!--\n<autoCommit>\n       <maxTime>${solr.autoCommit.maxTime:15000}</maxTime>\n       <openSearcher>false</openSearcher>\n </autoCommit>\n-->\n\n\n\n2) Add a document without commit:\n{{curl \"http://localhost:8983/solr/collection1/update?commit=false\" -H\n\"Content-type:text/xml; charset=utf-8\" --data-binary \"@solr.xml\"}}\n\n3) Solr generate empty tlog file (0 file size, the last one ends with 6):\n\n[hadoop@hdtest042 exampledocs]$ hadoop fs -ls\n/path/collection1/core_node1/data/tlog\nFound 5 items\n-rw-r--r--   1 hadoop hadoop        667 2014-07-18 08:47\n/path/collection1/core_node1/data/tlog/tlog.0000000000000000001\n-rw-r--r--   1 hadoop hadoop         67 2014-07-18 08:47\n/path/collection1/core_node1/data/tlog/tlog.0000000000000000003\n-rw-r--r--   1 hadoop hadoop        667 2014-07-18 08:47\n/path/collection1/core_node1/data/tlog/tlog.0000000000000000004\n-rw-r--r--   1 hadoop hadoop          0 2014-07-18 09:02\n/path/collection1/core_node1/data/tlog/tlog.0000000000000000005\n-rw-r--r--   1 hadoop hadoop          0 2014-07-18 09:02\n/path/collection1/core_node1/data/tlog/tlog.0000000000000000006\n\n\n\n4) Simulate Solr crash by killing the process with -9 option.\n\n5) restart the Solr process. Observation is that uncommitted document are\nnot replayed, files in tlog directory are cleaned up. Hence uncommitted\ndocument(s) is lost.\n\nAm I missing anything or this is a bug?\n\nBTW, additional observations:\na) If in step 4) Solr is stopped gracefully (i.e. without -9 option),\nnon-empty tlog file is geneated and after re-starting Solr, uncommitted\ndocument is replayed as expected.\n\nb) If Solr doesn't run on HDFS (i.e. on local file system), this issue is\nnot observed either.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Praneeth",
            "id": "comment-14263302",
            "date": "2015-01-02T23:02:32+0000",
            "content": "This is because of a bug here in the HdfsTransactionLog. The call fos.flushBuffer() should be fos.flush(). flushBuffer() in the underlying FastOutputStream does not actually flush. I could have taken up the task of making the minor change but I haven't contributed before and do not know the process. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14268705",
            "date": "2015-01-08T02:04:58+0000",
            "content": "miller: can you take a look at Venkata's proposed solution? is this a viable easy win for 5.0?\n\n(the HdfsTransactionLog code in question follows the same flushBuffer() instead of flush() model used in the TransactionLog parent class \u2013 but is flush() more appropriate in this case because of how FSDataOutputStream is wrapped in the HDFS case? ) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14271699",
            "date": "2015-01-09T18:52:42+0000",
            "content": "Praneeth, but we hflush or hsync on the stream that FastOutputStream wraps right after.\n\nThe non HdfsTransactionLog also only calls flushBuffer unless you configure to then fsync.\n\nIt may be necessary to call flush instead of flushBuffer here, but I don't understand why that would be yet. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14273844",
            "date": "2015-01-12T17:54:35+0000",
            "content": "I have not been able to reproduce this so far. With kill -9, I am not losing the doc. "
        },
        {
            "author": "Praneeth",
            "id": "comment-14273862",
            "date": "2015-01-12T18:02:34+0000",
            "content": "Sorry, I couldn't get to your previous comment earlier. I was too hasty making my first comment on the issue. I was wrong and call to flush() wouldn't make a difference. I have been able to consistently reproduce this. I suppose you tested it on latest version. I will give the latest version a try. I was testing it on Solr 4.4.0. \n\nI noticed that the stream is not being flushed properly and I came to a quick conclusion earlier. I will look into it further and post my findings here. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14274047",
            "date": "2015-01-12T19:48:49+0000",
            "content": "I did run into and file SOLR-6969 while looking into this. That could perhaps be involved in seeing this? "
        },
        {
            "author": "Praneeth",
            "id": "comment-14274229",
            "date": "2015-01-12T21:50:14+0000",
            "content": "I won't be able to get back fast enough during work hours. Sorry about that. I've tried it with Solr 4.10.3 and I am able to reproduce this consistently. You have your autocommit 'on' may be? and the document is getting committed before you kill it? I am running against cloudera distribution of hadoop - 2.0.0-cdh4.6.0.\n\nI did notice SOLR-6969. Though it kept switching between AlreadyBeingCreatedException and RecoveryInProgressException . I guess the later happens based on how fast you restart, whether the soft limit has expired or not may be?\n\nI don't think that these issues are related. I think SOLR-6969 happens on quick restart after every hard crash. But this current issue here is due to documents not making to the tlog. The file has nothing written to it before the crash. I haven't looked deeply into it but could this be because of underlying stream implementation with an intermediate buffer may be? That is why the local transaction log does not show such behaviour? This part complete speculation at this point and I will dig into it later tonight. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14278893",
            "date": "2015-01-15T16:22:10+0000",
            "content": "I'll try again from fresh.\n\nThe file has nothing written to it before the crash.\n\nI inspected the file before kill -9, and while it will be reported as 0 size, I could open and view the doc in the tlog file. "
        },
        {
            "author": "Praneeth",
            "id": "comment-14281636",
            "date": "2015-01-18T02:01:10+0000",
            "content": "Mark Miller, you are right that this could be related to SOLR-6969. I was mislead by the HDFS web UI. It tells me that the file is empty after the hard crash. I can see the contents of transaction log after a hard crash when I open a reader on it. \n\nWhen there is a problem appending to existing transaction log, it gets deleted here with the message Failure to open existing log file (non fatal) and there by losing the docs in that tlog. It looks like every time we see that message, there could be some events that are deleted from the tlog. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14307594",
            "date": "2015-02-05T17:20:47+0000",
            "content": "I think SOLR-6969 has resolved this. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14332962",
            "date": "2015-02-23T05:02:50+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}