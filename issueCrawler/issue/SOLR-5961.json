{
    "id": "SOLR-5961",
    "title": "Solr gets crazy on /overseer/queue state change",
    "details": {
        "affect_versions": "4.7.1",
        "status": "Closed",
        "fix_versions": [
            "4.10.4",
            "5.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "No idea how to reproduce it, but sometimes Solr stars littering the log with the following messages:\n\n419158 [localhost-startStop-1-EventThread] INFO  org.apache.solr.cloud.DistributedQueue  ? LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n\n419190 [Thread-3] INFO  org.apache.solr.cloud.Overseer  ? Update state numShards=1 message={\n  \"operation\":\"state\",\n  \"state\":\"recovering\",\n  \"base_url\":\"http://${IP_ADDRESS}/solr\",\n  \"core\":\"${CORE_NAME}\",\n  \"roles\":null,\n  \"node_name\":\"${NODE_NAME}_solr\",\n  \"shard\":\"shard1\",\n  \"collection\":\"${COLLECTION_NAME}\",\n  \"numShards\":\"1\",\n  \"core_node_name\":\"core_node2\"}\n\nIt continues spamming these messages with no delay and the restarting of all the nodes does not help. I have even tried to stop all the nodes in the cluster first, but then when I start one, the behavior doesn't change, it gets crazy nuts with this \" /overseer/queue state\" again.\n\nPS The only way to handle this was to stop everything, manually clean up all the data in ZooKeeper related to Solr, and then rebuild everything from scratch. As you should understand, it is kinda unbearable in the production environment.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Ugo Matrangolo",
            "id": "comment-14121411",
            "date": "2014-09-04T14:29:07+0000",
            "content": "Hi,\n\nwe got same behaviour that caused major outage. After a rolling restart of our production zk cluster the /overseer/queue started to be spammed by events and the Overseer was looping on the following:\n\n2014-08-28 22:43:06,753 [main-EventThread] INFO  org.apache.solr.cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-08-28 22:43:06,820 [Thread-125] INFO  org.apache.solr.cloud.Overseer  - Update state numShards=3 message={\n  \"operation\":\"state\",\n  \"state\":\"recovering\",\n  \"base_url\":\"http://\n{ip}:{port}\",\n  \"core\":\"warehouse-skus_shard2_replica2\",\n  \"roles\":null,\n  \"node_name\":\"{ip}\n:\n{port}\n_\",\n  \"shard\":\"shard2\",\n  \"collection\":\"warehouse-skus\",\n  \"numShards\":\"3\",\n  \"core_node_name\":\"core_node4\"}\n\nThe fix was to:\n1. Shutdown completely the cluster\n2. Using zkCli we rmr-ed all the /queue(s) under the /overseer\n3. Restarted\n\nSolr started fine after that as nothing has happened.\n\nThe outage was caused by clusterstate.json being in an inconsistent state (most of the nodes were incorrectly marked as down) with no chance to get things right given that the Overseer was too busy processing the spammed queues (more than 13k+ msgs in there) to fix the clusterstate.json with the latest situation. "
        },
        {
            "author": "Frans Lawaetz",
            "id": "comment-14125675",
            "date": "2014-09-08T15:33:47+0000",
            "content": "Also saw this issue as well.  Solr filled 2T worth of disk, recording hundreds of those messages per ms.  In our case it may have been related to zkeeper /overseer/queue becoming overloaded with znodes such that it was unable to function normally.   "
        },
        {
            "author": "Ugo Matrangolo",
            "id": "comment-14146217",
            "date": "2014-09-24T11:22:40+0000",
            "content": "Happened again :/\n\nAfter a routine maintenance of our network causing a 30 secs connectivity hiccup the SOLR cluster started to spam overseer/queue with more than 47k+ events.\n\n\n[zk: zookeeper4:2181(CONNECTED) 26] get /gilt/config/solr/overseer/queue\nnull\ncZxid = 0x290008df29\nctime = Fri Aug 29 02:06:47 GMT+00:00 2014\nmZxid = 0x290008df29\nmtime = Fri Aug 29 02:06:47 GMT+00:00 2014\npZxid = 0x290023cedd\ncversion = 60632\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 0\nnumChildren = 47822\n[zk: zookeeper4:2181(CONNECTED) 27]\n\n\n\nThis time we tried to wait for it to heal itself and we watched the numChildren count go down but then up again: no way it was going to fix alone.\n\nAs usual we had to shutdown all the cluster, rmr /overseer/queue and restart.\n\nAnnoying :/ "
        },
        {
            "author": "Gopal Patwa",
            "id": "comment-14308716",
            "date": "2015-02-06T06:50:55+0000",
            "content": "we also had similar problem today as Ugo mention in our Production system, this was cause after machine reboot for zookeeper (5 node) and 8 node solr cloud (single shard) to install some unix security patch.\n\nJDK 7, Solr 4.10.3, CentOS\n\nBut after reboot, we saw huge amount of message were in overseer/queue\n\n./zkCli.sh -server localhost:2181 ls /search/catalog/overseer/queue  | sed 's/,/\\n/g' | wc -l\n178587\n\nWe have very small cluster (8 nodes), how come overseer/queue should have 17k+ messages, due to this leader node took almost few hours to come from recovery.\n\nLogs from zookeeper:\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /overseer/queue "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14308870",
            "date": "2015-02-06T09:10:25+0000",
            "content": "Hi Gopal, is there anything else in the logs? Any other kinds of exceptions? What was the sequence of steps for installing this patch \u2013 did you take down ZooKeeper first or Solr nodes? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14310971",
            "date": "2015-02-07T22:39:50+0000",
            "content": "I think part of the problem here is SOLR-7033. The other part is that we should probably make sure there is a minimum time between recovery retries on failure in all cases. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14311079",
            "date": "2015-02-08T03:17:56+0000",
            "content": "The other part is that we should probably make sure there is a minimum time between recovery retries on failure in all cases.\n\nI've added that to my patch in SOLR-7033. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14311479",
            "date": "2015-02-08T18:36:13+0000",
            "content": "Commit 1658236 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1658236 ]\n\nSOLR-7033, SOLR-5961: RecoveryStrategy should not publish any state when closed / cancelled and there should always be a pause between recoveries even when recoveries are rapidly stopped and started as well as when a node attempts to become the leader for a shard. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14311484",
            "date": "2015-02-08T18:42:59+0000",
            "content": "Commit 1658237 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1658237 ]\n\nSOLR-7033, SOLR-5961: RecoveryStrategy should not publish any state when closed / cancelled and there should always be a pause between recoveries even when recoveries are rapidly stopped and started as well as when a node attempts to become the leader for a shard. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14311520",
            "date": "2015-02-08T19:45:10+0000",
            "content": "Commit 1658251 from Mark Miller in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1658251 ]\n\nSOLR-7033, SOLR-5961: RecoveryStrategy should not publish any state when closed / cancelled and there should always be a pause between recoveries even when recoveries are rapidly stopped and started as well as when a node attempts to become the leader for a shard. "
        },
        {
            "author": "Gopal Patwa",
            "id": "comment-14311681",
            "date": "2015-02-09T01:02:57+0000",
            "content": "Thanks Mark, here more details for our production issue, I will try to reproduce this issue.\n\nRestart sequence:\nSolr - Restarted 02/03/3015 (8 Nodes, 10 Collection)\nZKR - Restarted 02/04/2015 (5 Nodes)\n\nNormal Index Size are approx 5GB. Only few nodes had this issue\n\nWhen replica was in recovery, transaction logs size was over 100GB.\nPossible reason it starts writing all updates sent by the leader in this period to the transaction log .\n\nDue to overseer queue size large, Admin UI Cloud tree view hangs, may be similar to below jira issue\nhttps://issues.apache.org/jira/browse/SOLR-6395\n\nExceptions During this time:\n\nZookeeper Log:\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /overseer/queue\n\nSolr Log:\n\n2015-02-05 23:23:13,174 [] priority=ERROR app_name= thread=RecoveryThread location=RecoveryStrategy line=142 Error while trying to recover. core=city_shard1_replica2:java.util.concurrent.ExecutionException: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: I was asked to wait on state recovering for shard1 in city on srwp01usc002.stubprod.com:8080_solr but I still do not see the requested state. I see state: active live:true leader from ZK: http://srwp01usc001.stubprod.com:8080/solr/city_shard1_replica1/\n at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n at java.util.concurrent.FutureTask.get(FutureTask.java:188)\n at org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:615) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14313459",
            "date": "2015-02-10T03:15:03+0000",
            "content": "Yeah, once the Overseer work queue is flooded beyond keeping up, all bets are off because the cluster state won't change. SOLR-7033 puts in limits to avoid the behavior we know of that can cause this. We should not have any paths that can write to zk so frequently. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14340549",
            "date": "2015-02-27T18:45:21+0000",
            "content": "Fixed as part of SOLR-7033. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14340567",
            "date": "2015-02-27T18:48:48+0000",
            "content": "Commit 1662784 from Steve Rowe in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1662784 ]\n\nSOLR-7033, SOLR-5961: RecoveryStrategy should not publish any state when closed / cancelled and there should always be a pause between recoveries even when recoveries are rapidly stopped and started as well as when a node attempts to become the leader for a shard. (merged branch_5x r1658237) "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-14348924",
            "date": "2015-03-05T15:36:31+0000",
            "content": "Bulk close for 4.10.4 release "
        },
        {
            "author": "davidchiu",
            "id": "comment-15192198",
            "date": "2016-03-13T07:59:45+0000",
            "content": "I have the same problem in Solr 5.3.1. There are too many collections and shards(replicas)'s, when their state changes and will product a lot of task and write into /overseer/queue, but overseer is too busy and can't handle them in time, then the /overseer/queue will get very big and will make zookeeper down!!\n\nI think solr should limit the size of /overseer/queue according to the jute.buffersize of zookeeper. "
        }
    ]
}