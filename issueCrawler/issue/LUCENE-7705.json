{
    "id": "LUCENE-7705",
    "title": "Allow CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token length",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [],
        "fix_versions": [
            "6.7",
            "7.0"
        ]
    },
    "description": "SOLR-10186\n\nErick Erickson: Is there a good reason that we hard-code a 256 character limit for the CharTokenizer? In order to change this limit it requires that people copy/paste the incrementToken into some new class since incrementToken is final.\nKeywordTokenizer can easily change the default (which is also 256 bytes), but to do so requires code rather than being able to configure it in the schema.\nFor KeywordTokenizer, this is Solr-only. For the CharTokenizer classes (WhitespaceTokenizer, UnicodeWhitespaceTokenizer and LetterTokenizer) (Factories) it would take adding a c'tor to the base class in Lucene and using it in the factory.\nAny objections?",
    "attachments": {
        "LUCENE-7705.patch": "https://issues.apache.org/jira/secure/attachment/12854306/LUCENE-7705.patch",
        "LUCENE-7705": "https://issues.apache.org/jira/secure/attachment/12867128/LUCENE-7705"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15881239",
            "date": "2017-02-23T20:53:58+0000",
            "content": "I have cooked up a patch in SOLR-10186, and introduced new constructor in CharTokenizer and related Tokenizer factories, which takes maxCharLen and factory as parameters along with it.\n\nKindly provide your feedback and any comments on introducing new constructors in the classes. Thanks. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15881417",
            "date": "2017-02-23T22:26:07+0000",
            "content": "Patch that fixes up a few comments, regularized maxChars* to maxToken* and the like. I enhanced a test to test tokens longer than 256 characters.\n\nThere was a problem with LowerCaseTokenizerFactory, the getMultiTermComponent method constructed a LowerCaseFilterFactory with the original arguments including maxTokenLen, which then threw an error. There's a nocommit in there for the nonce, what's the right thing to do here?\n\n[~amrit sarkar] Do you have any ideas for a more elegant solution? The nocommit is there because this is feels just too hacky, but it does prove that this is the problem.\n\nIt seems like we should close SOLR-10186 and just make the code changes here. With this patch I successfully tested adding fields with tokens longer than 256 and shorter, so I don't think there's anything beyond this patch to do with Solr. I suppose we could add some maxTokenLen bits to some of the schemas just to exercise that (which would have found the LowerCaseTokenizerFactory bit). ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15881459",
            "date": "2017-02-23T22:41:01+0000",
            "content": "I think the patch I uploaded is a result of applying your most recent patch for SOLR-10186 but can you verify? We should probably consolidate two, I suggest we close the Solr one as a duplicate and continue iterating here.\n\nErick ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15881610",
            "date": "2017-02-24T00:15:13+0000",
            "content": "These two tests fail:\norg.apache.lucene.analysis.core.TestUnicodeWhitespaceTokenizer.testParamsFactory\norg.apache.lucene.analysis.core.TestRandomChains (suite)\n\n\nTestUnicideWhitespaceTokenizer is because I added \"to\" to one of the exception messages, a trivial fix\n\nNo idea what's happening with TestRandomChains though.\n\nAnd my nocommit insures that 'ant precommit' will fail too, that's rather the point. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15883188",
            "date": "2017-02-24T17:52:25+0000",
            "content": "Oops, forgot to \"git add\" on the new test file. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15883979",
            "date": "2017-02-25T03:19:14+0000",
            "content": "Erick,\n\nSuccessfully able to pass all the tests in the current patch uploaded with minor corrections and rectifications in exiting test-classes.\n\n\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java\nnew file:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java\nmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java\nmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java\n\n\n\nTest failure fixes:\n\n1. org.apache.lucene.analysis.core.TestRandomChains (suite):\n\n   Added the four tokenizer constructors failing to brokenConstructors map to bypass them without delay.\nThis class tends to check what arguments is legal for the constructors and create certain maps before-hand to check later. It doesn't take account of boxing/unboxing of primitive data types; hence when we are taking parameter in \"java.lang.Integer\", while creating map it is unboxing it into \"int\" itself and then fails because \"int.class\" and  \"java.lang.Integer.class\" doesn't match which doesn't make sense. Either we can fix how the maps are getting created or we skip these constructors for now.\n\n2.  the getMultiTermComponent method constructed a LowerCaseFilterFactory with the original arguments including maxTokenLen, which then threw an error:\n\n  Not sure what corrected that, but I see no suite failing, not even TestFactories which I suppose was throwing the error for incompatible constructors/noMethodFound etc. Kindly verify if we are still facing the issue or we need to harden the test cases for the same. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15884014",
            "date": "2017-02-25T04:28:18+0000",
            "content": "Actually, I didn't encounter the error in LowerCaseFilterFactory until I tried it out from a fully-compiled Solr instance with the maxTokenLen in the managed_schema file. I was thinking that it might make sense to add the maxTokenLen to a couple of the schemas used by some of the test cases, leaving it at the default value or 256 just to get some test coverage. I think this is really the difference between a test case at the Lucene level and one based on the schema from a Solr level. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15884209",
            "date": "2017-02-25T11:52:36+0000",
            "content": "Roger that! I will incorporate test-cases for the stated tokenizers and try to fix the issue without removing maxTokenLength from the method. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15884739",
            "date": "2017-02-26T13:02:47+0000",
            "content": "Erick,\n\nI wrote the test-cases and it is a problem, but removing \"maxTokenLen\" from original arguments which initialize LowerCaseFilterFactory makes sense, and it is not hack. We have to remove the argument for the FilterFactory init somewhere and it will be better if we do where we are making the call. I am not inclined towards removing this at FilterFactory init or AbstractAnalysisFactory func call. So we are left with two options, either we don't provide option for maxTokenLen for LowerCaseTokenizer or we remove the extra argument as you have done on getMultiTermComponent().\n\nLet me know your thoughts. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15884805",
            "date": "2017-02-26T16:40:46+0000",
            "content": "Removing the extra arguments in the getMulitTermComponent() is certainly better than in any of the superclasses, you'd have the possibility of interfering with someone else's filter that did coincidentally have a maxTokenLen parameter that should legitimately be passed through.\n\nI guess that removing it in getMultiTermComponent() is OK. At least the place that gets the maxTokenLength argument (i.e. the factory) being responsible for removing it before passing the args on to the Filter keeps things from sprawling.\n\nAlthough the other possibility is to just pass an empty map rather than munge the original ones. LowerCaseFilter's first act is to check whether the map is empty after all. Something like:\n\nreturn new LowerCaseFilterFactory(Collections.EMPTY_MAP); (untested).\n\nI see no justification for passing the original args anyway in this particular case, I'd guess it was just convenient. I think I like the EMPTY_MAP now that I think about it, but neither option is really all that superior IMO. The EMPTY_MAP will be slightly more efficient but I doubt it's really measurable. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15884831",
            "date": "2017-02-26T17:41:36+0000",
            "content": "Erick,\n\nFor every tokenizer init, two parameters are already included in their arguments as below:\n\n{class=solr.LowerCaseTokenizerFactory, luceneMatchVersion=7.0.0}\n\n\n\nwhich is consumed by AbstractAnalysisFactory while it instantiate:\n\n  originalArgs = Collections.unmodifiableMap(new HashMap<>(args));\n    System.out.println(\"orgs:: \"+originalArgs);\n    String version = get(args, LUCENE_MATCH_VERSION_PARAM);\n    if (version == null) {\n      luceneMatchVersion = Version.LATEST;\n    } else {\n      try {\n        luceneMatchVersion = Version.parseLeniently(version);\n      } catch (ParseException pe) {\n        throw new IllegalArgumentException(pe);\n      }\n    }\n    args.remove(CLASS_NAME);  // consume the class arg\n  }\n\n\nclass parameter is useless, we don't have to worry about it, while it do look up for luceneMatchVersion which is kind of sanity check for the versions, not sure anything important takes place at Version::parseLeniently(version) function. If we can confirm that, we can pass empty map there. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15884983",
            "date": "2017-02-27T00:34:23+0000",
            "content": "Ah, good catch. So yeah, we'll have to remove the maxTokenLen from the original args and pass the modified map through. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15885144",
            "date": "2017-02-27T05:04:33+0000",
            "content": "I did some adjustments, including removing maxTokenLen for LowerCaseFilterFactory init and included hard test cases for the tokenizers at solr level: TestMaxTokenLenTokenizer.java \n\n\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java\n\tmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java\n\tnew file:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java\n\tmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java\n\tmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java\n\tmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java\n\tnew file:   solr/core/src/test-files/solr/collection1/conf/schema-tokenizer-test.xml\n\tnew file:   solr/core/src/test/org/apache/solr/util/TestMaxTokenLenTokenizer.java\n\n\n\nI think we have covered everything, all tests passed.  ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-15897762",
            "date": "2017-03-06T18:18:09+0000",
            "content": "Patch looks good. I'm going to hang back on committing this until we figure out SOLR-10229 (control schema proliferation). The additional schema you put in here is about the only way currently to test Solr schemas, so that's perfectly appropriate. I'd just like to use this as a test case for what it would take to move constructing schemas to inside the tests rather than have each new case like this require another schema that we then have to maintain.\n\nBut if SOLR-10229 takes very long I'll just commit this one and we can work out the rest later. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15907737",
            "date": "2017-03-13T16:06:13+0000",
            "content": "May or may not depend on SOLR-10229, but we want to approach them both together. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15951049",
            "date": "2017-03-31T14:50:18+0000",
            "content": "Robert Muir Michael McCandless Wanted to check whether the discussion at LUCENE-7762 applies here or not.  ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15951158",
            "date": "2017-03-31T15:43:20+0000",
            "content": "I think that LUCENE-7762 discussion is unrelated: it has to do with exposing such things again at the Analyzer level, vs going the customanalyzer route ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-15951168",
            "date": "2017-03-31T15:47:46+0000",
            "content": "Thanks! ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16001105",
            "date": "2017-05-08T17:08:42+0000",
            "content": "Fixed a couple of precommit issues, otherwise patch is the same.\n\nAmrit:\n\nthis test always fails for me: TestMaxTokenLenTokenizer\n\nassertQ(\"Check the total number of docs\", req(\"q\", \"letter:lett\"), \"//result[@numFound=0]\");\n\nLooking at the code, numFound should be 1 I believe. The problem is that both the index time and query time analysis trims the term to 3 characters, so the finding a document when searching for \"lett\" here is perfectly legitimate. In fact all tokens no matter how long and no matter what follows \"let\" will succeed. I think all the rest of the tests for fields in this set will fail for a similar reason when checking for search terms > the length of the token. Do you agree?\n\nIf you agree, let's add a few tests explicitly showing this, that way future people looking at the code will know it's intended behavior. I.e. add lines like:\n\n// Anything that matches the first three letters should be found when maxLen=3\n assertQ(\"Check the total number of docs\", req(\"q\", \"letter:letXyz\"), \"//result[@numFound=1]\");\n\n\nOr I somehow messed up the patch. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16002611",
            "date": "2017-05-09T12:31:10+0000",
            "content": "Erick,\n\nI have absolutely no idea how I uploaded false/incomplete patch. I certainly understand the above and incorporated two different configurations to show the difference at first place, refined the same as per your latest comments.\n\nThere is one serious issue I am facing for a day, all the tests passes on IntelliJ IDE but the same gets failed when I do \"ant -Dtestcase=TestMaxTokenLenTokenizer test\". I don't know what to do with that.\n\nThe latest patch you uploaded is incomplete as the newly created files are not part of the patch. I have worked on the previous one (dated: 27-07-17). You may need to change the pre-commit changes on the latest patch.\n\nSorry for the hiccup. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-16002816",
            "date": "2017-05-09T14:50:34+0000",
            "content": "Precommit passes, let's use this patch as a basis going forward. The \"ant precommit\" task will show you all the failures.\n\nSo I'm not sure what's up with the test, \n\"ant -Dtestcase=TestMaxTokenLenTokenizer test\" \nworks just fine for me. What errors do you see? ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16002926",
            "date": "2017-05-09T15:53:58+0000",
            "content": "Yes Erick, I saw the \"ant precommit\" errors, tab instead of whitespaces, got it.\n\nI am still seeing this:\n\n   [junit4] Tests with failures [seed: C3F5B66314F27B5E]:\n   [junit4]   - org.apache.solr.util.TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers\n\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMaxTokenLenTokenizer -Dtests.method=testSingleFieldSameAnalyzers -Dtests.seed=C3F5B66314F27B5E -Dtests.slow=true -Dtests.locale=fr-CA -Dtests.timezone=Asia/Qatar -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   0.10s | TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers <<<\n   [junit4]    > Throwable #1: java.lang.RuntimeException: Exception during query\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([C3F5B66314F27B5E:A927890C4C11AB91]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:896)\n   [junit4]    > \tat org.apache.solr.util.TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers(TestMaxTokenLenTokenizer.java:104)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]    > Caused by: java.lang.RuntimeException: REQUEST FAILED: xpath=//result[@numFound=1]\n   [junit4]    > \txml response was: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   [junit4]    > <response>\n   [junit4]    > <lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">11</int></lst><result name=\"response\" numFound=\"0\" start=\"0\"></result>\n   [junit4]    > </response>\n   [junit4]    > \trequest was:q=letter0:lett&wt=xml\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:889)\n   [junit4]    > \t... 40 more\n\n\n\nBut if it working for you, I am good.\n\nYou didn't include the newly created files again in the latest patch, I have posted a new one with \"precommit\" sorted and included all the files.  ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-16002994",
            "date": "2017-05-09T16:39:45+0000",
            "content": "Sorry 'bout that, rushing out the door this morning and didn't do the git add bits.\n\nWe'll get it all right yet. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16004169",
            "date": "2017-05-10T06:52:02+0000",
            "content": "Erick, we got everything right this time. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-16005672",
            "date": "2017-05-10T23:47:51+0000",
            "content": "Amrit:\n\nI'm using LUCENE-7705 (note no .patch extension). Is that the right one?\n ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16005739",
            "date": "2017-05-11T01:01:16+0000",
            "content": "Why do the tokenizer ctor arg take Integer instead of int? They check for null and throws an exception in that case, but I think its better to encode the correct argument type in the method signature. This would also mean you can re-enable TestRandomChains for the newly-added ctors.\n\nI think if basic tokenizers like WhitespaceTokenizer have new ctors that take int, and throw exception on illegal values for this parameter because some aren't permitted, that these new ctors really could use a bit better docs:\n\n@param maxTokenLen max token length the tokenizer will emit\n\nInstead, maybe something like:\n\n@param maxTokenLen maximum token length the tokenizer will emit. Must not be negative.\n@throws IllegalArgumentException if maxTokenLen is invalid.\n\nAlso I am concerned about the allowable value ranges and the disabling of tests as far as correctness. Instead if int is used instead of Integer this test can be re-enabled which may find/prevent crazy bugs. e.g. I worry a bit what will these Tokenizers do in each case if maxTokenLen is 0 (which is allowed), will they loop forever or crash, etc?\n\nThe maximum value of the range is also suspicious. What happens with any CharTokenizer-based tokenizer if i supply a value > IO_BUFFER_SIZE? Maybe it behaves correctly, maybe not. I think Integer.MAX_VALUE is a trap for a number of reasons: enormous values will likely only blow up anyway (limited to 32766 in the index), if they don't they may create hellacious threadlocal memory usage in buffers, bad/malicious input could take out JVMs, etc.\n\nMaybe follow the example of StandardTokenizer here:\n\n\n/** Absolute maximum sized token */\npublic static final int MAX_TOKEN_LENGTH_LIMIT = 1024 * 1024;\n...\n   * @throws IllegalArgumentException if the given length is outside of the\n   *  range [1, {@value #MAX_TOKEN_LENGTH_LIMIT}].\n...\nif (length < 1) {\n  throw new IllegalArgumentException(\"maxTokenLength must be greater than zero\");\n} else if (length > MAX_TOKEN_LENGTH_LIMIT) {\n  throw new IllegalArgumentException(\"maxTokenLength may not exceed \" + MAX_TOKEN_LENGTH_LIMIT);\n}\n\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16005888",
            "date": "2017-05-11T05:01:35+0000",
            "content": "One nit and one suggestion and one question in addition to Robert's comments:\n\nThe nit:\nthere's a pattern of a bunch of these:\n\nupdateJ(\"{\\\"add\\\":{\\\"doc\\\": {\\\"id\\\":1,\\\"letter\\\":\\\"letter\\\"}},\\\"commit\\\":{}}\",null);\n.\n.\nthen:\nassertU(commit());\n\nIt's unnecessary to do the commit with the updateJ calls. the commit at the end will take care of it all. It's a little less efficient to commit with each doc. Frankly I doubt that'd be measurable, performance wise, but let's take them out anyway.\n\nThe suggestion:\nWhen we do stop accruing characters e.g. in CharTokenizer, let's log an INFO level message to that effect, something like\n\nlog.info(\"Splitting token at {} chars\", maxTokenLen);\n\nThat way people will have a clue where to look. I think INFO is appropriate rather than WARN or ERROR since it's perfectly legitimate to truncate input, I'm thinking OCR text for instance. Maybe dump the token we've accumulated so far?\n\nI worded it at \"splitting\" because (and there are tests for this) that the next token picks up where the first left off. So if the max length is 3, and the input is \"whitespace\", we get several tokens as a result, \n\n\"whi\", \"tes\", \"pac\", and \"e\". \n\nI suppose that means that the offsets are also incremented. Is that really what we want here? Or should we instead throw away the extra tokens? Robert Muir, what do you think is correct? This is not a change in behavior, the current code does the same thing just a hard-coded 255 limit. I'm checking if this is intended behavior. \n\nIf we do want to throw away the extra, we could spin through the buffer until we encountered a non char then return the part < maxTokenLen. If we did that we could also log the entire token and the truncated version if we wanted.\n\nOtherwise precommit passes and all tests pass. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16005892",
            "date": "2017-05-11T05:08:01+0000",
            "content": "Sorry, I think its a very bad idea to log tokens in lucene's analysis chains for any reason. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16005894",
            "date": "2017-05-11T05:10:03+0000",
            "content": "Fair enough, I suppose you could get log flies 20 bazillion bytes long.\n\nWDYT about generating multiple tokens when the length is exceeded? ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16006260",
            "date": "2017-05-11T11:23:39+0000",
            "content": "I don't think its within the scope of this issue. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16024855",
            "date": "2017-05-25T15:16:59+0000",
            "content": "Final patch, added CHANGES.txt etc. Incorporates Robert Muir's suggestions, thanks Robert!:\n1> Integer parameters are now int\n2> put tokenizers back in TestRandomChains (well, really took out the special handling of these tokenizers)\n3> added tests for 0 len and > StandardTokenizer.MAX_TOKEN_LENGTH_LIMIT (1M, which is still very large).\n4> Added better comments in the javadocs\n5> added test for input > I/O buffer size.\n\nUnless there are objections I'll commit this tomorrow. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16025386",
            "date": "2017-05-25T20:57:03+0000",
            "content": "Tests such as this are not effective. If no exception is thrown the test will pass.\n\n\n+    try {\n+      new LetterTokenizer(newAttributeFactory(), 0);\n+    } catch (Exception e) {\n+      assertEquals(\"maxTokenLen must be greater than 0 and less than 1048576 passed: 0\", e.getMessage());\n+    }\n\n\n\nI would use expectThrows in this case instead. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16027967",
            "date": "2017-05-28T22:37:48+0000",
            "content": "final patch, incorporates R. Muir's comments (thanks!). What bugs me is that I know that pattern is invalid but  didn't catch it.. Siiigggghhhh.\n\nCommitting momentarily, will backport to 6.7. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16027974",
            "date": "2017-05-28T23:40:38+0000",
            "content": "Oh bother. commit number. \nCommit to master is: 906679adc80f0fad1e5c311b03023c7bd95633d7\nCommit to 6x is: 3db93a1f0980bd098af892630c6dc95c4c962e14\n ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16029694",
            "date": "2017-05-30T16:38:49+0000",
            "content": "TestMaxTokenLenTokenizer seems to be failing in Jenkins\n\n1 tests failed.\nFAILED:  org.apache.solr.util.TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers\n\nError Message:\nException during query\n\nStack Trace:\njava.lang.RuntimeException: Exception during query\n        at __randomizedtesting.SeedInfo.seed([FE4BE1CA39C9E0DA:9499DEA5612A3015]:0)\n        at org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:895)\n        at org.apache.solr.util.TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers(TestMaxTokenLenTokenizer.java:104)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1713)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:907)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:943)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:957)\n        at com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:57)\n        at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)\n        at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n        at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)\n        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)\n        at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)\n        at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:916)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:802)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:852)\n        at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at com.carrotsearch.randomizedtesting.rules.SystemPropertiesRestoreRule$1.evaluate(SystemPropertiesRestoreRule.java:57)\n        at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)\n        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n        at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)\n        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: REQUEST FAILED: xpath=//result[@numFound=1]\n        xml response was: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><result name=\"response\" numFound=\"0\" start=\"0\"></result>\n</response>\n\n        request was:q=letter0:lett&wt=xml\n        at org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:888)\n        ... 40 more\n\n ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe"
        },
        {
            "id": "comment-16029872",
            "date": "2017-05-30T18:21:59+0000",
            "content": "Here's another reproducing failure https://jenkins.thetaphi.de/job/Lucene-Solr-6.x-Linux/3612: \n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMaxTokenLenTokenizer -Dtests.method=testSingleFieldSameAnalyzers -Dtests.seed=FE4BE1CA39C9E0DA -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=vi-VN -Dtests.timezone=Australia/NSW -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   0.07s J1 | TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers <<<\n   [junit4]    > Throwable #1: java.lang.RuntimeException: Exception during query\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([FE4BE1CA39C9E0DA:9499DEA5612A3015]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:895)\n   [junit4]    > \tat org.apache.solr.util.TestMaxTokenLenTokenizer.testSingleFieldSameAnalyzers(TestMaxTokenLenTokenizer.java:104)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]    > Caused by: java.lang.RuntimeException: REQUEST FAILED: xpath=//result[@numFound=1]\n   [junit4]    > \txml response was: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   [junit4]    > <response>\n   [junit4]    > <lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><result name=\"response\" numFound=\"0\" start=\"0\"></result>\n   [junit4]    > </response>\n   [junit4]    > \trequest was:q=letter0:lett&wt=xml\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.assertQ(SolrTestCaseJ4.java:888)\n[...]\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene62): {lowerCase0=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), whiteSpace=Lucene50(blocksize=128), letter=BlockTreeOrds(blocksize=128), lowerCase=Lucene50(blocksize=128), unicodeWhiteSpace=BlockTreeOrds(blocksize=128), letter0=FST50, unicodeWhiteSpace0=FST50, keyword0=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), keyword=Lucene50(blocksize=128), whiteSpace0=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{}, maxPointsInLeafNode=579, maxMBSortInHeap=5.430197160407458, sim=RandomSimilarity(queryNorm=false,coord=crazy): {}, locale=vi-VN, timezone=Australia/NSW\n   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=234332328,total=536870912\n\n ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-16029981",
            "date": "2017-05-30T19:23:09+0000",
            "content": "Hmmm, Steve's seed succeeds on my macbook pro, and I beasted this test 100 times on my mac pro without failures. Not quite sure what to do next...\n\nI wonder if SOLR-10562 is biting us again? Although that doesn't really make sense as the line before this failure finds the same document.\n\nI'll beast this on a different system I have access to when I can (it's shared). ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16030016",
            "date": "2017-05-30T19:43:52+0000",
            "content": "My seed reproduces for me both on Linux and on my Macbook pro (Sierra 10.12.5, Oracle JDK 1.8.0_112).  Note that the original failure was on branch_6x (and that's where I repro'd). ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-16030223",
            "date": "2017-05-30T22:03:50+0000",
            "content": "OK, I see what's happening. I noted earlier that the way this has always been implemented, multiple tokens are emitted when the token length is exceeded. In this case, the token sent in the doc is \"letter\". So two tokens are emitted:\n\"let\" and \"ter\". With positions incremented between I think.\n\nThe search is against \"lett\". For some reason, the parsed query in 6x is:\nPhraseQuery(letter0:\"let t\")\n\nwhile in master it's:\nletter0:let letter0:t\n\nEven this is wrong, it just happens to succeed because the default operator is OR, so the fact that and the tokens in the index do not include a bare \"t\" finds the doc by chance, not design.\n\nI think the right solution is to stop emitting tokens for a particular value once maxTokenLen is exceeded. I'll raise a new JIRA and we can debate it here.\n\nThis is not any change in behavior resulting from the changes in this JIRA, the tests just expose something that's always been the case but nobody's noticed. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-16034194",
            "date": "2017-06-02T06:04:45+0000",
            "content": "Following discussion on LUCENE-7857 and Erick's pointers,\n\nintroduced autoGeneratePhraseQueries=\"false\" on the all the FieldType definitions for this test case. all the test scenarios are successfully executed on both master and branch_6x. Patch attached. ",
            "author": "Amrit Sarkar"
        },
        {
            "id": "comment-16035781",
            "date": "2017-06-03T03:33:33+0000",
            "content": "Commit 15a8a2415280d50c982fcd4fca893a3c3224da14 in lucene-solr's branch refs/heads/master from Erick\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=15a8a24 ]\n\nLUCENE-7705: Allow CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token len (test fix) ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16035782",
            "date": "2017-06-03T03:33:37+0000",
            "content": "Commit 2eacf13def4dc9fbea1de9c79150c05682b0cdec in lucene-solr's branch refs/heads/master from Erick\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2eacf13 ]\n\nLUCENE-7705: Allow CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token length, fix test failure. ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16035783",
            "date": "2017-06-03T03:33:49+0000",
            "content": "Commit e4a43cf59a12ca39eb8278cc2533d409d792185a in lucene-solr's branch refs/heads/branch_6x from Erick\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e4a43cf ]\n\nLUCENE-7705: Allow CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token len (test fix)\n\n(cherry picked from commit 15a8a24) ",
            "author": "ASF subversion and git services"
        }
    ]
}