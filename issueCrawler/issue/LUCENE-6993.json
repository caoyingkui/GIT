{
    "id": "LUCENE-6993",
    "title": "Update UAX29URLEmailTokenizer TLDs to latest list, and upgrade all JFlex-based tokenizers to support Unicode 8.0",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "We did this once before in LUCENE-5357, but it might be time to update the list of TLDs again. Comparing our old list with a new list indicates 800+ new domains, so it would be nice to include them.\n\nAlso the JFlex tokenizer grammars should be upgraded to support Unicode 8.0.",
    "attachments": {
        "LUCENE-6993.patch": "https://issues.apache.org/jira/secure/attachment/12784181/LUCENE-6993.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15115456",
            "author": "Mike Drob",
            "date": "2016-01-25T16:09:59+0000",
            "content": "Attaching a patch against trunk that updates the TLD Macro file and the UAX29URLEmailTokenizerImpl.\n\nWhen running ant jflex I had to increase the amount of heap space available due to the increased number of TLDs, not sure if this will result in a negative impact to the rest of the build.\n\nThe .an and .tp domains were removed from the list, random.text.with.urls was updated accordingly. "
        },
        {
            "id": "comment-15126636",
            "author": "Mike Drob",
            "date": "2016-02-01T17:53:20+0000",
            "content": "Steve Rowe - you did the previous incarnation of this fix, do you have time to look at this one? "
        },
        {
            "id": "comment-15126654",
            "author": "Steve Rowe",
            "date": "2016-02-01T17:55:35+0000",
            "content": "Hi Mike Drob, sure, I'll try to look at it some time this week. "
        },
        {
            "id": "comment-15143997",
            "author": "Mike Drob",
            "date": "2016-02-12T04:02:59+0000",
            "content": "Hi Steve - do you have any updates or would you like me to ping somebody else? Thanks! "
        },
        {
            "id": "comment-15151296",
            "author": "Mike Drob",
            "date": "2016-02-17T22:21:59+0000",
            "content": "Robert Muir - Do you have any thoughts on this since you were involved in the previous patch too? "
        },
        {
            "id": "comment-15151332",
            "author": "Robert Muir",
            "date": "2016-02-17T22:42:35+0000",
            "content": "I with a major release looming we should update all this stuff. Also the unicode version (and icu library) to Unicode 8.0 because java has already done this for JDK 9 (http://openjdk.java.net/jeps/267), and we should not fall so far behind. \n\nWe should copy the current generated grammar with a 'std55' subdirectory and hook it in for backwards compatibility before applying grammar changes. Then I think just fix all this stuff at once? It sounds worse than it is, I think it can be done today, I will help.\n "
        },
        {
            "id": "comment-15151363",
            "author": "Mike Drob",
            "date": "2016-02-17T23:02:25+0000",
            "content": "That all makes sense. I was looking at the unicode spec changes between 6.3 and 8.0 and did not really understand what the impact to our grammars is.\n\nI'll add the current grammar to a std55 directory, but will need some help making sure that I've got all the right back-compat hooks. I'll post an updated patch shortly when I get stuck. "
        },
        {
            "id": "comment-15151375",
            "author": "Robert Muir",
            "date": "2016-02-17T23:08:48+0000",
            "content": "OK, I can look into the icu part in a separate issue, since its somewhat unrelated but I think worthwhile for consistency. "
        },
        {
            "id": "comment-15151445",
            "author": "Mike Drob",
            "date": "2016-02-17T23:50:44+0000",
            "content": "From analysis/common directory, I ran ANT_OPTS=\"-Xmx6g\" ant gen-tlds unicode-data jflex\n\nDo I need to manually update the %unicode X.X directive in the .jflex files or do we want to leave that for compatibility? I am unclear on the impacts here.\n\nAlso, I did not see any previous examples of keeping the tokenizers around for compatibility, so I guess I didn't quite understand where the hooks are.  "
        },
        {
            "id": "comment-15151457",
            "author": "Robert Muir",
            "date": "2016-02-18T00:01:13+0000",
            "content": "Basically the old versions of the Tokenizer and Impl are just \"saved\" to a subdirectory, and in the Analyzer and TokenizerFactory we conditionally use them, if you request that compatibility version.\n\nHave a look at branch_5x which still has std40 containing StandardTokenizer40, StandardTokenizerImpl40, UAX29URLEmailTokenizer40, and so on. TestStandardAnalyzer and TestUAX29URLEmailAnalyzer also have a testBackcompat40 which calls setVersion and ensures it works. Finally, see StandardAnalyzer/TokenizerFactory.java, and UAXURLEmailAnalyzer/TokenizerFactory.java which conditionally use StandardTokenizer40 depending on version.\n\nSo we should do a similar thing with the current stuff in master before modifying the files, and make them std55. We can just test that it works at all (e.g. foo bar -> foo,bar) initially and later maybe add a test ensuring \"old behavior\" stays the same.\n\nThen you can bump unicode version and tld lists and it won't change any behavior if someone asks for version < 6.0, because they will get the exact same tokenizer as before. "
        },
        {
            "id": "comment-15151461",
            "author": "Robert Muir",
            "date": "2016-02-18T00:02:48+0000",
            "content": "And i guess really we should call it std50 to keep things simple. if someone asks for 5.4 compatibility, they should get this one and then the logic in the Analyzer will be clear that is the case even going forward. "
        },
        {
            "id": "comment-15151606",
            "author": "Robert Muir",
            "date": "2016-02-18T02:30:00+0000",
            "content": "I took care of the icu parts here: LUCENE-7035\n\nplease ping me here if you have trouble setting up the back compat. I can always do that part, if it gets too frustrating. But it is better if more people can do it. "
        },
        {
            "id": "comment-15153009",
            "author": "Mike Drob",
            "date": "2016-02-18T20:16:03+0000",
            "content": "Attaching a new version of the patch that differentiates between the new and old tokenizers. A couple of notes...\n\nI used LUCENE_5_6 as the cutoff, since this could conceivably be backported to the 5.x branch. I know there aren't any plans for a 5.6 release ever, but this seems like a low cost change.\n\nThe UAX29URLEmailTokenizerImpl50.java currently shows up with private methods, which breaks compilation. I can't figure out how to make those public, even though the old and new jflex files look almost identical. Would appreciate a second set of eyes here. "
        },
        {
            "id": "comment-15153017",
            "author": "Robert Muir",
            "date": "2016-02-18T20:21:47+0000",
            "content": "I can't even imagine us releasing a 5.6 after a 6.0, I really do not think we should drag that idea into this issue. Its a bad one.\n\nLets target 6.0 here for all this stuff: these are major changes that impact backwards compatibility. The logic should be:\n\n\nif (version.onOrAfter(LUCENE_6_0_0)) {\n  // new tokenizer\n} else {\n  // old tokenizer\n}\n\n "
        },
        {
            "id": "comment-15153116",
            "author": "Mike Drob",
            "date": "2016-02-18T21:24:07+0000",
            "content": "Ok, switched to 6_0_0 in the next patch.\n\nJFlex 1.6.1 currently only supports Unicode 7.0, not 8.0 - Steve Rowe, do you know what the jflex timeline for upgrading looks like?\n\nI changed jflex-legacy to be a public target and ran it individually, and it generated public accessors like we need, so I'm still not sure what's going on, but we can move past it at this point. The jflex target still screws it up, though, so I'm going to take jflex-legacy out of it. "
        },
        {
            "id": "comment-15153131",
            "author": "Steve Rowe",
            "date": "2016-02-18T21:28:16+0000",
            "content": "JFlex 1.6.1 currently only supports Unicode 7.0, not 8.0 - Steve Rowe, do you know what the jflex timeline for upgrading looks like?\n\nUnicode 8.0 support is committed on JFlex master, but no release includes it yet. (So if you want to test I think you could build JFlex locally, change the JFlex dependency in Lucene to use the snapshot, then run the Lucene build.) No timeline for release has been set.  I'll ping JFlex founder Gerwin Klein, who has done all the releases, and get back to you here. "
        },
        {
            "id": "comment-15153209",
            "author": "Steve Rowe",
            "date": "2016-02-18T22:06:36+0000",
            "content": "Mike Drob, I haven't looked at your patch yet but there is a non-rote Unicode upgrade item that needs to be dealt with - from LUCENE-5357's TODO list:\n\n\n\tUpgrade the UAX#29-based grammars to the Unicode 6.3 8.0 word break rules, in StandardTokenizerImpl.jflex and UAX29URLEmailTokenizer.jflex.\n\n\n\nUAX#29 word break rules can (and usually do) change with each Unicode release, so we'll need to review the changes between 6.3 and 8.0 and see what, if anything, needs changing in the tokenizer grammars.  Another item from the LUCENE-5357 TODO list will confirm that this has been done correctly:\n\n\n\tTest the new scanners against the Unicode 6.3 8.0 word break test data\n\t\n\t\t[...]\n\t\n\t\n\n "
        },
        {
            "id": "comment-15153258",
            "author": "Mike Drob",
            "date": "2016-02-18T22:34:28+0000",
            "content": "Looking at http://unicode.org/reports/tr29/#Modifications I see\n\n\nRevision 27 [KW, LI]\n\n    Reissued for Unicode 8.0.\n    Modified rule SB7 to prevent sentence breaks within a word segment such as \u201cMr.Hamster\u201d.\n    Updated notes on tailoring using CLDR boundary suppressions.\n    Recast rule tables to use macros for compactness.\n    Updated table styles, removed inconsistently applied styles on character names and code points, and adjusted layout of various tables and figures.\n    Section 3.1 Default Grapheme Cluster Boundary Specification\n        Removed the New Tai Lue characters U+19B0..U+19B4, U+19B8..U+19B9, U+19BB..U+19C0, U+19C8..U+19C9 from the exception list for SpacingMark in Table 2, Grapheme_Cluster_Break Property Values.\n        Added U+11720 AHOM VOWEL SIGN A and U+11721 AHOM VOWEL SIGN AA to the same exception list for SpacingMark.\n\nRevision 26 being a proposed update, only changes between versions 27 and 25 are noted here.\nRevision 25\n\n    Reissued for Unicode 7.0.\n    General text cleanup, including \u201c_\u201d in property and property value names, use of curly-quotes and italics.\n    Section 3.1 Default Grapheme Cluster Boundary Specification\n        Added U+AA7D MYANMAR SIGN TAI LAING TONE-5 to the exception list for SpacingMark in Table 2, Grapheme_Cluster_Break Property Values.\n    Section 5.1 Default Sentence Boundary Specification\n        Added note to clarify that Format and Extend characters are not joined to separators like LF.\n        Added note about the fact that words can span a sentence break.\n\n\n\nI am by no means an expert in Unicode, but it looks like the Sentence Break rules are not relevant to us, right? But the Spacing Mark // Grapheme Cluster changes are relevant?\nWhen you refer to the word break test data, is that something that the Unicode Consortium publishes or do you mean our internal data? "
        },
        {
            "id": "comment-15153306",
            "author": "Steve Rowe",
            "date": "2016-02-18T23:02:15+0000",
            "content": "I think you're right, Mike, I don't see any default word break rule modifications in that list between versions 6.3 and 8.0.\n\nThe test data is here: http://www.unicode.org/Public/8.0.0/ucd/auxiliary/WordBreakTest.txt\n\nHere's the full TODO item from LUCENE-5357 (after s/6.3/8.0/g and s/6.1/6.3/g):\n\n\n\tTest the new scanners against the Unicode 8.0 word break test data\n\t\n\t\tUpdate generateJavaUnicodeWordBreakTest.pl to handle above-BMP characters in the Unicode character database's ucd/auxiliary/WordBreakTest.txt (previous Unicode versions included only BMP characters in that file). (one time operation, already done.)\n\t\tUsing generateJavaUnicodeWordBreakTest.pl, generate WordBreakTestUnicode_8_0_0.java under modules/analysis/common/src/test/org/apache/lucene/analysis/core/.\n\t\tUpdate TestStandardAnalyzer.java and TestUAX29URLEmailTokenizer.java to invoke WordBreakTestUnicode_8_0_0 rather than WordBreakTestUnicode_6_3_0.\n\t\tRemove WordBreakTestUnicode_6_3_0.java.\n\t\n\t\n\n "
        },
        {
            "id": "comment-15153725",
            "author": "Mike Drob",
            "date": "2016-02-19T05:02:15+0000",
            "content": "Ok, making progress...\n\nTo recap what I've done so far (so that it's easier next time)\n\n* ant gen-tlds\ncopy affected analyzers to a stdX directory\nupdate affected factories\nupdate jflex-legacy task in build.xml\nupdate test data to use valid domains\n* ANT_OPTS='-Xmx5g' ant jflex\n* perl generateJavaUnicodeWordBreakTest.pl -v 8.0.0\n* sed 's/6_3/8_0/g' TestStandardAnalyzer.java TestUAX29URLEmailTokenizer.java\n\n\n(starred entries are verbatim commands)\n\nI also had to add a special case to the perl script where if there is a double quote inside of output string text to add a java escape. I saw this in one of the other Unicode release notes relating to hebrew text. The specific test this is necessary for is \u05d0\"\u05d0\n\nI was not able to figure out how to install jflex snapshot locally \u2013 I thought by running mvn install on master it would work but turns out I don't understand the nuances of ivy. Consequentially, I have not updated the unicode directives to 8.0 for any of the parsers, but all the analyzer/tokenizer tests still seem to pass so we might be ok? "
        },
        {
            "id": "comment-15154933",
            "author": "Robert Muir",
            "date": "2016-02-19T21:51:51+0000",
            "content": "I think we need to regenerate still, because there are new characters/character property changes so the actual tokenizer will change (even if the rules stay the same: the alphabet got bigger). "
        },
        {
            "id": "comment-15154936",
            "author": "Mike Drob",
            "date": "2016-02-19T21:53:14+0000",
            "content": "Question about what is proper behaviour in terms of backwards compatibility here...\n\nUpgrading JFlex from 1.6.0 to 1.6.1 (and 1.7.0, I assume) changes the generated output. I have no idea if the behaviour is identical between the new class files and the old. I imagine that we want to keep the Impls generated by the old version when operating with an old lucene match version, rather than regenerating those with the new jflex. If so, I'll drop the work I did on updating jflex-legacy task, since it doesn't make sense to keep around (it woudn't generate code to match what is in source control).\n\nDoes this make sense? "
        },
        {
            "id": "comment-15154949",
            "author": "Mike Drob",
            "date": "2016-02-19T21:59:36+0000",
            "content": "I think we need to regenerate still, because there are new characters/character property changes so the actual tokenizer will change (even if the rules stay the same: the alphabet got bigger).\nOk. My current plan will be to copy all existing tokenizers to std50 packages, update the factories to be cognizant of lucene version, update current jflex files to all use unicode 8.0 and then regenerate all of the new tokenizer classes.\n\nSome of the tokenizers have a unicode 3.0 directive, which indicates that they haven't been touched in a long time. This worries me a bit, but I'll see how it goes. "
        },
        {
            "id": "comment-15155035",
            "author": "Robert Muir",
            "date": "2016-02-19T22:41:37+0000",
            "content": "I think we should be ok. As far as i understand it, jflex will respect that unicode directive and the grammar and generate the equivalent state machine. But regenerating the \"old grammars\" means we get bugfixes from jflex: e.g. performance or memory improvements or whatever improved there, so I think its the right thing to do. "
        },
        {
            "id": "comment-15155062",
            "author": "Steve Rowe",
            "date": "2016-02-19T22:52:37+0000",
            "content": "+1 "
        },
        {
            "id": "comment-15155154",
            "author": "Mike Drob",
            "date": "2016-02-19T23:46:40+0000",
            "content": "Using newer version of jflex breaks our existing macros...\n\n\n  <macrodef name=\"run-jflex-and-disable-buffer-expansion\">\n    <attribute name=\"dir\"/>\n    <attribute name=\"name\"/>\n    <sequential>\n      <jflex file=\"@{dir}/@{name}.jflex\" outdir=\"@{dir}\" nobak=\"on\" inputstreamctor=\"false\"/>\n      <!-- LUCENE-5897: Disallow scanner buffer expansion -->\n<!-- ... snip ... -->\n      <replaceregexp file=\"@{dir}/@{name}.java\"\n                     match=\"(zzFinalHighSurrogate = 1;)(\\r?\\n)\"\n                     replace=\"\\1\\2          if (totalRead == 1) { return true; }\\2\"/>\n    </sequential>\n  </macrodef>\n\n\n\nThere is no longer a totalRead variable tracked by the JFlex generated code, instead we could check numRead I think. However, from reading LUCENE-5897 it is unclear whether this behaviour would have been fixed in later JFlex releases and we don't need the \"-and-disable-buffer-expansion\" marco at all. "
        },
        {
            "id": "comment-15155160",
            "author": "Steve Rowe",
            "date": "2016-02-19T23:50:29+0000",
            "content": "Yeah, the generated code underwent some changes there, so the hack we use to disable buffer expansion will require some adjustment.  This technique should be in JFlex though, I'll take a look this weekend at getting it in before the 1.7 release. "
        },
        {
            "id": "comment-15167852",
            "author": "Mike Drob",
            "date": "2016-02-25T20:49:15+0000",
            "content": "Steve, did you get a chance to look at the buffer adjustment? I'm going to pre-emptively remove the hack from our code and trust that it will be in the next release of JFlex.\n\nUnrelated; the ClassicTokenizer is already advertised as a legacy option \u2013 I don't think it makes sense to create a new version of it here. WDYT? "
        },
        {
            "id": "comment-15168956",
            "author": "Steve Rowe",
            "date": "2016-02-26T13:10:28+0000",
            "content": "Steve, did you get a chance to look at the buffer adjustment? I'm going to pre-emptively remove the hack from our code and trust that it will be in the next release of JFlex.\n\nNot yet.  Planning on it though.\n\nUnrelated; the ClassicTokenizer is already advertised as a legacy option \u2013 I don't think it makes sense to create a new version of it here. WDYT?\n\nUwe Schindler has written that he still recommends this tokenizer in some cases, so if you're asking if we should remove it, I don't think so. "
        },
        {
            "id": "comment-15169481",
            "author": "Uwe Schindler",
            "date": "2016-02-26T18:30:46+0000",
            "content": "Uwe Schindler has written that he still recommends this tokenizer in some cases, so if you're asking if we should remove it, I don't think so.\n\nI think the question was if it should also be upgraded to newer Unicode. But it does not rely on any unicode version the JAVA files should be identical. Please don't remove it! "
        },
        {
            "id": "comment-15169499",
            "author": "Mike Drob",
            "date": "2016-02-26T18:38:55+0000",
            "content": "Yea, Uwe understood my question. I wasn't planning on removing it, but I also didn't quite understand if it needed to be updated to the new Unicode. Sounds like I can leave it as is. "
        },
        {
            "id": "comment-15169638",
            "author": "Steve Rowe",
            "date": "2016-02-26T19:39:21+0000",
            "content": "ClassicTokenizer does have direct Unicode version dependencies: [:digit:] and [:alpha:] are the equivalent of \\p{Digit} and \\p{Letter}, respectively.  Right now those definitions are pinned at Unicode 3.0, which means that characters added since Unicode 3.0 (released 15 years ago, in 2000) will not be properly tokenized.\n\nAlso, there are several effectively-pinned character sets (for CJK and Thai) that are hard-coded in the grammar, and don't include any supplementary characters at all.  If the Unicode version changes, these will need to be moved to use the appropriate Unicode properties instead.\n\nI guess I'm -0 on leaving the Unicode version as-is because of the above, but since this tokenizer will never be removed, it seems bad to me to keep it pinned to such an old Unicode version. "
        },
        {
            "id": "comment-15169658",
            "author": "Robert Muir",
            "date": "2016-02-26T19:53:18+0000",
            "content": "Yeah its tricky. I kinda view classictokenizer as a tokenizer for the ignorant... its got tons of bogus western only assumptions and is basically wrong in every possible way. But arguing with this is like arguing with donald trump, so better to give folks like this their own dedicated crappy tokenizer and keep them off our back. From this perspective, it can be wired to unicode 1.0 and it serves its intended purpose. "
        },
        {
            "id": "comment-15172324",
            "author": "Mike Drob",
            "date": "2016-02-29T18:34:07+0000",
            "content": "I think I am getting to a good place here, just a few more issues that I need some additional direction \u2013\n\n\n  /**\n   * Sets the scanner buffer size in chars\n   */\n   public final void setBufferSize(int numChars) {\n     ZZ_BUFFERSIZE = numChars;\n     char[] newZzBuffer = new char[ZZ_BUFFERSIZE];\n     System.arraycopy(zzBuffer, 0, newZzBuffer, 0, Math.min(zzBuffer.length, ZZ_BUFFERSIZE));\n     zzBuffer = newZzBuffer;\n   }\n\n\nThis is code that we inject directly from our jflex templates, not generated code. True to their promises, ZZ prefixed items in jflex are subject to change, and this one has become final between old and new versions. We could fix this with an additional post-processing step to take out the final modifier, or put changes in jflex to add a new constructor or something like that. It looks like all of the non-test usage of setting the size happens immediately post construction.\n\nAlso, there are several effectively-pinned character sets (for CJK and Thai) that are hard-coded in the grammar, and don't include any supplementary characters at all. If the Unicode version changes, these will need to be moved to use the appropriate Unicode properties instead.\nCurrently ClassicTokenizer has THAI       = [\\u0E00-\\u0E59]; ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+;. If I understand the Unicode spec correctly with Unicode 8.0 we can remove the THAI declaration and it would be correctly included in LETTER. But I have near zero confidence in this. Alternatively, leaving it as is should be fine because the assigned THAI characters have not gone outside of that range.\nFor CJK, we have a special call out for CJ, but K was apparently already included in LETTER? I don't understand the relationship between ALPHANUM, \\p{Letter} and CJK. "
        },
        {
            "id": "comment-15176520",
            "author": "Robert Muir",
            "date": "2016-03-02T21:29:07+0000",
            "content": "I wouldnt change any of the ClassicTokenizer ranges, it should just continue to do what it did before.\n\nnot all of the thai characters are letters, and its important not to e.g. split on tone marks or make other mistakes like that: http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[[%3AThai%3A]-[%3ALetter%3A]]%0D%0A&g=&i=\n\nCJ is a separate category because ClassicTokenizer will return each han character individually as token. On the other hand hangul (K) is kept with letter because it is an alphabet. "
        },
        {
            "id": "comment-15178739",
            "author": "Mike Drob",
            "date": "2016-03-03T22:36:27+0000",
            "content": "New patch, takes care of all 5 generated tokenizers.\n\nThis patch is built using jflex 1.6.1 and unicode 7, so that we can at least have something in time for 6.0.\n\nI looked at the new generated jflex code and I think it takes care of the buffer expansion issue. At the very least, our existing StandardAnalyzer tests pass. Still need to have a macro for fixing buffersize, though.\n\nHad issues with TestUAX29URLEmailTokenizer.testLongEMAILatomText taking a while, not sure if that's part of the same issue or not.\n\nAlso, moved jflex version to the properties file with everything else instead of set directly in the build.xml "
        },
        {
            "id": "comment-15178765",
            "author": "Robert Muir",
            "date": "2016-03-03T22:52:07+0000",
            "content": "\nHad issues with TestUAX29URLEmailTokenizer.testLongEMAILatomText taking a while, not sure if that's part of the same issue or not.\n\nWell this test is already marked @Slow and just took 41.2s on my machine. Were you seeing stuff like that? As far as i know from the original issue, there were tests for this bug that would basically never finish at all . "
        },
        {
            "id": "comment-15178886",
            "author": "Steve Rowe",
            "date": "2016-03-03T23:59:39+0000",
            "content": "Mike, can you please exclude generated files from your patch?  The patches here are way big, and reviewers/committers will want to regenerate anyway. "
        },
        {
            "id": "comment-15179308",
            "author": "Mike Drob",
            "date": "2016-03-04T04:40:06+0000",
            "content": "Well this test is already marked @Slow and just took 41.2s on my machine. Were you seeing stuff like that? As far as i know from the original issue, there were tests for this bug that would basically never finish at all .\nI left it to run and came back later and saw that it took 50 minutes. But it passed. 40 seconds on your machine sounds great, I won't worry about it, thanks.\n\nMike, can you please exclude generated files from your patch? The patches here are way big, and reviewers/committers will want to regenerate anyway.\nSure, this makes sense.\n\nSteps to generate everything:\n\n#!/usr/bin/env bash\n\npushd lucene/analysis/common\nANT_OPTS=\"-Xmx5g\" ant gen-tlds jflex\nant jflex-legacy # For some reason this needs to be run separately from the jflex command. I could never figure out why.\npushd src/test/org/apache/lucene/analysis/standard\nrm WordBreakTestUnicode_6_3_0.java\nperl generateJavaUnicodeWordBreakTest.pl -v 8.0.0\npopd\npopd\n\n "
        },
        {
            "id": "comment-15179311",
            "author": "Robert Muir",
            "date": "2016-03-04T04:43:58+0000",
            "content": "If that test really took 50 minutes, there may be some issue there... "
        },
        {
            "id": "comment-15180135",
            "author": "Mike Drob",
            "date": "2016-03-04T16:54:11+0000",
            "content": "Apologies for the patch churn here...\n\nI took another look at the buffer expansion pieces and ran through a bit of it with a debugger. Ended up making some slight tweaks to our post-generation regex replaces, tests pass in a reasonable amount of time now. I would really appreciate a second set of eyes there, though. "
        },
        {
            "id": "comment-15180146",
            "author": "Robert Muir",
            "date": "2016-03-04T17:04:40+0000",
            "content": "I don't understand this change:\n\n\n-      <fileset dir=\"src/java/org/apache/lucene/analysis/standard\" includes=\"**/*.java\">\n+      <fileset dir=\"src/java/org/apache/lucene/analysis/standard\" includes=\"*.java\">\n\n\n\nThis makes me worried it will not be applied to e.g. std50 subpackages, and re-introduce the performance bug? "
        },
        {
            "id": "comment-15180161",
            "author": "Robert Muir",
            "date": "2016-03-04T17:10:49+0000",
            "content": "sorry, that just affects the cleaning part. But still, it should stay, because otherwise clean-jflex does not really work on the subdirectories. "
        },
        {
            "id": "comment-15180171",
            "author": "Mike Drob",
            "date": "2016-03-04T17:13:57+0000",
            "content": "There's a clean-jflex-legacy target that takes care of the subdirectories.\n\nI had to split the jflex and jflex-legacy tasks because when run together all of the legacy code is generated with private methods instead of public ones and won't compile. I probably spent a full day looking at that trying to figure out what's going on, and eventually gave up. "
        },
        {
            "id": "comment-15181701",
            "author": "Robert Muir",
            "date": "2016-03-05T12:56:08+0000",
            "content": "OK, thanks for the work! I will try to review all the changes in detail and confirm things are ok. I do think it is best to target 6.1 here. "
        },
        {
            "id": "comment-15193818",
            "author": "Mike Drob",
            "date": "2016-03-14T18:18:41+0000",
            "content": "Robert Muir - did you get a chance to look at this? Should I wait to ping you again until after 6.0 has been released? "
        },
        {
            "id": "comment-15193824",
            "author": "Robert Muir",
            "date": "2016-03-14T18:21:48+0000",
            "content": "My mistake. thanks for the reminder. I have been working to get things off the old numeric encoding but need to break up that work with other things too.  "
        },
        {
            "id": "comment-15207199",
            "author": "Mike Drob",
            "date": "2016-03-22T20:14:51+0000",
            "content": "Any updates here? I'm not sure if there is anything I need to be doing to keep this patch up to date. "
        },
        {
            "id": "comment-15232225",
            "author": "Mike Drob",
            "date": "2016-04-08T14:05:26+0000",
            "content": "Steve Rowe - I pinged the jflex list about getting the release going, but it looks like there are still a few outstanding issues to be resolved on that end. Do you think it is still worth waiting on the release there, or should we move forward here until jflex catches up and re-engage then? "
        },
        {
            "id": "comment-15232236",
            "author": "Steve Rowe",
            "date": "2016-04-08T14:12:45+0000",
            "content": "Hi Mike, sorry I haven't had the bandwidth to engage on this issue and on JFlex recently.  Thanks for the work you've done here and in creating JFlex issues for some of the release blocking issues there.\n\nSince Lucene 6.0 will be released shortly, and there is usually a gap of at least a month between minor releases, I think it makes sense to delay the decision about waiting on JFlex release for a couple weeks.  I hope to be able to work on JFlex release blockers this weekend and next week.  If after a couple weeks no JFlex release is imminent, I'd say move forward here. "
        },
        {
            "id": "comment-15271363",
            "author": "Mike Drob",
            "date": "2016-05-04T20:17:06+0000",
            "content": "Steve Rowe - I see no movement coming from the JFlex community. How would you feel most comfortable proceeding? "
        },
        {
            "id": "comment-15271376",
            "author": "Steve Rowe",
            "date": "2016-05-04T20:22:54+0000",
            "content": "Thanks for persisting Mike.  \n\nI (and other JFlex community members) still haven't made any progress on the JFlex release blockers, so it's probably best to move forward using the current JFlex release rather than waiting for JFlex 1.7 to be released.\n\nI'll try to review your work on this issue this week. "
        },
        {
            "id": "comment-15281171",
            "author": "Steve Rowe",
            "date": "2016-05-12T03:53:45+0000",
            "content": "Hi Mike, my review of your latest patch:\n\n\n\tAll the on-or-after tests in analysis factories should switch to 6.1.0 (since that's where this will likely land)\n\tI agree with Robert that ClassicTokenizer should stay on Unicode 3.0  - you changed it to 7\u0010.0.  That means it doesn't need version-specific behavior, so the factory changes and the build.xml targets aren't required.\n\tWikipediaTokenizer is in the same boat as ClassicTokenizer - it's essentially a cloned ClassicTokenizer with some modifications for Wiki syntax.  I vote for keeping it at Unicode 3.0 and reverting the factory changes and the build.xml targets.  Performing an effective upgrade would probably mean cloning the current StandardTokenizer and then re-layering the wiki syntax.\n\tIn generateJavaUnicodeWordBreakTest.pl, you added the test for the double quote code point: push @tokens, '\"'.join('', map { $_ ~~ /0022/ ? \"\\\\\\\"\" : \"\u29f9\u29f9u$_\" } @chars).'\"'; - why use the smartmatch operator here? No recursion required or unknown types here. Just /0022/ instead of $_ ~~ /0022/ would work, right?\n\tTestStandardAnalyzer and TestUAX29URLEmailTokenizer refer to WordBreakTestUnicode_8_0_0, but that should be _7_0_0.\n\tIn the run-jflex-and-disable-buffer-expansion target in build.xml, you removed the comment with the link to the relevant JIRA - why?\n\tYou said:\nI looked at the new generated jflex code and I think it takes care of the buffer expansion issue.\\\n+1 - LGTM\n\tRobert said:\nTestStandardAnalyzer and TestUAX29URLEmailAnalyzer also have a testBackcompat40 which calls setVersion and ensures it works.\nbut AFAICT you didn't put any backcompat tests in place?\n\tyou said:\nant jflex-legacy # For some reason this needs to be run separately from the jflex command. I could never figure out why.\nWhat happens if you make it a dependency of the jflex target?\n\n "
        }
    ]
}