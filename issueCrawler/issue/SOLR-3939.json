{
    "id": "SOLR-3939",
    "title": "An empty or just replicated index cannot become the leader of a shard after a leader goes down.",
    "details": {
        "affect_versions": "4.0-BETA,                                            4.0",
        "status": "Closed",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "When a leader core is unloaded using the core admin api, the followers in the shard go into recovery but do not come out. Leader election doesn't take place and the shard goes down.\n\nThis effects the ability to move a micro-shard from one Solr instance to another Solr instance.\n\nThe problem does not occur 100% of the time but a large % of the time. \n\nTo setup a test, startup Solr Cloud with a single shard. Add cores to that shard as replicas using core admin. Then unload the leader core using core admin.",
    "attachments": {
        "cloud.log": "https://issues.apache.org/jira/secure/attachment/12549220/cloud.log",
        "SOLR-3939.patch": "https://issues.apache.org/jira/secure/attachment/12548929/SOLR-3939.patch",
        "cloud2.log": "https://issues.apache.org/jira/secure/attachment/12549990/cloud2.log"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13475136",
            "date": "2012-10-12T16:40:11+0000",
            "content": "I think I see two issues so far:\n\n1. SOLR-3940 - there can be a long wait that should not exist\n2. We should consider a sync attempt from leader to replica that fails due to 404 a success. That is either a core that has been unloaded or a starting or stopping Solr instance - treating it as a fail in the unloaded core (404) case can cause our current leader choice strategy to fail to make progress. A stopping or starting Solr instance will move on to recovery. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13475190",
            "date": "2012-10-12T17:42:28+0000",
            "content": "Patch attached.\n\nTreat 404 on leader sync to replicas as success + test.\n\nAlso includes fix for SOLR-3940. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13475207",
            "date": "2012-10-12T18:09:50+0000",
            "content": "The patch solved the issue. I was able to unload the leader core and one of the replicas became the leader without delay. Tested three times, worked each time. No error in the logs.\n\nGreat job! Thanks! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13475224",
            "date": "2012-10-12T18:33:59+0000",
            "content": "Thanks Joel! "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13475905",
            "date": "2012-10-14T21:28:12+0000",
            "content": "I think we need to re-open this issue.\n\nI tried unloading the shard leader when a replica is in another Solr instance and the leader election didn't take place. The initial test had the shard leader and replica in the same Solr instance, which works with this patch.\n\nHere is how to setup the test:\n\n1) Start initial solr instance, automatically creating collection1 and shard1.\njava -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -jar start.jar\n\n\n2) Add shard2 to the same solr instance using coreadmin.\n\nhttp://localhost:8983/solr/admin/cores?action=CREATE&name=mycore&collection=collection1&shard=shard2\n\n3) Feed exampledocs to collection1.\n\n4) Startup another solr instance and point to same zookeeper. This will create a replica for shard1 and replicate the data from shard1.\n\n5) Unload the shard1 leader (core collection1) on the first solr instance.\n\nhttp://localhost:8983/solr/admin/cores?action=UNLOAD&core=collection1\n\nThe leader election process doesn't take place. \n\nThis would be the basic scenario for creating a micro-shard and then migrating it to another Solr instance. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13475944",
            "date": "2012-10-15T03:30:16+0000",
            "content": "Thanks for the report - we don't have much testing around unload with SolrCloud yet - this is good stuff to address.\n\nI've setup a test that seems to show an issue. I'll try and look at it some more tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13476297",
            "date": "2012-10-15T17:47:54+0000",
            "content": "is this with an empty index? "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13476316",
            "date": "2012-10-15T18:06:39+0000",
            "content": "I tested with the exampledocs loaded. Step 3 in the test above. I loaded the shards before starting up the replica on the second solr instance. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13476400",
            "date": "2012-10-15T20:26:06+0000",
            "content": "interesting - I can see an issue when I run the test with empty indexes, but my current test is passing if I add some docs. The main reason I see for this at the moment is that a leader who tries to sync with his replicas will always fail with an empty tlog (no frame of reference).\n\nI'll have to dig deeper for the 'docs in index' case. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13476422",
            "date": "2012-10-15T20:39:29+0000",
            "content": "No sure if this helps. Here is stack trace from my second solr instance. This is the instance that would be the leader after the leader core was unloaded on the first instance.\n\nSEVERE: There was a problem finding the leader in zk:org.apache.solr.common.SolrException: Could not get leader props\n\tat org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:709)\n\tat org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:673)\n\tat org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1070)\n\tat org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:273)\n\tat org.apache.solr.cloud.ZkController.access$100(ZkController.java:82)\n\tat org.apache.solr.cloud.ZkController$1.command(ZkController.java:190)\n\tat org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:116)\n\tat org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:46)\n\tat org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:90)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:526)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /collections/collection1/leaders/shard1\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:102)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:927)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:244)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:241)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:63)\n\tat org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:241)\n\tat org.apache.solr.cloud.ZkController.getLeaderProps(ZkController.java:687)\n\t... 10 more\n\nOct 15, 2012 3:39:18 PM org.apache.solr.common.SolrException log\nSEVERE: :org.apache.solr.common.SolrException: There was a problem finding the leader in zk\n\tat org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1080)\n\tat org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:273)\n\tat org.apache.solr.cloud.ZkController.access$100(ZkController.java:82)\n\tat org.apache.solr.cloud.ZkController$1.command(ZkController.java:190)\n\tat org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:116)\n\tat org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:46)\n\tat org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:90)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:526)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502) "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13476449",
            "date": "2012-10-15T21:06:06+0000",
            "content": "I restarted the second solr instance and it came up as the leader for shard1, with no errors. \n\nI'll try to re-produce again. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13476487",
            "date": "2012-10-15T21:39:39+0000",
            "content": "I reproduced it again. I pulled again from the top of the 4x branch. I didn't apply the patch because it was committed, I believe.\n\nSame exact steps as described above. Attached is part of the log file from the second Solr instance that shows the replica going into recovery. It's looking for the collection1 core that was unloaded from the first solr instance. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13476488",
            "date": "2012-10-15T21:40:38+0000",
            "content": "The log output from solr. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13477222",
            "date": "2012-10-16T18:02:04+0000",
            "content": "It looks like after the leader is unloaded, the replica attempts to sync to the unloaded leader as part of the process to determine if it can be leader. When this fails, it thinks that there are better candidates to become leader. Then it goes into a recovery loop. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13477247",
            "date": "2012-10-16T18:29:47+0000",
            "content": "That's what I see when I have an empty index. The leader sync fails because sync always fails with no local versions.\n\nThe case with docs is perhaps a bit trickier since my simple test passes. I'll take a look at the logs. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13477250",
            "date": "2012-10-16T18:33:29+0000",
            "content": "I think I see the issue. While we have talked about it, we don't currently try to populate the transaction log after a replication.\n\nSo, the second core replica is replicating, it's got docs but no versions, then it tries to become the leader - but just like with the empty index, it cannot successfully sync with no versions as a frame of reference. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13477252",
            "date": "2012-10-16T18:35:33+0000",
            "content": "(My test was passing because I had the replica up initially, so it go the docs from the leader not through replication) "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13477898",
            "date": "2012-10-17T13:54:35+0000",
            "content": "So that was the difference between the initial test with all the cores in the single Solr instance. I ran this test starting up all the cores and then loaded, which hit all the transaction logs.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13477906",
            "date": "2012-10-17T14:11:29+0000",
            "content": "I'm not sure - I'd have to think about it. Could be as simple as timing. If the new leader candidate gets an updated cluster state fast enough, he won't even consider the core that just went down when trying to become the leader.\n\nAnyhow, for what I am seeing in my tests so far, I think have a solution that works for now - just have to test it out. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13478081",
            "date": "2012-10-17T17:59:47+0000",
            "content": "Mark & I were chatting a little about this.  The easiest fix would seem to be\n\n\tif a single remaining replica is active, it should always become the leader (even if it has no recent versions)\n\tif an existing replica comes back up and tries to sync with this new leader, it should fail (or somehow be forced to replicate from the new leader)\n\n\n\nThat still begs the question... what if two replicas are in the situation of having no recent versions because they both just finished replicating?\n\nAnther solution (which is more difficult and would take longer) is to store some number of latest versions in the commit data. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13478333",
            "date": "2012-10-17T20:22:55+0000",
            "content": "Curious as to what the procedure is when the leader VM is taken down, rather then the leader core is unloaded. This scenario seems to work even when the replica was just be replicated to.  "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13478386",
            "date": "2012-10-17T21:08:34+0000",
            "content": "Actually just tested, and it's a problem when you take down the lead VM as well. So it's not specific to unloading lead core. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13479322",
            "date": "2012-10-18T20:22:15+0000",
            "content": "Joel: It could just be timing - live nodes are likely faster to update than the cluster state - when you just unload a core, the live node stays - so the unloaded core will only not take part in leader election when the leader candidate gets the new cluster state. When you take down a vm, the live node goes. Perhaps this is noticed faster. In either case, it's a race.\n\nYonik:\n\nfor case one, I'd like to still sync if possible - but if there are no starting versions in the updatelog, don't worry if the sync was not a success?\n\nI still have to add a test and fix for case 2. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13479350",
            "date": "2012-10-18T20:58:48+0000",
            "content": "Sorry - not starting versions - recent versions. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13479368",
            "date": "2012-10-18T21:13:54+0000",
            "content": "Here is a patch that should address point 1 - point 2 still has no fix or test. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13480076",
            "date": "2012-10-19T15:17:31+0000",
            "content": "I applied the patch to a fresh pull of the 4x branch. \n\nThen I performed the test that I detailed in the Oct 14th comment. The versions issue still causes the recovery loop. I've attached the log (cloud2.log) that shows the recovery loop.\n\nWhen I do a similar test, all in the same instance it works, but as you mentioned this is likely because of the race.   "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13480094",
            "date": "2012-10-19T16:00:16+0000",
            "content": "I've got a test for the second case yonik mentioned now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13480126",
            "date": "2012-10-19T16:54:03+0000",
            "content": "for case one, I'd like to still sync if possible\n\nCase #1 only had a single active replica up, so there's no one to sync with (or you mean try to sync with even shards marked as down?)\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13480162",
            "date": "2012-10-19T17:38:02+0000",
            "content": "I mean, in the general case, don't just let someone be the leader if they are active. Make them sync and only let them be the leader if they are successful.\n\nHowever, if they have no recent versions, let them be the leader even if the sync fails. This lets us keep consistency in almost all cases. True, without the sync, there will still not be data loss, but I'd still like to try and force consistency if possible as well. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13481008",
            "date": "2012-10-21T18:12:36+0000",
            "content": "Actually, even if we someday replicate recent versions along with the index (by adding them to the commitData in the index, etc), it may still be good to support indexes w/o version info.  On the other side of the spectrum from having indexing completely automated, some people may want the ability to create a new shard off-line and then insert it into the cluster as read-only. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13482774",
            "date": "2012-10-23T22:54:13+0000",
            "content": "In two other issues I was working on, unrelated changes seemed to start causing test fails in one of the solrcloud tests - it's a fail I had seen sometimes in the past on Apache jenkins. A fail about waiting to notice a live node drop. It seems that was caused by this - it took some time to trace it back here. One of the nodes doesn't see a live node change because he is stuck in a leader election loop.\n\nGiven that, I plan on committing what I have so far - so it stops blocking my other two issues. We can then iterate further on trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13483120",
            "date": "2012-10-24T10:25:41+0000",
            "content": "Okay, I've finally got all tests passing reliably for me. I had to add SocketException to the list of exceptions that are okay to consider a peer sync success. I'll try and get this committed tonight. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13483567",
            "date": "2012-10-24T20:33:53+0000",
            "content": "Trying to think if this could happen when there are versions too... say that instead of having no versions, we just have old versions from before we did the replication.  This may argue for somehow marking the start of a replication in the transaction log and then never retrieving versions older than that. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13483595",
            "date": "2012-10-24T21:11:10+0000",
            "content": "Thinking of some scenarios where this could happen:\n\n1. R1,R2 both up and active, add docs 1,2,3\n2. bring R2 down\n3. add docs 4 through 1million\n4. bring R2 up, peersync fails, replication is kicked off\n5. R2 finishes replication and becomes active, but it's recent version still list 1,2,3\n6. bring R1 down, R2 becomes the leader\n7. bring R2 up, it does a peer-sync with R1, which looks like it has really old versions (and succeeds because of that)\n8. if the leader (R2) does a peer-sync back with R1, it will fail (not sure of the consequences of this)\n\n\nAnother variation... if there's an update between 6 and 7:\n6.5. add doc 1million+1\n\nThis will cause recent versions of R2 to be 1,2,3,1000001\nIt would be good to verify that peersync to the leader will either fail (causing full replication), or pick up the new document. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13483600",
            "date": "2012-10-24T21:18:19+0000",
            "content": "Currently the leader does not peer sync back to a replica coming up because it would have to buffer updates.\n\nI think that if a replica is somehow ahead of the leader when coming back, peersync should fail and it should replicate. I think since this is not a common case, that is much simpler than trying to peersync back from the leder to the replica in this case. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13483649",
            "date": "2012-10-24T22:08:55+0000",
            "content": "Currently the leader does not peer sync back to a replica coming up because it would have to buffer updates.\n\npeer sync doesn't require buffering updates.  AFAIK, we don't do that until we realize we need to replicate? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13483664",
            "date": "2012-10-24T22:26:43+0000",
            "content": "As far as I remember, if updates are coming in when you try and peer sync, we fail it? Isn't that what capturing the starting versions is all about?\n\nWhen a leader syncs with his replicas on leader election, we know docs are not coming in, so we don't worry about that starting versions check - but if you want to peer sync from the leader to a replica that is coming back up, if updates are coming in, you are going to force a replication anyway. Since it's already an uncommon case, it doesn't seem worth tackling. I mention buffering, because it seemed you would have to to be able to peer sync when updates are coming in (or block updates).\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13484611",
            "date": "2012-10-26T00:45:57+0000",
            "content": "I've committed my latest work to 4x Joel - can you do a bit more testing with a recent checkout? "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13484652",
            "date": "2012-10-26T02:17:10+0000",
            "content": "I ran the Oct 14th test and the leader election worked perfectly. Then I tested shutting down the leader VM instead of unloading the loader core and this worked fine.  \n\nThen I tried a leader with two replicas that had both just been replicated to. When I unloaded the leader neither replica became leader. But this was the case that was not yet accounted for I believe.\n\nI can't think of a use case where the second scenario would happen though.\n\nThe first scenario though is critical for migrating micro-shards, so it's great that you committed this.\n\nThanks for your work on this issue.\n\nJoel\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13484683",
            "date": "2012-10-26T04:07:53+0000",
            "content": "Isn't that what capturing the starting versions is all about?\n\nFor a node starting up, yeah.  For a leader syncing to someone else - I don't think it should matter.\nedit: OK - I think I got what you're saying now - if the new node coming up did have an extra doc, then the only way to guarantee the leader pick it up would be if not too many updates came in for either.  We could require that a sync from the leader to the replica have the list of recent versions overlap enough (else the replica would be forced to replicate), but as you say... if updates are coming in fast enough (and that is prob pretty slow) you're going to force a replication anyway.\n\nbut if you want to peer sync from the leader to a replica that is coming back up, if updates are coming in, you are going to force a replication anyway. \n\nIf updates were coming in fast enough during the \"bounce\"... I guess so. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13485000",
            "date": "2012-10-26T15:31:00+0000",
            "content": "Okay, I'm going to resolve this - we can make a new issue for the case where a replica comes up and is ahead somehow. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13494966",
            "date": "2012-11-11T20:27:56+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1397672\n\nSOLR-3939: Consider a sync attempt from leader to replica that fails due to 404 a success.\nSOLR-3940: Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over.\n "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610637",
            "date": "2013-03-22T16:23:16+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1402362\n\nSOLR-3933: Distributed commits are not guaranteed to be ordered within a request.\n\nSOLR-3939: An empty or just replicated index cannot become the leader of a shard after a leader goes down. \n\nSOLR-3971: A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards. \n\nSOLR-3932: SolrCmdDistributorTest either takes 3 seconds or 3 minutes. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610642",
            "date": "2013-03-22T16:23:37+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1402361\n\nSOLR-3933: Distributed commits are not guaranteed to be ordered within a request.\n\nSOLR-3939: An empty or just replicated index cannot become the leader of a shard after a leader goes down. \n\nSOLR-3971: A collection that is created with numShards=1 turns into a numShards=2 collection after starting up a second core and not specifying numShards. \n\nSOLR-3932: SolrCmdDistributorTest either takes 3 seconds or 3 minutes. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610678",
            "date": "2013-03-22T16:26:22+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1397672\n\nSOLR-3939: Consider a sync attempt from leader to replica that fails due to 404 a success.\nSOLR-3940: Rejoining the leader election incorrectly triggers the code path for a fresh cluster start rather than fail over. "
        }
    ]
}