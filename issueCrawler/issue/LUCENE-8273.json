{
    "id": "LUCENE-8273",
    "title": "Add a ConditionalTokenFilter",
    "details": {
        "components": [],
        "status": "Closed",
        "resolution": "Fixed",
        "fix_versions": [
            "7.4"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Major",
        "type": "New Feature"
    },
    "description": "Spinoff of LUCENE-8265.  It would be useful to be able to wrap a TokenFilter in such a way that it could optionally be bypassed based on the current state of the TokenStream.  This could be used to, for example, only apply WordDelimiterFilter to terms that contain hyphens.",
    "attachments": {
        "LUCENE-8273.patch": "https://issues.apache.org/jira/secure/attachment/12920458/LUCENE-8273.patch",
        "LUCENE-8273-2.patch": "https://issues.apache.org/jira/secure/attachment/12923754/LUCENE-8273-2.patch",
        "LUCENE-8273-part2-rebased.patch": "https://issues.apache.org/jira/secure/attachment/12923790/LUCENE-8273-part2-rebased.patch",
        "LUCENE-8273-part2.patch": "https://issues.apache.org/jira/secure/attachment/12923123/LUCENE-8273-part2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16449893",
            "author": "Alan Woodward",
            "content": "Here's a patch. ",
            "date": "2018-04-24T13:55:23+0000"
        },
        {
            "id": "comment-16449897",
            "author": "Alan Woodward",
            "content": "I added this to core rather than to the analysis module as it seems to me to be a utility class like FilteringTokenFilter, which is also in core.  But I'm perfectly happy to move it to analysis-common if that makes more sense to others. ",
            "date": "2018-04-24T13:57:07+0000"
        },
        {
            "id": "comment-16449993",
            "author": "David Smiley",
            "content": "Nice!\n\nCould you add a test with a filter that may produce multiple terms instead of just one-to-one? \u00a0And maybe try the scenario when the filter swallows it (e.g. WDF sees a token that is simply a symbol). \u00a0The documentation is ok\u00a0but I was confused about practically what would usage look like until I looked at the test, so maybe a simple example in the class javadocs could shed light on this.\u00a0\n\nWith such a general utility, I wonder if the existing TokenFilters that have precondition checks (e.g. stemmers that check conditions)\u00a0needn't bother\u00a0doing this anymore since\u00a0you could wrap the stemmer with the BypassingTokenFilter here with a check if the word is in a list? \u00a0Then\u00a0we\u00a0wouldn't\u00a0even need KeywordAttribute! \u00a0I realize this is taking your simple proposal\u00a0and taking it very far but I think it's worth discussing for 8.0.\n\nAn\u00a0alternative to your\u00a0BypassingTokenFilter\u00a0is creating an intermediate base class between existing TokenFilters that bypass (e.g. stemmers + ones that ought to like WDF) and TokenFilter. \u00a0But thinking about this more, this seems like a bigger disruptive change and wouldn't cast a net as wide as BypassingTokenFilter which can filter anything, even filters where the author forgot to consider being filtered. ",
            "date": "2018-04-24T14:49:29+0000"
        },
        {
            "id": "comment-16449994",
            "author": "Mike Sokolov",
            "content": "The name  \"resetting\" is a little confusing since it controls propagation of calls in end() and close() as well. Maybe call it \"recursing\" or \"once\" or something else? ",
            "date": "2018-04-24T14:49:45+0000"
        },
        {
            "id": "comment-16451265",
            "author": "Robert Muir",
            "content": "\nI added this to core rather than to the analysis module as it seems to me to be a utility class like FilteringTokenFilter, which is also in core. But I'm perfectly happy to move it to analysis-common if that makes more sense to others.\n\nThe idea is cool but I would like to see it more fleshed out (eg. marked experimental somewhere) before going into core/:\n\n\timproved testing:  i'd like to see some edge cases tested such as both \"true\" and \"false\" cases on the final token for end(), etc. what happens is a little sneaky,  think it should be hooked into TestRandomChains (this should probably be explicitly added to that test, wrapping with check of random.nextBoolean() or something simple that will test all cases). This may uncover some integration difficulties. In particular, it is not clear to me how some stuff such as end() works correctly in the general case with this filter right now.\n\tintegration with CustomAnalyzer: as this would add a generic \"if\" to allow branching in analysis chains (there is an issue somewhere for this), which would be very powerful, it would be good to plumb into CustomAnalyzer to make sure it can work well with the factory model. seems doable with the functional interface but needs to be proven out.\n\n ",
            "date": "2018-04-24T21:46:07+0000"
        },
        {
            "id": "comment-16451271",
            "author": "Robert Muir",
            "content": "Also I am not sure if the name BypassingTokenFilter is the best.\n\nIt works well for your case (but I think \"bypass\" may be due to some inertia/history and maybe not the best going forward). Maybe it should be \"if\" instead of \"unless\".\n\n\n// don't lowercase if the term contains an \"o\" character\nTokenStream t = new BypassingTokenFilter(cts, AssertingLowerCaseFilter::new) {\n  CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n  @Override\n  protected boolean bypass() throws IOException {\n    return termAtt.toString().contains(\"o\");\n  }\n};\n\n\n\nBut will look awkward for other cases:\n\n// apply greek stemmer (\"don't bypass\") if the token is written in the greek script.\nTokenStream t = new BypassingTokenFilter(ts, GreekStemmer::new) {\n  ScriptAttribute scriptAtt = addAttribute(ScriptAttribute.class);\n  @Override\n  protected boolean bypass() throws IOException {\n    return scriptAtt.getCode() != UScript.GREEK;\n  }     \n};\n\n ",
            "date": "2018-04-24T21:54:32+0000"
        },
        {
            "id": "comment-16452274",
            "author": "Alan Woodward",
            "content": "Here's an updated patch:\n\n\tnow works with wrapped filters that emit more than one token (thanks David!)\n\trenamed to ConditionalTokenFilter and the logic reversed (thanks Robert!)\n\tcleaned up all the logic around reset(), close() and end()\n\tintegrated into testRandomChains.\n\n\n\nThis latter one is a bit clunky, as this TokenFilter won't work with filters that consume more than one token at a time - eg ShingleFilter or SynonymGraphFilter.  At the moment I have a blacklist, but there may be a better way of isolating that - preferably one that throws errors when you build the TokenStream.  Speak up if you have any suggestions.\n\nI do like the idea of integrating things into CustomAnalyzer, will look at that next. ",
            "date": "2018-04-25T13:47:02+0000"
        },
        {
            "id": "comment-16452348",
            "author": "Mike Sokolov",
            "content": "Perhaps this can be extended to handle the case of ShingleFilter et al by tracking whether we are currently in a recursion. In the case of some filter that consumes multiple input tokens, it could call us multiple times while we are in DELEGATING, but if we remember that we called delegate.incrementToken() and it has not yet returned, then we should not recurse again, but should instead call input.incrementToken(). I haven't tried this, and my brain is getting contorted trying to hold all the cases, but I think it should work? ",
            "date": "2018-04-25T14:36:34+0000"
        },
        {
            "id": "comment-16452353",
            "author": "Mike Sokolov",
            "content": "Also - along the lines of making general purpose TokenFilter compositional tools, I think this general idea can be extended to handle multiple branches, so not just an 'if' statement, but we can also have a 'switch' statement that invokes one of a set of different wrapped filters, although if we did that, it should be a separate thing from this I think. ",
            "date": "2018-04-25T14:39:59+0000"
        },
        {
            "id": "comment-16452407",
            "author": "Alan Woodward",
            "content": "Perhaps this can be extended to handle the case of ShingleFilter\n\nI'm not sure that this makes sense in those cases though?  For example, what if the first token in the tokenstream matches the condition and is passed to the ShingleFilter, but the second one doesn't? ",
            "date": "2018-04-25T15:00:48+0000"
        },
        {
            "id": "comment-16452428",
            "author": "Robert Muir",
            "content": "\nThis latter one is a bit clunky, as this TokenFilter won't work with filters that consume more than one token at a time - eg ShingleFilter or SynonymGraphFilter. At the moment I have a blacklist, but there may be a better way of isolating that - preferably one that throws errors when you build the TokenStream. Speak up if you have any suggestions.\n\nI don't understand the problem yet, but I will try to fight with it. You mean any filter that uses captureState? ",
            "date": "2018-04-25T15:07:08+0000"
        },
        {
            "id": "comment-16452431",
            "author": "Mike Sokolov",
            "content": "I'm not sure that this makes sense in those cases though? For example, what if the first token in the tokenstream matches the condition and is passed to the ShingleFilter, but the second one doesn't?\nHmm right. Well you might be able to use that idea to detect illegal states at least ",
            "date": "2018-04-25T15:08:29+0000"
        },
        {
            "id": "comment-16452676",
            "author": "Alan Woodward",
            "content": "You mean any filter that uses captureState?\n\ncapture/restoreState works fine, the problem comes when you get a filter that needs to look ahead in the tokenstream, so for example if SynonymGraphFilter has a multiword synonym \"a b c -> d\", and you hit token \"a\", then the filter pulls in two more tokens to see if it matches the whole synonym; but ConditionalTokenFilter only allows you to pull in one token at a time, because it needs to distinguish between incrementToken() as called by the next filter down the line, and incrementToken() as called by its delegate. ",
            "date": "2018-04-25T17:20:09+0000"
        },
        {
            "id": "comment-16453806",
            "author": "Robert Muir",
            "content": "So you mean that the problem is not related to buffering state, its more that it only expects one call to incrementToken() and does the wrong thing if it gets called additional times? Sorry I'm slow here, i wanted to play with the patch last night but didn't have the time.\n\n\u00a0\n\n\u00a0 ",
            "date": "2018-04-26T10:30:43+0000"
        },
        {
            "id": "comment-16453807",
            "author": "Alan Woodward",
            "content": "Yes, exactly that. ",
            "date": "2018-04-26T10:32:15+0000"
        },
        {
            "id": "comment-16453814",
            "author": "Robert Muir",
            "content": "ok i get it, so basically the filter switches between two states right now with that boolean it has. So its basically hardcoding that the passed filter will only call incrementToken once with that. \n\nmaybe it could do this differently, like insert an \"end-if\" filter around the one passed in by the user, which would signal completion? ",
            "date": "2018-04-26T10:38:44+0000"
        },
        {
            "id": "comment-16453818",
            "author": "Robert Muir",
            "content": "i mean in the worst case, you could make it \"correct\" by using something like the StackWalker api right? Then we figure out how to make it more efficient. ",
            "date": "2018-04-26T10:41:55+0000"
        },
        {
            "id": "comment-16453852",
            "author": "Robert Muir",
            "content": "I feel like the OneTimeWrapper you have is close, it just needs to set the boolean on its \"parent\" after input.incrementToken() ? At that point we know the subfilter is \"done\" ",
            "date": "2018-04-26T11:03:06+0000"
        },
        {
            "id": "comment-16455543",
            "author": "Robert Muir",
            "content": "just imaging scenarios more, its probably useful if the thing can avoid corrupting graphs. By that i mean: conceptually the user has to understand that the filtering applies the condition based on the first token and that the filter gets whatever it pulls (based on its wanted context), and those are provided \"graph-aligned\" or something. I think its just inherent in what you are trying to do and not specific to the implementation: it needs to have some restrictions to avoid trouble? So maybe this filter should also consider positionLength... ",
            "date": "2018-04-26T23:20:32+0000"
        },
        {
            "id": "comment-16456717",
            "author": "Alan Woodward",
            "content": "I think I fixed it - attached is a patch including a test where we wrap SynonymGraphFilter, and everything seems to pass. ",
            "date": "2018-04-27T16:43:21+0000"
        },
        {
            "id": "comment-16457066",
            "author": "Mike Sokolov",
            "content": "Cool! I haven't read the patch carefully, but I looked at the tests, and I\nsee RandomChains test so this is probably covered, but I was wondering\nabout the case you mentioned earlier where say there is a multi-token\nsynonym and shouldFilter() is true for the first token and false for the\nsecond. I guess the filter just does not apply the synonym then?\n\nOn Fri, Apr 27, 2018 at 12:44 PM, Alan Woodward (JIRA) <jira@apache.org>\n ",
            "date": "2018-04-27T21:06:00+0000"
        },
        {
            "id": "comment-16458469",
            "author": "Alan Woodward",
            "content": "And here's a patch that integrates it into CustomAnalyzer ",
            "date": "2018-04-30T10:48:00+0000"
        },
        {
            "id": "comment-16458472",
            "author": "Alan Woodward",
            "content": "Mike Sokolov in that case, the filter would still get applied, because we only check the token that is passed to the synonym filter from outside.  Anything that's pulled by the synonym filter itself doesn't get checked.  Although thinking about it, it would be possible to run the check in the OneTimeWrapper as well and return 'false' from things that don't pass the check.  I'm not sure how that would work with the graph itself though, it might end up corrupting things. ",
            "date": "2018-04-30T10:50:14+0000"
        },
        {
            "id": "comment-16458501",
            "author": "Robert Muir",
            "content": "I like the custom analyzer integration. Can we rename ifMatches just to if? This makes it more natural for the user to pair with the necessary endif, doesn't imply any regex matching, etc. I think its useful to mention the necessary endif in the javadocs for both these methods, if users forget to call it they should get a compile error from build(), but it may not be obvious. I would also add to the ifTerm docs that it is just sugar, with snippet of how to implement it with if + CharTermAttribute. This gives an example in case the user needs to work on some other attribute. ",
            "date": "2018-04-30T11:46:47+0000"
        },
        {
            "id": "comment-16458507",
            "author": "Robert Muir",
            "content": "I'm also curious about common use cases where the condition is just matching a list of words, basically what the KeywordMarker factory provides today. Does the user have access to stuff like resource loaders to read from files / is it intuitive so they won't be reading the list of words in on every token or other mistakes? If we can make this simple, I think we can deprecate KeywordMarker and many other exception-list-type mechanisms hardcoded in all the filters, which would be a really nice cleanup.  ",
            "date": "2018-04-30T11:56:39+0000"
        },
        {
            "id": "comment-16458550",
            "author": "Alan Woodward",
            "content": "if is a keyword, unfortunately, so that won't work.  Maybe ifCondition instead?\n\nAccess to resources will be a bit trickier, I think maybe the best way to do that would be to have a specialised method ifInList or something similar that takes a path to a word list.  I'll see what I can come up with. ",
            "date": "2018-04-30T13:02:29+0000"
        },
        {
            "id": "comment-16458569",
            "author": "Robert Muir",
            "content": "sounds good. yeah i know the resource stuff/keywork marking is tricky, i looked at what the existing factory is doing and its pretty crazy. \n\nit seems you need to make the ConditionalTokenFilterFactory implement the resourceloaderaware stuff always, because its separately a bug that the current patch will \"hide\" the resourceloader from anything inside the if? So I think it should implement the interface and pass the loader in its inform() method to stuff inside. Maybe this leads towards a solution to what you need for the conditional part, too. ",
            "date": "2018-04-30T13:32:34+0000"
        },
        {
            "id": "comment-16459103",
            "author": "Steve Rowe",
            "content": "if is a keyword, unfortunately, so that won't work. Maybe ifCondition instead?\n\nShorter and almost as good?: when (used e.g. by Mockito) ",
            "date": "2018-04-30T21:35:46+0000"
        },
        {
            "id": "comment-16459564",
            "author": "Alan Woodward",
            "content": "Updated patch.  ConditionalTokenFilterFactory is now a top-level class, distinct from ConditionBuilder.  I've added a TermExclusionFilter that accepts a list of terms and only runs its child filters if the current token is not in its list, and demonstrated how to use it in TestCustomAnalyzer.  At the moment it just reads a word file, but we can expand it to accept patterns or a directly passed in list of terms in follow ups.  I've also changed the CustomAnalyzerBuilder to use when rather than ifXXX - thanks for the suggestion Steve! ",
            "date": "2018-05-01T09:45:11+0000"
        },
        {
            "id": "comment-16459643",
            "author": "Alan Woodward",
            "content": "it seems you need to make the ConditionalTokenFilterFactory implement the resourceloaderaware stuff always\n\nJust spotted Robert's comment here, I've added a new patch which fixes this.  Thanks! ",
            "date": "2018-05-01T11:57:24+0000"
        },
        {
            "id": "comment-16466012",
            "author": "Alan Woodward",
            "content": "Patch, up to date with master and passing all tests and precommit.  I think this is ready? ",
            "date": "2018-05-07T15:16:25+0000"
        },
        {
            "id": "comment-16466309",
            "author": "David Smiley",
            "content": "Looks good Alan.\n\n\tMaybe make ConditionBuilder whenTerm(Predicate<CharTermAttribute> predicate) even simpler by referring to CharSequence in the predicate instead of CharTermAttribute? This is just a convenience method after all; and it keeps the caller immune to some lower level details like Attributes. And it somewhat prevents them from using mutable methods on CharTermAttribute which would be pretty nasty.\n\n ",
            "date": "2018-05-07T18:45:47+0000"
        },
        {
            "id": "comment-16467353",
            "author": "Alan Woodward",
            "content": "Thanks David, I'll do that.  Will commit later on if nobody else has any comments. ",
            "date": "2018-05-08T12:32:37+0000"
        },
        {
            "id": "comment-16467407",
            "author": "Robert Muir",
            "content": "For the new TermExclusionFilterFactory:\n\n\npublic static final String EXCLUDED_TOKENS = \"excludeFile\";\n\n\n\nKeywordMarkerFilterFactory currently uses a different parameter name: \"protected\". So does WordDelimiterFilterFactory (which I think was the one that inspired this JIRA issue). Maybe we want it to be consistent, to support migrating away from that stuff? ",
            "date": "2018-05-08T13:22:39+0000"
        },
        {
            "id": "comment-16468547",
            "author": "Alan Woodward",
            "content": "New patch, whenTerm() now takes Predicate<CharSequence>, and TermExclusionFilterFactory uses protected instead of excludeFile ",
            "date": "2018-05-09T08:27:13+0000"
        },
        {
            "id": "comment-16471731",
            "author": "ASF subversion and git services",
            "content": "Commit 7f13e5642924aa7f97e21b059553cfd6b5b56c21 in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7f13e56 ]\n\nLUCENE-8273: Add ConditionalTokenFilter ",
            "date": "2018-05-11T10:35:10+0000"
        },
        {
            "id": "comment-16471732",
            "author": "ASF subversion and git services",
            "content": "Commit 1ce3ebadbd60d4485145c71b48330d0aabde77ad in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1ce3eba ]\n\nLUCENE-8273: Add ConditionalTokenFilter ",
            "date": "2018-05-11T10:35:12+0000"
        },
        {
            "id": "comment-16471733",
            "author": "Alan Woodward",
            "content": "Thanks all!  I'll start to look at applying this to the various language analyzers soon. ",
            "date": "2018-05-11T10:36:17+0000"
        },
        {
            "id": "comment-16471900",
            "author": "Alan Woodward",
            "content": "The elasticsearch CI had some failures due to this:\n\nant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=EF8BCF910EB1138C -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=es-CR -Dtests.timezone=Asia/Ashgabat -Dtests.asserts=true -Dtests.file.encoding=UTF8\n\n\n\nThey look to be caused by FingerprintFilter being wrapped in a ConditionalTokenStream, which I don't think makes any sense?  So the simplest solution is probably to blacklist it. ",
            "date": "2018-05-11T13:28:38+0000"
        },
        {
            "id": "comment-16471991",
            "author": "ASF subversion and git services",
            "content": "Commit aea073e029f0aa3dd32acb41ba0209c12da9725c in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aea073e ]\n\nLUCENE-8273: Fix end() propagation ",
            "date": "2018-05-11T14:08:11+0000"
        },
        {
            "id": "comment-16471992",
            "author": "ASF subversion and git services",
            "content": "Commit 2225d0e464eeeabcec682edcabd51b136ac1aee2 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2225d0e ]\n\nLUCENE-8273: Fix end() propagation ",
            "date": "2018-05-11T14:08:13+0000"
        },
        {
            "id": "comment-16471993",
            "author": "Alan Woodward",
            "content": "Turns out it was an error in end() propagation, now fixed.  I'll keep an eye out for further failures. ",
            "date": "2018-05-11T14:08:38+0000"
        },
        {
            "id": "comment-16472184",
            "author": "Robert Muir",
            "content": "Thanks for debugging the failure: TestRandomChains is cruel but it works. Should we open a followup issue to clean up the analyzers and stuff? This is something i can help with. ",
            "date": "2018-05-11T16:11:33+0000"
        },
        {
            "id": "comment-16472206",
            "author": "Alan Woodward",
            "content": "Thanks for debugging the failure\n\nThere will be more, I'm sure!\n\nShould we open a followup issue to clean up the analyzers and stuff?\n\n+1 ",
            "date": "2018-05-11T16:25:01+0000"
        },
        {
            "id": "comment-16472235",
            "author": "Robert Muir",
            "content": "I opened LUCENE-8308, I think we have to work our way through that one first. Thanks for the work here, great improvement. ",
            "date": "2018-05-11T16:39:27+0000"
        },
        {
            "id": "comment-16472342",
            "author": "Mike Sokolov",
            "content": "Yes, thanks Alan Woodward this is a great addition ",
            "date": "2018-05-11T17:44:55+0000"
        },
        {
            "id": "comment-16472594",
            "author": "Michael McCandless",
            "content": "+1, very cool addition to Lucene! ",
            "date": "2018-05-11T20:25:40+0000"
        },
        {
            "id": "comment-16472709",
            "author": "Alan Woodward",
            "content": "Some more failures in testRandomChains:\nhttps://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/1885/\nhttps://jenkins.thetaphi.de/job/Lucene-Solr-master-Solaris/1857/\n\nI'm going to AwaitsFix it for the weekend, and look at it again on Monday. ",
            "date": "2018-05-11T21:45:26+0000"
        },
        {
            "id": "comment-16472733",
            "author": "Robert Muir",
            "content": "maybe ConditionalTokenFilter's toString() can be improved which will help in debugging chains like that. ",
            "date": "2018-05-11T22:04:06+0000"
        },
        {
            "id": "comment-16472873",
            "author": "Steve Rowe",
            "content": "I've belatedly reviewed the changes committed under this issue - awesome additions!\n\nI noticed a few small things that could be improved, and in putting together a patch I found what I think is a bug/omission in the design: TermExclusionFilterFactory can't be used outside of CustomAnalyzer because ConditionalTokenFilterFactory.inform() expects setInnerFilters() to have already been called, but TermExclusionFilterFactory doesn't have the ability to do that. As a result, the protected word list can't be loaded. AFAICT this will prevent TermExclusionFilterFactory from being used in Solr, e.g.\n\nMy idea to address the problem is to allow TermExclusionFilterFactory to accept (but not require) as args wrapped filters and those filters' args, and then call setInnerFilters() from the ctor. I've added a factory test suite that appears to validate the idea. See javadocs in the patch for interface details.\n\nBecause of the way I produced it, the patch (forthcoming) includes several other changes, some of which may be controversial. I'd be happy to factor out any of these changes if you don't agree with all of them, Alan Woodward. The other changes, in no particular order:\n\n\tI think TermExclusionFilter should be renamed to ProtectedTermFilter (or similar) - forcing devs&users to deal with both \"protected\" and \"excluded\" seems pointless to me; it should be either one or the other everywhere.\n\tProtectedTermFilter ctor should require() the protected terms file param rather than get() it, since it is in fact required.\n\tAdded TestConditionalTokenFilter.testMultipleConditionalFilters()\n\tThe boolean sense of ConditionalTokenFilter.shouldFilter() is backward in the class javadocs: it says that shouldFilter:false will execute the wrapped filters, but AFAICT it's the opposite.\n\tSome other minor javadoc improvements.\n\n\n\nIf the approach is okay, there is a little bit more work to do: some additional testing (more tests for graceful handling of bad input, and a Solr test), and adding Solr ref guide doc. \u00a0I haven't run precommit or all tests yet, so there may be some outstanding problems. ",
            "date": "2018-05-12T01:55:00+0000"
        },
        {
            "id": "comment-16473985",
            "author": "Alan Woodward",
            "content": "Thanks Steve, your patch looks great. ",
            "date": "2018-05-14T09:54:43+0000"
        },
        {
            "id": "comment-16474120",
            "author": "Steve Rowe",
            "content": "Okay, I'll finish up the remaining work today. ",
            "date": "2018-05-14T12:37:50+0000"
        },
        {
            "id": "comment-16475166",
            "author": "Steve Rowe",
            "content": "I stumbled on what looks like a ProtectedTermFilter bug when a wrapped filter is a filtering token filter, and the content to be analyzed contains at least one non-protected term prior to a protected term; in this case protection fails:\n\nTestProtectedTermFilter.java\n  public void testWrappedFilteringTokenFilter() throws IOException {\n    CharArraySet protectedTerms = new CharArraySet(5, true);\n    protectedTerms.add(\"foobar\");\n    TokenStream stream = whitespaceMockTokenizer(\"foobar abc\");\n    stream = new ProtectedTermFilter(protectedTerms, stream, in -> new LengthFilter(in, 1, 4));\n    assertTokenStreamContents(stream, new String[]{ \"foobar\", \"abc\" }); // succeeds\n\n    stream = whitespaceMockTokenizer(\"wuthering foobar abc\");\n    stream = new ProtectedTermFilter(protectedTerms, stream, in -> new LengthFilter(in, 1, 4));\n    assertTokenStreamContents(stream, new String[]{ \"foobar\", \"abc\" }); // fails @ term 0: Actual: abc\n  }\n\n\n\nI haven't yet figured out what the problem is.  Alan, do you understand what's happening here? ",
            "date": "2018-05-15T01:49:55+0000"
        },
        {
            "id": "comment-16477116",
            "author": "Alan Woodward",
            "content": "There's a bug in the way that tokens are buffered if the wrapped TokenFilter needs to read ahead.  I'm working on a fix for that now.\n\nTestRandomChains has found quite a few problems with this, I'm tempted to back it out and work on a branch for a while as it's clearly not ready for release yet. ",
            "date": "2018-05-16T09:04:36+0000"
        },
        {
            "id": "comment-16477590",
            "author": "Steve Rowe",
            "content": "Attaching an updated version of my part2 patch.  Changes:\n\n\tAdded a Solr test and ref guide text for ProtectedTermFilterFactory\n\tMoved the failing read-ahead test over to TestConditionalFilter\n\tIn TestConditionalFilter, converted most CannedTokenStream's to MockTokenizer's, which causes error IllegalStateException: end() called in wrong state=END! - I'm guessing you already know about this and are working on it\n\tExcept for the end()-related failures, tests succeed\n\tPrecommit succeeds\n\n ",
            "date": "2018-05-16T15:41:06+0000"
        },
        {
            "id": "comment-16477910",
            "author": "Alan Woodward",
            "content": "Every time I think I have this fixed, TestRandomChains finds another failure...  I'm attaching my latest patch, which includes the failing seed.  Steve Rowe, can you rebase on top of this? ",
            "date": "2018-05-16T18:56:23+0000"
        },
        {
            "id": "comment-16478105",
            "author": "Steve Rowe",
            "content": "Every time I think I have this fixed, TestRandomChains finds another failure... I'm attaching my latest patch, which includes the failing seed. Steve Rowe, can you rebase on top of this?\n\nSure, will do. ",
            "date": "2018-05-16T21:11:54+0000"
        },
        {
            "id": "comment-16478177",
            "author": "Steve Rowe",
            "content": "I'm attaching my latest patch, which includes the failing seed. Steve Rowe, can you rebase on top of this?\n\nDone: LUCENE-8273-part2-rebased.patch.  (I haven't run tests or precommit though.) ",
            "date": "2018-05-16T21:58:44+0000"
        },
        {
            "id": "comment-16478208",
            "author": "Robert Muir",
            "content": "\nIn TestConditionalFilter, converted most CannedTokenStream's to MockTokenizer's, which causes error IllegalStateException: end() called in wrong state=END! - I'm guessing you already know about this and are working on it\n\nGood approach, TestRandomChains is rather inefficient (basically an integration test) and its best to always make the fails reproduce with simpler unit tests. ",
            "date": "2018-05-16T22:33:20+0000"
        },
        {
            "id": "comment-16480549",
            "author": "Alan Woodward",
            "content": "I think I have now chased down the last failures, all around end() propagation and how to deal with position increments when FilteredTermFilter is skipped.  Attached is a patch that includes Steve Rowe's test improvements.  I'll commit this now, and un-awaitsfix TestRandomChains and see if that throws out any new bugs in the next while. ",
            "date": "2018-05-18T11:59:21+0000"
        },
        {
            "id": "comment-16480559",
            "author": "Robert Muir",
            "content": "is the TokenBuffer class in the patch actually used? ",
            "date": "2018-05-18T12:04:09+0000"
        },
        {
            "id": "comment-16480574",
            "author": "Alan Woodward",
            "content": "Nope, that's a relic from a failed attempt to fix something, and precommit has just pulled me up on it not having any javadocs \u00a0Will nuke it before I commit. ",
            "date": "2018-05-18T12:14:50+0000"
        },
        {
            "id": "comment-16480594",
            "author": "ASF subversion and git services",
            "content": "Commit 443bd2ac526a40079e9028a5e7d8ad2f24ca6a4b in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=443bd2a ]\n\nLUCENE-8273: Fix end() and posInc handling ",
            "date": "2018-05-18T12:30:01+0000"
        },
        {
            "id": "comment-16480595",
            "author": "ASF subversion and git services",
            "content": "Commit b1ee23c525a64017242148bca43111168fe1be3a in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b1ee23c ]\n\nLUCENE-8273: Fix end() and posInc handling ",
            "date": "2018-05-18T12:30:03+0000"
        },
        {
            "id": "comment-16480698",
            "author": "Steve Rowe",
            "content": "Attached is a patch that includes Steve Rowe's test improvements. \n\nWould you like me to make a new patch from the other stuff in my patch? ",
            "date": "2018-05-18T14:06:21+0000"
        },
        {
            "id": "comment-16480712",
            "author": "Alan Woodward",
            "content": "Would you like me to make a new patch from the other stuff in my patch?\n\nYes please!  Feel free to commit it if you think it's ready, I wasn't sure if you still wanted to add more testing or docs. ",
            "date": "2018-05-18T14:17:03+0000"
        },
        {
            "id": "comment-16480717",
            "author": "Steve Rowe",
            "content": "\nWould you like me to make a new patch from the other stuff in my patch?\n\nYes please! Feel free to commit it if you think it's ready, I wasn't sure if you still wanted to add more testing or docs.\nOK, will do. Yeah, I think it's ready to go, but I'll re-run precommit and tests before I commit. ",
            "date": "2018-05-18T14:20:40+0000"
        },
        {
            "id": "comment-16482079",
            "author": "Steve Rowe",
            "content": "Attached remaining parts from my -rebased patch. \u00a0Tests and precommit pass. Committing shortly. ",
            "date": "2018-05-20T23:39:56+0000"
        },
        {
            "id": "comment-16482084",
            "author": "ASF subversion and git services",
            "content": "Commit 97490ac7c66313625c9526a466ab96981e249746 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=97490ac ]\n\nLUCENE-8273: Rename TermExclusionFilter -> ProtectedTermFilter.  Allow ProtectedTermFilterFactory to be used outside of CustomAnalyzer, including in Solr, by allowing wrapped filters and their parameters to be specified on construction.  Add tests for ProtectedTermFilterFactory in lucene/common/analysis/ and in solr/core/.  Add Solr ref guide documentation for ProtectedTermFilterFactory.  Improve javadocs for CustomAnalyzer, ConditionalTokenFilter, and ProtectedTermFilter. ",
            "date": "2018-05-20T23:52:54+0000"
        },
        {
            "id": "comment-16482085",
            "author": "ASF subversion and git services",
            "content": "Commit d91273ddf0b524e14ebcc2a90bbedd8d9ae319d4 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d91273d ]\n\nLUCENE-8273: Rename TermExclusionFilter -> ProtectedTermFilter.  Allow ProtectedTermFilterFactory to be used outside of CustomAnalyzer, including in Solr, by allowing wrapped filters and their parameters to be specified on construction.  Add tests for ProtectedTermFilterFactory in lucene/common/analysis/ and in solr/core/.  Add Solr ref guide documentation for ProtectedTermFilterFactory.  Improve javadocs for CustomAnalyzer, ConditionalTokenFilter, and ProtectedTermFilter. ",
            "date": "2018-05-20T23:52:56+0000"
        },
        {
            "id": "comment-16482582",
            "author": "ASF subversion and git services",
            "content": "Commit a69321a4d05d30f06248d0a33a237d8978942a9f in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a69321a ]\n\nLUCENE-8273: TestRandomChains found some more end() handling problems ",
            "date": "2018-05-21T15:00:52+0000"
        },
        {
            "id": "comment-16482583",
            "author": "ASF subversion and git services",
            "content": "Commit 0c0fce3e98c9a01c330329eca5153fb78c7decaf in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0c0fce3 ]\n\nLUCENE-8273: TestRandomChains found some more end() handling problems ",
            "date": "2018-05-21T15:00:54+0000"
        },
        {
            "id": "comment-16483624",
            "author": "ASF subversion and git services",
            "content": "Commit 0934e2a998ac43e46594e049daab751d8cae2476 in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0934e2a ]\n\nLUCENE-8273: Don't wrap MinHashFilter in a condition\n\nMinHashFilter needs to consume the entire tokenstream, so wrapping it in a\nrandomized condition makes no sense, and breaks offsets. ",
            "date": "2018-05-22T08:09:55+0000"
        },
        {
            "id": "comment-16483625",
            "author": "ASF subversion and git services",
            "content": "Commit 24c186eff9a9b2b2c0a86fc0a828bd81ba0993e8 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=24c186e ]\n\nLUCENE-8273: Don't wrap MinHashFilter in a condition\n\nMinHashFilter needs to consume the entire tokenstream, so wrapping it in a\nrandomized condition makes no sense, and breaks offsets. ",
            "date": "2018-05-22T08:09:57+0000"
        },
        {
            "id": "comment-16486886",
            "author": "Alan Woodward",
            "content": "The elastic CI has found some reproducing seeds in TestRandomChains that look like the following:\n\nSuite: org.apache.lucene.analysis.core.TestRandomChains\n01:47:39    [junit4]   2> Exception from random analyzer: \n01:47:39    [junit4]   2> charfilters=\n01:47:39    [junit4]   2>   org.apache.lucene.analysis.fa.PersianCharFilter(java.io.StringReader@36de1051)\n01:47:39    [junit4]   2>   org.apache.lucene.analysis.charfilter.MappingCharFilter(org.apache.lucene.analysis.charfilter.NormalizeCharMap@31483c67, org.apache.lucene.analysis.fa.PersianCharFilter@51a9d324)\n01:47:39    [junit4]   2> tokenizer=\n01:47:39    [junit4]   2>   org.apache.lucene.analysis.core.UnicodeWhitespaceTokenizer(org.apache.lucene.util.AttributeFactory$1@27232fb3, 35)\n01:47:39    [junit4]   2> filters=ConditionalTokenFilter: \n01:47:39    [junit4]   2>   org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter(OneTimeWrapper@5f621e45 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1, org.apache.lucene.analysis.compound.hyphenation.HyphenationTree@40cdd67e)ConditionalTokenFilter: \n01:47:39    [junit4]   2>   org.apache.lucene.analysis.in.IndicNormalizationFilter(OneTimeWrapper@2de2e47c term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1)ConditionalTokenFilter: \n01:47:39    [junit4]   2>   org.apache.lucene.analysis.MockRandomLookaheadTokenFilter(java.util.Random@4ced13ac, OneTimeWrapper@7d30a80d term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1)\n01:47:39    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=72E157E8E16C0F79 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=en-US -Dtests.timezone=America/Anguilla -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n01:47:39    [junit4] FAILURE 0.57s J0 | TestRandomChains.testRandomChainsWithLargeStrings <<<\n01:47:39    [junit4]    > Throwable #1: java.lang.AssertionError\n01:47:39    [junit4]    > \tat __randomizedtesting.SeedInfo.seed([72E157E8E16C0F79:18BAE8F9B8222F8A]:0)\n01:47:39    [junit4]    > \tat org.apache.lucene.analysis.LookaheadTokenFilter.peekToken(LookaheadTokenFilter.java:140)\n\n\n\nThe root cause is that LookaheadTokenFilter doesn't play well with ConditionalTokenFilter when we have stacked tokens:\n\n\tCTF works by presenting the underlying TokenStream to its wrapped filter as a series of snippets, demarcated by tokens that don't pass the shouldFilter() test.  When a new snippet is started (i.e. when a token that passes shouldFilter() appears after one that doesn't) then reset() is called on the delegate, and when it stops (i.e. when a token that doesn't pass shouldFilter() appears) then end() is called.\n\tThis means that if we have stacked tokens, with the first not passing shouldFilter() and the second passing it, the wrapped filter can see a TokenStream that has an initial position increment of 0\n\tLookaheadTokenFilter has an explicit assertion that checks we don't have an initial posInc of 0\n\n\n\nI think this can be fixed by having a posInc adjustment when we're delegating, so that the delegated snippet starts with a posInc of 1, but this is then adjusted downwards by the CTF before it's emitted. ",
            "date": "2018-05-23T08:14:24+0000"
        },
        {
            "id": "comment-16489903",
            "author": "ASF subversion and git services",
            "content": "Commit 23ab3d19ab0239bae4cc0acdbc01a8b65c637d68 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=23ab3d1 ]\n\nLUCENE-8273: Move test resources to where they belong ",
            "date": "2018-05-24T22:14:43+0000"
        },
        {
            "id": "comment-16489904",
            "author": "ASF subversion and git services",
            "content": "Commit 2f38342687f1af3a092f1fbfa183a09a55490cb3 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2f38342 ]\n\nLUCENE-8273: Move test resources to where they belong ",
            "date": "2018-05-24T22:14:45+0000"
        },
        {
            "id": "comment-16491222",
            "author": "Steve Rowe",
            "content": "Not sure if this is the same sort of problem you already know about, Alan Woodward, but my Jenkins found a reproducing TestRandomChains master seed for a chain including ConditionalTokenFilter:\n\n\nChecking out Revision 18ad8d137afa8e2017f4121ddced4d630b1c86a1 (refs/remotes/origin/master)\n[...]\n   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains\n   [junit4]   2> TEST FAIL: useCharFilter=false text='\\u0bd2\\u0ba7\\u0bdc\\u0bf5\\u0b96\\u0ba5 qrhnhrlyeh \\u6d41\\u0601\\u033a\\ue4e1 bvmoocaycg dtazdd fn]|b({0,5  vjsrn'\n   [junit4]   2> Exception from random analyzer: \n   [junit4]   2> charfilters=\n   [junit4]   2> tokenizer=\n   [junit4]   2>   org.apache.lucene.analysis.standard.ClassicTokenizer()\n   [junit4]   2> filters=ConditionalTokenFilter: \n   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.TypeAsSynonymFilter(OneTimeWrapper@588fa3b0 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1, tzqrpnf)\n   [junit4]   2>   org.apache.lucene.analysis.no.NorwegianLightStemFilter(ValidatingTokenFilter@63bdcc8b term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false)ConditionalTokenFilter: \n   [junit4]   2>   org.apache.lucene.analysis.core.TypeTokenFilter(OneTimeWrapper@28da9033 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,keyword=false, [<EMOJI>, <NUM>, <HANGUL>], true)\n   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=735F260A3B6964C3 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=ca -Dtests.timezone=Europe/Isle_of_Man -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   39.0s J2 | TestRandomChains.testRandomChainsWithLargeStrings <<<\n   [junit4]    > Throwable #1: java.lang.IllegalStateException: last stage: inconsistent startOffset at pos=6: 48 vs 53; token=tzqrpnf<ALPHANUM>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([735F260A3B6964C3:1904991B62274430]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:106)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:746)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:657)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:559)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestRandomChains.testRandomChainsWithLargeStrings(TestRandomChains.java:882)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/analysis/common/test/J2/temp/lucene.analysis.core.TestRandomChains_735F260A3B6964C3-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {dummy=PostingsFormat(name=LuceneVarGapDocFreqInterval)}, docValues:{}, maxPointsInLeafNode=1103, maxMBSortInHeap=7.435649827990908, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@5743798e), locale=ca, timezone=Europe/Isle_of_Man\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_151 (64-bit)/cpus=16,threads=1,free=293435744,total=411566080\n\n ",
            "date": "2018-05-25T20:18:08+0000"
        },
        {
            "id": "comment-16493674",
            "author": "ASF subversion and git services",
            "content": "Commit 4ea9d2ea8cbb036bac6aa1e61161afc65d04a1be in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4ea9d2e ]\n\nLUCENE-8273: Adjust position increments when filtering stacked tokens ",
            "date": "2018-05-29T15:11:39+0000"
        },
        {
            "id": "comment-16493678",
            "author": "ASF subversion and git services",
            "content": "Commit 8577aee08f8cf348a674dfbcc7b408358380d260 in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8577aee ]\n\nLUCENE-8273: Adjust position increments when filtering stacked tokens ",
            "date": "2018-05-29T15:12:55+0000"
        },
        {
            "id": "comment-16504369",
            "author": "Alan Woodward",
            "content": "A couple more failing seeds:\n\nSuite: org.apache.lucene.analysis.core.TestRandomChains\n07:41:21    [junit4]   2> TEST FAIL: useCharFilter=true text='t \\u0af5\\u0a9f\\u0acb\\u0ada\\u0aa6 \\u0011\\u02eb^ q hnhpwei txslx  e \\u22c8\\u22d9 \\u2e06\\u2e15\\u2e6a\\u2e05 uv im i \\u1387\\u1391\\u1398\\u1386\\u138c  j'\n07:41:21    [junit4]   2> Exception from random analyzer: \n07:41:21    [junit4]   2> charfilters=\n07:41:21    [junit4]   2>   org.apache.lucene.analysis.MockCharFilter(java.io.StringReader@495f18ce)\n07:41:21    [junit4]   2> tokenizer=\n07:41:21    [junit4]   2>   org.apache.lucene.analysis.core.UnicodeWhitespaceTokenizer(org.apache.lucene.util.AttributeFactory$1@37e10425)\n07:41:21    [junit4]   2> filters=ConditionalTokenFilter: \n07:41:21    [junit4]   2>   org.apache.lucene.analysis.MockGraphTokenFilter(java.util.Random@4d3a58d5, OneTimeWrapper@7c12a0d4 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1)ConditionalTokenFilter: \n07:41:21    [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(OneTimeWrapper@75d26c0e term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1, <KATAKANA>)\n07:41:21    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=A61F0C126076A16B -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=ar-TN -Dtests.timezone=US/Arizona -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n07:41:21    [junit4] ERROR   0.55s J2 | TestRandomChains.testRandomChainsWithLargeStrings <<<\n07:41:21    [junit4]    > Throwable #1: java.lang.IllegalStateException: last stage: inconsistent endOffset at pos=10: 41 vs 55; token=L\u5bc2,\uee14\u91df\ue767'\u0187\u2a04{\ue13c\ued72\ud802\udda0\u058d\n\n\n\nSuite: org.apache.lucene.analysis.core.TestRandomChains\n15:11:36    [junit4]   2> TEST FAIL: useCharFilter=true text='\\u2d8e\\u2dbb\\u2daf\\u2d8b\\u2d97\\u2dd5\\u2d97\\u2dcc \\u035b\\u5996\\u07ca\\u0003\\u12e3\\u6450\\uf36f '\n15:11:36    [junit4]   2> Exception from random analyzer: \n15:11:36    [junit4]   2> charfilters=\n15:11:36    [junit4]   2> tokenizer=\n15:11:36    [junit4]   2>   org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer()\n15:11:36    [junit4]   2> filters=\n15:11:36    [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(ValidatingTokenFilter@3f23768a term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1, lbay)ConditionalTokenFilter: \n15:11:36    [junit4]   2>   org.apache.lucene.analysis.SimplePayloadFilter(OneTimeWrapper@114147eb term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,payload=null)ConditionalTokenFilter: \n15:11:36    [junit4]   2>   org.apache.lucene.analysis.shingle.ShingleFilter(OneTimeWrapper@680fa1a5 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1,payload=null, 7)\n15:11:36    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=49B6EB935C8F8B35 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=nl-NL -Dtests.timezone=BST -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n15:11:36    [junit4] ERROR   0.05s J2 | TestRandomChains.testRandomChains <<<\n15:11:36    [junit4]    > Throwable #1: java.lang.IllegalStateException: last stage: inconsistent endOffset at pos=9: 12 vs 14; token=\u07ca \u12e3\u12e3\n\n\nI'm looking at these now ",
            "date": "2018-06-07T07:58:36+0000"
        },
        {
            "id": "comment-16504540",
            "author": "Alan Woodward",
            "content": "Both of these failures are due to ShingleFilter not properly handling graphs.  Without being wrapped in a condition, the ShingleFilter is mangling its input graph, but it's doing it in a consistent way, so the ValidatingTokenFilter is happy.  However, if it's randomly turned off, then occasionally the ValidatingTokenFilter gets the plain input graph as opposed to the mangled one, and so it complains because offsets are no longer consistent.\n\nI'm not quite sure how best to fix this.  Ideally, we'd just fix ShingleFilter, but that's not as simple as it sounds.  Perhaps the simplest thing to do is to add ShingleFilter to the blacklist, and document that ConditionalTokenFilter won't work with broken graph inputs?\n ",
            "date": "2018-06-07T11:14:51+0000"
        },
        {
            "id": "comment-16504554",
            "author": "Robert Muir",
            "content": "If shinglefilter is the buggy one it should be banned from the test for sure, and a issue opened for its bugginess.\n\nIts not a test coverage concern: this can't replace unit tests: it exists to find new buggy interactions between the analysis components, like this one here. ",
            "date": "2018-06-07T11:40:57+0000"
        },
        {
            "id": "comment-16504560",
            "author": "Alan Woodward",
            "content": "Yeah, it's the same issue as LUCENE-4170.  I'll add it to the blacklist. ",
            "date": "2018-06-07T11:48:55+0000"
        },
        {
            "id": "comment-16504575",
            "author": "ASF subversion and git services",
            "content": "Commit dfb679a93cc9aae2620e7ce110c9515181bd17c6 in lucene-solr's branch refs/heads/branch_7x from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=dfb679a ]\n\nLUCENE-8273: Don't wrap ShingleFilter in conditions in testRandomChains ",
            "date": "2018-06-07T12:03:21+0000"
        },
        {
            "id": "comment-16504576",
            "author": "ASF subversion and git services",
            "content": "Commit a4fa16896225e08b72bf64fba97a216bb6a83fbb in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4fa168 ]\n\nLUCENE-8273: Don't wrap ShingleFilter in conditions in testRandomChains ",
            "date": "2018-06-07T12:03:23+0000"
        },
        {
            "id": "comment-16504577",
            "author": "Alan Woodward",
            "content": "I'll let RandomChains stew for another 24 hours, but I think this is ready to be resolved for the upcoming 7.4 release. ",
            "date": "2018-06-07T12:04:20+0000"
        }
    ]
}