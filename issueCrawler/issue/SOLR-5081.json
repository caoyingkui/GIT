{
    "id": "SOLR-5081",
    "title": "Highly parallel document insertion hangs SolrCloud",
    "details": {
        "affect_versions": "4.3.1",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "If I do a highly parallel document load using a Hadoop cluster into an 18 node solrcloud cluster, I can deadlock solr every time.\n\nThe ulimits on the nodes are:\ncore file size          (blocks, -c) 0\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 1031181\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 32768\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 515590\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n\nThe open file count is only around 4000 when this happens.\n\nIf I bounce all the servers, things start working again, which makes me think this is Solr and not ZK.\n\nI'll attach the stack trace from one of the servers.",
    "attachments": {
        "threads.txt": "https://issues.apache.org/jira/secure/attachment/12594560/threads.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mike Schrag",
            "id": "comment-13721749",
            "date": "2013-07-27T20:42:40+0000",
            "content": "stack dump of one of the nodes while hung "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13721756",
            "date": "2013-07-27T21:15:47+0000",
            "content": "Can you do a jstack on one or more of your Solr servers? There's some distributed deadlock possibility and it would be good to see if this is the same problem. "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13721761",
            "date": "2013-07-27T21:22:41+0000",
            "content": "I attached a jstack of one of them (threads.txt) during this event. Do you want me to reproduce and grab more? "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13721764",
            "date": "2013-07-27T21:26:32+0000",
            "content": "btw, I dropped the hadoop cluster to doing single-record batches in the run corresponding to that stack dump. I saw a note somewhere (I think from you?) that suggested increasing the semaphore permits, which I was about to test, too. It's not clear what a reasonable value is, but I jacked it up from *16 to *1024 and figured I'd go for broke  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13721767",
            "date": "2013-07-27T21:33:55+0000",
            "content": "You really want to go for broke? Try SOLR-4816 (note assuming you're indexing from SolrJ). The deadlock I've seen has to do with intra-shard routing, essentially forwarding the packets to other shards, if there are enough packets, can lead to this situation. That JIRA is about having SolrJ just send the documents to the right leader so it will not have to route the docs to other shards. We'd be really interested to see if that worked in the real world...\n\nNOTE: I'm not sure what the current state of that patch is, I think it was ready to rock-n-roll but just missed the cut for 4.4.\n "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13721780",
            "date": "2013-07-27T22:28:46+0000",
            "content": "No luck   Whatever this hang is doesn't appear to be the same as that. "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13721781",
            "date": "2013-07-27T22:29:05+0000",
            "content": "(that's with the latest SOLR-4816 patch applied) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13723378",
            "date": "2013-07-30T04:18:26+0000",
            "content": "Can you please throw some more light into the system\n\n\n\tnumShards\n\tReplication factor\n\tmaxShardsPerNode (I guess it is 1)\n\tAverage size per doc\n\tVM startup params (-Xmx -Xms, GC params etc)\n\tHow are you indexing? Are you using SolrJ and the CloudSolrServer? How many clients are used to index the data?\n\n "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13723402",
            "date": "2013-07-30T04:57:36+0000",
            "content": "1. numShards=20\n2. RF=3\n3. maxShardsPerNode=1000 (aka \"just a big number\" .. we overcommit shards in this environment)\n4. not very big ... maybe 0.5-1k\n5. -Xms10g -Xmx10g -XX:MaxPermSize=1G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:CMSInitiatingOccupancy\nFraction=60 -XX:-OmitStackTraceInFastThrow\n6. SolrJ + CloudSolrServer + when you say clients, do you mean threads, or actual client JVM instances? Talking more generically in terms of threads, I know it works at around 15-20 threads, but 100 threads makes it go sadfaced "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13723853",
            "date": "2013-07-30T13:32:58+0000",
            "content": "This is likely the same issue that has come up before - and it has nothing to do with cloudsolrserver - its more likely how we limit the number of threads that are used to forward on updates- and the nodes can talk back and forth to each other, run out of threads, and deadlock. It's similar to the distrib deadlock issue. It's been a known issue for many months, just have not had a chance to look into it closely yet. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13723856",
            "date": "2013-07-30T13:36:46+0000",
            "content": "Agreed, although we should be able to see the deadlock on the semaphore that we saw before in SolrCmdDistributor in here somewhere, and it's not in the stack trace we've seen so far. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13723878",
            "date": "2013-07-30T14:04:23+0000",
            "content": "the stack trace we've seen so far.\n\nThose traces are suspect for the problem described I think. Regardless, for this type of thing, it would be great to get the traces from a couple machines rather than just one. "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13723996",
            "date": "2013-07-30T16:03:06+0000",
            "content": "I'll kill it again today and grab traces from a few of the nodes. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13726746",
            "date": "2013-08-01T18:59:39+0000",
            "content": "Mike Schrag COuld you get any more thread dumps? "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13726801",
            "date": "2013-08-01T20:03:26+0000",
            "content": "I grabbed more and they all look basically the same as the attached, which is to say, it sort of looks like Solr isn't doing ANYTHING. I'm going to look into whether I'm crushing ZooKeeper, and maybe my requests aren't even getting to Solr. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13726831",
            "date": "2013-08-01T20:24:42+0000",
            "content": "Yeah, that is odd. The stack traces you sent basically showed no deadlocks, nothing interesting at all. I suspect pursuing whether anything is getting to Solr or not is a good idea....\n\nHmmmm, blunt-instrument test when the cluster is hung. What happens if you, say, submit a query directly to one of the nodes? Does it respond or do you see anything in the solr log on that node? Tip: adding &distrib=false to the query will not try to send sub-queries to other shards.\n\nAnd I wonder what happens if you, say, use post.jar (comes with the example) to try to send a doc to Solr when it's hung, anything?\n\nClearly I'm grasping at straws here, but I'm kind of out of good ideas. "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13726848",
            "date": "2013-08-01T20:32:51+0000",
            "content": "I actually did this exact test when I was in this state originally, and the insert worked, which totally confused the situation for me. However, in light of seeing nothing in the traces, it supports the theory that the cluster isn't hung, but rather I'm somehow not even getting that far in the Hadoop cluster. ZK was my best guess as something that maybe could be an earlier stage failure, but even that I would expect to have hang the test-insert. So I need to do a little more forensics here and see if I can get a better picture of wtf is going on. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13730269",
            "date": "2013-08-06T02:16:23+0000",
            "content": "I actually did this exact test when I was in this state originally, and the insert worked, which totally confused the situation for me. \n\nok ... hold up ... basically what you're saying is \"the first time i saw this problem (solrcloud \"hangs\" and is \"deadlocked\" under heavy document insertion load) i tried to insert a single document and it worked.\"\n\n...which makes no sense to me because if that's the case, then what exactly do you mean by \"hangs\" and \"deadlocked\" ? \n\nSo let's back up: \n\n\n\twhat do you observe about your system that leads you to believe there is a problem?\n\twhat aspect of your observations doesn't match what you expect?\n\twhat do you expect to observe?\n\thow are you making these observations?\n\n\n\nWild shot in the dark: what does your indexng code look like?  is it possible that your indexing code is encountering some \"deadlock\" of it's own, independent of anything happening in solr?  If you are using solrj, can you get thread dumps from your indexing client apps when you observe this \"deadlock\" sitaution (again: this info is useless unless we have a better understanding of what exactly you are observing that you think indicates a problem)\n\n\n "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13730995",
            "date": "2013-08-06T17:32:29+0000",
            "content": "I have this problem too, but in my case, Solr hangs and I can done more insertions without restart the nodes.\n\nI do the insertion using a culr post in json format like:\n\ncurl http://127.0.0.1:8983/solr/collection/update --data-binary @data -H 'Content-type:application/json' "
        },
        {
            "author": "Kevin Osborn",
            "id": "comment-13748047",
            "date": "2013-08-22T23:20:01+0000",
            "content": "I may have this issue as well. I am posting batches of 1000 through SolrJ. I have autoCommit set to 15000 with openSearcher=false. autoSoftCommit is set to 30000. During my initial testing, I was able to recreate it after just a couple updates. I then change the limit of the number of open files for the process from 4096 to 15000. This seemed to help, but only to a point.\n\nIf all my updates are at once, it seems to succeed. But if I have pauses between updates, it seems to have problems. I have also only seen this error when I have more than 1 node in my SolrCloud cluster.\n\nI also took a look at netstat. There seemed to be a lot of connections between my two nodes. Could the the frequency of my updates be overwhelming the connection from the leader to the replica?\n\nDeletes also fail, but queries still seem to work.\n\nRestarting the nodes fixes the problem. "
        },
        {
            "author": "Mike Schrag",
            "id": "comment-13752699",
            "date": "2013-08-28T18:38:09+0000",
            "content": "I think we tracked this down on our side. We noticed when testing another part of the system that we had SYN flood warnings in the system logs. I believe the kernel was blocking traffic to the Solr port once it believed that Hadoop was attacking it. By turning off net.ipv4.tcp_syncookies and increasing the net.ipv4.tcp_max_syn_backlog, the problem seems to have gone away. This also explains why I was able to connect to Solr and insert still from another machine even when accessed died from the Hadoop cluster. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13752814",
            "date": "2013-08-28T20:31:32+0000",
            "content": "Mike:\n\nThanks for letting us know! This is a tricky one.... "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13759001",
            "date": "2013-09-05T11:35:37+0000",
            "content": "Mike identified this particular problem and a fix as being unrelated to Solr, so I'm closing it. In particular it's not related to the SolrCmdDistributor issue. "
        }
    ]
}