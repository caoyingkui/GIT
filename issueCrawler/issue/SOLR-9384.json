{
    "id": "SOLR-9384",
    "title": "Add randomization to the train Streaming Expression to support very large training sets",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "The train (SOLR-9252) Streaming Expression optimizes a logistic regression model on text.\n\nThe initial implementation instantiates a doc vector for each document in the training set on each iteration. The doc vectors are held in memory so, the size of the training set is limited by memory constraints.\n\nThis ticket will add randomization to the algorithm so that a random set of documents from the training set are processed on each iteration. \n\nThis will allow the train Streaming Expression to be run on much larger training sets.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-08-05T04:11:59+0000",
            "author": "Cao Manh Dat",
            "content": "Hi Joel,\n\nI think we should support both randomization & full training of large dataset. \nFor full training a large dataset we can split documents into batches ( for example : docId % batchId ) and run the train in sequence for each batch. So the number of TermEnum seeks will be equal to number of batches. ",
            "id": "comment-15408837"
        }
    ]
}