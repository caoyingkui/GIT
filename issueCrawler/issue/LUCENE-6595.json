{
    "id": "LUCENE-6595",
    "title": "CharFilter offsets correction is wonky",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Bug"
    },
    "description": "Spinoff from this original Elasticsearch issue: https://github.com/elastic/elasticsearch/issues/11726\n\nIf I make a MappingCharFilter with these mappings:\n\n\n  ( -> \n  ) -> \n\n\n\ni.e., just erase left and right paren, then tokenizing the string\n\"(F31)\" with e.g. WhitespaceTokenizer, produces a single token F31,\nwith start offset 1 (good).\n\nBut for its end offset, I would expect/want 4, but it produces 5\ntoday.\n\nThis can be easily explained given how the mapping works: each time a\nmapping rule matches, we update the cumulative offset difference,\nconceptually as an array like this (it's encoded more compactly):\n\n\n  Output offset: 0 1 2 3\n   Input offset: 1 2 3 5\n\n\n\nWhen the tokenizer produces F31, it assigns it startOffset=0 and\nendOffset=3 based on the characters it sees (F, 3, 1).  It then asks\nthe CharFilter to correct those offsets, mapping them backwards\nthrough the above arrays, which creates startOffset=1 (good) and\nendOffset=5 (bad).\n\nAt first, to fix this, I thought this is an \"off-by-1\" and when\ncorrecting the endOffset we really should return\n1+correct(outputEndOffset-1), which would return the correct value (4)\nhere.\n\nBut that's too naive, e.g. here's another example:\n\n\n  cccc -> cc\n\n\n\nIf I then tokenize cccc, today we produce the correct offsets (0, 4)\nbut if we do this \"off-by-1\" fix for endOffset, we would get the wrong\nendOffset (2).\n\nI'm not sure what to do here...",
    "attachments": {
        "Lucene-6595.pptx": "https://issues.apache.org/jira/secure/attachment/12744396/Lucene-6595.pptx",
        "LUCENE-6595.patch": "https://issues.apache.org/jira/secure/attachment/12740922/LUCENE-6595.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14595099",
            "author": "Cao Manh Dat",
            "date": "2015-06-21T16:14:22+0000",
            "content": "Initial patch (It not pass all the test but it solved above problem). I will continue working on this bug. "
        },
        {
            "id": "comment-14595169",
            "author": "David Smiley",
            "date": "2015-06-21T19:18:34+0000",
            "content": "Cao,\nMike's last words were \"I'm not sure what to do here...\"   Could you please describe how you fixed this?  And do you agree this issue is the same as LUCENE-5734 ? "
        },
        {
            "id": "comment-14595289",
            "author": "Cao Manh Dat",
            "date": "2015-06-22T01:45:51+0000",
            "content": "The root of problems is we mapping N -> 1 and then asking an inverse mapping 1 -> 1.\n\nCurrently CharFilter have two problems.\nProblem 1:\n\nInput :       A B C ) ) )\nOutput :      A B C\n\n\nWhen Tokenizer ask to correct offset of 3 (which is C in output). This offset related to offset 3 4 5 6 in the input. CharFilter will correct offset of C to 6 ( end of range ).\n\nSo why cccc -> cc have correct offset?\n\nInput :     c c c c\nOutput :    c c\n\n\nBecause offset 2 (which is the second c in output) related to offset 2 3 4 in the input. CharFilter will correct offset 2 to 4 (end of range, which is correct). \n\nThe different of two examples, In Ex1 : the replacement happen right in the correct point (at 3) and in Ex2 : the replacement happen before the correct point (at 0). So I store an inputOffsets[] which is the start for each replacements.\n\nProblem 2:\n\nInput :   A <space> ( C\nOutput :  A <space> C\n\n\nWhen Tokenizer ask to correct offset of 3 (which is C in output). This offset related to offset 3 4 in the input. CharFilter will correct offset of C to 4 (end of range, which is correct). But in this example the replacement also happen right in the correct point. So there is a difference between correct startOffset and endOffset. So I add correctEndOffset method in Tokenizer\n\nDavid Smiley I will look at LUCENE-5734 and try to fix that bug. "
        },
        {
            "id": "comment-14596742",
            "author": "Michael McCandless",
            "date": "2015-06-22T22:10:11+0000",
            "content": "Thanks Cao Manh Dat, I'll try to understand your proposed change.  But some tests seem to be failing with this patch, e.g.:\n\n\n   [junit4] Suite: org.apache.lucene.analysis.core.TestBugInSomething\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBugInSomething -Dtests.method=test -Dtests.seed=FD8C8301DD07CEFD -Dtests.locale=hr -Dtests.timezone=SystemV/PST8PDT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 0.00s J2 | TestBugInSomething.test <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: finalOffset expected:<16> but was:<20>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([FD8C8301DD07CEFD:75D8BCDB73FBA305]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:280)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:295)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:299)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:812)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:674)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:670)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestBugInSomething.test(TestBugInSomething.java:77)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4] IGNOR/A 0.01s J2 | TestBugInSomething.testUnicodeShinglesAndNgrams\n   [junit4]    > Assumption #1: 'slow' test group is disabled (@Slow())\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene53): {}, docValues:{}, sim=DefaultSimilarity, locale=hr, timezone=SystemV/PST8PDT\n   [junit4]   2> NOTE: Linux 3.13.0-46-generic amd64/Oracle Corporation 1.8.0_40 (64-bit)/cpus=8,threads=1,free=370188896,total=519569408\n\n\n\nand\n\n\n   [junit4] Suite: org.apache.lucene.analysis.charfilter.TestHTMLStripCharFilterFactory\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestHTMLStripCharFilterFactory -Dtests.method=testSingleEscapedTag -Dtests.seed=FD8C8301DD07CEFD -Dtests.locale=lt_LT -Dtests.timezone=America/Thule -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.00s J3 | TestHTMLStripCharFilterFactory.testSingleEscapedTag <<<\n   [junit4]    > Throwable #1: java.lang.NullPointerException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([FD8C8301DD07CEFD:36A72464D080D0F1]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.charfilter.BaseCharFilter.correctEnd(BaseCharFilter.java:82)\n   [junit4]    > \tat org.apache.lucene.analysis.CharFilter.correctEndOffset(CharFilter.java:93)\n   [junit4]    > \tat org.apache.lucene.analysis.Tokenizer.correctEndOffset(Tokenizer.java:84)\n   [junit4]    > \tat org.apache.lucene.analysis.MockTokenizer.incrementToken(MockTokenizer.java:176)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:177)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:295)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:299)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:303)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:327)\n   [junit4]    > \tat org.apache.lucene.analysis.charfilter.TestHTMLStripCharFilterFactory.testSingleEscapedTag(TestHTMLStripCharFilterFactory.java:99)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n\n "
        },
        {
            "id": "comment-14597883",
            "author": "Cao Manh Dat",
            "date": "2015-06-23T16:19:13+0000",
            "content": "Thanks Michael McCandless. \nI quite confuse about finalOffset of Tokenizer. For example\n\nInput : ABC))) \nOutput : ABC\n\n\nThe end offset of last term is 3. So finalOffset should be 3 or 6? "
        },
        {
            "id": "comment-14597912",
            "author": "Cao Manh Dat",
            "date": "2015-06-23T16:38:46+0000",
            "content": "Here is the patch that pass all test for CharFilter. I think this patch is only prototype because it change the API of Tokenizer which need the agreement of committers. "
        },
        {
            "id": "comment-14604408",
            "author": "Michael McCandless",
            "date": "2015-06-27T22:48:23+0000",
            "content": "So finalOffset should be 3 or 6?\n\nIn this example finalOffset should be 6. "
        },
        {
            "id": "comment-14604419",
            "author": "Michael McCandless",
            "date": "2015-06-27T23:18:18+0000",
            "content": "And do you agree this issue is the same as LUCENE-5734 ?\n\nThis looks like the same issue to me, although since HTMLStripCharFilter \"knows\" it's replacing HTML entities (I think?) it could be smarter about correcting offsets, vs e.g. MappingCharFilter which needs to be generic/agnostic as to what exactly it's remapping.\n\nMy first idea was the same idea proposed on LUCENE-5734: add a new correctEndOffset method, which defaults to correctOffset(endOffset-1)+1 but then this \"fails\" the cccc -> cc case.\n\nCao Manh Dat's approach here is to store another int per correction, which is the input offset where the correction first applied, which is a neat solution: it seems to solve my two examples, and I think would solve LUCENE-5734 as well?  Any HTML entity that maps to empty string (e.g. <em>, </em>, <b>, etc., I think?) would not be included within the output token's start/endOffset, unless that entity was \"inside\" a token. "
        },
        {
            "id": "comment-14604599",
            "author": "Michael McCandless",
            "date": "2015-06-28T09:28:00+0000",
            "content": "I think the API change here is necessary, but maybe we can minimize it?\n\nE.g., can we fix the existing BaseCharFilter.addOffCorrectMap method to forward to the new one that now takes an inputOffset?  And can it just pass off as the inputOffset (instead of filling with 0)?\n\nI think we may not need the new method BaseCharFilter.correctEnd, but we do need Tokenizer.correctEndOffset, but can we just implement it as LUCENE-5734 proposed (correctOffset(endOffset-1)+1)?\n "
        },
        {
            "id": "comment-14604614",
            "author": "Michael McCandless",
            "date": "2015-06-28T10:08:30+0000",
            "content": "I think we'll also need to conditionalize this behavior change by version for back compat ... "
        },
        {
            "id": "comment-14604729",
            "author": "Cao Manh Dat",
            "date": "2015-06-28T15:24:40+0000",
            "content": "\nAny HTML entity that maps to empty string (e.g. <em>, </em>, <b>, etc., I think?) would not be included within the output token's start/endOffset, unless that entity was \"inside\" a token.\nI think it will not a problem because we only ask for start/end offset of a token. "
        },
        {
            "id": "comment-14616411",
            "author": "Michael McCandless",
            "date": "2015-07-07T09:21:52+0000",
            "content": "Cao Manh Dat will you have time to fold in some of the feedback above, to minimize API changes?  Or I can try to, if you're too busy... "
        },
        {
            "id": "comment-14618122",
            "author": "Cao Manh Dat",
            "date": "2015-07-08T07:15:49+0000",
            "content": "Michael McCandless Sorry for the late, I will submit a patch tonight (6 hours later) "
        },
        {
            "id": "comment-14618720",
            "author": "Cao Manh Dat",
            "date": "2015-07-08T14:41:48+0000",
            "content": "Refactored some code inside BaseCharFilter to make it cleaner. I think this patch is final.\n\nMichael McCandless I changed \n\naddOffCorrectMap(off, cumulativeDiff, 0);\n\n\nto\n\naddOffCorrectMap(off, cumulativeDiff, off);\n\n\nBut it fail with some test of HTMLStripCharFilterTest. I'm not sure what going on HTMLStripCharFilter. "
        },
        {
            "id": "comment-14619292",
            "author": "Michael McCandless",
            "date": "2015-07-08T20:23:53+0000",
            "content": "Thanks Cao Manh Dat!\n\nIs this the failing case if you pass off instead of 0 to addOffCorrectMap?\n\n\n@@ -215,7 +230,8 @@\n     };\n     \n     int numRounds = RANDOM_MULTIPLIER * 10000;\n-    checkRandomData(random(), analyzer, numRounds);\n+//    checkRandomData(random(), analyzer, numRounds);\n+    checkAnalysisConsistency(random(),analyzer,true,\"m?(y '&\");\n     analyzer.close();\n   }\n\n\n\nBest to add // nocommit comment when making such temporary changes... and it's spooky the test fails because with the right default here (hmm maybe it should be off + cumulativeDiff since it's an input offset, it should behave exactly has before?\n\nCan you mark the old addCorrectMap as deprecated?  We can remove that in trunk but leave deprecated in 5.x ... seems like any subclasses here really need to tell us the input offset...\n\nFor the default impl for CharFilter.correctEnd should we just use CharFilter.correct?\n\nCan we rename correctOffset --> correctStartOffset now that we also have a correctEndOffset?\n\nDoes (correctOffset(endOffset-1)+1) not work?  It would be nice not to add the new method to CharFilter (only to Tokenizer). "
        },
        {
            "id": "comment-14619449",
            "author": "Robert Muir",
            "date": "2015-07-08T21:58:59+0000",
            "content": "I am lost in all the correct() methods now for charfilters. I think at most tokenizer should only have one such method. "
        },
        {
            "id": "comment-14619814",
            "author": "Cao Manh Dat",
            "date": "2015-07-09T03:09:38+0000",
            "content": "Thanks Michael McCandless!\n\n@@ -215,7 +230,8 @@\n     };\n\n     int numRounds = RANDOM_MULTIPLIER * 10000;\n\n\tcheckRandomData(random(), analyzer, numRounds);\n+//    checkRandomData(random(), analyzer, numRounds);\n+    checkAnalysisConsistency(random(),analyzer,true,\"m?(y '&\");\n     analyzer.close();\n   }\n\n\nMy fault, I played around with the test and forgot to roll back. \n\n\nIt's spooky the test fails because with the right default here (hmm maybe it should be \n\n off + cumulativeDiff \n\n since it's an input offset, it should behave exactly has before?\nNice idea, I changed it will \n\n off - cumulativeDiff \n\n and i work perfectly\n\n\nFor the default impl for CharFilter.correctEnd should we just use CharFilter.correct?\nCan we rename correctOffset --> correctStartOffset now that we also have a correctEndOffset?\nNice refactoring.\n\n\nDoes (correctOffset(endOffset-1)+1) not work? It would be nice not to add the new method to CharFilter (only to Tokenizer).\nI tried to do that, but it cant be. Because the information for the special case lie down in BaseCharFilter.\n\nRobert Muir I will try to explain the solution in a slide, I'm quite not good at it   "
        },
        {
            "id": "comment-14619831",
            "author": "Cao Manh Dat",
            "date": "2015-07-09T03:39:01+0000",
            "content": "Attached the slide.\nRobert Muir I think the solution quite clear now  "
        },
        {
            "id": "comment-14625935",
            "author": "David Smiley",
            "date": "2015-07-14T06:50:01+0000",
            "content": " At first I was a little confused by the interleaved representation you used but then I figured it out.  Nice work on the PPT Cao   "
        },
        {
            "id": "comment-14626212",
            "author": "Cao Manh Dat",
            "date": "2015-07-14T11:28:29+0000",
            "content": "I changed correctEndOffset to correctEndTokenOffset because in correctFinalOffset we still use correctOffset().\nI also apply suggestions of Michael McCandless to this patch. "
        }
    ]
}