{
    "id": "LUCENE-7538",
    "title": "Uploading large text file to a field causes \"this IndexWriter is closed\" error",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.5.1",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.4",
            "7.0"
        ],
        "priority": "Major",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "We have seen \"this IndexWriter is closed\" error after we tried to upload a large text file to a single Solr text field. The field definition in the schema.xml is:\n\n<field name=\"fileContent\" type=\"text_general\" indexed=\"true\" stored=\"true\" termVectors=\"true\" termPositions=\"true\" termOffsets=\"true\"/>\n\n\n\nAfter that, the IndexWriter remained closed and couldn't be recovered until we reloaded the Solr core.  The text file had size of 800MB, containing only numbers and English characters.\n\nStack trace is shown below:\n\n2016-11-02 23:00:17,913 [http-nio-19082-exec-3] ERROR org.apache.solr.handler.RequestHandlerBase - org.apache.solr.common.SolrException: Exception writing document id 1487_0_1 to the index; possible analysis error.\n        at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:180)\n        at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:68)\n        at org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:48)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:934)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1089)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:712)\n        at org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n        at org.apache.solr.handler.extraction.ExtractingDocumentLoader.doAdd(ExtractingDocumentLoader.java:126)\n        at org.apache.solr.handler.extraction.ExtractingDocumentLoader.addDoc(ExtractingDocumentLoader.java:131)\n        at org.apache.solr.handler.extraction.ExtractingDocumentLoader.load(ExtractingDocumentLoader.java:237)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:69)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:155)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:2082)\n        at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:651)\n        at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:458)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:229)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:184)\n        at veeva.ecm.common.interfaces.web.SolrDispatchOverride.doFilter(SolrDispatchOverride.java:43)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:212)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:141)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79)\n        at org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:616)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:521)\n        at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1096)\n        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:674)\n        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1500)\n        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1456)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed\n        at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:720)\n        at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:734)\n        at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)\n        at org.apache.solr.update.DirectUpdateHandler2.doNormalUpdate(DirectUpdateHandler2.java:282)\n        at org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:214)\n        at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:169)\n        ... 34 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 56\n        at org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(UnicodeUtil.java:201)\n        at org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(UnicodeUtil.java:183)\n        at org.apache.lucene.codecs.compressing.GrowableByteArrayDataOutput.writeString(GrowableByteArrayDataOutput.java:72)\n        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.writeField(CompressingStoredFieldsWriter.java:292)\n        at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:382)\n        at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:321)\n        at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)\n        at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1477)\n        ... 37 more\n\n\n\n\nWe debugged and traced down the issue.  It was an integer overflow problem that was not properly handled.  The GrowableByteArrayDataOutput::writeString(String string) method is shown below:\n\n@Override\n  public void writeString(String string) throws IOException {\n    int maxLen = string.length() * UnicodeUtil.MAX_UTF8_BYTES_PER_CHAR;\n    if (maxLen <= MIN_UTF8_SIZE_TO_ENABLE_DOUBLE_PASS_ENCODING)  {\n      // string is small enough that we don't need to save memory by falling back to double-pass approach\n      // this is just an optimized writeString() that re-uses scratchBytes.\n      scratchBytes = ArrayUtil.grow(scratchBytes, maxLen);\n      int len = UnicodeUtil.UTF16toUTF8(string, 0, string.length(), scratchBytes);\n      writeVInt(len);\n      writeBytes(scratchBytes, len);\n    } else  {\n      // use a double pass approach to avoid allocating a large intermediate buffer for string encoding\n      int numBytes = UnicodeUtil.calcUTF16toUTF8Length(string, 0, string.length());\n      writeVInt(numBytes);\n      bytes = ArrayUtil.grow(bytes, length + numBytes);\n      length = UnicodeUtil.UTF16toUTF8(string, 0, string.length(), bytes, length);\n    }\n  }\n\n\nThe 800MB text file stored in the string parameter of the method had a length of 800 million, the maxLen became negative integer as the result of the length times 3. The negative integer was then passed into ArrayUtil.grow(scratchBytes, maxLen):\n\npublic static byte[] grow(byte[] array, int minSize) {\n    assert minSize >= 0: \"size must be positive (got \" + minSize + \"): likely integer overflow?\";\n    if (array.length < minSize) {\n      byte[] newArray = new byte[oversize(minSize, 1)];\n      System.arraycopy(array, 0, newArray, 0, array.length);\n      return newArray;\n    } else\n      return array;\n  }\n\n\nAssertion was disabled in production so the execution won't stop. The original array was returned from the method call without increasing the size, which caused an ArrayIndexOutOfBoundsException to be thrown.  The ArrayIndexOutOfBoundsException was wrapped into AbortingException, and later on caused the IndexWriter to be closed in IndexWriter class.\n\nThe code should fail faster with a more-specific error for the integer overflow problem, and shouldn't cause the IndexWriter to be closed.",
    "attachments": {
        "LUCENE-7538.patch": "https://issues.apache.org/jira/secure/attachment/12837091/LUCENE-7538.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15635896",
            "author": "Michael McCandless",
            "date": "2016-11-04T10:12:31+0000",
            "content": "Thanks Steve Chen\n\nI'll fix this logic to use Math.multiplyExact instead so we find out right away that the massive string is too large for us to convert to UTF8. "
        },
        {
            "id": "comment-15635908",
            "author": "Michael McCandless",
            "date": "2016-11-04T10:17:25+0000",
            "content": "Simple patch ... I factored out a new UnicodeUtil method to safely compute the max UTF8 length from a UTF16 length. "
        },
        {
            "id": "comment-15636513",
            "author": "Eric B",
            "date": "2016-11-04T14:31:58+0000",
            "content": "Michael McCandless thanks for the quick response on this.   \n\nwith this patch, will we continue to see the AlreadyClosedException after this issue is encountered?  Not sure if this is a separate issue or all part of this one, but is it expected that an attempt to index a too-large document is considered an \"unrecoverable disaster\"* such that subsequent requests to index different documents will be rejected with an AlreadyClosedException?  I'm pretty sure that's what we experience... that once we attempt to index a too-large document, subsequent requests to index different documents of \"normal\" size are rejected with the AlreadyClosedException error.  \n\nAt this point, we find the only way to recover is to reload the core.  For example..\n\ncurl http://localhost:19081/solr/admin/cores?action=UNLOAD&core=instance_7889_CONTENT\ncurl http://localhost:19081/solr/admin/cores?action=CREATE&name=instance_7889_CONTENT&collection=instance_7889_CONTENT&instanceDir=instance_7889_CONTENT&shard=shard1&collection.configName=contentConf&property.instanceId=7889&loadOnStartup=true&transient=false\n\n\n\n* I used the word \"unrecoverable disaster\" since that's how it's referred to in the code.  From org.apache.lucene.index.IndexWriter:\n\n  // when unrecoverable disaster strikes, we populate this with the reason that we had to close IndexWriter\n  volatile Throwable tragedy;\n\n\n\nThanks,\n\nEric\n "
        },
        {
            "id": "comment-15636551",
            "author": "Michael McCandless",
            "date": "2016-11-04T14:50:32+0000",
            "content": "Unfortunately, yes, IndexWriter will still close itself if you try to index a too-massive text field.\n\nAll my patch does is catch the int overflow, but the resulting exception isn't that much better \n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIndexWriter -Dtests.method=testMassiveField -Dtests.seed=251DAB4FB5DAD334 -Dtests.locale=uk-UA -Dtests.timezone=Atlantic/Jan_Mayen -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   37.4s | TestIndexWriter.testMassiveField <<<\n   [junit4]    > Throwable #1: org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([251DAB4FB5DAD334:E1314D20F8A334C3]:0)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:740)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:754)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1558)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1307)\n   [junit4]    > \tat org.apache.lucene.index.TestIndexWriter.testMassiveField(TestIndexWriter.java:2791)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]    > Caused by: java.lang.ArithmeticException: integer overflow\n   [junit4]    > \tat java.lang.Math.multiplyExact(Math.java:867)\n   [junit4]    > \tat org.apache.lucene.util.UnicodeUtil.maxUTF8Length(UnicodeUtil.java:618)\n   [junit4]    > \tat org.apache.lucene.codecs.compressing.GrowableByteArrayDataOutput.writeString(GrowableByteArrayDataOutput.java:67)\n   [junit4]    > \tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.writeField(CompressingStoredFieldsWriter.java:292)\n   [junit4]    > \tat org.apache.lucene.codecs.asserting.AssertingStoredFieldsFormat$AssertingStoredFieldsWriter.writeField(AssertingStoredFieldsFormat.java:143)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:434)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:373)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:231)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:478)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1562)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1307)\n   [junit4]    > \tat org.apache.lucene.index.TestIndexWriter.testMassiveField(TestIndexWriter.java:2784)\n   [junit4]    > \t... 36 more\n\n\n\nI'll think about how to catch it more cleanly up front w/o closing the IndexWriter... "
        },
        {
            "id": "comment-15637022",
            "author": "Steve Chen",
            "date": "2016-11-04T17:06:37+0000",
            "content": "Michael McCandless I think the code piece is in DefaultIndexingChain::processField method.\n\n// Add stored fields:\n    if (fieldType.stored()) {\n      if (fp == null) {\n        fp = getOrAddField(fieldName, fieldType, false);\n      }\n      if (fieldType.stored()) {\n        try {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        } catch (Throwable th) {\n          throw AbortingException.wrap(th);\n        }\n      }\n\n\nIt wraps up all the Throwable from writeField method call into an AbortingException. "
        },
        {
            "id": "comment-15637193",
            "author": "Michael McCandless",
            "date": "2016-11-04T18:03:53+0000",
            "content": "I think the code piece is in DefaultIndexingChain::processField method.\n\nIndeed, that's a great place ... I'll add the check there. "
        },
        {
            "id": "comment-15637348",
            "author": "Michael McCandless",
            "date": "2016-11-04T19:05:20+0000",
            "content": "OK here's an updated patch.  Now you get an IllegalArgumentException if the attempted stored String field is too big.  I added a test case with @Ignore (because it requires a larger heap than default) and confirmed it's working. "
        },
        {
            "id": "comment-15653739",
            "author": "ASF subversion and git services",
            "date": "2016-11-10T10:51:27+0000",
            "content": "Commit 2902727a1570544869271cf177ed299fdef6863f in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2902727 ]\n\nLUCENE-7538: throw IllegalArgumentException if you attempt to store a too-massive text field "
        },
        {
            "id": "comment-15653745",
            "author": "ASF subversion and git services",
            "date": "2016-11-10T10:53:49+0000",
            "content": "Commit be47009ce765f75661f3eda4878b4bb14a9688a1 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=be47009 ]\n\nLUCENE-7538: throw IllegalArgumentException if you attempt to store a too-massive text field "
        },
        {
            "id": "comment-15653747",
            "author": "Michael McCandless",
            "date": "2016-11-10T10:54:41+0000",
            "content": "Thanks Steve Chen! "
        },
        {
            "id": "comment-15654658",
            "author": "Steve Chen",
            "date": "2016-11-10T17:44:53+0000",
            "content": "Michael McCandless Thank you for getting this fixed so quickly! "
        },
        {
            "id": "comment-15655055",
            "author": "Michael McCandless",
            "date": "2016-11-10T20:27:53+0000",
            "content": "Thank you for taking the time to report it Steve Chen! "
        },
        {
            "id": "comment-16202681",
            "author": "nkeet",
            "date": "2017-10-12T21:47:43+0000",
            "content": "Thanks for having this fixed. I have been using SOLR 6.1.0 for just about a year now, and i recently stated seeing this issue. On ivestigation, i found that, at times i have files which are 1GB huge. Now i do unsterstand that this fix will prevent the IndexWriter from closing. but how do you index such a huge file, are we saying that there is a limit to the file size that can be indexed?, if yes what is the limit? "
        },
        {
            "id": "comment-16202705",
            "author": "Michael McCandless",
            "date": "2017-10-12T22:13:27+0000",
            "content": "nkeet see IndexWriter.MAX_STORED_STRING_LENGTH for the limit; you'll have to either not store that huge file, or break it up across several documents. "
        },
        {
            "id": "comment-16202707",
            "author": "Erick Erickson",
            "date": "2017-10-12T22:13:55+0000",
            "content": "There are probably some a-priori limits, somewhere  someplace lots of data has to be kept. The document has to be indexed in memory-resident segments (essentially) before being flushed. I question whether it's reasonable to even try to index documents this large.\n\nFrom a user's perspective, what's the value in indexing such a huge document? Assuming it's text, it's quite likely that it'll be found for almost every search... and have such a low score that it won't be seen by the users. It can't be displayed in a browser, it's too big. Even transmitting it to the client is prohibitive.\n\nSo I'd ask a little different question: \"What's the largest document it makes sense to index?\". Personally I think it's far, far less than a gigabyte.... "
        },
        {
            "id": "comment-16208235",
            "author": "nkeet shah",
            "date": "2017-10-17T19:56:43+0000",
            "content": "Erick, we are not using SOLR purely as a search engine. we are using it slightly in a non-traditional manner, and our user case of 1GB file are very valid. Thanks Michael for the pointer. "
        }
    ]
}