{
    "id": "LUCENE-4198",
    "title": "Allow codecs to index term impacts",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Sub-task",
        "fix_versions": [
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Subtask of LUCENE-4100.\n\nThats an example of something similar to impact indexing (though, his implementation currently stores a max for the entire term, the problem is the same).\n\nWe can imagine other similar algorithms too: I think the codec API should be able to support these.\n\nCurrently it really doesnt: Stefan worked around the problem by providing a tool to 'rewrite' your index, he passes the IndexReader and Similarity to it. But it would be better if we fixed the codec API.\n\nOne problem is that the Postings writer needs to have access to the Similarity. Another problem is that it needs access to the term and collection statistics up front, rather than after the fact.\n\nThis might have some cost (hopefully minimal), so I'm thinking to experiment in a branch with these changes and see if we can make it work well.",
    "attachments": {
        "LUCENE-4198.patch": "https://issues.apache.org/jira/secure/attachment/12904419/LUCENE-4198.patch",
        "TestSimpleTextPostingsFormat.sarowe.jenkins.nightly.master.681.consoleText.excerpt.txt": "https://issues.apache.org/jira/secure/attachment/12908902/TestSimpleTextPostingsFormat.sarowe.jenkins.nightly.master.681.consoleText.excerpt.txt",
        "LUCENE-4198-BMW.patch": "https://issues.apache.org/jira/secure/attachment/12905909/LUCENE-4198-BMW.patch",
        "LUCENE-4198_flush.patch": "https://issues.apache.org/jira/secure/attachment/12535255/LUCENE-4198_flush.patch",
        "TestSimpleTextPostingsFormat.asf.nightly.master.1466.consoleText.excerpt.txt": "https://issues.apache.org/jira/secure/attachment/12908903/TestSimpleTextPostingsFormat.asf.nightly.master.1466.consoleText.excerpt.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-05T20:36:41+0000",
            "content": "here's a patch fixing how we compute stats in FreqProxTermsWriter: but the codec api is unchanged.\n\nNext ill look at merge, which is trickier, and then see about changing the codec api. ",
            "author": "Robert Muir",
            "id": "comment-13407463"
        },
        {
            "date": "2018-01-03T15:50:59+0000",
            "content": "I have been working on a prototype that adds skip data so that postings could know the best potential score for each block of documents. It would be nice to not make it Similarity-dependant so that Similarities that use the same norm encoding could still be switched at search time like today. So the current approach is to store the maximum freq per block when norms are disabled, or all competitive (freq,norm) pairs when norms are enabled. This leverages the work that has been done on similarities in order to make sure that scores do not decrease when freq increases or when the norm increases. This means that (freq,norm) is always more competitive than (freq-1,norm) or (freq,norm+1), so we don't need to store all (freq,norm) pairs, only competitive ones. At search time, the sim scorer is passed to the postings producer so that it can compute the maximum score of a block by computing the score for all competitive (freq,norm) pairs.\n\nNote that the attached patch is a rough prototype, it is hacky and not everything compiles. I just did the bare minimum so that some basic tests and luceneutil can run. There is very little testing. Some notes about the approach:\n\n\tThis patch adds the assumption than (unsigned) greater norms produce equal or lower scores. I liked this better than adding a new API on Similarity so that it could tell us how to compare norms.\n\tSkip lists do not store the competitive (freq,norm) pairs on level 0 since it could take more storage than the postings block, only level 1 and greater.\n\tI had to add norms producers to the postings consumers so that they could know about norms.\n\tHaving to pass the sim scorer to the postings producer is a bit ugly but I couldn't figure a way to make it nicer.\n\tThe similarity API doesn't make it easy to integrate, it currently gives a score(docID, freq) API while we'd rather need a score(freq,norm) API, especially because this optimization only works if freq and norm are the only per-document parameters that can influence the score.\n\n\n\nHere is what it gives on luceneutil when disabling total hit counts on both master and the patch:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n             AndHighHigh      127.39      (1.4%)      100.94      (2.4%)  -20.8% ( -24% -  -17%)\n              AndHighMed      240.66      (2.0%)      212.11      (1.3%)  -11.9% ( -14% -   -8%)\n               OrHighMed       76.60      (3.6%)       69.37      (2.3%)   -9.4% ( -14% -   -3%)\n              OrHighHigh       27.37      (3.9%)       24.78      (2.4%)   -9.4% ( -15% -   -3%)\n                  Fuzzy1      328.61      (6.5%)      316.04      (5.4%)   -3.8% ( -14% -    8%)\n                Wildcard       56.88      (7.6%)       55.64     (10.0%)   -2.2% ( -18% -   16%)\n                  Fuzzy2      144.68      (3.5%)      142.07      (5.8%)   -1.8% ( -10% -    7%)\n                 Prefix3      372.69      (6.1%)      366.43      (7.7%)   -1.7% ( -14% -   12%)\n   HighTermDayOfYearSort      132.88      (6.6%)      131.18      (7.7%)   -1.3% ( -14% -   13%)\n             LowSpanNear       53.14      (1.8%)       52.48      (1.9%)   -1.2% (  -4% -    2%)\n       HighTermMonthSort      109.37      (7.8%)      108.12      (7.1%)   -1.1% ( -14% -   14%)\n         LowSloppyPhrase       54.79      (1.2%)       54.20      (1.1%)   -1.1% (  -3% -    1%)\n                 Respell      293.10      (2.9%)      290.77      (5.7%)   -0.8% (  -9% -    8%)\n        HighSloppyPhrase       35.60      (1.6%)       35.33      (1.6%)   -0.8% (  -3% -    2%)\n            OrNotHighLow     1686.91      (3.4%)     1675.46      (1.8%)   -0.7% (  -5% -    4%)\n              HighPhrase       24.98      (1.9%)       24.82      (1.7%)   -0.6% (  -4% -    3%)\n             MedSpanNear      228.02      (3.4%)      226.69      (3.6%)   -0.6% (  -7% -    6%)\n         MedSloppyPhrase       46.13      (1.4%)       45.87      (1.3%)   -0.6% (  -3% -    2%)\n               MedPhrase      642.58      (3.7%)      639.51      (3.1%)   -0.5% (  -6% -    6%)\n               LowPhrase       82.99      (2.1%)       82.63      (1.6%)   -0.4% (  -3% -    3%)\n            HighSpanNear       34.77      (2.8%)       34.66      (3.1%)   -0.3% (  -5% -    5%)\n                  IntNRQ       32.59     (15.2%)       32.61     (14.9%)    0.1% ( -26% -   35%)\n              AndHighLow     1719.37      (3.8%)     1915.66      (2.8%)   11.4% (   4% -   18%)\n               OrHighLow     1290.65      (3.1%)     1808.66      (3.7%)   40.1% (  32% -   48%)\n                 LowTerm      873.82      (3.1%)     1527.34      (7.2%)   74.8% (  62% -   87%)\n            OrNotHighMed      285.74      (2.5%)      590.09      (3.9%)  106.5% (  97% -  115%)\n                 MedTerm      180.74      (3.6%)      970.40     (20.2%)  436.9% ( 398% -  477%)\n           OrNotHighHigh       63.41      (0.8%)      529.76     (20.0%)  735.5% ( 709% -  762%)\n            OrHighNotLow       71.04      (0.6%)      649.36     (30.4%)  814.1% ( 778% -  850%)\n                HighTerm       85.02      (3.7%)      804.33     (35.6%)  846.1% ( 778% -  919%)\n            OrHighNotMed      107.76      (0.6%)     1929.95     (48.1%) 1691.0% (1633% - 1749%)\n           OrHighNotHigh       24.38      (0.5%)      478.53     (56.8%) 1862.7% (1796% - 1928%)\n\n\n\nIt make HighTerm about 8x faster. If you wonder why it also helps some boolean queries, this is because boolean queries propagate information about the minimum competitive score to sub clauses. Disk usage increase is negligible: only 0.5% on .doc files and 0.2% overall. I have not measured indexing speed however.\n\nIf someone has ideas what the API could look like, I'd be happy to discuss. ",
            "author": "Adrien Grand",
            "id": "comment-16309823"
        },
        {
            "date": "2018-01-03T16:07:26+0000",
            "content": "Some things I forgot to mention:\n\n\tthis doesn't optimize the omitFreqs case, but we should be able to do that just like the patch already optimizes omitNorms\n\tin the future we could leverage this to speed up phrase queries as well\n\n ",
            "author": "Adrien Grand",
            "id": "comment-16309842"
        },
        {
            "date": "2018-01-03T18:47:28+0000",
            "content": "Once we have block-based impacts, it should also be possible to take some of the ideas of block-max WAND (http://engineering.nyu.edu/~suel/papers/bmw.pdf) to speed up our WANDScorer and also potentially conjunctions. ",
            "author": "Adrien Grand",
            "id": "comment-16310061"
        },
        {
            "date": "2018-01-04T06:21:05+0000",
            "content": "\nThe similarity API doesn't make it easy to integrate, it currently gives a score(docID, freq) API while we'd rather need a score(freq,norm) API, especially because this optimization only works if freq and norm are the only per-document parameters that can influence the score.\n\nWell I think it is fair game to simplify the api so its not strange, i mean we need to fix it so you can make changes like this  A lot of the stuff in Similarity was geared at just hiding away the classic tf/idf stuff so that other things can work. But it should be the term weighting api and limited to that, and there are only 3 components of that: term specificity, term frequency, doc length.\n\nSimple example: boosting doesn't need to be in this api, its only there because it was needed for crazy queryNorm before. But it never belonged and it just adds complexity that isn't needed (and bugs if you forget to multiply it in).\n\nBut along the path of this change, I think its best to change the api to score(freq,norm). But i don't think we should use a Long/boxing, we could just call score(freq,1) for the omitNorms case and thats it (similar to how we pass freq=1 when frequencies are omitted). Seems like it would simplify things there. This is already what SimilarityBase is doing internally, and it doesn't much matter what you substitute in there. ",
            "author": "Robert Muir",
            "id": "comment-16310818"
        },
        {
            "date": "2018-01-04T19:30:10+0000",
            "content": "OK, new iteration. I integrated LUCENE-8116, started to fix corner-cases and I've been looking into ways to make the API nicer. Current take is to add PostingsEnum.setMinCompetitiveScore which defaults to a no-op, and TermsEnum.topPostings(SimScorer) which returns a postings that should be able to skip low-scoring documents and delegates to TermsEnum.postings(null, PostingsEnum.FREQS) by default.\n\nI still need to work on tests and stop creating a new IndexInput slice for every term at index-time. I suppose I could implement getMergeInstance on Lucene70NormsProducer to reuse the same slice across invocations to getNorms on the same field.\n\nI'll keep working on this in the next days. ",
            "author": "Adrien Grand",
            "id": "comment-16311904"
        },
        {
            "date": "2018-01-05T14:25:55+0000",
            "content": "New patch. This time it has tests, does basic testing in CheckIndex and does not clone too much.\n\nResults are very good on queries that score on a single term, almost too good, I'm currently thinking about how we could change the API to have something that is easier to propagate with boolean queries, even if it means term queries can't be as fast.\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n              AndHighLow     2050.37      (4.2%)     1745.54      (2.0%)  -14.9% ( -20% -   -9%)\n               OrHighLow      922.62      (3.7%)      844.54      (2.4%)   -8.5% ( -14% -   -2%)\n              AndHighMed      277.85      (1.8%)      258.11      (2.6%)   -7.1% ( -11% -   -2%)\n            OrNotHighLow     1105.41      (3.6%)     1044.69      (2.0%)   -5.5% ( -10% -    0%)\n             AndHighHigh      128.97      (1.1%)      121.89      (2.7%)   -5.5% (  -9% -   -1%)\n                  Fuzzy2      166.62      (6.2%)      158.38      (6.3%)   -4.9% ( -16% -    8%)\n               OrHighMed      177.56      (2.3%)      170.05      (1.9%)   -4.2% (  -8% -    0%)\n                  Fuzzy1      199.16      (4.4%)      193.05      (5.5%)   -3.1% ( -12% -    7%)\n         MedSloppyPhrase       53.92      (2.2%)       52.40      (2.3%)   -2.8% (  -7% -    1%)\n               LowPhrase      201.13      (1.7%)      195.87      (1.0%)   -2.6% (  -5% -    0%)\n             LowSpanNear      363.85      (3.0%)      355.07      (2.5%)   -2.4% (  -7% -    3%)\n              HighPhrase       62.68      (1.6%)       61.32      (1.2%)   -2.2% (  -4% -    0%)\n       HighTermMonthSort      218.42      (9.8%)      214.35      (8.3%)   -1.9% ( -18% -   18%)\n             MedSpanNear       46.65      (1.4%)       45.89      (1.5%)   -1.6% (  -4% -    1%)\n               MedPhrase      178.02      (1.5%)      175.24      (1.2%)   -1.6% (  -4% -    1%)\n            HighSpanNear       10.21      (3.4%)       10.11      (3.4%)   -1.0% (  -7% -    6%)\n        HighSloppyPhrase       32.32      (7.3%)       32.01      (7.1%)   -1.0% ( -14% -   14%)\n         LowSloppyPhrase       18.01      (2.7%)       17.85      (2.7%)   -0.9% (  -6% -    4%)\n                 Respell      320.99      (2.1%)      321.02      (2.4%)    0.0% (  -4% -    4%)\n                  IntNRQ       29.29     (11.6%)       29.42     (12.5%)    0.4% ( -21% -   27%)\n                Wildcard      189.97      (4.6%)      191.87      (3.9%)    1.0% (  -7% -    9%)\n                 Prefix3      166.43      (6.2%)      169.95      (5.4%)    2.1% (  -8% -   14%)\n              OrHighHigh       48.00      (3.7%)       49.09      (3.9%)    2.3% (  -5% -   10%)\n   HighTermDayOfYearSort      146.88      (7.4%)      150.76      (8.0%)    2.6% ( -11% -   19%)\n                 LowTerm      830.79      (2.6%)     2246.40      (9.9%)  170.4% ( 153% -  187%)\n            OrNotHighMed      180.11      (1.5%)     1454.55     (15.7%)  707.6% ( 680% -  735%)\n                 MedTerm      216.16      (1.7%)     3834.73     (37.0%) 1674.0% (1608% - 1742%)\n                HighTerm      109.49      (2.0%)     1944.44     (45.3%) 1675.9% (1597% - 1757%)\n            OrHighNotMed       57.55      (1.1%)     1292.66     (57.7%) 2146.2% (2064% - 2229%)\n            OrHighNotLow       84.00      (1.1%)     1996.82     (75.4%) 2277.2% (2176% - 2379%)\n           OrNotHighHigh       58.22      (1.3%)     1479.53     (53.5%) 2441.4% (2356% - 2528%)\n           OrHighNotHigh       66.91      (1.2%)     2042.54     (55.1%) 2952.6% (2862% - 3045%)\n\n ",
            "author": "Adrien Grand",
            "id": "comment-16313194"
        },
        {
            "date": "2018-01-06T14:35:33+0000",
            "content": "\nCurrent take is to add PostingsEnum.setMinCompetitiveScore which defaults to a no-op, and TermsEnum.topPostings(SimScorer) which returns a postings that should be able to skip low-scoring documents and delegates to TermsEnum.postings(null, PostingsEnum.FREQS) by default.\n\nI like it, clean simple and minimally invasive. Don't think we should feel bad about passing the scoring function in here, it makes sense.\n\n ",
            "author": "Robert Muir",
            "id": "comment-16314678"
        },
        {
            "date": "2018-01-12T12:14:50+0000",
            "content": "I have taken another approach. Issue with setMinCompetitiveScore is that it usually cannot be efficiently leveraged to speed up eg. conjunctions. So I went with implementing ideas from the block-max WAND (BMW) paper (http://engineering.nyu.edu/~suel/papers/bmw.pdf): the patch introduces a new ImpactsEnum which extends PostingsEnum and introduces two APIs instead of setMinCompetitiveScore:\n\n\tint advanceShallow(int target) to get scoring information for documents that start at target. The benefit compared to advance is that it only advances the skip list reader, which is much cheaper: no decoding is happening.\n\tfloat getMaxScore(int upTo) wich gives information about scores for doc ids between the last target to advanceShallow and upTo, both included.\n\n\n\nCurrently only TermScorer leverages this, but the benefit is that we could add these APIs to Scorer as well in a follow-up issue so that WANDScorer and ConjunctionScorer could leverage them. I built a prototype already to make sure that there is an actual speedup for some queries, but I'm leaving it to a follow-up issue as indexing impacts is already challenging on its own. One thing that it made me change though is that the new patch also stores all impacts on the first level, which is written every 128 documents. This seemed important for conjunctions, since the maximum score on a given block is not always reached, on the contrary to term queries since they match all documents in the block. It makes it more important to have good bounds of the score with conjunctions than it is with term queries. The disk overhead is still acceptable to me: the wikimedium10 index is only 1.4% larger overall, and postings alone (the .doc file) is only 3.1% larger.\n\nHere are the benchmark results:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n              AndHighLow     1128.91      (3.5%)      875.48      (2.3%)  -22.4% ( -27% -  -17%)\n              AndHighMed      409.67      (2.0%)      331.98      (1.7%)  -19.0% ( -22% -  -15%)\n               OrHighMed      264.99      (3.5%)      229.15      (3.0%)  -13.5% ( -19% -   -7%)\n               OrHighLow      111.47      (4.5%)       98.00      (3.2%)  -12.1% ( -18% -   -4%)\n              OrHighHigh       34.88      (4.2%)       31.69      (4.0%)   -9.1% ( -16% -   -1%)\n            OrNotHighLow     1373.74      (5.2%)     1291.72      (4.1%)   -6.0% ( -14% -    3%)\n               LowPhrase       78.14      (1.6%)       75.28      (1.2%)   -3.7% (  -6% -    0%)\n               MedPhrase       47.49      (1.6%)       45.92      (1.2%)   -3.3% (  -5% -    0%)\n         LowSloppyPhrase      208.43      (2.8%)      202.37      (2.7%)   -2.9% (  -8% -    2%)\n                  Fuzzy1      300.99      (7.7%)      292.78      (8.0%)   -2.7% ( -17% -   13%)\n             LowSpanNear       62.73      (1.4%)       61.09      (1.3%)   -2.6% (  -5% -    0%)\n                  Fuzzy2      188.37      (7.9%)      184.16      (6.7%)   -2.2% ( -15% -   13%)\n             MedSpanNear       57.41      (1.8%)       56.17      (1.5%)   -2.2% (  -5% -    1%)\n         MedSloppyPhrase       23.21      (2.3%)       22.73      (2.3%)   -2.1% (  -6% -    2%)\n              HighPhrase       48.75      (3.2%)       47.80      (3.6%)   -1.9% (  -8% -    4%)\n            HighSpanNear       40.04      (2.9%)       39.35      (2.7%)   -1.7% (  -7% -    4%)\n       HighTermMonthSort      228.21      (8.4%)      224.66      (7.9%)   -1.6% ( -16% -   16%)\n        HighSloppyPhrase       25.96      (2.8%)       25.61      (3.0%)   -1.4% (  -6% -    4%)\n                 Respell      284.85      (3.7%)      282.42      (4.0%)   -0.9% (  -8% -    7%)\n                  IntNRQ       18.87      (5.3%)       18.86      (6.8%)   -0.1% ( -11% -   12%)\n                Wildcard       85.50      (5.0%)       86.79      (4.0%)    1.5% (  -7% -   11%)\n                 Prefix3      137.41      (6.5%)      141.61      (4.9%)    3.1% (  -7% -   15%)\n   HighTermDayOfYearSort      116.58      (6.3%)      121.38      (7.2%)    4.1% (  -8% -   18%)\n             AndHighHigh       37.64      (1.5%)      118.12      (6.7%)  213.8% ( 202% -  225%)\n                 LowTerm      909.13      (2.2%)     3379.38     (11.2%)  271.7% ( 252% -  291%)\n            OrNotHighMed      196.21      (1.7%)     1509.92     (28.9%)  669.6% ( 627% -  712%)\n                 MedTerm      305.82      (1.7%)     2897.01     (42.5%)  847.3% ( 789% -  907%)\n                HighTerm      108.94      (1.7%)     1191.54     (61.3%)  993.8% ( 915% - 1075%)\n            OrHighNotMed       81.83      (0.5%)     1082.94     (63.2%) 1223.5% (1153% - 1294%)\n           OrHighNotHigh       63.16      (0.7%)      995.35     (72.0%) 1475.8% (1393% - 1559%)\n            OrHighNotLow       34.53      (0.9%)      557.35     (94.8%) 1514.1% (1406% - 1623%)\n           OrNotHighHigh       51.95      (0.9%)      943.81     (73.7%) 1716.6% (1626% - 1808%)\n\n\n\nResults are a bit less good than previously for two reasons:\n\n\tThe new API doesn't allow term queries to skip more than one block (128 docs) at a time. But I think the perspective of speeding up boolean queries too makes it a good trade-off, especially given that this is already quite a significant speed up for term queries.\n\tThe new implementation is more complex since it also (optionally) gives access to positions, offsets and payloads. I did not specialize the case that only frequencies are requested (hopefully this won't be necessary). Information about positions could be useful in the future to speed up phrase queries.\n\n\n\nI wouldn't worry too much about the slowdown for some boolean queries. First the most affected queries are queries that are already quite fast. Plus I hope that follow-ups will allow us to get some performance back. ",
            "author": "Adrien Grand",
            "id": "comment-16323881"
        },
        {
            "date": "2018-01-12T15:14:14+0000",
            "content": "I tested wikibigall as well, which has the benefit of not having artificially truncated lengths like wikimedium:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n              AndHighLow     1440.24      (3.0%)      794.43      (2.9%)  -44.8% ( -49% -  -40%)\n              AndHighMed      121.80      (1.4%)       94.75      (1.5%)  -22.2% ( -24% -  -19%)\n             AndHighHigh       56.62      (1.2%)       45.26      (1.4%)  -20.1% ( -22% -  -17%)\n               OrHighMed       93.16      (3.3%)       78.18      (3.1%)  -16.1% ( -21% -   -9%)\n               OrHighLow      827.62      (2.6%)      748.49      (3.5%)   -9.6% ( -15% -   -3%)\n              OrHighHigh       35.14      (4.4%)       32.25      (4.6%)   -8.2% ( -16% -    0%)\n                  Fuzzy1      265.67      (4.7%)      246.12      (5.0%)   -7.4% ( -16% -    2%)\n               LowPhrase      166.32      (1.3%)      157.61      (1.6%)   -5.2% (  -8% -   -2%)\n                  Fuzzy2      184.41      (4.3%)      176.40      (3.5%)   -4.3% ( -11% -    3%)\n             LowSpanNear      749.77      (2.1%)      726.14      (2.2%)   -3.2% (  -7% -    1%)\n               MedPhrase       23.77      (2.0%)       23.14      (1.9%)   -2.6% (  -6% -    1%)\n              HighPhrase       18.73      (3.0%)       18.24      (3.0%)   -2.6% (  -8% -    3%)\n             MedSpanNear      113.11      (2.3%)      110.17      (2.0%)   -2.6% (  -6% -    1%)\n         MedSloppyPhrase       10.28      (6.5%)       10.07      (6.9%)   -2.0% ( -14% -   12%)\n         LowSloppyPhrase       12.68      (6.6%)       12.43      (7.1%)   -2.0% ( -14% -   12%)\n        HighSloppyPhrase        9.47      (7.0%)        9.29      (7.5%)   -1.9% ( -15% -   13%)\n                  IntNRQ       27.89      (7.0%)       27.58      (8.7%)   -1.1% ( -15% -   15%)\n            HighSpanNear        9.05      (4.9%)        8.98      (4.7%)   -0.8% (  -9% -    9%)\n                 Respell      273.80      (2.3%)      273.79      (2.2%)   -0.0% (  -4% -    4%)\n       HighTermMonthSort       68.77      (7.1%)       69.60      (7.8%)    1.2% ( -12% -   17%)\n                Wildcard       92.81      (5.8%)       94.67      (6.2%)    2.0% (  -9% -   14%)\n   HighTermDayOfYearSort       61.99     (10.3%)       64.18     (10.9%)    3.5% ( -16% -   27%)\n                 Prefix3       41.42      (8.3%)       42.96      (8.2%)    3.7% ( -11% -   22%)\n                 LowTerm      694.99      (2.5%)     3126.69     (17.7%)  349.9% ( 321% -  379%)\n                HighTerm       58.04      (2.7%)      490.60     (58.6%)  745.3% ( 666% -  828%)\n                 MedTerm      120.80      (2.6%)     1053.44     (55.1%)  772.1% ( 695% -  852%)\n\n\n\n.doc file is 5.2% larger and the index is 1.5% larger overall. ",
            "author": "Adrien Grand",
            "id": "comment-16324101"
        },
        {
            "date": "2018-01-12T16:55:20+0000",
            "content": "Just so I'm clear on something: does this performance improvement only apply when the search doesn't track the total hits, and generally wouldn't work with a faceted search as well? ",
            "author": "David Smiley",
            "id": "comment-16324234"
        },
        {
            "date": "2018-01-12T17:08:32+0000",
            "content": "Correct, it is only good at computing the top-scoring hits. ",
            "author": "Adrien Grand",
            "id": "comment-16324247"
        },
        {
            "date": "2018-01-12T17:10:08+0000",
            "content": "There are a lot of approaches for getting the same benefits with facets, such as returning fast top-N search results immediately, then do exact facet search as a separate query. this can be asynchronous from a user interface perspective so it still reduces latency to the user. There is also the idea of approximate facet counts which i think has been explored a bit with lucene facets module. But we have to start somewhere. ",
            "author": "Robert Muir",
            "id": "comment-16324250"
        },
        {
            "date": "2018-01-12T19:14:50+0000",
            "content": "To give some insight into future work on scorers, here is an untested patch (the only tests for now are that luceneutil gives the same hits back) that implements some ideas from the BMW paper.\n\nThe new BlockMaxConjunctionScorer skips blocks whose sum of max scores is less than the max competitive score, and also skips hits when the score of the max scoring clause is less than the minimum required score minus max scores of other clauses.\n\nWANDScorer uses the block max scores to get an upper bound of the score of the current candidate, which already helps OrHighLow. It could also skip over blocks when the sum of the max scores is not competitive, but the impl needs a bit more work than for conjunctions.\n\nBaseline is LUCENE-4198.patch, patch is LUCENE-4198.patch and LUCENE-4198-BMW.patch combined.\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n                 LowTerm     2365.07      (2.8%)     2313.92      (2.5%)   -2.2% (  -7% -    3%)\n               OrHighMed       73.78      (2.9%)       72.70      (2.5%)   -1.5% (  -6% -    4%)\n   HighTermDayOfYearSort       88.44     (11.4%)       87.15     (13.0%)   -1.5% ( -23% -   25%)\n                HighTerm      650.28      (5.8%)      646.81      (5.7%)   -0.5% ( -11% -   11%)\n                 Respell      228.08      (2.5%)      227.84      (2.4%)   -0.1% (  -4% -    4%)\n                 MedTerm     1189.63      (4.2%)     1189.27      (4.6%)   -0.0% (  -8% -    9%)\n             MedSpanNear       12.21      (5.0%)       12.24      (5.5%)    0.2% (  -9% -   11%)\n            HighSpanNear        7.26      (5.5%)        7.28      (5.8%)    0.2% ( -10% -   12%)\n                Wildcard      108.43      (7.0%)      108.95      (6.8%)    0.5% ( -12% -   15%)\n                 Prefix3      128.80      (8.1%)      129.46      (7.8%)    0.5% ( -14% -   17%)\n       HighTermMonthSort      172.27      (8.0%)      173.28      (8.0%)    0.6% ( -14% -   18%)\n                  Fuzzy2      104.86      (5.7%)      105.79      (6.5%)    0.9% ( -10% -   13%)\n         LowSloppyPhrase       14.80      (5.6%)       14.93      (6.1%)    0.9% ( -10% -   13%)\n             LowSpanNear       95.06      (3.4%)       96.07      (4.2%)    1.1% (  -6% -    8%)\n        HighSloppyPhrase        3.96      (8.6%)        4.02      (9.7%)    1.6% ( -15% -   21%)\n                  IntNRQ       29.80      (7.0%)       30.50      (6.9%)    2.4% ( -10% -   17%)\n                  Fuzzy1      281.25      (4.8%)      288.77      (9.5%)    2.7% ( -11% -   17%)\n         MedSloppyPhrase       53.95      (8.0%)       55.43      (9.0%)    2.7% ( -13% -   21%)\n              OrHighHigh       23.86      (4.1%)       24.70      (2.7%)    3.5% (  -3% -   10%)\n               MedPhrase       42.45      (2.2%)       44.10      (3.2%)    3.9% (  -1% -    9%)\n               LowPhrase       19.57      (2.7%)       20.47      (3.6%)    4.6% (  -1% -   11%)\n              HighPhrase       15.76      (4.1%)       16.91      (5.3%)    7.3% (  -1% -   17%)\n               OrHighLow      209.91      (2.3%)      261.10      (3.5%)   24.4% (  18% -   30%)\n             AndHighHigh       27.22      (2.1%)       47.66      (5.1%)   75.1% (  66% -   84%)\n              AndHighLow      514.84      (3.5%)      920.46      (6.0%)   78.8% (  66% -   91%)\n              AndHighMed       56.15      (2.0%)      107.60      (5.4%)   91.6% (  82% -  101%)\n\n\n ",
            "author": "Adrien Grand",
            "id": "comment-16324415"
        },
        {
            "date": "2018-01-12T19:21:45+0000",
            "content": "Yeah, its good to split it up into sizable chunks: we can always mark the new API experimental to be safe\n\nI like the idea of the more general solution to speed up boolean and maybe proxy queries in the future. Will look over the API to the new enum. ",
            "author": "Robert Muir",
            "id": "comment-16324427"
        },
        {
            "date": "2018-01-19T14:42:34+0000",
            "content": "Here is a new patch with a minor modification that makes impacts computed lazily. It seems to help a bit with conjunctions. ",
            "author": "Adrien Grand",
            "id": "comment-16332335"
        },
        {
            "date": "2018-01-23T09:06:42+0000",
            "content": "The patch looks great. The new API seems contained and easy to use. I like the simplicity of the implementation of max score for the term scorer.\n\nI have some questions though, the SlowImpactEnum returns NO_MORE_DOCS when advanceShallow is used, is it allowed (the contract is that this API should always return a docId greater than the current doc ?) ?\u00a0 The name is also a bit misleading since it seems that it is only for postings that don't index impacts or that contain a single document.Why do you need to compute the impact lazily ? Is it for queries that don't use score for sorting ? You said it helps with conjunctions but the bmw patch uses the impacts for conjunctions as well so I guess it's for all other queries that don't require this information ? ",
            "author": "Jim Ferenczi",
            "id": "comment-16335533"
        },
        {
            "date": "2018-01-24T15:09:50+0000",
            "content": "Thanks Jim for having a look!\n\nthe SlowImpactEnum returns NO_MORE_DOCS when advanceShallow is used, is it allowed (the contract is that this API should always return a docId greater than the current doc ?)\n\nI had a look at the patch and it says gte so that should be fine? When working on the patch I went back and forth between requiring that upTo is either gt or gte the current doc and settled on gte which made the API easier to use. Otherwise I would have needed to make it illegal to call once on NO_MORE_DOCS, which makes it harder to use in scorers.\n\nWhy do you need to compute the impact lazily ? Is it for queries that don't use score for sorting ?\n\nQueries that don't use the score for sorting should be ok: they won't be using an ImpactsEnum anyway. Laziness doesn't help term queries, this is more for conjunctions (and disjunctions that progressively turn into conjunctions like WANDScorer). Imagine a conjunction between two term queries: one that matches lots of docs and the other one that matches about 100x fewer documents. The latter will be used to lead the iteration and the former will be used to confirm matches. It is quite likely that almost every time it is advanced, the clause with a higher cost will need to decode an entire block, which also involves computing scores for all competitive (freq,norm) pairs. If there is a match, this CPU time is not necessarily wasted, but it is also quite likely that there is no match, in which case the computed impacts are useless.\n\nConjunctions do use impacts, but they use the block boundaries (the return value of advanceShallow) of the clause that has the higher score contribution (see LUCENE-8135). So in case of the conjunction described above, we will likely need only impacts that are computed on the second level for the clause with a higher cost, which is stored every 8 blocks (instead of every block for the first level). Laziness helps skip computing scores on the lower levels if they are unused. ",
            "author": "Adrien Grand",
            "id": "comment-16337720"
        },
        {
            "date": "2018-01-24T18:05:58+0000",
            "content": "Ok thanks for the explanation. I looked briefly at LUCENE-8135 and I understand the intent better now. ",
            "author": "Jim Ferenczi",
            "id": "comment-16338004"
        },
        {
            "date": "2018-01-29T03:51:38+0000",
            "content": "Sorry, took me a long time (been traveling). I think the work is impressive and clean, but just have a few thoughts, maybe for the future: it would be nice if we didn't have to pass SimScorer down to this low level.\n\nI guess I am suggesting we could explore an even lower-level API for impactsenum where the consumer (likely gonna be org.apache.lucene.search) is the only one aware of the scoring function, so the codec api is really exposing raw data instead. I feel like this would map better to how the other codec apis work, give a bit better separation?\n\nThat being said, I think its more important to make progress for now... the api is labelled experimental so we could improve it in the future. Its also not stupid-complicated or anything, just two methods! Also admittedly, I don't have any real solid use cases for the \"rawer\" api besides CheckIndex, maybe searchAfter..., its just more of a stretch idea. ",
            "author": "Robert Muir",
            "id": "comment-16342865"
        },
        {
            "date": "2018-01-31T13:48:04+0000",
            "content": "Thanks Jim and Robert for the reviews! I just merged this change and opened LUCENE-8142 for the idea that Robert mentioned of exposing raw data. ",
            "author": "Adrien Grand",
            "id": "comment-16346850"
        },
        {
            "date": "2018-01-31T13:55:03+0000",
            "content": "Commit f410df81136291f61b8deaace22bf4bb650de1ad in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f410df8 ]\n\nLUCENE-4198: Give codecs the opportunity to index impacts. ",
            "author": "ASF subversion and git services",
            "id": "comment-16346864"
        },
        {
            "date": "2018-01-31T15:31:26+0000",
            "content": "My Jenkins found two reproducing Solr test failures that git bisect blames on the f410df8 commit on this issue:\n\n\n   [junit4] Suite: org.apache.solr.uninverting.TestUninvertingReader\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestUninvertingReader -Dtests.method=testSortedSetIntegerManyValues -Dtests.seed=120215CB40DFC75D -Dtests.slow=true -Dtests.locale=de-LU -Dtests.timezone=America/North_Dakota/New_Salem -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.20s J7  | TestUninvertingReader.testSortedSetIntegerManyValues <<<\n   [junit4]    > Throwable #1: java.lang.UnsupportedOperationException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([120215CB40DFC75D:F3DEA4F6D55E4A1B]:0)\n   [junit4]    > \tat org.apache.lucene.index.MultiTermsEnum.impacts(MultiTermsEnum.java:373)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.checkFields(CheckIndex.java:1615)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.testPostings(CheckIndex.java:1873)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:337)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:319)\n   [junit4]    > \tat org.apache.solr.uninverting.TestUninvertingReader.testSortedSetIntegerManyValues(TestUninvertingReader.java:300)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@2818cbd7), locale=de-LU, timezone=America/North_Dakota/New_Salem\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_151 (64-bit)/cpus=16,threads=1,free=54883016,total=531103744\n\n\n\n\n   [junit4] Suite: org.apache.solr.uninverting.TestDocTermOrds\n   [junit4] IGNOR/A 0.00s J2  | TestDocTermOrds.testTriggerUnInvertLimit\n   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestDocTermOrds -Dtests.method=testRandomWithPrefix -Dtests.seed=120215CB40DFC75D -Dtests.slow=true -Dtests.locale=th -Dtests.timezone=SystemV/MST7MDT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.03s J2  | TestDocTermOrds.testRandomWithPrefix <<<\n   [junit4]    > Throwable #1: java.lang.UnsupportedOperationException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([120215CB40DFC75D:4E81207B2E152EC5]:0)\n   [junit4]    > \tat org.apache.lucene.index.MultiTermsEnum.impacts(MultiTermsEnum.java:373)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.checkFields(CheckIndex.java:1615)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.testPostings(CheckIndex.java:1873)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:337)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:319)\n   [junit4]    > \tat org.apache.solr.uninverting.TestDocTermOrds.testRandomWithPrefix(TestDocTermOrds.java:357)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestDocTermOrds -Dtests.method=testRandom -Dtests.seed=120215CB40DFC75D -Dtests.slow=true -Dtests.locale=th -Dtests.timezone=SystemV/MST7MDT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.02s J2  | TestDocTermOrds.testRandom <<<\n   [junit4]    > Throwable #1: java.lang.UnsupportedOperationException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([120215CB40DFC75D:604E30C4F1BF712E]:0)\n   [junit4]    > \tat org.apache.lucene.index.MultiTermsEnum.impacts(MultiTermsEnum.java:373)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.checkFields(CheckIndex.java:1615)\n   [junit4]    > \tat org.apache.lucene.index.CheckIndex.testPostings(CheckIndex.java:1873)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:337)\n   [junit4]    > \tat org.apache.lucene.util.TestUtil.checkReader(TestUtil.java:319)\n   [junit4]    > \tat org.apache.solr.uninverting.TestDocTermOrds.testRandom(TestDocTermOrds.java:271)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-tests-master/workspace/solr/build/solr-core/test/J2/temp/solr.uninverting.TestDocTermOrds_120215CB40DFC75D-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {field=BlockTreeOrds(blocksize=128), foo=BlockTreeOrds(blocksize=128), id=PostingsFormat(name=LuceneFixedGap)}, docValues:{}, maxPointsInLeafNode=1391, maxMBSortInHeap=6.814533240370871, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@177c82a3), locale=th, timezone=SystemV/MST7MDT\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16347016"
        },
        {
            "date": "2018-01-31T15:42:18+0000",
            "content": "Woops, thanks Steve. I'll fix asap. ",
            "author": "Adrien Grand",
            "id": "comment-16347035"
        },
        {
            "date": "2018-01-31T15:46:57+0000",
            "content": "Commit 5cf9b9f704251adceed04edb66e8ec9e994ea543 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5cf9b9f ]\n\nLUCENE-4198: Make MultiTermsEnum implement impacts. ",
            "author": "ASF subversion and git services",
            "id": "comment-16347039"
        },
        {
            "date": "2018-02-02T01:57:47+0000",
            "content": "TestSimpleTextPostingsFormat has failed reproducibly a couple times in Jenkins today (ASF and my Jenkins), and git bisect blames the f410df8 commit on this issue for the one from my Jenkins.  \n\nI'll attach the relevant (voluminous) output from https://builds.apache.org/job/Lucene-Solr-NightlyTests-master/1466/ and from my Jenkins. ",
            "author": "Steve Rowe",
            "id": "comment-16349648"
        },
        {
            "date": "2018-02-02T13:00:17+0000",
            "content": "Commit 666f93ad4f597edb1a88ef48374ac79a1c09e862 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=666f93a ]\n\nLUCENE-4198: Fix propagation of flags in SimpleTextPostingsFormat. ",
            "author": "ASF subversion and git services",
            "id": "comment-16350276"
        },
        {
            "date": "2018-02-05T10:31:50+0000",
            "content": "This change had an impact on the nightly benchmarks:\n\n\u00a0- AndHighHigh: -3% http://people.apache.org/~mikemccand/lucenebench/AndHighHigh.html\n\n\u00a0- AndHighMed: -4% http://people.apache.org/~mikemccand/lucenebench/AndHighMed.html\n\n\u00a0- AndHighOrMedMed: -4% http://people.apache.org/~mikemccand/lucenebench/AndHighOrMedMed.html\n\n\u00a0- AndMedOrHighHigh: -5% http://people.apache.org/~mikemccand/lucenebench/AndMedOrHighHigh.html\n\n\u00a0\n\nHowever indexing speed and term queries seem unaffected. And phrase queries only have a very minor slowdown, which might be noise (~1%). I'll look into it later, but I think this is an acceptable slowdown given how it allows to speed up top-k queries. ",
            "author": "Adrien Grand",
            "id": "comment-16352218"
        }
    ]
}