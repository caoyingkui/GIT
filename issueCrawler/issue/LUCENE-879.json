{
    "id": "LUCENE-879",
    "title": "Document number integrity merge policy",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.1",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This patch allows for document numbers stays the same even after merge of segments with deletions.\n\nConsumer needs to do this:\nindexWriter.setSkipMergingDeletedDocuments(false);\n\nThe effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.\n\nAlso see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880",
    "attachments": {
        "LUNCENE-879.diff": "https://issues.apache.org/jira/secure/attachment/12357124/LUNCENE-879.diff"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-05-11T17:36:18+0000",
            "content": "Forgot to tell about all the effects:\n\n1. Replaces deleted documents with a new Document()\n2. Stores a null term frequency vector\n3. Sets norm to Similarity.encodeNorm(0f)\n ",
            "author": "Karl Wettin",
            "id": "comment-12495103"
        },
        {
            "date": "2007-05-11T18:41:43+0000",
            "content": "I skimmed through the patch and I understand that all terms and postings \nof deleted docs are discarded, and, instead, an empty doc is added.\n\nI would like to comment on the idea behind this.\n\nI think that this satisfies part of (some) applications needs, \nassuming it is mainly documents updating that causes deletions.\n\nFor example, assume initial 5 documents \n{A,B,C,D,E}\n, their internal ids \nare \n{0,1,2,3,4}\n, and used as keys to consumer's secondary storage.\n\nNow, docs B and D are updated - so the internal ids would change.\nAs of now, they become:  \n{A:0, C:1, E:2, B`:3, D`:4}\n.\nWith this patch, I believe they would become:  \n{A:0, _:1, C:2, _:3, E:4, B`:5, D`:6}\n.\n\nSo, accessing the secondary storage is now working nicely for the unchanged \ndocs A, C, E, but the keys in the secondary storage have to be modified for the \nupdated documents B and D.\n\nThis is probably not too bad, because the application updated the secondary \nstorage anyhow, so why not updating the access key at the same\ntime - especially if the application keeps track of number of added documents.\n\nI like this idea, but can see a few issues:\n\n1) statistics are somewhat distorted - docCount used at search time \n    computations (idf) now (always) includes docs that were deleted. \n\n2) In the long run, norms size grow, so more memory is used.\n     Eventually a merge-and-clean/squeeze might be required, but I guess the \n     application can do that in a controlled and efficient manner, updating the \n     secondary storage ids at the same time.\n\nHow about a different - more external - approach, not changing the internal-ids \nbehavior, but rather using payloads for storing external IDs, and, when opening a \nnew reader, reading (once) these IDs to an int array, that maps from\ninternal IDs to application IDs. This information is now readily available \nat search time for referencing the secondary repository. Having these IDs as \npayloads should allow to load them relatively fast, so hopefully warming a new \nreader would not be too slow as result of this. That was part 1 of the price of this \napproach. Part 2 is the memory taken for the IDs - 4 bytes per doc per reader.\nPart 3 is the complexity of using this, but I didn't think of API yet.\n\nDoron ",
            "author": "Doron Cohen",
            "id": "comment-12495127"
        },
        {
            "date": "2007-05-11T20:02:30+0000",
            "content": "Doron, thanks for the input. \n\nI have not had time to read and think everything though that you wrote yet, but I will tell you of what I'm doing and what I'm aiming at.\n\nI use this patch in conjunction with an Oracle (Sleepycat) BDB object storage. The Lucene document number (LDN) is used as secondary key. I do no unmarshalling to object from data stored in Lucene fields, I only use it as an index. I never have to read the document from Lucene. I have no clue how much CPU ticks or bits of RAM this might save me, I'll have to bench that later on. This is just me fooling around with technology solutions for fun, a proof of concept. There is no real project.\n\nWhen I update an instance of the object storage, I'll create a new document in Lucene and then update the LDN in the instace to be updated in the object storage, then delete the old document in Lucene.\n\nEven though it works, I do not like this solution. I want to fully retain the document number integrity for updated document. I belive this can be solved if i limit the warranty to an index in an optimized state. \n\nAn instance of DocumentIdentityFactory, capable of identifying and create queried to uniquely identify documents, will be passed to the SegmentMerger. It might look at field \"_type\" and \"_pk\", or so. \n\nAs SegmentMerger.mergeFields reach a deleted document it will use the factory to find replacements for the deleted document in the index. The one with the top document number is latest one and thus the winner. This document will be added at the current position and added to a list of document number to treat as deleted. \n\nTa-da, and there we have safe(tm) document numbers. ",
            "author": "Karl Wettin",
            "id": "comment-12495145"
        },
        {
            "date": "2007-05-11T23:15:41+0000",
            "content": "This new patch allows consumer to, based on a primary key, delete a document and add a new document with the same document number as the deleted. The events will occur on merging. ",
            "author": "Karl Wettin",
            "id": "comment-12495201"
        },
        {
            "date": "2007-05-12T08:16:15+0000",
            "content": "Karl, in your application, you store nothing in Lucene isn't it ?\nDoes it cost so much to just store an field id in Lucene ? ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12495239"
        },
        {
            "date": "2007-05-12T12:07:51+0000",
            "content": "Nicolas Lalev\u00e9e [12/May/07 01:16 AM]\n> Karl, in your application, you store nothing in Lucene isn't it ?\n> Does it cost so much to just store an field id in Lucene ? \n\nI have no clue how much CPU ticks or bits of RAM this might save me, I'll have to bench that later on. This is just me fooling around with technology solutions for fun, a proof of concept. There is no real project.\n\nBut it is not the cost that conserns me. It is having the data spread around diffrent layers. I want to use BDB as object storage, not Lucene. ",
            "author": "Karl Wettin",
            "id": "comment-12495257"
        },
        {
            "date": "2007-05-12T17:18:56+0000",
            "content": "That was I was talking about, storing in Lucene just an ID referencing the data in another storage. So this Lucene-stored ID became the document-id you try to fix.\n\nI have also done some experimentation about making the storage external, but I realized that what I was coding was exactly the same as storing an ID in Lucene. But I didn't tried to \"fix\" the document id. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12495310"
        },
        {
            "date": "2011-01-26T04:59:56+0000",
            "content": "Closing for long inactivity. Also, there seems to be a reasonable workaround proposed that would not require Lucene to handle mock empty documents to preserve the other doc IDs. In addition, we're working towards out of order merges etc., so I don't see how this would fit in the future at all. ",
            "author": "Shai Erera",
            "id": "comment-12986846"
        },
        {
            "date": "2011-01-26T05:08:24+0000",
            "content": "The workaround of using an ID turns out to be very slow.  The worst part is if you need to build a filter - if your database query spits out some other ID scheme then you need to map them to lucene IDs to build the filter, which effectively comes down to one search per document matching the filter.\n\nWas there another workaround proposed which was actually reasonable?  Maybe I'm just not seeing it? ",
            "author": "Trejkaz",
            "id": "comment-12986848"
        }
    ]
}