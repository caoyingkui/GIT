{
    "id": "LUCENE-6322",
    "title": "IndexSearcher.doc(int docID, SetfieldsToLoad)  is slower in Lucene 4.9 when compared to Lucene 2.9",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "4.9",
        "components": [
            "core/codecs"
        ],
        "labels": "",
        "fix_versions": [
            "4.10.5"
        ],
        "priority": "Major",
        "status": "Open",
        "type": "Bug"
    },
    "description": "We use IndexSearcher.doc(int docID, SetfieldsToLoad) method to get the document with selected stored fields. If we did not mention few stored fields which have data more than 500KB, this call is slower in Lucene 4.9 when compared to Lucene 2.9.\n\nI debugged the above method with Lucene 4.9 and found that CompressingStoredFieldsReader#visitDocument(int docID, StoredFieldVisitor visitor) is spending more time while loading file content and decompressing in chunks of 16kb, even to skip the fields. It is noticeable degrade if the document's field size is more than 1MB, and we call this method in loop for more than 1000 such documents.\n\nIn case of Lucene 2.9, there was no compression, and if we want to skip the field, it just does file seek to set the next pointer to read the stored field. For example see Lucene3xStoredFieldsReader#skipField() method how it works for skipping a field in Lucene 2.9 which is VERY faster compared to Lucene 4.9.\n\nWe should have something in CompressingStoredFieldsReader to know the field\u2019s compressed length in file and just do the file seek to set the next pointer instead of loading content from file and decompress that in 16KB chunks to just skip the field from the file.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14343325",
            "author": "Adrien Grand",
            "date": "2015-03-02T16:17:19+0000",
            "content": "Agreed it would be nice to skip over compressed blocks when they are not needed instead of decompressing and then discarding the decompressed bytes. I was just looking at the impl and it seems that to make it work we would need to store the compressed length of each block and implement skipBytes on the anonymous DataInput created in CompressingStoredFieldsReader.document. "
        },
        {
            "id": "comment-14344673",
            "author": "Mikhail Khludnev",
            "date": "2015-03-03T07:27:37+0000",
            "content": "Sekhar,\nCan you revise your storage scheme and move to binary docvalues, or so?   "
        },
        {
            "id": "comment-15265919",
            "author": "Stanislav Palatnik",
            "date": "2016-05-01T20:05:54+0000",
            "content": "Is there an alternative 4.x codec that does not use CompressingStoredFieldsFormat? "
        },
        {
            "id": "comment-15266229",
            "author": "Adrien Grand",
            "date": "2016-05-02T08:39:27+0000",
            "content": "You could make a codec that uses a different stored fields format, but this codec would not be supported in terms of backward compatibility. So you would have to mave back to the default codec and then again to your custom codec on every upgrade. "
        }
    ]
}