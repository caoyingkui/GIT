{
    "id": "SOLR-1979",
    "title": "Create LanguageIdentifierUpdateProcessor",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.5",
            "4.0-ALPHA"
        ],
        "components": [
            "contrib - LangId",
            "update"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Language identification from document fields, and mapping of field names to language-specific fields based on detected language.\n\nWrap the Tika LanguageIdentifier in an UpdateProcessor.\n\nSee user documentation at http://wiki.apache.org/solr/LanguageDetection",
    "attachments": {
        "SOLR-1979-branch_3x.patch": "https://issues.apache.org/jira/secure/attachment/12497882/SOLR-1979-branch_3x.patch",
        "SOLR-1979.patch": "https://issues.apache.org/jira/secure/attachment/12465289/SOLR-1979.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Chris A. Mattmann",
            "id": "comment-12884070",
            "date": "2010-06-30T21:35:29+0000",
            "content": "I would look at the Language Identifier in Tika (which is based on the Nutch work) as it is likely to be the one that is more maintained going forward IMHO... "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12899568",
            "date": "2010-08-17T20:38:16+0000",
            "content": "I have implemented a first shot patch using the Tika LanguageIdentifier. It is unfortunately quite limited in features, and for short text segments, isReasonablyCertain() always returns false  Also, the number of languages supported is still quite low. But it works as a start, and then we can focus on improving the Tika code in future releases.\n\nI plan on putting the patch in contrib/extraction, since it depends on Tika. If I put it relative to main, Solr will not compile unless you put tika jar in lib. Agree? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12966732",
            "date": "2010-12-03T23:32:16+0000",
            "content": "First raw patch implementing language identification. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12966955",
            "date": "2010-12-05T14:33:28+0000",
            "content": "See http://wiki.apache.org/solr/LanguageDetection for the start of documentation.\n\nisReasonablyCertain() always returns false\n\nSee TIKA-568. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12966964",
            "date": "2010-12-05T15:17:51+0000",
            "content": "Simply allowing to set the threshold for isReasonablyCertain() is probably not enough to get a robust detection. This is because the distance measure is very sensitive to the length of the profiles in use. Thus, it is a bit dangerous to expose getDistance() as in TIKA-568, cause that distance measure is kind of an internal value, not very normalized and is bound to change in future versions of TIKA.\n\nSee TIKA-369 and TIKA-496.\n\nI think the right way to go is solving these two issues first. By fixing so that getDisance() is not biased towards profile length, we can make a new isReasonablyCertain() implementation taking into account the relative distance between first and second candidate languages... "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12966970",
            "date": "2010-12-05T15:27:30+0000",
            "content": "The idField input parameter is just used for decent logging if detection fails. It would be more elegant to get the id field name automatically through SolrCore... "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12966972",
            "date": "2010-12-05T15:39:32+0000",
            "content": "cause that distance measure is kind of an internal value, not very normalized and is bound to change in future versions of TIKA.\n\nwe can make a new isReasonablyCertain() implementation taking into account the relative distance between first and second candidate languages...\n\nI don't follow the logic: if its not very normalized then it seems like this approach doesnt tell you anything... language 1 could be uncertain,\n and language 2 is just completely uncertain, but that tells you nothing: isn't it like trying to determine if a good lucene search result score is \"certainly a hit\" and not really the right way to go?\n\nFor example: consider the case where the language isn't supported at all by Tika (i dont see a list of supported languages anywhere by the way!).\nIt would be good for us to know that the detection is uncertain at all... how relatively uncertain it is with regards to the next language, is not very important.\n\nI think its also important we be able to get this uncertainty or whatever different agnostic of the implementation.\nFor example, we should be able to somehow think of chaining detectors... \n\nIts really important to \"cheat\" and not use heuristics for languages that don't need them.\nFor example, disregarding some strange theoretical/historical cases, you can simply look at the unicode properties \nin the document to determine that its in the Greek language, as its basically the only modern language using the greek alphabet "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12966976",
            "date": "2010-12-05T16:15:59+0000",
            "content": "I took Jan's and Tommaso's patches and reworked them a bit.  It seems to me that there isn't much point in merely identifying the language if you aren't going to do something about it.  So, this patch builds on what Jan and Tommaso did and then will remap the input fields to new per language fields (note, we could make this optional).  I also tried to standardize the input parameters a bit.  I dropped the outputField setting and a number of other settings and I made the language detection to be per input field.  The basic gist of it is that if you input two fields: name, subject, it will detect the language of each field and then attempt to map them to a new field.  The new field is made by concatenating the original field name with \"_\" + the ISO 639 code.  For example, if en is the detected language, then the new field for name would be name_en.  If that field doesn't exist, it will fall back to the original field (i.e. name).\n\nLeft to do:\n\n\tFix the tests.  I don't like how we currently tests UpdateProcessorChains.  It should not require writing your own little piece of update mechanism.  You should be able to simply setup the appropriate configuration, hook it into an update handler and then hit that update handler.\n\tNeed to check the license headers, builds, etc.\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12966978",
            "date": "2010-12-05T16:26:34+0000",
            "content": "We really need to not be using ISO 639-1 here. \n\nFor example,\nIts not expressive enough, not differentiating between Simplified and Traditional chinese, yet SmartChineseAnalyzer only works on Simplified.\n\nI would like to see RFC 3066 instead "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967010",
            "date": "2010-12-05T20:19:56+0000",
            "content": "I would like to see RFC 3066 instead\n\nYeah, that makes sense, however, I believe Tika returns 639. (Tika doesn't recognize Chinese yet at all).  One approach is we could normalize, I suppose.  Another is to fix Tika.  I'd really like to see Tika support more languages, too.\n\nLonger term, I'd like to not do the fieldName_LangCode thing at all and instead let the user supply a string that could have variable substitution if they want, something like fieldName_${langCode}, or it could be ${langCode}_fieldName or it could just be another literal. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967011",
            "date": "2010-12-05T20:21:20+0000",
            "content": "Another thought, here, is that, over time, this class becomes a base class and it becomes easy to replace the language detection piece, that way one gets all the infrastructure of this class, but can plugin their own detection.  In fact, I'm going to do that right now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12967016",
            "date": "2010-12-05T20:57:01+0000",
            "content": "The new field is made by concatenating the original field name with \"_\" + the ISO 639 code. \n\nThis could be problematic given a large set of language codes since they could collide with existing dynamic field definitions.\nPerhaps something with \"text\" in the name also?\n\nPerhaps fieldName_${langCode}Text\n\nExamples:\nname_enText\nname_frText\n\nIt would probably also be nice to be able to map a number of languages to a single field.... say you have a single analyzer that can handle CJK, then you may want that whole collection of languages mapped to a single _cjk field.\n\nAnd just because you can detect a language doesn't mean you know how to handle it differently... so also have an optional catchall that handles all languages not specifically mapped.\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12967019",
            "date": "2010-12-05T21:03:46+0000",
            "content": "Yeah, that makes sense, however, I believe Tika returns 639.\n\nRight, but 639 is just a subset of 3066 etc. \n\nSo, ignore what tika does. its 639 identifiers are also valid 3066.\n\nOur API should at least be 3066, Java7/ICU already support BCP47 locale identifiers etc, so you get the normalization there for free.\n\n\nIt would probably also be nice to be able to map a number of languages to a single field.... say you have a single analyzer that can handle CJK, then you may want that whole collection of languages mapped to a single _cjk field.\n\nAnd just because you can detect a language doesn't mean you know how to handle it differently... so also have an optional catchall that handles all languages not specifically mapped.\n\nBoth of these are good reasons why we must avoid 639-1.\nWe should be able to use things like macrolanguages and undetermined language.\n\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12967032",
            "date": "2010-12-05T22:05:11+0000",
            "content": "@Robert: Yes, there must be a way to tell whether or not the language even has a profile, through some well defined method. It's not important HOW we improve detection certainty, but comparing the top n distances could help. I'm also a fan of including other metrics than profile similarity if that can help, however for unique scripts that will automatically be covered by profile similarity. Detailed solution discussions should continue in TIKA-369.\n\nMacro languages: See TIKA-493\n\nIt makes sense to allow for detecting languages outside 639-1, and I believe RFC3066 and BCP47 are both re-using the 639 codes, so that if there is a 2-letter code for a language it will be used. 639-1 is what \"everyone\" already knows.\n\nIn general, improvements should be done in Tika space, then use those in Solr, thus building one strong language detection library.\n\n@Grant: I actually planned to do the regEx based field name mapping in a separate UpdateProcessor, to make things more flexible. Example:\n\n \n  <processor class=\"org.apache.solr.update.processor.LanguageFieldMapperUpdateProcessor\">\n    <str name=\"languageField\">language</str>\n    <str name=\"fromRegEx\">(.*?)_lang</str>\n    <str name=\"toRegEx\">$1_$lang</str>\n    <str name=\"notSupportedLanguageToRegEx\">$1_t</str>\n    <str name=\"supportedLanguages\">de,en,fr,it,es,nl</str>\n  </processor>\n\n \n\nYour thought of allowing to detect language for individual fields in one go is also interesting. I'd love to see metadata support in SolrInputDocument, so that one processor could annotate a @language on the fields analyzed. Then next processor could act on metadata to rename field...\n\n@Yonik: By allowing regex naming of field names, we give users a generic tool to avoid field name clashes, by picking the pattern.. Mapping multiple languages to same suffix also makes sense. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967046",
            "date": "2010-12-05T23:23:22+0000",
            "content": "@Grant: I actually planned to do the regEx based field name mapping in a separate UpdateProcessor, to make things more flexible\n\nI don't really see that it makes it any more flexible.  If it was a general purpose mapper, maybe, but since it is tied to the language field, why not just put in the language processor?  I've already got the method that choose the output field as a protected.  With that, one merely would need to extend it to provide an alternate method from what you have proposed. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967047",
            "date": "2010-12-05T23:29:06+0000",
            "content": "Here's a patch that passes the tests.  Note, I modified the Solr base test case to have some new methods to properly call update handlers and then validate the results. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967048",
            "date": "2010-12-05T23:30:11+0000",
            "content": "Note, the patch still needs more tests and needs to check headers, etc. as well as the better field mapping and the proper language support that Robert is talking about. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12967076",
            "date": "2010-12-06T03:26:39+0000",
            "content": "\nIt makes sense to allow for detecting languages outside 639-1, and I believe RFC3066 and BCP47 are both re-using the 639 codes, so that if there is a 2-letter code for a language it will be used. 639-1 is what \"everyone\" already knows.\n\nIn general, improvements should be done in Tika space, then use those in Solr, thus building one strong language detection library.\n\nyes they do, the 639-1 codes that tika outputs are also valid BCP47 codes \n\nbut in solr, when designing up front, i was just saying we shouldn't limit any abstract portion to 639-1 when another implementation might support 3066 or BCP47... we should make sure we allow that. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967186",
            "date": "2010-12-06T13:49:17+0000",
            "content": "but in solr, when designing up front, i was just saying we shouldn't limit any abstract portion to 639-1 when another implementation might support 3066 or BCP47... we should make sure we allow that.\n\nAgreed.The only thing we are doing now is using the language that the language detector returns as part of the field name.  Both of these steps are easily overridable.  Both also rely on those fields existing.\n\nThis could be problematic given a large set of language codes since they could collide with existing dynamic field definitions.\n\nYonik, I wasn't planning on relying on dynamic fields necessarily.  It may make sense to have users either predeclare the variations.\n\nAll in all, I would like to see Solr have better support for languages in both the schema and the config.  In my experience, in apps that have to support a lot of languages, there is a lot of redundancy in both the schema and the config. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12967191",
            "date": "2010-12-06T14:08:41+0000",
            "content": "Agreed.The only thing we are doing now is using the language that the language detector returns as part of the field name. Both of these steps are easily overridable. Both also rely on those fields existing.\n\n\"Easily overridable\" does not solve the problem!\n\nPlease don't commit this, its so easy to just change the code, variable names, documentation here to say these interfaces are BCP47 language ids.\n\nWe should not be using 639-1 codes in any APIs!!!!!!! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12967201",
            "date": "2010-12-06T14:37:03+0000",
            "content": "Both also rely on those fields existing.\n\nI don't think this check should be at \"runtime\" either.\n\nWhat if you are indexing lots of documents and suddenly you encounter a thai document (or mis-detected as Thai!) and the whole thing fails?\n\nCan't we validate the output mapping (and log it!) at initialization time? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12967204",
            "date": "2010-12-06T14:42:47+0000",
            "content": "Yonik, I wasn't planning on relying on dynamic fields necessarily. It may make sense to have users either predeclare the variations.\n\nSure, but the problem was the ease by which a generated field of originalname_${langcode} could clash with existing fields (regardless of if they are dynamic fields) due to there being many different language codes.\n\nIf we use regex naming as Jan suggests (or another configurable mechanism) then the issue comes down to what we configure by default or by example. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12967211",
            "date": "2010-12-06T15:03:21+0000",
            "content": "@Grant: \"I dropped the outputField setting and a number of other settings\"\n\nThere should be a way to output the language for the whole document to some field as some applications need to filter on language.\n\nI like making most things configurable, but with good defaults which fits most needs. The default could be to detect a document wide langauge from all input fields and output this to a \"language_s\" field, unless you specify params docLangInputFields=f1,f2.. and docLangOutputField=nn. Likewise make it easy to disable field renaming. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967214",
            "date": "2010-12-06T15:17:10+0000",
            "content": "There should be a way to output the language for the whole document to some field as some applications need to filter on language.\n\nThere is.  It's the langField.\n\nCan't we validate the output mapping (and log it!) at initialization time?\n\nTo some extent, but users can also pass it in.  \n\nWe should not be using 639-1 codes in any APIs!!!!!!!\n\nI'll update the patch. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12967215",
            "date": "2010-12-06T15:22:26+0000",
            "content": "Removes mentions of ISO 639. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12967271",
            "date": "2010-12-06T17:33:47+0000",
            "content": "In skimming the current patch, it looks like fields get mapped no matter what.  What if I just want the language detected and added as another field, but no field mapping desired?  (one might have decent enough analysis already on a general \"title\" field for example that it doesn't need to be mapped to anything else at all) \n\nAlso, if there are multiple input fields, the current patch would create multiple language field values requiring that field to be multi-valued.  Is the goal here to identify a single language for a document?  Or a separate language value for each of the input fields (which seems odd to me)?\n\nOn field name mapping, maybe we want to have a generic concept of a FieldNameMapper such that various implementations could be plugged in rather than having to subclass the update processor? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12968445",
            "date": "2010-12-06T22:14:59+0000",
            "content": "In skimming the current patch, it looks like fields get mapped no matter what. What if I just want the language detected and added as another field, but no field mapping desired?\n\nYeah, that's sort of in line with my:\nAnd just because you can detect a language doesn't mean you know how to handle it differently... so also have an optional catchall that handles all languages not specifically mapped.\n\nSo for all unmapped languages, you may want to map to a single generic field, or not map at all (leave field as is).\nI guess it also depends on the general strategy... if you are detecting language on the \"body\" field, are we using a copyField type approach and only storing the body field while indexing as body_enText, or are we moving the field from \"body\" to \"body_enText\"?\n\nAlso, if there are multiple input fields, the current patch would create multiple language field values requiring that field to be multi-valued. Is the goal here to identify a single language for a document?\n\nI could see both making sense. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12968528",
            "date": "2010-12-07T01:07:49+0000",
            "content": "So for all unmapped languages, you may want to map to a single generic field, or not map at all (leave field as is).\n\nIt currently leaves it in the original field.\n\nAlso, if there are multiple input fields, the current patch would create multiple language field values requiring that field to be multi-valued. Is the goal here to identify a single language for a document? Or a separate language value for each of the input fields (which seems odd to me)?\n\nCurrent patch requires multivalued language field.  I figure the main thing you want the lang. field for is faceting and filtering, but it can be changed.  As for the broader goal, I think it makes sense to detect languages per field and not per document.  In other words, you can have multiple languages in a single document. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12968576",
            "date": "2010-12-07T05:28:46+0000",
            "content": "If a list of fields (by name) is mapped into a corresponding parallel identified language code field, do we leave it up to search clients to also know the list of field names to jive a field (say title) with its identified language?\n\nA language field shouldn't have to be multivalued - it just doesn't match the domain model of many search applications where there will only ever be one and only one language per document. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12968582",
            "date": "2010-12-07T05:52:44+0000",
            "content": "Oh, and don't get me wrong, I get the multivalued language per document need too, here.  Anyway, it'll be easy enough add support for this to be controlled through configuration.  In single language per doc mode, basically concatenate all of the fields specified and detect on that and map into a singled value language field.  Language-per-field I get too, of course... just depends on the domain being modeled and in my experience I've seen apps designed both ways.  Neither way is the one true way, it just depends. \n\nAnd of course Muir is smirking and saying \"heck, you have multiple languages within a field often too, so we need to account for that somehow too\".  But probably not here, yet. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12968627",
            "date": "2010-12-07T08:31:20+0000",
            "content": "Allow for both a \"language\" field and a \"languages\" (multivalued) field.\nIf fields are mapped, the new name reflect the language, so I don't know if we need a field->lang mapping.\nHowever, have you considered extending the document model to allow metadata per field? Then @language would be a valid field metadata, mostly as a means for later processing to pick up and act on. This can be a valuable mechanism for other inter processor communication as well as to pass info between document centric processing and Analysis. "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-12968633",
            "date": "2010-12-07T08:52:53+0000",
            "content": "However, have you considered extending the document model to allow metadata per field? Then @language would be a valid field metadata, mostly as a means for later processing to pick up and act on. This can be a valuable mechanism for other inter processor communication as well as to pass info between document centric processing and Analysis.\n\nI've also thought about this option and it sounds somehow reasonable but I think that it'd be a very huge change on the API; so from one point of view I like the idea but from another standpoint I think it could lead to a proliferation of @metadata.\nSo in the end I've not a strong opinion on that but I also have to say that I've seen such customizations in a production environment to leverage per field metadata.\n\nRegarding per field and per document language fields I think that a document language field could be handled with two fixed strategies/policies (that can be also extended):\n\n\trestrictive strategy - if different languages result to be mapped inside the document language field than say that document language is, for example, \"x-unspecified\"\n\tsimple strategy - map all the retrieved languages (per field) inside the document language field as different values (so multivalued=\"true\")\n\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12968748",
            "date": "2010-12-07T14:22:37+0000",
            "content": "I'm going to be out of pocket for the next week.  If someone can put the field mapping stuff up, then I think we will have the basis for a good first pass at this, which we can then iterate on.  I also think we need to get together and add a bunch more languages to Tika b/c it is pretty unacceptable to not have, at a minimum, support for the big Asian languages of CJK. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12968753",
            "date": "2010-12-07T14:27:13+0000",
            "content": "I also think we need to get together and add a bunch more languages to Tika b/c it is pretty unacceptable to not have, at a minimum, support for the big Asian languages of CJK.\n\nWhat languages does tika support in its identifier? I couldnt find an actual list only a ref to Europarl (http://www.statmt.org/europarl/), is it just those languages?\n\nAlso is there docs on whats necessary (legally and technically) to contribute a new profile... is just recording ngrams from creative commons text acceptable? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12968757",
            "date": "2010-12-07T14:39:07+0000",
            "content": "Have a look at http://tika.apache.org/0.8/detection.html\n\nReally, though, you need to dig into the Tika class: LanguageIdentifier.  Adding languages, AFAICT, involves building the model accordingly and then letting Tika know about it via a properties file. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12968760",
            "date": "2010-12-07T14:45:37+0000",
            "content": "Have a look at http://tika.apache.org/0.8/detection.html\n\nThat page does not have a list of languages. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12968777",
            "date": "2010-12-07T15:12:27+0000",
            "content": "Sorry, you are right.  See http://svn.apache.org/repos/asf/tika/trunk/tika-core/src/main/resources/org/apache/tika/language/tika.language.properties \n\n\nname.da=Danish\nname.de=German\nname.et=Estonian\nname.el=Greek\nname.en=English\nname.es=Spanish\nname.fi=Finnish\nname.fr=French\nname.hu=Hungarian\nname.is=Icelandic\nname.it=Italian\nname.nl=Dutch\nname.no=Norwegian\nname.pl=Polish\nname.pt=Portuguese\nname.ru=Russian\nname.sv=Swedish\nname.th=Thai\n\nKind of random that Thai is thrown in there! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12968786",
            "date": "2010-12-07T15:25:26+0000",
            "content": "Kind of random that Thai is thrown in there!\n\nI agree, i tend to detect thai by the characters being between U+0E00 and U+0E7F.\n\nanyway, if we add more languages it would be good if one of us could document the process, because many important ones are missing. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12968806",
            "date": "2010-12-07T16:28:18+0000",
            "content": "Discussion on the process for adding language profiles to TIKA should be continued in TIKA-546\n\nI have a plan to add profiles for the Norwegian and Sami languages when time allows: TIKA-491 TIKA-492 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12968813",
            "date": "2010-12-07T16:39:41+0000",
            "content": "I have a plan to add profiles for the Norwegian and Sami languages when time allows: TIKA-491 TIKA-492\n\nDid you plan to also upgrade tika from 639-1 for the Sami languages? the only 639-1 code i see is \"se\" but this seems to be appropriate only for North Sami. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12968820",
            "date": "2010-12-07T16:56:33+0000",
            "content": ">>I have a plan to add profiles for the Norwegian and Sami languages when time allows: TIKA-491 TIKA-492\n>Did you plan to also upgrade tika from 639-1 for the Sami languages? the only 639-1 code i see is \"se\" but this seems to be appropriate only for North Sami.\n\nExactly. That's one example which will need a wider range of codes. I was planning to use 639-2 for those that do not have a 2-letter code, but BCP47 it will be now (although the end result may be more or less the same)\n\nWe also need to detect whether a language is part of a macro language, and add both to languages multivalue field, because it should be possible to filter on Norwegian (no) without specifying both nn and nb, and also for sami (smi) without specifying all of the specific languages. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12968827",
            "date": "2010-12-07T17:16:05+0000",
            "content": "We also need to detect whether a language is part of a macro language, and add both to languages multivalue field, because it should be possible to filter on Norwegian (no) without specifying both nn and nb, and also for sami (smi) without specifying all of the specific languages.\n\nmacrolangs: http://www.sil.org/iso639-3/iso-639-3-macrolanguages_20100128.tab\ncollections: http://www.loc.gov/standards/iso639-5/iso639-5.tab.txt "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12969138",
            "date": "2010-12-08T02:01:02+0000",
            "content": "A use case for multi-language fields: PDFs with different languages in different columns.  "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12969140",
            "date": "2010-12-08T02:03:35+0000",
            "content": "About Thai: there is a lot of South and East Asian language text out there written in phonetic USASCII, especially older pre-Unicode. Samples of these texts from different languages have ngram profiles just as distinct as the European languages.\n\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12969404",
            "date": "2010-12-08T18:15:30+0000",
            "content": "What about leveraging payloads (we can output term|payload strings to the payload field type) for associating languages with fields? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12971322",
            "date": "2010-12-14T17:19:33+0000",
            "content": "What about leveraging payloads (we can output term|payload strings to the payload field type) for associating languages with fields? \n\nYeah, that could be used with mixed language text (or a marker token).  \n\nJan, do you have any updates to the patch?  I'd like to move forward with the basic functionality at least, but I still think we need the field mapping stuff, or we should punt all field mapping stuff to another processor.  WDYT? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12971338",
            "date": "2010-12-14T17:43:44+0000",
            "content": "\nJan, do you have any updates to the patch? I'd like to move forward with the basic functionality at least, but I still think we need the field mapping stuff, or we should punt all field mapping stuff to another processor. WDYT?\n\nI don't have any updates.\n\nKeep it basic in first version. Allow for per-document and per-field detection.\n\nMake field-mapping configurable and optional (default off), allowing people to chain in their own mapper downstream if they choose.\n\nMixed-language per field is a different beast and should be dealt with to later. Probably requires analysis changes as well if we want analyzers to pick up language from payloads or something.\n\nMy 2 cents "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-12971400",
            "date": "2010-12-14T20:24:27+0000",
            "content": "Keep it basic in first version. Allow for per-document and per-field detection. Make field-mapping configurable and optional (default off), allowing people to chain in their own mapper downstream if they choose.\n\nI agree, this sounds good for a basic implementation. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13043448",
            "date": "2011-06-03T16:27:20+0000",
            "content": "Continuing on this implementing the ideas above... "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13053223",
            "date": "2011-06-22T12:51:54+0000",
            "content": "New version. Example of accepted params:\n\n\n <processor class=\"org.apache.solr.update.processor.LanguageIdentifierUpdateProcessorFactory\">\n   <defaults>\n     <str name=\"langid\">true</str>\n     <str name=\"langid.fl\">title,subject,text,keywords</str>\n     <str name=\"langid.langField\">language_s</str>\n     <str name=\"langid.langsField\">languages</str>\n     <str name=\"langid.overwrite\">false</str>\n     <float name=\"langid.threshold\">0.5</float>\n     <str name=\"langid.whitelist\">no,en,es,dk</str>\n     <str name=\"langid.map\">true</str>\n     <str name=\"langid.map.fl\">title,text</str>\n     <bool name=\"langid.map.overwrite\">false</bool>\n     <bool name=\"langid.map.keepOrig\">false</bool>\n     <bool name=\"langid.map.individual\">false</bool>\n     <str name=\"langid.map.individual.fl\"></str>\n     <str name=\"langid.fallbackFields\">meta_content_language,lang</str>\n     <str name=\"langid.fallback\">en</str>\n   </defaults>\n </processor>\n\n\n\nThe only mandatory parameter is langid.fl\nTo enable field name mapping, set langid.map=true. It will then map field names for all fields in langid.fl. If the set of fields to map is different from langid.fl, supply langid.map.fl. Those fields will then be renamed with a language suffix equal to the language detected from the langid.fl fields.\n\nIf you require detecting languages separately for each field, supply langid.map.individual=true. The supplied fields will then be renamed according to detected language on an individual basis. If the set of fields to detect individually is different from the already supplied langid.fl or langid.map.fl, supply langid.map.individual.fl. The fields listed in langid.map.individual.fl will then be detected individually, while the rest of the mapping fields will be mapped according to global document language. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13053227",
            "date": "2011-06-22T12:57:07+0000",
            "content": "One question regarding the JUnit test: I now use\n\nassertU(commit());\n\n\nHow can I add update request params to this commit? To select another update chain from different tests, I'd like to add update params on the fly, e.g.:\n\nassertU(commit(), \"update.chain=langid2\");\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13055228",
            "date": "2011-06-26T23:13:22+0000",
            "content": "Fixed threshold so that Tika distance 0.1 gives certainty 0.5 and distance 0.02 gives certainty 0.9. The default threshold of 0.5 now works pretty well, at least for the tests...\n\nNew parameters:\nField name mapping is now configurable to user defined pattern, so to map ABC_title to title_<lang>, you set:\n\n&langid.map.pattern=ABC_(.*)\n&langid.map.replace=$1_{lang}\n\n\nA parameter to map multiple detected languages to same field regex. I.e. to map both Japanese, Korean and Chinese texts to a field *_cjk, do:\n\nlangid.map.lcmap=jp:cjk zh:cjk ko:cjk\n\nTurn off validation of field names against schema (useful if you want to rename or delete fields later in the UpdateChain):\n\n&langid.enforceSchema=false\n\n\nOther changes\nRemoved default on langField, i.e. if langField is not specified, the detected language will not be written anywhere. A typical minimal config for only detecting language and writing to a field is now:\n\n<processor class=\"org.apache.solr.update.processor.LanguageIdentifierUpdateProcessorFactory\">\n   <defaults>\n     <str name=\"langid.fl\">title,subject,text,keywords</str>\n     <str name=\"langid.langField\">language_s</str>\n   </defaults>\n</processor>\n\n\n\nAlso added multiple other languages to the tests. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13076259",
            "date": "2011-08-02T15:14:51+0000",
            "content": "This has been tested on a real, several hundred thousand docs dataset, including HTML, office docs and multiple other formats and it works well.\n\nI'd like some more pairs of eyes on this however.\n\nOne thing which is less than perfect is that the threshold conversion from Tika currently parses out the (internal) distance value from a String, in lack of a getDistance() method (TIKA-568). This is a bit of a hack, but I argue it's a beneficial one since we can now configure langid.threshold to something meaningful for our own data instead of the preset binary isReasonablyCertain(). As we also normalize to a value between 0-1, we abstract away the TIKA implementation detail, and are free to use any improved distance measures from TIKA in the future e.g. as a result of TIKA-369, or even plug in a non-Tika identifier or a hybrid solution. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13078710",
            "date": "2011-08-03T12:24:15+0000",
            "content": "Updated to latest trunk, simplified build file, added clean target "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13101605",
            "date": "2011-09-09T22:19:36+0000",
            "content": "Moving to 3.5 "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13101612",
            "date": "2011-09-09T22:44:36+0000",
            "content": "I'm impressed! This is a lot of work and empirical testing for a difficult problem.\n\nComments:\nThere are a few parameters that are true/false, but in the future you might want a third answer. It might be worth making the decision via a keyword so you can add new keywords later.\n\nAbout the multiple languages in one field problem: you can't solve everything at once. The other document analysis components like UIMA should be able to identify parts of documents, and then you use this on one part at a time. This is the point of a modular toolkit: you combine the tools to solve advanced problems. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102374",
            "date": "2011-09-11T23:20:58+0000",
            "content": "An updated documentation of the Processor is now at http://wiki.apache.org/solr/LanguageDetection\n\n@Lance: What params were on your mind as candidates for keyword instead of true/false, and for what potential future reasons? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13102520",
            "date": "2011-09-12T09:41:35+0000",
            "content": "Hi Jan,\n\nCan we also use the mapping feature without detection? Our detection is done in a Nutch cluster so we already identified many millions of docs.\n\nThanks "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102573",
            "date": "2011-09-12T12:01:14+0000",
            "content": "@Markus: Sure. If you put your pre-known language code in the same field configured in langid.langField and use langid.overwrite=false, you will obtain that behavior. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13102578",
            "date": "2011-09-12T12:11:39+0000",
            "content": "Hi. This is not what i understood from reading the wiki doc. Will the update processor skip detection with these settings? It's rather costly on many docs.\n\nAnyway, this is great work already, thanks! "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102646",
            "date": "2011-09-12T13:29:51+0000",
            "content": "Yep, it will skip detection if the field defined in langid.langField is not emtpty and langid.overwrite==false "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102647",
            "date": "2011-09-12T13:32:08+0000",
            "content": "Patch updated to fit new directory structure, updated comments to point to Wiki doc.\n\nAlso optimized regex, now pre-compiling patterns instead of using String.replace directly. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102714",
            "date": "2011-09-12T14:58:51+0000",
            "content": "New patch with these improvements:\n\n\n\tNow also allows config at first level, without <lst name=\"default\">\n\tAdded langid to example schema (commented out), so it is really easy to demonstrate\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13102723",
            "date": "2011-09-12T15:16:05+0000",
            "content": "Any changes you'd like before committing this? Lance, what config param changes did you have in mind? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13107810",
            "date": "2011-09-19T13:26:19+0000",
            "content": "Some further improvements:\n\n\tDefault fallback language if none set is now \"\" to avoid nullpointer exception\n\tAll individually detected languages are now added to \"langsField\" array\n\tMore tests\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13107850",
            "date": "2011-09-19T14:20:14+0000",
            "content": "Added link to Wiki in example update chain in solrconfig "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13107867",
            "date": "2011-09-19T14:37:45+0000",
            "content": "Question: Since I plan to commit this for both 3.x and 4.x, I will be adding the CHANGES entry under 3.5 section, also for TRUNK. I know there have been some discussion around where to log changes, but as long as 4.0 is not released before 3.5, it will always be true that the feature was released in 3.5 and exists for all later revisions, not? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13109326",
            "date": "2011-09-21T07:58:18+0000",
            "content": "Fixed java.lang.IndexOutOfBoundsException bug in resolveLanguage() when no languages detected. Added more corner case tests. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13117730",
            "date": "2011-09-29T23:17:16+0000",
            "content": "New patch:\n\n\tAdded contrib folders to eclipse dot.classpath\n\tAdded javadoc entries to build.xml\n\tFixed Javadoc errors\n\tUpgraded test case to use schema v1.4\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13121428",
            "date": "2011-10-05T20:16:05+0000",
            "content": "Added final patches which will be committed now. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13121433",
            "date": "2011-10-05T20:23:13+0000",
            "content": "Finally committed this long-lived issue "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13121569",
            "date": "2011-10-05T22:58:11+0000",
            "content": "Nice! Great feature to get in - thanks guys. "
        },
        {
            "author": "T Jake Luciani",
            "id": "comment-13124343",
            "date": "2011-10-10T18:04:36+0000",
            "content": "build on 3x branch still failing because solr/contrib/langid/src/java/overview.html was only committed to trunk. This file needs to be added to branch_3x as well. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13124366",
            "date": "2011-10-10T18:26:13+0000",
            "content": "Fixed overview.html in branch "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13157835",
            "date": "2011-11-27T12:36:02+0000",
            "content": "Bulk close after 3.5 is released "
        }
    ]
}