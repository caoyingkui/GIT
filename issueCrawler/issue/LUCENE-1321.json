{
    "id": "LUCENE-1321",
    "title": "Highlight fragment does not extend to maxDocCharsToAnalyze",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.\n\nFor example, consider the phrase \"this is a text with searchterm in it\". \"In\" and \"it\" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for \"searchterm\" gives a fragment ending in \"searchterm\". The expected behaviour is to have \"in it\" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered.",
    "attachments": {
        "LUCENE-1321.patch": "https://issues.apache.org/jira/secure/attachment/12384938/LUCENE-1321.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-06-30T06:23:16+0000",
            "content": "Patch changing \"text.length()< maxDocCharsToAnalyze\" to \"text.length()<= maxDocCharsToAnalyze\" and adding a unit test to verify this behaviour. ",
            "author": "Lars Kotthoff",
            "id": "comment-12609161"
        },
        {
            "date": "2008-06-30T12:34:22+0000",
            "content": "Thanks Lars. Nice catch - not an easy spot <g> Looks good to me. When I get a few free minutes I'll go over it a bit more, but on first inspection, certainly looks like the right fix and all tests pass. ",
            "author": "Mark Miller",
            "id": "comment-12609220"
        }
    ]
}