{
    "id": "LUCENE-3767",
    "title": "Explore streaming Viterbi search in Kuromoji",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I've been playing with the idea of changing the Kuromoji viterbi\nsearch to be 2 passes (intersect, backtrace) instead of 4 passes\n(break into sentences, intersect, score, backtrace)... this is very\nmuch a work in progress, so I'm just getting my current state up.\nIt's got tons of nocommits, doesn't properly handle the user dict nor\nextended modes yet, etc.\n\nOne thing I'm playing with is to add a double backtrace for the long\ncompound tokens, ie, instead of penalizing these tokens so that\nshorter tokens are picked, leave the scores unchanged but on backtrace\ntake that penalty and use it as a threshold for a 2nd best\nsegmentation...",
    "attachments": {
        "LUCENE-3767.patch": "https://issues.apache.org/jira/secure/attachment/12514048/LUCENE-3767.patch",
        "compound_diffs.txt": "https://issues.apache.org/jira/secure/attachment/12514836/compound_diffs.txt",
        "SolrXml-5498.xml": "https://issues.apache.org/jira/secure/attachment/12516300/SolrXml-5498.xml",
        "LUCENE-3767_branch_3x.patch": "https://issues.apache.org/jira/secure/attachment/12517372/LUCENE-3767_branch_3x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-02-10T10:52:05+0000",
            "content": "New patch.\n\nI cleaned up some silly nocommits, got extended mode and user dict working, added a few more nocommits.\n\nAll tests now pass if you set KuromojiTokenizer2.DO_OUTPUT_COMPOUND to false, which does the same score penalty logic as the current tokenizer. ",
            "author": "Michael McCandless",
            "id": "comment-13205355"
        },
        {
            "date": "2012-02-11T21:06:06+0000",
            "content": "I created a branch for this issue: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3767 ",
            "author": "Michael McCandless",
            "id": "comment-13206266"
        },
        {
            "date": "2012-02-16T17:52:49+0000",
            "content": "I think the branch is ready to land... I'll post an applyable patch\nsoon.\n\nIn Mode.SEARCH the tokenizer produces the same tokens as current\ntrunk.\n\nThe only real end-user visible change is the addition of\nMode.SEARCH_WITH_COMPOUNDS, which can produce two paths (compound\ntoken + its segmentation).  This mode uses the new\nPositionLengthAttribute to record how \"long\" the compound token is.\n\nIn this mode, the Viterbi search first runs without penalties, but\nthen, if a too-long token (a token where the penalty would have been >\n0) is in the best path, we effectively re-run the Viterbi under that\ncompound token, this time with penalties included.  If this results in\na different backtrace, we add that into the output tokens as well.\n\nNote that this will not produce congruent results as Mode.SEARCH,\nbecause the 2nd segmentation runs \"in context\" of the best path,\nmeaning the chosen best wordID before and after the compound token are\n\"enforced\" in the 2nd segmentation.  Sometimes this results in still\npicking only the compound token where trunk today would have split it\nup.  From TestQuality, the total number of edits was 4418 vs trunk's\n4828.\n\nI didn't explore this, but, we may want to use harsher penalties in\nSEARCH_WITH_COMPOUNDS mode, ie, since we're going to output the\ncompound as well we may as well \"try harder\" to produce the 2nd best\nsegmentation.\n\nI left the default mode as Mode.SEARCH... maybe if we can somehow\nrun some relevance tests we can make the default SEARCH_WITH_COMPOUNDS.\nBut it'd also be tricky at query time...\n\nIt looks like the rolling Viterbi is a bit faster (~16%: 1460\nbytes/msec vs 1700 bytes/msec on TestQuality.testSingleText). ",
            "author": "Michael McCandless",
            "id": "comment-13209540"
        },
        {
            "date": "2012-02-16T18:25:11+0000",
            "content": "Applyable patch. ",
            "author": "Michael McCandless",
            "id": "comment-13209579"
        },
        {
            "date": "2012-02-16T18:34:39+0000",
            "content": "Attaching a file showing the segmentation change when you run with\nSEARCH_WITH_COMPOUNDS vs SEARCH.\n\nLeft is SEARCH and right is SEARCH_WITH_COMPOUNDS.\n\nI believe these diffs are due to the forced \"best path context\" that\nwe re-tokenize a given compound word in.  Ie, the left / right wordIDs\nwere picked because they had best path with this compound word, but\nthen we force the segmentation to also use these words. ",
            "author": "Michael McCandless",
            "id": "comment-13209586"
        },
        {
            "date": "2012-02-21T14:41:14+0000",
            "content": "Mike,\n\nThanks a lot for this.  I'd meant to comment on this earlier and I'd like to look further into the details, but I really like your idea of running the Viterbi in a streaming fashion.\n\nKuromoji originally split input using two punctuation characters as this would be an articulation point in the lattice/graph in practice, but your idea is much more elegant and also faithful to the statistical model.\n\nAs for dealing with compounds, the penalization is a crude hack as you know, but it turns to work quite well in practice as many of the \"decompounds\" are known to the statistical model.  However, in cases where not not all of them are known, we sometimes get wrong decomounds.  I've done some analysis of these cases and it's possible to add more heuristics to deal with some that are obviouslt wrong, such a word starting with a long vowel sound in katakana.  This is a slippery slope that I'm reluctant to pursue...\n\nRobert mentioned earlier that he believes IPADIC could have been annotated with compounds as the documentation mentions them, but they're not part of the IPADIC model we are using.  If it is possible to get the decompounds from the training data (Kyoto Corpus), a better overall approach is then to do regular segmentation (normal mode) and then provide the decompounds directly from the token info for the compounds.  We might need to retrain the model and preserving the decompounds in order for this to work, but I think it is worth investigating. ",
            "author": "Christian Moen",
            "id": "comment-13212623"
        },
        {
            "date": "2012-02-23T00:59:08+0000",
            "content": "\nRobert mentioned earlier that he believes IPADIC could have been annotated with compounds as the documentation mentions them, but they're not part of the IPADIC model we are using. If it is possible to get the decompounds from the training data (Kyoto Corpus), a better overall approach is then to do regular segmentation (normal mode) and then provide the decompounds directly from the token info for the compounds. We might need to retrain the model and preserving the decompounds in order for this to work, but I think it is worth investigating.\n\nThe dictionary documentation for the original ipadic has the ability to hold compound data (not in mecab-ipadic though, so maybe it was never implemented?!), \nbut I don't actually see it in any implementations. So yeah, we would need to find a corpus containing compound information (and of course extend the file format\nand add support to kuromoji) to support that.\n\nHowever, would this really solve the total issue? Wouldn't that really only help for known kanji compounds... whereas most katakana compounds \n(e.g. the software engineer example) are expected to be OOV anyway? So it seems like, even if we ensured the dictionary was annotated for \nlong kanji such that we always used decompounded forms, we need a 'heuristical' decomposition like search-mode either way, at least for \nthe unknown katakana case?\n\nAnd I tend to like Mike's improvements from a relevance perspective for these reasons:\n\n\tkeeping the original compound term for improved precision\n\tpreventing compound decomposition from having any unrelated negative impact on the rest of the tokenization\n\n\n\nSo I think we should pursue this change, even if we want to separately train a dictionary in the future, because in that case, \nwe would just disable the kanji decomposition heuristic but keep the heuristic (obviously re-tuned!) for katakana? ",
            "author": "Robert Muir",
            "id": "comment-13214148"
        },
        {
            "date": "2012-02-23T01:13:58+0000",
            "content": "\nI've done some analysis of these cases and it's possible to add more heuristics to deal with some that are obviouslt wrong, such a word starting with a long vowel sound in katakana.  This is a slippery slope that I'm reluctant to pursue...\n\nI've wondered about that also at least for the unknown katakana case: (though I don't know all the rules that could be applied).\n\nAdding such heuristics isn't really unprecedented, in a way its very similar to the compounds/ package (geared towards german, etc)\nusing TeX hyphenation rules to restrict word splits to hyphenation breaks; and similar to DictionaryBasedBreakIterators in\nICU/your JRE that use orthographic rules in combination with a dictionary to segment southeast asian languages like Thai, and \nnot too far from simple rules like \"don't separate a base character from any combining characters that follow it\", or \"don't\nseparate a lead surrogate from a trail surrogate\" that you would generally use across all languages. ",
            "author": "Robert Muir",
            "id": "comment-13214159"
        },
        {
            "date": "2012-02-23T02:02:59+0000",
            "content": "I agree completely; we should definitely proceed with Mike's improvements.  A big +1 from me.\n\nI'm sorry if this wasn't clear.  My comments on search mode are unrelated and I didn't mean to confuse these with the improvements made.  None of this is necessary for Mike's improvements to be used, of course. ",
            "author": "Christian Moen",
            "id": "comment-13214196"
        },
        {
            "date": "2012-02-23T02:11:27+0000",
            "content": "Well my comments are only questions based on your previous JIRA comments,\nI don't really know anything about decompounding Japanese and my suggestions\nare only intuition, mostly based on patterns that have worked for other\nlanguages... so just consider it thinking out loud.\n\nI think your points on search mode are actually related here: though we can't \nreally have a plan its good to think about possible paths we might take in the \nfuture so that we don't lock ourselves out. ",
            "author": "Robert Muir",
            "id": "comment-13214201"
        },
        {
            "date": "2012-02-23T02:28:09+0000",
            "content": "Robert, some comments are below.\n\n\nAnd I tend to like Mike's improvements from a relevance perspective for these reasons:\n\n\n\tkeeping the original compound term for improved precision\n\tpreventing compound decomposition from having any unrelated negative impact on the rest of the tokenization\n\n\n\nVery good points.\n\n\nSo I think we should pursue this change, even if we want to separately train a dictionary in the future, because in that case, we would just disable the kanji decomposition heuristic but keep the heuristic (obviously re-tuned!) for katakana?\n\nI agree completely.\n\n\nThe dictionary documentation for the original ipadic has the ability to hold compound data (not in mecab-ipadic though, so maybe it was never implemented?!), but I don't actually see it in any implementations. So yeah, we would need to find a corpus containing compound information (and of course extend the file format and add support to kuromoji) to support that.\n\nHowever, would this really solve the total issue? Wouldn't that really only help for known kanji compounds... whereas most katakana compounds (e.g. the software engineer example) are expected to be OOV anyway? So it seems like, even if we ensured the dictionary was annotated for long kanji such that we always used decompounded forms, we need a 'heuristical' decomposition like search-mode either way, at least for the unknown katakana case?\n\nI've made an inquiry to a friend who did his PhD work at Prof. Matsumoto's lab at NAIST (where ChaSen was made) and I've made en inquiry regarding compound information and the Kyoto Corpus.\n\n\nYou are perfectly right that this doesn't solve the complete problem as unknown words can actually be compounds \u2013 unknown compounds.  The approach used today is basically adding all the potential decompounds the model knows about to the lattice and see if a short path can be found often in combination with an unknown word.\n\nWe get errors such as \u30af\u30a4\u30fc\u30f3\u30ba\u30b3\u30df\u30c3\u30af\u30b9 (Queen's Comics) becoming \u30af\u30a4\u30fc\u30f3\u3000\u30ba\u30b3\u30df\u30c3\u30af\u30b9 (Queen Zukuomikkusu) because \u30af\u30a4\u30fc\u30f3 (Queen) is known.\n\nI'll open up a separate JIRA for discussing search-mode improvements.  ",
            "author": "Christian Moen",
            "id": "comment-13214217"
        },
        {
            "date": "2012-02-23T02:34:26+0000",
            "content": "\nI left the default mode as Mode.SEARCH... maybe if we can somehow run some relevance tests we can make the default SEARCH_WITH_COMPOUNDS.\nBut it'd also be tricky at query time...\n\nMike, could you share some details on any query-time trickiness here?  Are you thinking about composing a query with both the compound and its parts/decompounds?  Thanks. ",
            "author": "Christian Moen",
            "id": "comment-13214227"
        },
        {
            "date": "2012-02-23T02:39:26+0000",
            "content": "\nMike, could you share some details on any query-time trickiness here? Are you thinking about composing a query with both the compound and its parts/decompounds? Thanks.\n\nI don't understand Mike's comment there either. positionIncrement=0 is really going to be treated like synonyms by the QP,\nso it shouldn't involve any trickiness. ",
            "author": "Robert Muir",
            "id": "comment-13214232"
        },
        {
            "date": "2012-02-24T16:09:17+0000",
            "content": "\nI left the default mode as Mode.SEARCH... maybe if we can somehow run some relevance tests we can make the default SEARCH_WITH_COMPOUNDS. But it'd also be tricky at query time...\n\nMike, could you share some details on any query-time trickiness here? Are you thinking about composing a query with both the compound and its parts/decompounds? Thanks.\n\nSorry, I was wrong about this!  Apparently QueryParser works fine if\nthe analyzer produces a graph (it just creates a\nsomewhat-scary-yet-works MultiPhraseQuery).\n\nSo I think we have no issue at query time... and I guess we should\ndefault to SEARCH_WITH_COMPOUNDS.\n\nI agree we can explore better tweaking the decompounding in a new\nissue. ",
            "author": "Michael McCandless",
            "id": "comment-13215713"
        },
        {
            "date": "2012-02-27T11:38:37+0000",
            "content": "New patch, making Mode.SEARCH always produce a graph output (ie no\nmore separate SEARCH_WITH_COMPOUNDS).  Mode.NORMAL and Mode.EXTENDED\nare as they were before...\n\nI think it's ready! ",
            "author": "Michael McCandless",
            "id": "comment-13217156"
        },
        {
            "date": "2012-02-28T09:40:09+0000",
            "content": "Mike,\n\nThanks a lot for this.  I've been taking this patch for a spin and I'm seeing some exceptions being thrown when indexing some Wikipedia test documents.\n\nI haven't had a chance to analyze this in detail, but when indexing the attached SolrXml-5498.xml I'm getting:\n\n\njava.lang.ArrayIndexOutOfBoundsException: -69\n\tat org.apache.lucene.util.RollingCharBuffer.get(RollingCharBuffer.java:98)\n\tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.computePenalty(KuromojiTokenizer.java:236)\n\tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.computeSecondBestThreshold(KuromojiTokenizer.java:227)\n\tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:910)\n\tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:567)\n\tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:394)\n\tat org.apache.lucene.analysis.kuromoji.TestKuromojiTokenizer.doTestBocchan(TestKuromojiTokenizer.java:567)\n\tat org.apache.lucene.analysis.kuromoji.TestKuromojiTokenizer.testBocchan(TestKuromojiTokenizer.java:528)\n\t...\n\n\n\nI believe you can reproduce this by changing TestKuromojiTokenizer.doTestBocchan() to read the attached SolrXml-5498.xml instead of bocchan.utf-8.\n ",
            "author": "Christian Moen",
            "id": "comment-13218026"
        },
        {
            "date": "2012-02-28T13:23:30+0000",
            "content": "Egads, I'll dig... thanks Christian. ",
            "author": "Michael McCandless",
            "id": "comment-13218154"
        },
        {
            "date": "2012-02-28T23:18:39+0000",
            "content": "New patch, fixing the exc Christian found: the 2nd best search was corrupting the bestIDX on the backtrace in the case where a compound wasn't selected.\n\nI also set a limit on the max UNK word length, and pulled the limits into static final private constants.\n\nI was able to parse the 2012/02/20 jaenwiki export with this (see commented out test case in TestKuromojiTokenizer).  I think it's ready (again!). ",
            "author": "Michael McCandless",
            "id": "comment-13218685"
        },
        {
            "date": "2012-02-29T09:15:20+0000",
            "content": "Thanks, Mike.\n\nI've tried the latest patch and things look fine here now.  I haven't had a chance to review the code changes yet, but I'm happy to do that over the next couple of days and commit this \u2013 unless you'd like to do that yourself.  (This would be my first commit.) ",
            "author": "Christian Moen",
            "id": "comment-13219015"
        },
        {
            "date": "2012-02-29T11:33:20+0000",
            "content": "Hi Christian, sure feel free to commit this!  Thanks  ",
            "author": "Michael McCandless",
            "id": "comment-13219115"
        },
        {
            "date": "2012-03-03T09:03:04+0000",
            "content": "Thanks, Mike.  I'll commit this to trunk tomorrow and backport to branch_3x as well. ",
            "author": "Christian Moen",
            "id": "comment-13221544"
        },
        {
            "date": "2012-03-03T14:50:51+0000",
            "content": "Thanks Christian! ",
            "author": "Michael McCandless",
            "id": "comment-13221599"
        },
        {
            "date": "2012-03-03T21:45:57+0000",
            "content": "+1 ",
            "author": "Robert Muir",
            "id": "comment-13221707"
        },
        {
            "date": "2012-03-04T14:36:57+0000",
            "content": "Committed to trunk with revision 1296805.\n\nWill backport to branch_3x and then mark as fixed. ",
            "author": "Christian Moen",
            "id": "comment-13221903"
        },
        {
            "date": "2012-03-07T06:34:42+0000",
            "content": "I've attached a patch for branch_3x and I'd be very thankful if Robert or Mike could have a quick look before I commit.\n\nThese are some comments and questions:\n\n\n\tI haven't backported the test for PositionLengthAttribute since the structure seems very different in branch_3x.  Perhaps this is something we could look to as we start using PositionLength in more of the other analyzers?\n\tThere will be some minor discrepancies between trunk and branch_3x with regards to Set<?> and CharArraySet types.  Perhaps we should use CharArraySet on across the board here in the future?\n\tDoes the svn:mergeinfo look okay in the patch?\n\n\n\nTests pass on branch_3x and I've verified that the new feature seems fine in a Solr build as well. ",
            "author": "Christian Moen",
            "id": "comment-13224025"
        },
        {
            "date": "2012-03-07T17:53:09+0000",
            "content": "I haven't backported the test for PositionLengthAttribute since the structure seems very different in branch_3x. Perhaps this is something we could look to as we start using PositionLength in more of the other analyzers?\n\nHmm you mean TestSimpleAttributeImpl?  I think that's fine.  We'll\nimprove testing of pos length as we make more tokenizers/filters\ngraphs...\n\nDoes the svn:mergeinfo look okay in the patch?\n\nHmm looks spooky... there was a trick for this... (and I know it's\nhard to merge back since the path changed in trunk).  Maybe do svn\npropdel svn:mergeinfo on all affected files, and then at the top level\nyou can just do an \"svn merge --record-only\" to update toplevel\nmergeinfo?\n\nThere will be some minor discrepancies between trunk and branch_3x with regards to Set<?> and CharArraySet types. Perhaps we should use CharArraySet on across the board here in the future?\n\nThis was from LUCENE-3765; I think it's OK to require\nCharArraySet to Kuromoji...\n\nLooks like the added files are not included in the patch (eg\nRollingCharBuffer)... if you're using svn >= 1.7, you can run \"svn\ndiff --show-copies-as-adds\" maybe?  Or, use\ndev-tools/scripts/diffSources.py... but I don't think it's\nreally necessary before committing...\n\nOtherwise patch looks great! ",
            "author": "Michael McCandless",
            "id": "comment-13224551"
        },
        {
            "date": "2012-03-09T18:14:59+0000",
            "content": "Patch looks good: KuromojiAnalyzer can take CharArraySet because its not yet released, the only reason\nits kinda funky in LUCENE-3765 is to avoid backwards compatibility breaks in 3.x. ",
            "author": "Robert Muir",
            "id": "comment-13226279"
        },
        {
            "date": "2012-03-10T14:38:43+0000",
            "content": "Thanks for the feedback.\n\nMike, you are right that the added files aren't included in the patch, but I believe they will be added on check-in. (I've verified that they're marked for addition, including RollingCharBuffer. Sorry for the confusion.)\n\nI believe the patch is correct with regards to svn:mergeinfo \u2013 only the root directory, lucene and solr should have them.\n\nI'll commit to branch_3x shortly. ",
            "author": "Christian Moen",
            "id": "comment-13226868"
        },
        {
            "date": "2012-03-10T14:55:31+0000",
            "content": "Committed revision 1299213 on branch_3x ",
            "author": "Christian Moen",
            "id": "comment-13226872"
        },
        {
            "date": "2012-03-10T15:56:21+0000",
            "content": "Confirmed this working in a branch_3x build.\n\nThanks, Mike and Robert!  ",
            "author": "Christian Moen",
            "id": "comment-13226881"
        }
    ]
}