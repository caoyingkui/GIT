{
    "id": "SOLR-4744",
    "title": "Version conflict error during shard split test",
    "details": {
        "affect_versions": "4.3",
        "status": "Closed",
        "fix_versions": [
            "4.3.1",
            "4.4"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "ShardSplitTest fails sometimes with the following error:\n\n\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state invoked for collection: collection1\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1 to inactive\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_0 to active\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_1 to active\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.873; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={wt=javabin&version=2} {add=[169 (1432319507166134272)]} 0 2\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.884; org.apache.solr.update.processor.LogUpdateProcessor; [collection1_shard1_1_replica1] webapp= path=/update params={distrib.from=http://127.0.0.1:41028/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {} 0 1\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.885; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={distrib.from=http://127.0.0.1:41028/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {add=[169 (1432319507173474304)]} 0 2\n[junit4:junit4]   1> ERROR - 2013-04-14 19:05:26.885; org.apache.solr.common.SolrException; shard update error StdNode: http://127.0.0.1:41028/collection1_shard1_1_replica1/:org.apache.solr.common.SolrException: version conflict for 169 expected=1432319507173474304 actual=-1\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:404)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)\n[junit4:junit4]   1> \tat org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:332)\n[junit4:junit4]   1> \tat org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:306)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n[junit4:junit4]   1> \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n[junit4:junit4]   1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n[junit4:junit4]   1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   1> \tat java.lang.Thread.run(Thread.java:679)\n[junit4:junit4]   1> \n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.886; org.apache.solr.update.processor.DistributedUpdateProcessor; try and ask http://127.0.0.1:41028 to recover\n\n\n\nThe failure is hard to reproduce and very timing sensitive. These kind of failures have always been seen right after \"updateshardstate\" action.",
    "attachments": {
        "SOLR-4744__no_more_NPE.patch": "https://issues.apache.org/jira/secure/attachment/12585972/SOLR-4744__no_more_NPE.patch",
        "SOLR-4744.patch": "https://issues.apache.org/jira/secure/attachment/12584859/SOLR-4744.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13655181",
            "date": "2013-05-11T07:02:02+0000",
            "content": "Consider the following scenario\n\n\tThe overseer collection processor asks overseer to update the state of parent to INACTIVE and the sub shards to ACTIVE\n\tThe parent shard leader receives an update request\n\tThe parent shard leader thinks that it is still the leader of an ACTIVE shard and therefore tries to send the request to the sub shard leaders (FROMLEADER update containing \"from.shard.parent\" param). This is done asynchronously so the client has already been given a success status.\n\tThe sub shard leader receives such a request but it's cluster state is already up to date and therefore rejects the update saying that it is already a leader and not in construction state any more.\n\tThe parent shard leader asks sub shard leader to recover which is basically no-op for sub shard leaders\n\tThe sub shard misses such a document update\n\n\n\nSOLR-4795 exposed the underlying problem clearly. The exceptions in the log on jenkins are now:\n\n[junit4:junit4]   1> INFO  - 2013-05-10 17:12:00.128; org.apache.solr.update.processor.LogUpdateProcessor; [collection1_shard1_1_replica1] webapp=/sx path=/update params={distrib.from=http://127.0.0.1:47193/sx/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {} 0 1\n[junit4:junit4]   1> INFO  - 2013-05-10 17:12:00.128; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp=/sx path=/update params={distrib.from=http://127.0.0.1:47193/sx/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {add=[296 (1434667890899943424)]} 0 1\n[junit4:junit4]   1> ERROR - 2013-05-10 17:12:00.129; org.apache.solr.common.SolrException; org.apache.solr.common.SolrException: Request says it is coming from parent shard leader but we are not in construction state\n[junit4:junit4]   1> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.doDefensiveChecks(DistributedUpdateProcessor.java:327)\n[junit4:junit4]   1> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.setupRequest(DistributedUpdateProcessor.java:232)\n[junit4:junit4]   1> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:394)\n[junit4:junit4]   1> \tat org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:100)\n[junit4:junit4]   1> \tat org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:246)\n[junit4:junit4]   1> \tat org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)\n[junit4:junit4]   1> \tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)\n[junit4:junit4]   1> \tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n[junit4:junit4]   1> \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n[junit4:junit4]   1> \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1832)\n[junit4:junit4]   1> \tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:656)\n[junit4:junit4]   1> \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:359)\n[junit4:junit4]   1> \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:155)\n[junit4:junit4]   1> \tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)\n\n\n\nThese happen right after the state is switched until both parent leader and sub shard leader have the latest cluster state.\n\nThe possible fixes are:\n\n\tCreate a new recovery strategy for sub shard replication\n\tReplicate to sub shard leader synchronously (before local update)\n\tSwitch parent shard to INACTIVE first, wait for it to receive the cluster state and then switch sub shards to ACTIVE \u2013 Clients would receive failures on updates for a short time but such failures should already be handled by clients (because of host failures), we should be okay. Sub shards failures must be handled so that we always end up with the shard range being available somewhere.\n\n\n\nThoughts? Yonik Seeley, Mark Miller, Anshum Gupta "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13657460",
            "date": "2013-05-14T20:23:48+0000",
            "content": "Nice job tracking that down.\n\nReplicate to sub shard leader synchronously (before local update)\n\nThis seems like the right fix (I had thought it was this way already).  This should include returning failure to the client of course. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13667234",
            "date": "2013-05-26T08:51:49+0000",
            "content": "Changes:\n\n\tNew syncAdd and syncDelete methods in SolrCmdDistributor which add/delete synchronously and propagate exceptions\n\tDistributedUpdateProcessor calls cmdDistrib.syncAdd inside versionAdd because that's the only place where we have the version and the full doc and an opportunity to do remote synchronous add before local add\n\tSimilarly cmdDistrib.syncDelete is called by versionDelete and doDeleteByQuery\n\tShardSplitTest tests for delete-by-id\n\n\n\nWith these changes, any exception while forwarding updates to sub shard leaders, will result in an exception being thrown to the client. The client can then retry the operation. \n\nA code review would be helpful.\n\nConsidering that without this fix, shard splitting can, in some cases, lead to data loss, we should add this to 4.3.1 "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13667683",
            "date": "2013-05-27T10:31:43+0000",
            "content": "Looks fine to me other than one small change which I don't think is a part of your patch but would be good if fixed.\n\nDistributedUpdateProcessor.updateAdd(): Line 404\n\n\n if (isLeader) \nUnknown macro: {   params.set(\"distrib.from\", ZkCoreNodeProps.getCoreUrl(       zkController.getBaseUrl(), req.getCore().getName()));   } \n\n   params.set(\"distrib.from\", ZkCoreNodeProps.getCoreUrl(\n       zkController.getBaseUrl(), req.getCore().getName())); "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13668310",
            "date": "2013-05-28T13:54:41+0000",
            "content": "Hmmm, can we get away with fixing this with a much less invasive change?\nWhat if we just send to sub-shards like we send to other replicas, and return a failure if the sub-shard fails (don't worry about trying to change the logic to send to a sub-shard before adding locally).  The shard is going to become inactive anyway, so it shouldn't matter if we accidentally add a document locally that goes on to be rejected by the sub-shard, right?\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13668335",
            "date": "2013-05-28T14:34:10+0000",
            "content": "What if we just send to sub-shards like we send to other replicas, and return a failure if the sub-shard fails (don't worry about trying to change the logic to send to a sub-shard before adding locally). The shard is going to become inactive anyway, so it shouldn't matter if we accidentally add a document locally that goes on to be rejected by the sub-shard, right?\n\nWhat happens with partial updates in that case? Suppose an increment operation is requested which succeeds locally but is not propagated to the sub shard. If the client retries, the index will have wrong values. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13668344",
            "date": "2013-05-28T14:51:18+0000",
            "content": "What happens with partial updates in that case? Suppose an increment operation is requested which succeeds locally but is not propagated to the sub shard.\n\nIf we're talking about failures due to the sub-shard already being active when it receives an update from the old shard who thinks it's still the leader, then I think we're fine.  This isn't a new failure mode, but just another way that the old shard can be out of date.  For example, once a normal update is received by the new shard, the old shard will be out of date anyway.\n\nIf the client retries, the index will have wrong values.\n\nIf the client retries to the same old shard that is no longer the leader, then the update will fail again because the sub-shard will reject it again?  We could perhaps return an error code suggesting that the client is using stale cluster state (i.e. re-read before trying the update again). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13668351",
            "date": "2013-05-28T15:01:12+0000",
            "content": "If we're talking about failures due to the sub-shard already being active when it receives an update from the old shard who thinks it's still the leader, then I think we're fine.\n\nYes, that's true. I was thinking of the general failure scenario but perhaps we can ignore it because both parent and sub shard leaders are on the same JVM? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13668426",
            "date": "2013-05-28T16:37:49+0000",
            "content": "Yes, that's true. I was thinking of the general failure scenario but perhaps we can ignore it because both parent and sub shard leaders are on the same JVM?\n\nYeah, I think that's best for now.  If it actually becomes an issue (which should be really rare), we could just cancel the split and maybe retry it from the start. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13668522",
            "date": "2013-05-28T18:22:10+0000",
            "content": "Okay, I'll put up a patch. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13671788",
            "date": "2013-05-31T20:02:00+0000",
            "content": "Changes:\n\n\tAdd and delete requests are processed after local operations but before distributed operations\n\n\n\nThis patch makes no changes to the versionAdd, versionDelete methods and is much less invasive. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13673498",
            "date": "2013-06-03T20:07:36+0000",
            "content": "Committed.\n\ntrunk r1489138\nbranch_4x 1489139\nlucene_solr_4_3 r1489141 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13673673",
            "date": "2013-06-03T22:14:09+0000",
            "content": "this change completlye broke almost every test of solr that doesn't use solrcloud...\n\n\n\norg.apache.solr.common.SolrException: java.lang.NullPointerException\n        at __randomizedtesting.SeedInfo.seed([E1CD32BA68ADB059:42A275854FFF05B6]:0)\n        at org.apache.solr.util.TestHarness.update(TestHarness.java:271)\n        at org.apache.solr.util.BaseTestHarness.checkUpdateStatus(BaseTestHarness.java:261)\n        at org.apache.solr.util.BaseTestHarness.validateUpdate(BaseTestHarness.java:231)\n        at org.apache.solr.SolrTestCaseJ4.checkUpdateU(SolrTestCaseJ4.java:481)\n        at org.apache.solr.SolrTestCaseJ4.assertU(SolrTestCaseJ4.java:460)\n        at org.apache.solr.SolrTestCaseJ4.assertU(SolrTestCaseJ4.java:454)\n        at org.apache.solr.SolrTestCaseJ4.clearIndex(SolrTestCaseJ4.java:827)\n        at org.apache.solr.BasicFunctionalityTest.testDefaultFieldValues(BasicFunctionalityTest.java:623)\n...\nCaused by: java.lang.NullPointerException\n        at\norg.apache.solr.update.processor.DistributedUpdateProcessor.doDeleteByQuery(DistributedUpdateProcessor.java:872)\n        at\norg.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:774)\n        at org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121)\n        at org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:346)\n        at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:277)\n        at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:173)\n        at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1843)\n        at org.apache.solr.servlet.DirectSolrConnection.request(DirectSolrConnection.java:131)\n        at org.apache.solr.util.TestHarness.update(TestHarness.java:267)\n\n\n\nInvetigating. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13673697",
            "date": "2013-06-03T22:32:27+0000",
            "content": "patch that seems to fix the NPE currenlty happening in 250+Solr tests w/o causing the shard splitting test to break  (still running full suite to be certain). "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13673747",
            "date": "2013-06-03T23:04:23+0000",
            "content": "FWIW... full test on trunk seem to pass with my patch, so i've start committing and backporting.\n\ni have no idea if this fix is \"correct' but it is certainly better then what we had.\n\nI'll be leaving this issue open for Shalin to review & correct as needed. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13673802",
            "date": "2013-06-03T23:47:31+0000",
            "content": "Committed revision 1489222.\nCommitted revision 1489224.\nCommitted revision 1489231.\n\ni have no idea if this fix is \"correct' but it is certainly better then what we had.\n\nI'll be leaving this issue open for Shalin to review & correct as needed. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13673931",
            "date": "2013-06-04T01:46:35+0000",
            "content": "Sorry for breaking the build. Lesson learned. I'm on a flight to Mumbai,  will take a look once I land in a couple of hours. \n\nThanks Hoss for fixing this.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13676370",
            "date": "2013-06-05T21:28:51+0000",
            "content": "Your changes look fine Hoss.\n\nIt's not clear to me why the forward to subshard needs to be synchronous in the original committed patch, but I guess that can always be revisited later as an optimization. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13677311",
            "date": "2013-06-06T17:56:01+0000",
            "content": "It's not clear to me why the forward to subshard needs to be synchronous in the original committed patch, but I guess that can always be revisited later as an optimization.\n\nIt does not need to be that way. The syncAdd and syncDelete method seemed to be the most straight forward way to achieve the goal. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13686925",
            "date": "2013-06-18T16:52:39+0000",
            "content": "Bulk close after 4.3.1 release "
        }
    ]
}