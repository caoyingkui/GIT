{
    "id": "SOLR-2444",
    "title": "Update fl syntax to support: pseudo fields, AS, transformers, and wildcards",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The ReturnFields parsing needs to be improved.  It should also support wildcards",
    "attachments": {
        "SOLR-2444-fl-parsing.patch": "https://issues.apache.org/jira/secure/attachment/12474649/SOLR-2444-fl-parsing.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-13011355",
            "date": "2011-03-25T18:59:27+0000",
            "content": "This patch takes Yoniks patch from SOLR-1566 and updates to trunk\n\ni don't really understand the proposed syntax, so will need some help "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13011379",
            "date": "2011-03-25T19:30:03+0000",
            "content": "adding missing file "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13011544",
            "date": "2011-03-26T04:04:23+0000",
            "content": "In SOLR-1566, we added this first draft at paramater parsing.  As is, we have a few things happening:\n\nMapping Field Names\n\nwith fl=id,score we get:\n\n<doc>\n    <str name=\"id\">GB18030TEST</str>\n    <float name=\"score\">1.0</float></doc>\n <doc>\n\n\n\nwith &fl=xxx=id,score\nwe get:\n\n<doc>\n  <float name=\"score\">1.0</float>\n  <str name=\"xxx\">GB18030TEST</str>\n</doc>\n\n\nid has been mapped to xxx\n\n\nDocTransformers\n\nAdded support to select transformers in the fl param.  See http://wiki.apache.org/solr/DocTransformers for more info (or help fill it in!)\n\n&fl=id,_explain_ will give:\n\n<doc>\n    <str name=\"id\">GB18030TEST</str>\n    <str name=\"_explain_\">1.0 = (MATCH) MatchAllDocsQuery, product of:\n  1.0 = queryNorm\n\n</str></doc>\n\n\n\nPassing argument to transformer\n\nwe can change the format with: &fl=id,_explain:nl_ \n\n<doc>\n    <str name=\"id\">GB18030TEST</str>\n    <lst name=\"_explain:nl_\">\n\n      <bool name=\"match\">true</bool>\n      <float name=\"value\">1.0</float>\n      <str name=\"description\">MatchAllDocsQuery, product of:</str>\n      <arr name=\"details\">\n        <lst>\n          <bool name=\"match\">true</bool>\n          <float name=\"value\">1.0</float>\n\n          <str name=\"description\">queryNorm</str>\n        </lst>\n      </arr>\n    </lst></doc>\n\n\n\nSimilarly, we can use the _values_ transformer to add a constant value to the output\n\n&fl=id,_value:hello_\n\n<doc>\n   <doc>\n    <str name=\"id\">GB18030TEST</str>\n    <str name=\"_value:hello_\">hello</str></doc>\n\n\n\nor specify a type:\n\n&fl=id,_value:int:10_\n\n<doc>\n  <doc>\n    <str name=\"id\">GB18030TEST</str>\n    <int name=\"_value:int:10_\">10</int></doc>\n\n\n\naliasing also works with transformers\n\n&fl=id,ten=_value:int:10_\n\n<doc>\n  <doc>\n    <str name=\"id\">GB18030TEST</str>\n    <int name=\"ten\">10</int></doc>\n\n\n\n\nWildcard parsing\n\nThe parser excepts wildcards \u2013 we are not doing anything with them yet\n\nMultiple fl parameters\n\n&fl=id,score\n\nis equivalent to:\n\n&fl=id&id=score\n\n\n\n\n\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13011545",
            "date": "2011-03-26T04:09:06+0000",
            "content": "I am not wild about the field mapping syntax.  To display the field 'id' as 'xxx', we use: fl=xxx=id,score\n\nWhat about using SQL style syntax? \n\n&fl=id AS xxx,score\n\n\n\nI think this reads better and is less confusing (for me) since there are not so many = signs\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13011547",
            "date": "2011-03-26T04:14:55+0000",
            "content": "I debated different ways to pass parameters to transformers, and now think i like the simple short method:\n\n&fl=field,_transformer:args_,...\n\n\nAnother option would be to reuse the LocalParams syntax... something like:\n\n&fl=field,{!transformer name=explain style=html},score\n\n\nbut that feels a bit ridiculous and most transformers don't need arguments and the few that do just take simple ones.\n\nthoughts? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13012317",
            "date": "2011-03-29T02:14:33+0000",
            "content": "It's not even clear to me why invoking a transformer would even be part of \"fl\".\n\n\"fl=name,docid\" makes sense because the user is asking for the docid field back - the fact that it's a transformer is an implementation detail.\n\nIf one wants to add a transformer that can do anything to a document, it feels like that should be specified elsewhere, and not in the field list? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13012320",
            "date": "2011-03-29T02:39:44+0000",
            "content": "I think it makes sense because it is the place you select what goes in the output.\n\nIf I add fl=name,_stuff_from_my_db_  it is reasonable that the field will have stuff from my db (whatever they are called)\n\nIf that is specified elsewhere, it seems odd to have to keep them in sync.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13012323",
            "date": "2011-03-29T02:45:05+0000",
            "content": "In the case where some fields may come from a DB, all of the clients definitely shouldn't be exposed to that mapping.  The goal should be to have those fields look like any other fields, with the clients isolated from any mapping changes. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13012327",
            "date": "2011-03-29T03:17:29+0000",
            "content": "That seems fine \u2013 if you don't want people to add a transformer as in the fl parameter, don't register the factory in solrconfig.xml, and add it to ReturnFields in a Component/Handler/whatever\n\nThe Component/Handler/Whatever could be configured in solrconfig.xml with whatever it needs to make/edit the Transformer\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13012582",
            "date": "2011-03-29T17:54:53+0000",
            "content": "I think it makes sense because it is the place you select what goes in the output.\n\npart of the complexity here is that in a lot of cases you want the client to specy the \"target\" of the transformation, w/o knowing the source.\n\nin your previous example: clients may e in a situation where they know they want the \"xxx\" and \"score\" fields w/o knowing that \"xxx\" is the result of a transformation from the concrete \"id\" field.\n\nIn an ideal world, a solr admin named Bob should be able to tell his client Carl that the \"price\" field is the one Carl wants to use.  Carl could then query solr with \"...&fl=price\" or \"...&sort=price desc\" w/o ever necessarily knowing that price is really the result of a function query that takes into account the current exchange rate (or some other factors driven by a transformer configured in solrconfig.xml for hte handler Carl is querying)\n\nCarl is selecting what data goes in the output, but that doesn't mean carl should (have to) know where that data comes from "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13012606",
            "date": "2011-03-29T18:37:41+0000",
            "content": "This makes sense \u2013 i can see letting a component set up some list of pseudo fields you could ask for, and the FL parser picking them out of a map and makeing the right transformer/whatever\n\nI'd like to make sure we have a simple way to configure basic things inline \u2013 SQL SELECT 1 is remarkably useful! "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13012642",
            "date": "2011-03-29T19:39:20+0000",
            "content": "I'd like to make sure we have a simple way to configure basic things inline \u2013 SQL SELECT 1 is remarkably useful!\n\ni agree completely, my point is just that in picking a syntax/api we should prioritize making the syntax for \"select price\" (where price is something dynamicly generated by a transformer behind the scenes) simpler then the syntax for saying \"select foo() AS price\" (where the client knows the nitty-gritty details of how things work under the covers)\n\nclients asking for \"select price\" should be more common then clients asking for \"select foo()\" or \"select foo() as price\"\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13014828",
            "date": "2011-04-01T21:12:02+0000",
            "content": "I just started a new branch and implemented some of the things we have suggested.  Check:\nhttps://svn.apache.org/repos/asf/lucene/dev/branches/pseudo/\n\nThis implements:\n\nSQL style AS\n\n?fl=id,field AS display\n\n\nwill display 'field' with the name 'display'\n\n\nPseudo Fields\n\nYou can define pseudo fields with  ?hl.pseudo=key:value\n\nAny key that matches something in the fl param gets replaced with value.  For example:\n\n?fl=id,price&fl.pseudo=price:real_price_field\n\n\nis the same as\n\n?fl=id,real_price_field AS price\n\n\n\n\nTransformer Syntax [name]\n\nThe previous underscore syntax is replaced with brackets.\n\n?fl=id,[value:10] AS 10\n\n\n\nHopefully this will make it more clear that it is calling a function.\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13014838",
            "date": "2011-04-01T21:29:50+0000",
            "content": "Just commited the changes \u2013 yonik, i replaced your fancy parsing with something i can understand (StringTokenizer and indexof)\n\nI figure we should agree on a syntax first, and then optimize the fl parsing  (out of my league) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13015202",
            "date": "2011-04-03T20:09:41+0000",
            "content": "\nI'm not particularly a fan of the specific syntax \"x AS y\", but i'm not opposed to it \u2013 although your specific example (with a number on the right side) confuses me ... i think you must have actually ment something like this, correct? ...\n\n?fl=id,[value:10] as hard_coded_price\n\n\n\nThe one change i'd like argue in favor of is ensuring we have some way to deal with fieldnames that are esoteric. ie: containing whitespaces or special characters\n\nI don't think we need stress out about a lot of quoting/escaping in the common case \u2013 the rules used in the FunctionQParser to identify \"simple\" field names should be good enough for most people, and helps keep the syntax clean for the majority of users who have straight forward field names, but it would definitely be nice if there was some way for people to refer to complex field names (either as \"input\" (for referring to esoteric field names in their documents) or as \"output\" (for generating esoteric field names as the result of an alias/transformation)\n\nI think the two changes to what Ryan has already described that might make this totally feasible would be...\n\n1) a quote character discovered where fields like fl expect to encounter a field name (like \"fl\") should trigger quoted terminated string parsing (where white space and punctuation are considered part of hte string, and backslash can be used escapes quotes) and the reuslting string will be used as the field name.\n\n2) the psuedo field param mapping ryan described should move the fieldname to the \"key\" so there is no ambiguity if the seperator appears twice, ie... \n\n\n?hl.pseudo.xxx%3Ayyy=zzz\n   ..instead of...\n?hl.pseudo=xxx%3Ayyy%3Azzz\n\n\n\n\nIf we did those two things, then these would all be equivilent...\n\n\n?fl=id,price&fl.pseudo.price=real_price_field\n?fl=id,real_price_field+AS+price\n?fl=id,\"real_price_field\"+AS+\"price\"\n\n\n\n...but it would also be possible to have either of these...\n\n\n?fl=id,\"external+price+alias\"&fl.pseudo.external+price+alias=internal+price+field\n?fl=id,\"internal+price+field\"+AS+\"external+price+alias\"\n\n\n\n...this shouldn't cause a problem with the syntax for echoing back literal values, since that would already require a transformer...\n\n\n?fl=id,[value:\"No+it+is+Not\"]+AS+\"Is+Product+In+Stock\"\n(all products temporarily out of stock)\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015256",
            "date": "2011-04-04T01:36:57+0000",
            "content": "I'm not particularly a fan of the specific syntax \"x AS y\", but i'm not opposed to it\n\nOther ideas?  Yoniks patch uses:\n\n?fl=id,hard_coded_price=[value:10]\n\n\nMy problem (but not strong) is that = is used for both the parameter and the name mapping.  \n\nother options?\n\n\ni think you must have actually ment something like this, correct?   ?fl=id,[value:10] as hard_coded_price\n\nyes, sorry poor example\n\n\na quote character...\n\n+1\n\n\nshould move the fieldname to the \"key\"\n\nI like the syntax you suggest.  The reason i suggested fl.pseudo=key:value is more to do with the implementation then the syntax.  With fl.pseudo.key=value we have to iterate and compare all parameters to parse the pseudo fields rather then just getParams( \"fl.pseudo\" ).\n\nI am happy with either syntax, but I like the implementation simplicity of fl.pseudo=key:value\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015393",
            "date": "2011-04-04T13:14:43+0000",
            "content": "i replaced your fancy parsing with something i can understand (StringTokenizer and indexof)\n\nheh - good luck with that   Not using the qparser framework is pretty much doomed to failure (due to the need to then exactly replicate that parsing logic). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015397",
            "date": "2011-04-04T13:24:54+0000",
            "content": "i'm not suggesting the StringTokenizer approach is the right way to go \u2013 i just have no way to futz with the qparser stuff since it is so far out of my league.\n\nThe stuff i did (in the branch), still uses the qparser \u2013 but only when it thinks something is a function.  Is it required elsewhere?\n\nThe plan would be to figure out what syntax we want then optimize the fl parsing from there (i think)\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015410",
            "date": "2011-04-04T13:46:11+0000",
            "content": "The stuff i did (in the branch), still uses the qparser \u2013 but only when it thinks something is a function.\n\nI should have explained further... doing indexOf(\"any random delimiter\") and then feeding that to the qparser is what is pretty much doomed to failure.  Whatever delimiter you're trying to use could easily be contained within the query/function syntax itself.  This is why the qparser framework must be used to find the end of the query/funtion.  The first crack at sort-by-function had the same problems (although it was trying to be more clever and parse parens, params, etc, to find the end of the function) and needed to be rewritten. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015413",
            "date": "2011-04-04T13:54:02+0000",
            "content": "got it \u2013 yes, this first splits on comma \u2013 so if a field has a comma it would be busted.\n\nThere is also no way to implement the quoted field names that hoss suggests  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015432",
            "date": "2011-04-04T14:21:37+0000",
            "content": "There is also no way to implement the quoted field names that hoss suggests\n\nActually it should be trivial since it's a single value and StrParser already supports it.  Anyway, as you say, we should concentrate first on the desired syntax.\n\nI brought up \"add(a,b) AS foo\" in SOLR-1298 as one option, but no one was thrilled with it.\n\nOK, so for the basic syntax of how to name pseudofields, I think these are the top 3 options we have?\n\nfl=name,title,dist=geodist(),nterms=termfreq(text,solr)\nfl=name,title,dist:geodist(),nterms:termfreq(text,solr)\nfl=name,title,geodist() AS dist,termfreq(text,solr) AS nterms\n\n\n\nI think I'm fine with any of these, but perhaps we should get feedback from more people?  Once we decide, we should pretty much stick with it forever IMO. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015447",
            "date": "2011-04-04T14:58:08+0000",
            "content": "but perhaps we should get feedback from more people?\n\n+1  I'd be fine with any of these options, but lean towards AS and shy away from = "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13015495",
            "date": "2011-04-04T16:55:05+0000",
            "content": "I like the syntax you suggest. The reason i suggested fl.pseudo=key:value is more to do with the implementation then the syntax. With fl.pseudo.key=value we have to iterate and compare all parameters to parse the pseudo fields rather then just getParams( \"fl.pseudo\" ).\n\nYeah ... one possibility is to use the same approach we use for field overrides in other params...\n\n\nfl.pseudo=external+price+alias\nfl.pseudo=external+popularity+alias\nfl.pseudo.external+price+alias=internal+price+field\nfl.pseudo.external+popularity+alias=internal+popularity+field\n\n\n\n...it's a little verbose, but since the main use of this is likely to be \"default\" params anyway (because people specifying it at request time could just include the aliasing directly in hte param value) it might not be that bad.\n\nOK, so for the basic syntax of how to name pseudofields, I think these are the top 3 options we have?\n\ni vote for the colon.  \"=\" is evil since it's easy to confuse as a key=val delimiter in the URL (and requires extra escaping in most docs to explain it correctly).  \"x AS y\" just seems unnecessarily verbose.\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13015502",
            "date": "2011-04-04T17:10:06+0000",
            "content": "I'm kinda liking having fl stay unadorned, such that we have fl=name,title,dist,nterms and fl.dist=geodist() and fl.nterms=termfreq(text,solr)  (or fl.pseudo prefix).  This allows for indirection on what dist and nterms really maps to, keeping that out of the main fl. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015509",
            "date": "2011-04-04T17:31:47+0000",
            "content": "Erik \u2013 so you are suggesting that the fl list is always the display value, and it may map to a pseudo field with a different parameter\n\nfl=name,dist,nice_looking_field_name\nfl.pseudo.dist=geodist()\nfl.pseudo.nice_looking_field_name=crazy_field_name\n\nIn this case, each 'fl' value would be checked to see if it actually maps to a pseudo field.  As for supporting crazy field names we could either:\n1. support quoting in the fl param so that fields with ',' aren't split\n2. if you index a field with ',' in the name, you can get it but it needs to be mapped as a pseudo field.\n\n\nI like this suggestion.  It avoids the ':' vs '=' vs 'AS' issue and makes the parsing rules easy to explain.\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015515",
            "date": "2011-04-04T17:52:40+0000",
            "content": "From the high level user perspective, a field and a function of fields both yield values.\nI like that simplicity - it's the same type of simplicity we have in programming languages.\n\nI can call foo(a,b), but instead of a variable(think fieldname) I can simply substitute another function:\nfoo(a,bar(c,d))\n\nI think this best matches people's expectations - directly specify what they want returned in \"fl\", as they do today.  It's also less syntax to remember.  It's also consistent with how we enable sort-by-function... anywhere a fieldname can be, a function can be substituted.  It's also cleaner by default since a name is not required and the label that will be used is the function itself.\n\ni vote for the colon\n\n+1, I think that's the best option.\n\nI think this is an independent issue from setting up transparent user selectable aliases/pseudofields such that when a user puts \"foo\" in the field, they get the value but have no idea it came from a source other than stored fields. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015522",
            "date": "2011-04-04T18:06:42+0000",
            "content": "Thinking about the pseudo field mapping \u2013 I like the idea that each parsed 'fl' element gets checked and possibly replaced with something from the pseudo field list.  \n\nAs a crazy example, this would mean:\n\nfl=name,dist:geodist()&\nfl.pseudo.geodist()=distance_field\n\nwould use the lucene field 'distance_field' and put it in a solr document with the name 'dist'\n\nDoes that match your expectation?\n\n\ni vote for colon\n\n+1 ok me too\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015531",
            "date": "2011-04-04T18:20:07+0000",
            "content": "Ok, so it seems like we're agreeing on two different usecases.\n\nFor user directly asking for functions, we'll support (as we already do today)\nfl=a,b,foo(c),d\n\nAnd if the user wants to change the name, they can use \"mykey:foo(c)\"\n\nAnd for separately setting up fields that a user could add to fl w/o needing to know where they come from,\nwe can support\nfield.mykey=foo(c)\n\nIt seems like the latter should almost be a separate issue - IMO, it's not as immediately important, and there are other issues to figure out (like how to specify if it's implicit and included with *, or with globs, or if it's only added if explicitly listed). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015551",
            "date": "2011-04-04T19:01:54+0000",
            "content": "bq, For user directly asking for functions, we'll support (as we already do today)\nfl=a,b,foo(c),d]\nAnd if the user wants to change the name, they can use \"mykey:foo(c)\"\n\nWith the addition of the inline transformer syntax\n\nfl=a,b,[explain]\n\nfl=a,b,price:[value:10]\n\n\nI don't think we can treat the pseudo fields as a totally different issue since the field list parsing depends on the pseudo fields.  That is, if you have fl=id,foo(c)&fl.pseudo.foo(c)=field_name  we never actually parse foo(c) as a function \u2013 it needs to  get mapped to the field 'field_name'\n\nI think the fl parser needs a few passes:\n1. split fl into tokens (or whatever we call them)\n2. each token may get replaced with the value from fl.pseudo.token\n3. check if the token is a function, transformer, or wildcard\n\nbut yes, it is a conceptually different issue.\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015554",
            "date": "2011-04-04T19:12:47+0000",
            "content": "I don't think we can treat the pseudo fields as a totally different issue since the field list parsing depends on the pseudo fields.\n\nWell sure, more development wherever it is.  I meant more that it's different issue as far as features go.\nBeing able to return function values finally completes basic geosearch.\n\nif you have fl=id,foo(c)&fl.pseudo.foo(c)=field_name we never actually parse foo(c) as a function\n\nThat seems like more complexity than it's worth, and would really only work with parameter-less functions (if implemented as a hashmap lookup) since different arguments to the function would cause the match to fail.\n\n\"transformer syntax\" also seems like an somewhat orthoginal issue.  Has anyone commented on the proposed syntax (what is the full proposed syntax anyway? I need to see some examples with more than one parameter).  Is it important to allow these transformer parameters inline in \"fl\"? etc. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015559",
            "date": "2011-04-04T19:23:28+0000",
            "content": "Hmmm, I've tried changing '=' to ':' for the key... but things now fail because of tests in trunk w/ syntax like\nexplain:nl\n\nAlso, why is testAugmentFields in SolrExampleTests (the stuff that tests the example)?\nedit: Oh, I think I see - it's an easy way to see that both binary and non-binary response writers are tested, right? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015585",
            "date": "2011-04-04T19:53:05+0000",
            "content": "That seems like more complexity than it's worth, and would really only work with parameter-less functions (if implemented as a hashmap lookup) since different arguments to the function would cause the match to fail.\n\nthis is why i bring it up, and why I think it is the same issue.  We need to agree on what it means. and i'm pretty sure that has consequences on how we implement the basic parsing.  \n\nAs you say, i would expect different arguments to the function should not match a pseudo field.\nfl=id,foo(a)\nwould not use the pseudo field defined in\nfl.pseudo.foo(a)=something\n\nI think we only need to say that exact matches would get replaced.  For example\nfl=id,foo( a )\ndoes not need to match\nfl.pseudo.foo(a)=something\n\nWe can say that functions/transformers are not supported by pseudo fields \u2013 i'm fine with that, but think we need to be explicit.  One argument to support it is so that you could swap the meaning of some function w/out updating clients.  \n\n\n\n\"transformer syntax\" also seems like an somewhat orthoginal issue\n\nNot really, it is about how we parse the fl. \n\n\nHas anyone commented on the proposed syntax\n\nnope \u2013 other then hoss agreeing that SQL SELECT 1 is very useful and we should make it simple\n\n(what is the full proposed syntax anyway? I need to see some examples with more than one parameter)\n\nThe proposed syntax is:\n\n[name] and [name:argument]\n\nFor the key use cases I can think of having a single inline parameter is very useful [value:10], for more complex args, the transformer can use SolrQueryRequest\n\n\n\nIs it important to allow these transformer parameters inline in \"fl\"? etc.\n\nFor me they are equally important to inline functions.  I plan to use them for things that do not map cleanly to functions.  A simple example, if you have a geohash point that encodes X and Y in a single field, i want to return that with well typed difference between X and Y.  With a transformer, i can return \n{x:10, y:20}\n rather then just \n{point:'10 20'}\n and make the client figure out if I mean x y or lat lon.\n\nThe other key place I see them getting used is with returning highlighed fields inline\n\n?fl=id,name,[hl:name]\n\nwould return the raw name field and the highlighted name field.  All the other highlight parameters would be fetched from getParams() "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015593",
            "date": "2011-04-04T20:04:51+0000",
            "content": "[name] and [name:argument]\n\nIt just seems both strange and limiting to say that an augmenter may only have one argument.\nBut I suppose if that argument is always just a string, the augmentor could always parse it into multiple arguments.  What is the syntax of \"argment\"? is it backslash escaped so the value can contain \"]\"? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015597",
            "date": "2011-04-04T20:07:43+0000",
            "content": "For the key use cases I can think of having a single inline parameter is very useful [value:10],\n\nStill too complex for my tastes.  I think that should be fl=myvalue:10\nrather than fl=myvalue:[value:10]\n\nBut it doesn't hurt anything if we keep the \"value\" transformer around anyway  "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015667",
            "date": "2011-04-04T22:40:50+0000",
            "content": "but things now fail because of tests in trunk w/ syntax like\n\nya, i tried messing with that too \u2013 also tried changing the transformer syntax from _ to [] but could not understand how the parser works.  This is why i made the branch to see how the rest feels.\n\nI just updated the branch to use a space as the transformer args deliminator.  I also refactored to support the pseudo field mapping I think we agree on (though yonik thinks we should do it as a different issues)\n\nThis adds a parameter fl.pseudo=true/false \u2013 if that is on, it will check if each field has an alternative in fl.pseudo.key=value\n\nThis just uses fl.split( \",\" ) but that should really be a fancy parser that knows about quotes.  \n\n\nit's an easy way to see that both binary and non-binary response writers are tested, right?\n\nYes, this is the high level place that hits XML and binary response writers \u2013 it used to use JSON too, but looks like that has changed.  It is also tested there because I want to make sure solrj works correctly with complex structures like [explain nl]\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015673",
            "date": "2011-04-04T22:52:03+0000",
            "content": "It just seems both strange and limiting to say that an augmenter may only have one argument.\nBut I suppose if that argument is always just a string, the augmentor could always parse it into multiple arguments. \n\nYa, the value augmenter actually does this \u2013 you can specify a type [value int 10] vs [value 10] \n\n\nWhat is the syntax of \"argment\"? is it backslash escaped so the value can contain \"]\"?\n\nI guess that is a good idea \u2013 if it makes things complicated, i'm not too worried about it.  You could use another parameter if if there is a need for something complex.  \n\n\nStill too complex for my tastes. I think that should be fl=myvalue:10\n\nwould this mean that any unknown string becomes a literal value?  I would rather have an error then shorten the SELECT 10 case.  See SOLR-2441\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015679",
            "date": "2011-04-04T23:33:57+0000",
            "content": "> What is the syntax of \"argment\"? is it backslash escaped so the value can contain \"]\"?\nI guess that is a good idea \u2013 if it makes things complicated, i'm not too worried about it. \n\nI'm not concerned about the complexity of implementation at all - I'm just trying to figure out what the proposal actually is.\n\nYou could use another parameter if if there is a need for something complex.\n\nAnother parameter for the augmenter?  That's essentially what I was asking about.  Or do you mean a different query parameter?\nedit: oops... just saw your previous message \"I just updated the branch to use a space as the transformer args deliminator.\"  I guess that's what you meant.\n\n> Still too complex for my tastes. I think that should be fl=myvalue:10\nwould this mean that any unknown string becomes a literal value?\n\nNope.  By default, the function parser treats an unquoted string literal as a field name, and an error will be thrown if it isn't.  If you want a string literal, you quote it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015681",
            "date": "2011-04-04T23:45:28+0000",
            "content": "This adds a parameter fl.pseudo=true/false \u2013 if that is on, it will check if each field has an alternative in fl.pseudo.key=value\n\nI think I like the shorter form fl.x=y better (rather than fl.pseudo.x=y)?  Anyone else?\n\nA pseudofields=false parameter is a good idea to aid in debugging though. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015710",
            "date": "2011-04-05T01:41:34+0000",
            "content": "FYI: just so we don't overlap effort, I'm busy adding objectVal(doc) to DocValues so that we can support all types of function queries (which is important beyond pseudo-fields too) "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015711",
            "date": "2011-04-05T01:44:34+0000",
            "content": "you have seen SOLR-2443.... right? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13015722",
            "date": "2011-04-05T02:05:33+0000",
            "content": "you have seen SOLR-2443.... right?\n\nHeh - no I had not.\nI lose track (and in this case had never even seen the issue).  We should try to link all of these related issues in one place - it's hard to keep track of otherwise. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13016089",
            "date": "2011-04-05T19:30:26+0000",
            "content": "We should try to link all of these related issues in one place - it's hard to keep track of otherwise.\n\nour current version of jira makes it possible to convert an issue into a \"sub task\" of another issue ... it's a little more visible that way then just using the dependency/realted issue linking. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13016095",
            "date": "2011-04-05T19:51:17+0000",
            "content": "Random thought about augmenter parameter syntax (when it's actually needed)... what about reusing most of localParams syntax, but change \n{!stuff}\n to [stuff]?  This would give us named parameters, param dereferencing, and allow passing something like SolrParams to an augmenter, which is probably easier for most people to deal with than parsing themselves?\n\nSo instead of this:\n\ntitlehl:[_hl_:title]\n\n\n\nWe could have this:\n\ntitlehl:[_hl_ f=title]\n\n\n\nWhich would then easily allow multiple params like this:\n\ntitlehl:[_hl_ f=title snippets=3 fragsize=800]\n\n\n\nJust a note, I'm making good progress on supporting all common object types in function queries, so basic literals won't need any transformer syntax and you should be able to just do stuff like\n\n fl=mystr:'hello',myint:10, myfloat:25.5\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13016102",
            "date": "2011-04-05T20:18:20+0000",
            "content": "no need for the '_' with transformers \u2013 i would hope that the brackets tell you that it is a tranformer.\n\n?fl=id,[explain]\nnot\n?fl=id,[_explain_]\n\n\nwe could have titlehl:[_hl_ f=title]\n\nya, i think that would be fine.\n\n\n\nso basic literals won't need any transformer syntax\n\nI still don't get this.  How do you know that the literal is not referring to a field (or invalid field)?  How do you know it is an int vs float vs double vs string?  Seems like too much magic to me.\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13016107",
            "date": "2011-04-05T20:40:55+0000",
            "content": "no need for the '_' with transformers \u2013 i would hope that the brackets tell you that it is a tranformer.\n\nRight, but it seems like the name of the transformer should match the field that it adds to the document by default?  That's just a convention of course... for example, fl=docid adds the docid field to the documents.  It seems natural to refer to the transformer that does that as the docid transformer?\n\nI still don't get this. How do you know that the literal is not referring to a field (or invalid field)? How do you know it is an int vs float vs double vs string? Seems like too much magic to me.\n\nIt's magic people expect, and easy to understand, because most of their programming languages work that way. It's unlikely that float vs double would matter for returning a constant anyway - and things like JSON don't even distinguish.  A string would be quoted, and an int would lack characteristics of a float/doube.\n\nWe could even add float() int() double() long() functions in the future if we really need them.\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13016114",
            "date": "2011-04-05T20:54:10+0000",
            "content": "Right, but it seems like the name of the transformer should match the field that it adds to the document by default?\n\nSince changing from the '_' syntax to the bracket syntax, i would now expect the brackets in the name for the return field.\n\n\n?fl=id,[explain]\n\n\nwould return the document:\n\n<str name=\"[explain]\">...</str>\n\n\n\n\nIt's magic people expect,\n\nHow do you know it is a literal and not just a missing field name?  See SOLR-2441\n\nWhat about a literal that matches a field name?  quotes?  Didn't hoss suggest that we should use quotes to wrap crazy field names?\n\nis:\n\n?fl=id,avalue:'some crazy field name',score\n\n\n\nreferring to a field or a literal?  In the fl parameter, i would expect everything to be a field name unless you explicitly say it is a literal.\n\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13016186",
            "date": "2011-04-05T23:52:07+0000",
            "content": "What about a literal that matches a field name? quotes? Didn't hoss suggest that we should use quotes to wrap crazy field names?\n\n'foo bar' would be a string literal\nfield('foo bar') would be the whacky field name with a space in it "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13016222",
            "date": "2011-04-06T02:48:27+0000",
            "content": "Aaa \u2013 my instinct would be the reverse.  If something is listed, it is most likely a field name, and then only if you explicitly make it a value would it be a value.\n\nother people have opinions?\n\nwhat about a field name 10?  does that need special escaping just because it is also a number?  How would this handle ?fl=id,foo  when foo is not a real field name?  is foo a literal or a field name that does not exist? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13032239",
            "date": "2011-05-12T02:46:23+0000",
            "content": "Does this issue cover wildcard syntax like fl=*_s ? Because SOLR-2503 has been committed, I want the wildcard syntax for fl.\n\n&fl=*_s\n\n<doc>\n  <str name=\"PERSON_S\">Barack Obama</str>\n  <str name=\"TITLE_S\">the President</str>\n</doc>\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13046139",
            "date": "2011-06-08T19:12:01+0000",
            "content": "in #1133505, I updated Transformers to take a Map<String,String> that is parsed using the LocalParams syntax.\n\nIn trunk, things now look like:\n\n?fl=id,[shard],[value v=10]\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13046149",
            "date": "2011-06-08T19:25:08+0000",
            "content": "Yep, we get the full power/familiarity of local params, including param substitution (e.g. myvar=$other_request_param)\n\nI updated Transformers to take a Map<String,String> that is parsed using the LocalParams syntax.\n\nIn the template parsing code I committed first, I had used SolrParams... one reason being that for some time I've thought that we might want multi-valued parameters in localParams.  If back compat of transformers isn't a big deal, we can change Map<String,String> to Map<String,String[]> later... but it seems like the additional parsing logic of SolrParams might add enough value to use that instead of a bare Map anyway? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13046161",
            "date": "2011-06-08T19:42:29+0000",
            "content": "I used Map<String,String> because i figured most Transformes won't use the params anyway, so it is less \"work\" \u2013 I don't feel strongly either way.  \n\nI'll change it to SolrParams "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13046174",
            "date": "2011-06-08T19:55:06+0000",
            "content": "changed in r1133534 "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13091375",
            "date": "2011-08-25T22:31:47+0000",
            "content": "I added some quick docs to:\nhttp://wiki.apache.org/solr/CommonQueryParameters#glob\n\nwe should make sure that is accurate and flush out better examples "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13124363",
            "date": "2011-10-10T18:24:32+0000",
            "content": "since the majority of this has already been committed to trunk, i'm marking this for 4.0 \u2013 if there is any outstanding work to consider this issue \"finished\" it either needs spun off into a new issue, or wrapped up before 4.0 is released. "
        },
        {
            "author": "Jan Rasehorn",
            "id": "comment-13170934",
            "date": "2011-12-16T12:26:07+0000",
            "content": "It does not seem to be working in Solr 4 Trunk from 16th Dec 2011. \nI added a transformer with name \"testtrans\" as a copy of the existing examples to solrconfig.xml and tried to incorporate it into the fl parameter. \n\nSolr returns an error message saying:\n\nundefined field: [testtrans]\n\nIf i add a pseudo field \"constval:sum(1,2)\" to fl - parameter, solr returns an error message also:\n\nundefined field: constval:sum(1\n\nAm I missing some steps to enable it? "
        },
        {
            "author": "Jan Rasehorn",
            "id": "comment-13175358",
            "date": "2011-12-23T09:40:30+0000",
            "content": "Found the reason: \n\nI had the term vector component enabled. As described in SOLR-2352, TVC causes an error \"undefined field\" for \"*\" and \"score\" and as it seems for all pseudo fields, transformers and functions used in the fl-parameter.\n\nI disabled TVC in solrconfig.xml and now it is working. "
        },
        {
            "author": "Luca Cavanna",
            "id": "comment-13201292",
            "date": "2012-02-06T14:05:00+0000",
            "content": "Hi all, \nare there any plan to support an exclusion syntax? I'm not sure the fl (field list) is the right place, but I would like to exclude some of the fields (by default) from the output. This would be better than specifying a (long) list of fields that I want, which would need to be updated every time I add a field to my schema.\nWouldn't this be an useful feature? "
        },
        {
            "author": "Luca Cavanna",
            "id": "comment-13219553",
            "date": "2012-02-29T21:11:51+0000",
            "content": "When I wrote the last comment I didn't realize this was already committed! I think the fact that the issue is still open is a bit misleading, can't we close it? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13221232",
            "date": "2012-03-02T20:14:48+0000",
            "content": "Lets discuss any problems in new issues "
        }
    ]
}