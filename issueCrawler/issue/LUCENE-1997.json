{
    "id": "LUCENE-1997",
    "title": "Explore performance of multi-PQ vs single-PQ sorting API",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.9",
        "resolution": "Unresolved",
        "status": "Reopened"
    },
    "description": "Spinoff from recent \"lucene 2.9 sorting algorithm\" thread on java-dev,\nwhere a simpler (non-segment-based) comparator API is proposed that\ngathers results into multiple PQs (one per segment) and then merges\nthem in the end.\n\nI started from John's multi-PQ code and worked it into\ncontrib/benchmark so that we could run perf tests.  Then I generified\nthe Python script I use for running search benchmarks (in\ncontrib/benchmark/sortBench.py).\n\nThe script first creates indexes with 1M docs (based on\nSortableSingleDocSource, and based on wikipedia, if available).  Then\nit runs various combinations:\n\n\n\tIndex with 20 balanced segments vs index with the \"normal\" log\n    segment size\n\n\n\n\n\tQueries with different numbers of hits (only for wikipedia index)\n\n\n\n\n\tDifferent top N\n\n\n\n\n\tDifferent sorts (by title, for wikipedia, and by random string,\n    random int, and country for the random index)\n\n\n\nFor each test, 7 search rounds are run and the best QPS is kept.  The\nscript runs singlePQ then multiPQ, and records the resulting best QPS\nfor each and produces table (in Jira format) as output.",
    "attachments": {
        "LUCENE-1997.patch": "https://issues.apache.org/jira/secure/attachment/12422688/LUCENE-1997.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-10-20T15:05:19+0000",
            "content": "Attached patch.\n\nNote that patch is based on 2.9.x branch, so first checkout 2.9.x,\napply the patch, then:\n\n  cd contrib/benchmark\n  ant compile\n  <edit constants @ top of sortBench.py>\n  python -u sortBench.py -run results\n  python -u sortBench.py -report results\n\nThe important constants are INDEX_DIR_BASE (where created indexes are\nstored), WIKI_FILE (points to .tar.bz2 or .tar export of wikipedia; if\nthis file can't be found the script just skips the wikipedia tests).\nYou can also change INDEX_NUM_DOCS and INDEX_NUM_THREADS.\n\nIf you don't have the wiki export downloaded, that's fine... the\nscript should just run the tests based on the random index. ",
            "author": "Michael McCandless",
            "id": "comment-12767823"
        },
        {
            "date": "2009-10-20T16:40:14+0000",
            "content": "OK I ran sortBench.py on opensolaris 2009.06 box, Java 1.6.0_13.\n\nIt'd be great if others with more mainstream platforms (Linux,\nWindows) could run this and post back.\n\nRaw results (only ran on the log-sized segments):\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n1\n318481\ntitle\n10\n114.26\n112.40\n-1.6%\n\n\nlog\n1\n318481\ntitle\n25\n117.59\n110.08\n-6.4%\n\n\nlog\n1\n318481\ntitle\n50\n116.22\n106.96\n-8.0%\n\n\nlog\n1\n318481\ntitle\n100\n114.48\n100.07\n-12.6%\n\n\nlog\n1\n318481\ntitle\n500\n103.16\n73.98\n-28.3%\n\n\nlog\n1\n318481\ntitle\n1000\n95.60\n57.85\n-39.5%\n\n\nlog\n<all>\n1000000\ntitle\n10\n95.71\n109.41\n14.3%\n\n\nlog\n<all>\n1000000\ntitle\n25\n111.56\n101.73\n-8.8%\n\n\nlog\n<all>\n1000000\ntitle\n50\n110.56\n98.84\n-10.6%\n\n\nlog\n<all>\n1000000\ntitle\n100\n104.09\n93.02\n-10.6%\n\n\nlog\n<all>\n1000000\ntitle\n500\n93.36\n66.67\n-28.6%\n\n\nlog\n<all>\n1000000\ntitle\n1000\n97.07\n50.03\n-48.5%\n\n\nlog\n<all>\n1000000\nrand string\n10\n118.10\n109.63\n-7.2%\n\n\nlog\n<all>\n1000000\nrand string\n25\n107.68\n102.33\n-5.0%\n\n\nlog\n<all>\n1000000\nrand string\n50\n107.12\n100.37\n-6.3%\n\n\nlog\n<all>\n1000000\nrand string\n100\n110.63\n95.17\n-14.0%\n\n\nlog\n<all>\n1000000\nrand string\n500\n79.97\n72.09\n-9.9%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n76.82\n54.67\n-28.8%\n\n\nlog\n<all>\n1000000\ncountry\n10\n129.49\n103.63\n-20.0%\n\n\nlog\n<all>\n1000000\ncountry\n25\n111.74\n102.60\n-8.2%\n\n\nlog\n<all>\n1000000\ncountry\n50\n108.82\n100.90\n-7.3%\n\n\nlog\n<all>\n1000000\ncountry\n100\n108.01\n96.84\n-10.3%\n\n\nlog\n<all>\n1000000\ncountry\n500\n97.60\n72.02\n-26.2%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n85.19\n54.56\n-36.0%\n\n\nlog\n<all>\n1000000\nrand int\n10\n151.75\n110.37\n-27.3%\n\n\nlog\n<all>\n1000000\nrand int\n25\n138.06\n109.15\n-20.9%\n\n\nlog\n<all>\n1000000\nrand int\n50\n135.40\n106.49\n-21.4%\n\n\nlog\n<all>\n1000000\nrand int\n100\n108.30\n101.86\n-5.9%\n\n\nlog\n<all>\n1000000\nrand int\n500\n94.45\n73.42\n-22.3%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n88.30\n54.71\n-38.0%\n\n\n\n\n\nSome observations:\n\n\n\tMultiPQ seems like it's generally slower, thought it is faster in\n    one case, when topN = 10, sorting by title.  It's only faster with\n    the : (MatchAllDocsQuery) query, not with the TermQuery for\n    term=1, which is odd.\n\n\n\n\n\tMultiPQ slows down, relatively, as topN increases.\n\n\n\n\n\tSorting by int acts differently: MultiPQ is quite a bit slower\n    across the board, except for topN=100 \n\n ",
            "author": "Michael McCandless",
            "id": "comment-12767870"
        },
        {
            "date": "2009-10-20T16:41:26+0000",
            "content": "New patch attached:\n\n\n\tTurn off testing on the balanced index by default (set DO_BALANCED to True if you want to change this)\n\n\n\n\n\tMinor formatting fixes in generating the report\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12767871"
        },
        {
            "date": "2009-10-23T03:13:43+0000",
            "content": "Results from John Wang:\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n<all>\n1000000\nrand string\n10\n91.76\n108.63\n18.4%\n\n\nlog\n<all>\n1000000\nrand string\n25\n92.39\n106.79\n15.6%\n\n\nlog\n<all>\n1000000\nrand string\n50\n91.30\n104.02\n13.9%\n\n\nlog\n<all>\n1000000\nrand string\n500\n86.16\n63.27\n-26.6%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n76.92\n64.85\n-15.7%\n\n\nlog\n<all>\n1000000\ncountry\n10\n92.42\n108.78\n17.7%\n\n\nlog\n<all>\n1000000\ncountry\n25\n92.60\n106.26\n14.8%\n\n\nlog\n<all>\n1000000\ncountry\n50\n92.64\n103.76\n12.0%\n\n\nlog\n<all>\n1000000\ncountry\n500\n83.92\n50.30\n-40.1%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n74.78\n46.59\n-37.7%\n\n\nlog\n<all>\n1000000\nrand int\n10\n114.03\n114.85\n0.7%\n\n\nlog\n<all>\n1000000\nrand int\n25\n113.77\n112.92\n-0.7%\n\n\nlog\n<all>\n1000000\nrand int\n50\n113.36\n109.56\n-3.4%\n\n\nlog\n<all>\n1000000\nrand int\n500\n103.90\n66.29\n-36.2%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n89.52\n70.67\n-21.1%\n\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12769039"
        },
        {
            "date": "2009-10-23T03:17:28+0000",
            "content": "Hah!  Thanks for posting that, Mark!   Much easier to read. \n\nHey John, can you comment with your hardware specs on this, so it can be recorded for posterity?  ",
            "author": "Jake Mannix",
            "id": "comment-12769042"
        },
        {
            "date": "2009-10-23T03:26:53+0000",
            "content": "My machine HW spec:\n\nModel Name:\tMacBook Pro\n  Model Identifier:\tMacBookPro3,1\n  Processor Name:\tIntel Core 2 Duo\n  Processor Speed:\t2.4 GHz\n  Number Of Processors:\t1\n  Total Number Of Cores:\t2\n  L2 Cache:\t4 MB\n  Memory:\t4 GB\n  Bus Speed:\t800 MHz ",
            "author": "John Wang",
            "id": "comment-12769045"
        },
        {
            "date": "2009-10-23T04:12:53+0000",
            "content": "Another run:\n\nI made the changes to int/string comparator to do the faster compare.\nJava 1.5.0_20\nLaptop - 64bit OS - 64bit JVM - 64bit\nQuad Core - 2.0 Ghz\nUbuntu 9.10 Kernel 2.6.31\n4 GB RAM\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n1\n317925\ntitle\n10\n87.38\n75.42\n-13.7%\n\n\nlog\n1\n317925\ntitle\n25\n86.55\n74.49\n-13.9%\n\n\nlog\n1\n317925\ntitle\n50\n90.49\n71.90\n-20.5%\n\n\nlog\n1\n317925\ntitle\n100\n88.07\n83.08\n-5.7%\n\n\nlog\n1\n317925\ntitle\n500\n76.67\n54.34\n-29.1%\n\n\nlog\n1\n317925\ntitle\n1000\n69.29\n38.54\n-44.4%\n\n\nlog\n<all>\n1000000\ntitle\n10\n109.01\n92.78\n-14.9%\n\n\nlog\n<all>\n1000000\ntitle\n25\n108.30\n89.43\n-17.4%\n\n\nlog\n<all>\n1000000\ntitle\n50\n107.19\n85.86\n-19.9%\n\n\nlog\n<all>\n1000000\ntitle\n100\n94.84\n80.25\n-15.4%\n\n\nlog\n<all>\n1000000\ntitle\n500\n78.84\n49.10\n-37.7%\n\n\nlog\n<all>\n1000000\ntitle\n1000\n72.52\n26.90\n-62.9%\n\n\nlog\n<all>\n1000000\nrand string\n10\n115.32\n101.53\n-12.0%\n\n\nlog\n<all>\n1000000\nrand string\n25\n115.22\n91.82\n-20.3%\n\n\nlog\n<all>\n1000000\nrand string\n50\n114.40\n89.70\n-21.6%\n\n\nlog\n<all>\n1000000\nrand string\n100\n91.30\n81.04\n-11.2%\n\n\nlog\n<all>\n1000000\nrand string\n500\n76.31\n43.94\n-42.4%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n67.33\n28.29\n-58.0%\n\n\nlog\n<all>\n1000000\ncountry\n10\n115.40\n101.46\n-12.1%\n\n\nlog\n<all>\n1000000\ncountry\n25\n115.06\n92.15\n-19.9%\n\n\nlog\n<all>\n1000000\ncountry\n50\n114.03\n90.06\n-21.0%\n\n\nlog\n<all>\n1000000\ncountry\n100\n99.30\n80.07\n-19.4%\n\n\nlog\n<all>\n1000000\ncountry\n500\n75.64\n43.44\n-42.6%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n66.05\n27.94\n-57.7%\n\n\nlog\n<all>\n1000000\nrand int\n10\n118.47\n109.30\n-7.7%\n\n\nlog\n<all>\n1000000\nrand int\n25\n118.72\n99.37\n-16.3%\n\n\nlog\n<all>\n1000000\nrand int\n50\n118.25\n95.14\n-19.5%\n\n\nlog\n<all>\n1000000\nrand int\n100\n97.57\n83.39\n-14.5%\n\n\nlog\n<all>\n1000000\nrand int\n500\n86.55\n46.21\n-46.6%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n78.23\n28.94\n-63.0%\n\n\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12769051"
        },
        {
            "date": "2009-10-23T04:25:40+0000",
            "content": "While Java5 numbers are still important, I'd say that Java6 (-server of course) should be weighted far heavier?  That must be what a majority of people are running in production for new systems? ",
            "author": "Yonik Seeley",
            "id": "comment-12769053"
        },
        {
            "date": "2009-10-23T04:36:12+0000",
            "content": "Hey John, did you pull from a wiki dump or use the random index?\n\nedit\n\nNM - that explains your shortened table - no wiki results - I go it. ",
            "author": "Mark Miller",
            "id": "comment-12769055"
        },
        {
            "date": "2009-10-23T04:40:07+0000",
            "content": "Java6 is standard in production servers, since when?  What justified lucene staying java1.4 for so long if this is the case?  In my own experience, my last job only moved to java1.5 a year ago, and at my current company, we're still on 1.5, and I've seen that be pretty common, and I'm in the Valley, where things update pretty quickly. ",
            "author": "Jake Mannix",
            "id": "comment-12769056"
        },
        {
            "date": "2009-10-23T04:42:51+0000",
            "content": "I would say that of course weighting more highly linux and solaris should be done over results on macs, because while I love my mac, I've yet to see a production cluster running on MacBook Pros...  ",
            "author": "Jake Mannix",
            "id": "comment-12769058"
        },
        {
            "date": "2009-10-23T04:53:25+0000",
            "content": "Java6 is standard in production servers, since when?\n\nMaybe I'm wrong... it  was just a guess. It's just what I've seen most customers deploying new projects on.\n\nWhat justified lucene staying java1.4 for so long if this is the case?\n\nThe decision of what JVM a business should use to deploy their new app is a very different one than what Lucene should require.\nA minority of users may be justification enough to avoid requring a new JVM... unless the benefits are really that huge.  Lucene does not target the JVM that most people will be deploying on - if that were the case, I have a feeling we'd be switching to Java6 instead of Java5. ",
            "author": "Yonik Seeley",
            "id": "comment-12769059"
        },
        {
            "date": "2009-10-23T04:54:49+0000",
            "content": "Same system, Java 1.6.0_15\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n1\n317925\ntitle\n10\n105.46\n97.11\n-7.9%\n\n\nlog\n1\n317925\ntitle\n25\n109.08\n98.34\n-9.8%\n\n\nlog\n1\n317925\ntitle\n50\n108.01\n93.99\n-13.0%\n\n\nlog\n1\n317925\ntitle\n100\n105.79\n84.08\n-20.5%\n\n\nlog\n1\n317925\ntitle\n500\n91.12\n50.28\n-44.8%\n\n\nlog\n1\n317925\ntitle\n1000\n80.51\n33.59\n-58.3%\n\n\nlog\n<all>\n1000000\ntitle\n10\n113.89\n105.39\n-7.5%\n\n\nlog\n<all>\n1000000\ntitle\n25\n113.14\n102.13\n-9.7%\n\n\nlog\n<all>\n1000000\ntitle\n50\n111.30\n96.51\n-13.3%\n\n\nlog\n<all>\n1000000\ntitle\n100\n86.77\n83.86\n-3.4%\n\n\nlog\n<all>\n1000000\ntitle\n500\n78.00\n42.15\n-46.0%\n\n\nlog\n<all>\n1000000\ntitle\n1000\n70.50\n27.02\n-61.7%\n\n\nlog\n<all>\n1000000\nrand string\n10\n107.78\n106.09\n-1.6%\n\n\nlog\n<all>\n1000000\nrand string\n25\n103.09\n102.53\n-0.5%\n\n\nlog\n<all>\n1000000\nrand string\n50\n106.42\n95.17\n-10.6%\n\n\nlog\n<all>\n1000000\nrand string\n100\n86.28\n85.41\n-1.0%\n\n\nlog\n<all>\n1000000\nrand string\n500\n76.69\n37.76\n-50.8%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n68.48\n22.95\n-66.5%\n\n\nlog\n<all>\n1000000\ncountry\n10\n103.36\n106.79\n3.3%\n\n\nlog\n<all>\n1000000\ncountry\n25\n103.43\n102.69\n-0.7%\n\n\nlog\n<all>\n1000000\ncountry\n50\n102.93\n94.97\n-7.7%\n\n\nlog\n<all>\n1000000\ncountry\n100\n108.49\n85.71\n-21.0%\n\n\nlog\n<all>\n1000000\ncountry\n500\n80.87\n38.23\n-52.7%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n67.24\n22.79\n-66.1%\n\n\nlog\n<all>\n1000000\nrand int\n10\n120.59\n112.03\n-7.1%\n\n\nlog\n<all>\n1000000\nrand int\n25\n119.80\n107.49\n-10.3%\n\n\nlog\n<all>\n1000000\nrand int\n50\n119.96\n98.84\n-17.6%\n\n\nlog\n<all>\n1000000\nrand int\n100\n88.58\n89.24\n0.7%\n\n\nlog\n<all>\n1000000\nrand int\n500\n83.50\n40.13\n-51.9%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n74.80\n23.83\n-68.1%\n\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12769060"
        },
        {
            "date": "2009-10-23T05:05:13+0000",
            "content": "Java6 is standard in production servers, since when?\n\nMaybe I'm wrong... it was just a guess. It's just what I've seen most customers deploying new projects on.\n\nThats my impression too - Java 1.6 is mainly just a bug fix and performance release and has been out for a while, so its usually the choice I've seen.\nSounds like Uwe thinks its more buggy though, so who knows if thats a good idea  ",
            "author": "Mark Miller",
            "id": "comment-12769085"
        },
        {
            "date": "2009-10-23T05:09:32+0000",
            "content": "John, what happened to your topn:100 results? ",
            "author": "Mark Miller",
            "id": "comment-12769088"
        },
        {
            "date": "2009-10-23T05:12:34+0000",
            "content": "There was a bad stretch in Java6... they plopped in a major JVM upgrade (not just bug fixes) and there were bugs.  I think that's been behind us for a little while now though.  If someone were starting a project today, I'd recommend the latest Java6 JVM. ",
            "author": "Yonik Seeley",
            "id": "comment-12769089"
        },
        {
            "date": "2009-10-23T05:15:22+0000",
            "content": "bq: topn:100\nI had made changes to sortBench.py to look at each run. And forgot to add back in 100  My bad. ",
            "author": "John Wang",
            "id": "comment-12769090"
        },
        {
            "date": "2009-10-23T05:22:02+0000",
            "content": "Anyone got a Windows box to run this on? I'm only running windows on a VM these days. ",
            "author": "Mark Miller",
            "id": "comment-12769092"
        },
        {
            "date": "2009-10-23T05:37:04+0000",
            "content": "There was a bad stretch in Java6... \n\nBut how can that be ? Number 10 of the top 10 of whats new is the -lites! \n\n10. The -lities: Quality, Compatibility, Stability ",
            "author": "Mark Miller",
            "id": "comment-12769097"
        },
        {
            "date": "2009-10-23T06:03:48+0000",
            "content": "Thats my impression too - Java 1.6 is mainly just a bug fix and performance release and has been out for a while, so its usually the choice I've seen. Sounds like Uwe thinks its more buggy though, so who knows if thats a good idea \n\nBecause of this, for Lucene 3.0 we should say, it's a Java 1.5 compatible release. As Mark said, Java 6 does not contain anything really new useable for Lucene, so we are fine with staying on 1.5. If somebody wants to use 1.5 or 1.6 it's his choice, but we should not force people to use 1.6. If at least one developer uses 1.5 for developing, we have no problem with maybe some added functions in core classes we accidently use (like String.isEmpty() - which is a common problem because it was added in 1.6 and many developers use it intuitive).\n\nEven though 1.5 is EOLed by Sun, they recently added a new release 1.5.0_21. I was also wondering about that, but it seems that Sun is still providing \"support\" for it.\n\nAbout the stability: maybe it is better now, but I have seen so many crashed JVMs in the earlier versions <= _12, so I stayed on 1.5. But we are also thinking of switching here at some time. ",
            "author": "Uwe Schindler",
            "id": "comment-12769099"
        },
        {
            "date": "2009-10-23T07:04:32+0000",
            "content": "I think I found the reason for the discrepancy: 32 vs 64 bit:\n\n32-bit, run\njwang-mn:benchmark jwang$ python -u sortBench.py -report john3\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n<all>\n1000000\nrand string\n10\n92.24\n103.65\n12.4%\n\n\nlog\n<all>\n1000000\nrand string\n25\n91.88\n102.06\n11.1%\n\n\nlog\n<all>\n1000000\nrand string\n50\n91.72\n99.07\n8.0%\n\n\nlog\n<all>\n1000000\nrand string\n100\n106.26\n90.61\n-14.7%\n\n\nlog\n<all>\n1000000\nrand string\n500\n86.38\n59.88\n-30.7%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n74.88\n39.93\n-46.7%\n\n\nlog\n<all>\n1000000\ncountry\n10\n92.33\n103.79\n12.4%\n\n\nlog\n<all>\n1000000\ncountry\n25\n92.27\n101.60\n10.1%\n\n\nlog\n<all>\n1000000\ncountry\n50\n91.58\n99.14\n8.3%\n\n\nlog\n<all>\n1000000\ncountry\n100\n100.76\n82.25\n-18.4%\n\n\nlog\n<all>\n1000000\ncountry\n500\n75.18\n48.65\n-35.3%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n67.68\n32.67\n-51.7%\n\n\nlog\n<all>\n1000000\nrand int\n10\n88.14\n101.93\n15.6%\n\n\nlog\n<all>\n1000000\nrand int\n25\n95.02\n96.14\n1.2%\n\n\nlog\n<all>\n1000000\nrand int\n50\n96.54\n89.61\n-7.2%\n\n\nlog\n<all>\n1000000\nrand int\n100\n88.58\n92.06\n3.9%\n\n\nlog\n<all>\n1000000\nrand int\n500\n103.60\n62.25\n-39.9%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n92.36\n40.84\n-55.8%\n\n\n\n\n\n64bit run:\njwang-mn:benchmark jwang$ python -u sortBench.py -report john4\n\n\n\n\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nlog\n<all>\n1000000\nrand string\n10\n119.59\n107.52\n-10.1%\n\n\nlog\n<all>\n1000000\nrand string\n25\n119.25\n105.05\n-11.9%\n\n\nlog\n<all>\n1000000\nrand string\n50\n117.22\n101.99\n-13.0%\n\n\nlog\n<all>\n1000000\nrand string\n100\n95.78\n86.19\n-10.0%\n\n\nlog\n<all>\n1000000\nrand string\n500\n76.05\n54.71\n-28.1%\n\n\nlog\n<all>\n1000000\nrand string\n1000\n68.37\n38.94\n-43.0%\n\n\nlog\n<all>\n1000000\ncountry\n10\n119.68\n108.12\n-9.7%\n\n\nlog\n<all>\n1000000\ncountry\n25\n119.10\n105.72\n-11.2%\n\n\nlog\n<all>\n1000000\ncountry\n50\n115.85\n99.70\n-13.9%\n\n\nlog\n<all>\n1000000\ncountry\n100\n97.44\n91.03\n-6.6%\n\n\nlog\n<all>\n1000000\ncountry\n500\n78.92\n40.97\n-48.1%\n\n\nlog\n<all>\n1000000\ncountry\n1000\n68.48\n30.43\n-55.6%\n\n\nlog\n<all>\n1000000\nrand int\n10\n121.64\n108.82\n-10.5%\n\n\nlog\n<all>\n1000000\nrand int\n25\n121.68\n113.92\n-6.4%\n\n\nlog\n<all>\n1000000\nrand int\n50\n120.80\n110.45\n-8.6%\n\n\nlog\n<all>\n1000000\nrand int\n100\n101.36\n95.68\n-5.6%\n\n\nlog\n<all>\n1000000\nrand int\n500\n90.15\n60.29\n-33.1%\n\n\nlog\n<all>\n1000000\nrand int\n1000\n80.23\n40.67\n-49.3%\n\n\n\n\n ",
            "author": "John Wang",
            "id": "comment-12769116"
        },
        {
            "date": "2009-10-23T07:15:59+0000",
            "content": "wrote a small test and verified that 64bit vm's string compare is much faster than that of 32-bit. (kinda makes sense)\nand the above numbers now all make sense. ",
            "author": "John Wang",
            "id": "comment-12769119"
        },
        {
            "date": "2009-10-23T07:53:15+0000",
            "content": "So it does not have something to do with Java 1.5/1.6 but more with 32/64 bit. As most servers are running 64 bit, I think the new 2.9 search API is fine?\n\nI agree with you, the new API is cleaner at all, the old API could only be reimplemented with major refactorings, as it does not fit well in multi-segment search.\n\nBy the way, I found during refactoring for Java5 some inconsistenceies in MultiSearcher/ParallelMultiSearcher, which uses FieldDocSortedHitQueue (its used nowhere else anymore): During sorting it uses (when merging the queues of all Searchers) some native compareTo operations, which may not work correctly with custom comparators. Is this correct? In my opinion this queue should also somehow use at least the FieldComparator's compare functions.\nMark, do not understand it completely, but how does this fit together. I added a warning because of very strange casts in the source code (unsafe casts) and a SuppressWarnings(\"unchecked\") so its easy to find in FieldDocSortedHitQueue. The temp variable is just there for the unchecked warning supress (but it really needs to be fixed). ",
            "author": "Uwe Schindler",
            "id": "comment-12769127"
        },
        {
            "date": "2009-10-23T13:30:59+0000",
            "content": "but how does this fit together.\n\nThats what Comparable FieldComparator#value is for - fillFields will grab all those and load up FieldDoc fields - so the custom FieldComparator is tied into it - it creates Comparable objects that can be compared by the native compareTos. (the old API did the same thing)\n\n\n  /**\n   * Given a queue Entry, creates a corresponding FieldDoc\n   * that contains the values used to sort the given document.\n   * These values are not the raw values out of the index, but the internal\n   * representation of them. This is so the given search hit can be collated by\n   * a MultiSearcher with other search hits.\n   * \n   * @param entry The Entry used to create a FieldDoc\n   * @return The newly created FieldDoc\n   * @see Searchable#search(Weight,Filter,int,Sort)\n   */\n  FieldDoc fillFields(final Entry entry) {\n    final int n = comparators.length;\n    final Comparable[] fields = new Comparable[n];\n    for (int i = 0; i < n; ++i) {\n      fields[i] = comparators[i].value(entry.slot);\n    }\n    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores\n    return new FieldDoc(entry.docID, entry.score, fields);\n  }\n\n ",
            "author": "Mark Miller",
            "id": "comment-12769221"
        },
        {
            "date": "2009-10-23T13:33:23+0000",
            "content": "As most servers are running 64 bit,\n\nAren't we at the tipping point where even non servers are 64bit now? My consumer desktop/laptops have been 64-bit for years now. ",
            "author": "Mark Miller",
            "id": "comment-12769222"
        },
        {
            "date": "2009-10-23T13:54:43+0000",
            "content": "it creates Comparable objects that can be compared by the native compareTos. (the old API did the same thing)\n\nOK understood. I will try to fix the generics somehow to be able to remove the SuppressWarnings. ",
            "author": "Uwe Schindler",
            "id": "comment-12769227"
        },
        {
            "date": "2009-10-23T16:06:37+0000",
            "content": "New patch attached:\n\n\n\tMade some basic code level optimizations, eg created an explicit\n    DocIDPriorityQueue (that deals in int not Object, to avoid\n    casting), subclassed that directly to a SortByStringQueue and a\n    SortByIntQueue.  It turns out that if statement (when comparing\n    int values) must stay because the subtraction can overflow int.\n\n\n\n\n\tAdded \"sortBench.py -verify\" that quickly runs each API across all\n    tests and confirms results are identical \u2013 proxy for real unit\n    tests\n\n\n\n\n\tAdded \"Source\" (wiki or random) to Jira table output\n\n\n\n\n\tPrint java/os version at start\n\n\n\nI'll re-run my test. ",
            "author": "Michael McCandless",
            "id": "comment-12769275"
        },
        {
            "date": "2009-10-23T16:08:39+0000",
            "content": "New patch, fixes silly bug in sortBench.py. ",
            "author": "Michael McCandless",
            "id": "comment-12769277"
        },
        {
            "date": "2009-10-23T16:33:17+0000",
            "content": "Env:\n\nJAVA:\njava version \"1.5.0_19\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02)\nJava HotSpot(TM) 64-Bit Server VM (build 1.5.0_19-b02, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\nResults:\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n318481\ntitle\n10\n98.47\n104.60\n6.2%\n\n\nwiki\nlog\n1\n318481\ntitle\n25\n97.90\n103.63\n5.9%\n\n\nwiki\nlog\n1\n318481\ntitle\n50\n105.12\n101.50\n-3.4%\n\n\nwiki\nlog\n1\n318481\ntitle\n100\n102.30\n108.59\n6.1%\n\n\nwiki\nlog\n1\n318481\ntitle\n500\n89.43\n79.40\n-11.2%\n\n\nwiki\nlog\n1\n318481\ntitle\n1000\n82.83\n63.75\n-23.0%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n10\n152.56\n157.40\n3.2%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n25\n151.95\n148.52\n-2.3%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n50\n148.52\n142.90\n-3.8%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n100\n127.70\n138.72\n8.6%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n500\n104.30\n90.30\n-13.4%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n1000\n99.10\n66.05\n-33.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n10\n153.13\n157.74\n3.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n25\n128.79\n150.62\n17.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n50\n122.46\n153.95\n25.7%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n100\n116.26\n141.43\n21.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n500\n98.24\n96.17\n-2.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n1000\n86.38\n71.95\n-16.7%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n10\n148.65\n153.23\n3.1%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n25\n148.52\n152.69\n2.8%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n50\n122.01\n149.52\n22.5%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n100\n120.39\n145.99\n21.3%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n500\n99.70\n95.65\n-4.1%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n1000\n90.18\n69.46\n-23.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n10\n150.85\n171.22\n13.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n25\n151.13\n167.94\n11.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n50\n152.51\n162.23\n6.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n100\n130.54\n145.04\n11.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n500\n108.38\n43.74\n-59.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n1000\n98.27\n63.56\n-35.3%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12769287"
        },
        {
            "date": "2009-10-23T16:58:00+0000",
            "content": "\n32 bit 1.5 JRE:\n\nJAVA:\njava version \"1.5.0_19\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02)\nJava HotSpot(TM) Server VM (build 1.5.0_19-b02, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n318481\ntitle\n10\n97.31\n92.69\n-4.7%\n\n\nwiki\nlog\n1\n318481\ntitle\n25\n96.74\n92.09\n-4.8%\n\n\nwiki\nlog\n1\n318481\ntitle\n50\n98.57\n90.03\n-8.7%\n\n\nwiki\nlog\n1\n318481\ntitle\n100\n97.20\n103.72\n6.7%\n\n\nwiki\nlog\n1\n318481\ntitle\n500\n84.14\n78.23\n-7.0%\n\n\nwiki\nlog\n1\n318481\ntitle\n1000\n77.84\n63.62\n-18.3%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n10\n114.99\n136.86\n19.0%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n25\n114.63\n125.92\n9.8%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n50\n113.33\n130.58\n15.2%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n100\n115.36\n111.81\n-3.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n500\n107.30\n86.16\n-19.7%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n1000\n98.07\n55.39\n-43.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n10\n115.55\n140.86\n21.9%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n25\n125.66\n137.15\n9.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n50\n123.58\n133.82\n8.3%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n100\n115.51\n134.82\n16.7%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n500\n102.73\n93.24\n-9.2%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n1000\n88.70\n65.09\n-26.6%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n10\n113.92\n139.72\n22.6%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n25\n113.44\n131.36\n15.8%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n50\n122.88\n128.62\n4.7%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n100\n121.88\n135.58\n11.2%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n500\n96.94\n79.38\n-18.1%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n1000\n82.01\n62.31\n-24.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n10\n124.58\n134.20\n7.7%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n25\n123.46\n134.82\n9.2%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n50\n117.96\n128.61\n9.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n100\n113.92\n122.09\n7.2%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n500\n105.49\n38.92\n-63.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n1000\n92.27\n53.14\n-42.4%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12769295"
        },
        {
            "date": "2009-10-23T17:56:10+0000",
            "content": "Mike, thanks for all the hard work on this - it's clearly far more work than anyone has spent yet on just doing the upgrade to the newer api, and that's appreciated.  \n\nAm I wrong in thinking that these results are pretty ambiguous?  How often to people take the top 500 or top 1000 sorted hits?  If you don't focus on that case (that of looking for pages 50 through 100 of normal 10-per-page search results), there's a bunch of green, a bunch of red, both techniques are +/- 10-20% of each other?\n\nIs that what everyone else sees of Mike's newest numbers here, or am I misreading them? ",
            "author": "Jake Mannix",
            "id": "comment-12769328"
        },
        {
            "date": "2009-10-23T18:04:53+0000",
            "content": "Mike's latest results are more ambiguous - let me run the new stuff on Linux too. ",
            "author": "Mark Miller",
            "id": "comment-12769332"
        },
        {
            "date": "2009-10-23T19:04:52+0000",
            "content": "JAVA:\njava version \"1.5.0_20\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_20-b02)\nJava HotSpot(TM) 64-Bit Server VM (build 1.5.0_20-b02, mixed mode)\n\nOS:\nLinux quad-laptop 2.6.31-14-generic #48-Ubuntu SMP  x86_64 GNU/Linux\n\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n317914\ntitle\n10\n80.09\n78.20\n-2.4%\n\n\nwiki\nlog\n1\n317914\ntitle\n25\n80.12\n79.51\n-0.8%\n\n\nwiki\nlog\n1\n317914\ntitle\n50\n78.61\n76.03\n-3.3%\n\n\nwiki\nlog\n1\n317914\ntitle\n100\n77.18\n75.13\n-2.7%\n\n\nwiki\nlog\n1\n317914\ntitle\n500\n75.01\n54.74\n-27.0%\n\n\nwiki\nlog\n1\n317914\ntitle\n1000\n67.77\n41.29\n-39.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n10\n109.30\n119.29\n9.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n25\n108.34\n116.02\n7.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n50\n106.86\n110.70\n3.6%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n100\n94.72\n101.10\n6.7%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n500\n78.69\n62.04\n-21.2%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n1000\n71.93\n43.05\n-40.2%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n10\n112.81\n117.80\n4.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n25\n113.92\n115.73\n1.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n50\n113.55\n110.08\n-3.1%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n100\n90.30\n95.35\n5.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n500\n76.77\n51.88\n-32.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n1000\n66.78\n36.93\n-44.7%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n10\n114.26\n118.72\n3.9%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n25\n113.96\n115.81\n1.6%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n50\n113.59\n109.78\n-3.4%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n100\n91.97\n94.05\n2.3%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n500\n75.03\n27.37\n-63.5%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n1000\n66.62\n34.85\n-47.7%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n10\n118.06\n124.42\n5.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n25\n117.76\n120.76\n2.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n50\n117.35\n115.81\n-1.3%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n100\n96.35\n103.60\n7.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n500\n87.04\n50.63\n-41.8%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n1000\n77.59\n35.47\n-54.3%\n\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12769365"
        },
        {
            "date": "2009-10-23T19:08:01+0000",
            "content": "I agree the new results are now more ambiguous. ",
            "author": "Michael McCandless",
            "id": "comment-12769368"
        },
        {
            "date": "2009-10-25T19:04:58+0000",
            "content": "Time for the reevaluations?\n\nWith the previous numbers, I would have said I'd -1 it. Now the numbers have changed. Its less clear.\n\nHowever - I'm still leaning against. I don't like the 30-50% drops even if top500,1000 are not as common as top 10,100. Its a nasty hit for those that do it. It doesn't carry tons of weight, but I don't like it.\n\nI also really don't like shifting back to this API right after rolling out the new one. Its very ugly. Its not a good precedent to set for our users. And unless we make a change in our back compat policy, we are stuck with both API's till 4.0. Managing two API's is something else I don't like.\n\nFinally, creating a custom sort is an advanced operation. The far majority of Lucene users will be good with the built in sorts. If you need a new custom one, you are into some serious stuff already. You can handle the new API. We have seen users handle it. Uwe had ideas for helping in that regard, and documentation can probably still be improved based on future user interactions.\n\nI'm not as dead set against it as I was, but I still don't think I'm for the change myself. ",
            "author": "Mark Miller",
            "id": "comment-12769850"
        },
        {
            "date": "2009-10-25T20:31:10+0000",
            "content": "Mark, you say with the previous numbers, you'd say \"-1\", but if you look at the most common use case (top 10), the simpler API is faster in almost all cases, and in some cases it's 10-20% faster.   Top 500, top 1000 are not only just \"not as common\", they're probably at the 1% level, or less.\n\nAs far as shifting back, API-wise, that really shouldn't be a factor: 2.9 just came out, and what, we stick with a slightly slower API (for the most common use case across all Lucene users), which happens to be more complex, and more importantly: just very nonstandard - Comparable is very familiar to everyone, even if you have to have two forms, one for primitives, one for Objects - an api which doesn't have the whole slew of compare(), compareBottom(), copy(), setBottom(), value() and setNextReader() has a tremendous advantage over one which does.  \n\nIt's \"advanced\" to implement a custom sort, but it will be easier if it's not complex, and then it doesn't need to be \"advanced\" (shouldn't we be striving to make there be less APIs which are listed as \"advanced\", and instead more features which can do complex things but are still listed as things \"normal users\" can do).\n\nI think it's great precedent to set with users to say, \"oops!  we found that this new (just now as of this version) api was unnecessarily clumsy, we're shifting back to a simpler one which is just like the one you used to have\".  Sticking with a worse api because it performs better in only extreme scenarios because \"we already moved on to this new api, shouldn't go back now, don't want to admit we ever made a mistake!\" is what is \"ugly\".\n\nThe main thing to remember is that the entire thinking around making this different from the old was only because it seemed that using a simpler api would perform much worse than this one, and it does not appear that this is the case.  If that original reasoning turns out to have been incorrect, then the answer is simple: go with the simpler API now before users do get used to using the new one.\n\nIf it turns out I'm wrong, and lots of users sort based on field values for the top 1000 entries often, or that the most recent runs turn out to be flukes and are not typical performance, only then would I'd change my opinion. ",
            "author": "Jake Mannix",
            "id": "comment-12769860"
        },
        {
            "date": "2009-10-25T20:42:14+0000",
            "content": "Given good enough reasons, I could see saying we made a mistake and switching back - as it is, for the reasons I've said, I don't find that to be the case. I don't feel the new API was a mistake yet. \n\nLots of other guys to weigh in though. If everyone else feels like its the right move, I'm not going to -1 it - just weighing in with how I feel. \n\nI'm not seeing 10-20% faster across the board - on my system it doesnt even hit 10% and I'm a linux user and advocate. I'm all for performance, but < 10% here and there is not enough to sway me against 30-50% loses in the large queue cases, combined with having to shift back. Its not a clear win either way, but I've said which way I lean.\n\nLuckily, its not just me you have to convince. Lots of smart people still to weigh in. ",
            "author": "Mark Miller",
            "id": "comment-12769863"
        },
        {
            "date": "2009-10-26T10:37:59+0000",
            "content": "Looking more at this, I think there are a few problems with the\ncurrent test:\n\n\n\tThe \"random\" index is smallish \u2013 it has only 7 segments.  I'd\n    like to test across a wider range of segments.\n\n\n\n\n\tMy \"optimized\" code for the multi-PQ (old) API is too optimized \u2013\n    I conflated the comparator directly into the PQ (eg I have\n    SortByIntQueue, SortByStringQueue, that directly subclass\n    DocIDPriorityQueue and override only compare).  For source code\n    specialization this would be appropriate (it is \"correct\"), but in\n    order to test the two extension APIs, it's not.  I'll rework it to\n    pull it back out into a separate comparator.\n\n\n\n\n\tWe should test more realistic queries & index.  Of all tested so\n    far, query \"1\" on wiki index is the most realistic, and we see the\n    least gains there.  The : query is unnatural, in part because\n    the I think first N title in wiki'd index appear to be roughly\n    alpha sorted.  Random strings & random ints are very realistic.\n\n\n\n\n\tWe need more results from diverse envs \u2013 eg Windows (different\n    versions), different Linux's, JREs, etc.\n\n\n\nAlso, I really do not like the large perf hits at highish topN sizes.\nAdmittedly it's unusual (though I suspect not THAT rare) to use such a\nlarge topN, but, I esepically don't like that it doesn't degrade\ngracefully \u2013 it's surprising.  Likewise I'd like to see with highish\nnumber of segments how things degrade.\n\nI would also give more weight to the JRE 1.6, 64 bit, results.  Yes we\ncan debate what's the popular JRE in production today, but this API is\ngoing to last a looong time in Lucene so I think we should bias\ntowards what will be the future JRE.  Maybe we should even test on JDK\n7 preview... though it's probably too early to make any decisisions\nbased in its performance. ",
            "author": "Michael McCandless",
            "id": "comment-12769974"
        },
        {
            "date": "2009-10-26T22:33:57+0000",
            "content": "Waiting for Mike's changes to try and do some different varying tests, but while I wait, here is a 1.6 test to go with that 1.5 test (kind of funny how it likes 100 over 50 and 500 like it does):\n\nJAVA:\njava version \"1.6.0_15\"\nJava(TM) SE Runtime Environment (build 1.6.0_15-b03)\nJava HotSpot(TM) 64-Bit Server VM (build 14.1-b02, mixed mode)\n\nOS:\nLinux quad-laptop 2.6.31-14-generic #48-Ubuntu SMP x86_64 GNU/Linux\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n317914\ntitle\n10\n105.49\n106.68\n1.1%\n\n\nwiki\nlog\n1\n317914\ntitle\n25\n108.78\n110.37\n1.5%\n\n\nwiki\nlog\n1\n317914\ntitle\n50\n106.79\n106.35\n-0.4%\n\n\nwiki\nlog\n1\n317914\ntitle\n100\n105.26\n100.10\n-4.9%\n\n\nwiki\nlog\n1\n317914\ntitle\n500\n91.03\n72.09\n-20.8%\n\n\nwiki\nlog\n1\n317914\ntitle\n1000\n80.17\n53.61\n-33.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n10\n114.44\n116.63\n1.9%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n25\n113.03\n116.77\n3.3%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n50\n111.52\n112.33\n0.7%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n100\n86.97\n101.80\n17.1%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n500\n77.79\n66.45\n-14.6%\n\n\nwiki\nlog\n<all>\n1000000\ntitle\n1000\n70.76\n46.45\n-34.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n10\n118.47\n112.77\n-4.8%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n25\n117.98\n109.12\n-7.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n50\n117.51\n105.05\n-10.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n100\n86.44\n96.08\n11.2%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n500\n77.03\n60.82\n-21.0%\n\n\nrandom\nlog\n<all>\n1000000\nrand string\n1000\n68.84\n40.33\n-41.4%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n10\n118.25\n110.85\n-6.3%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n25\n117.55\n110.41\n-6.1%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n50\n117.22\n103.35\n-11.8%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n100\n87.01\n94.02\n8.1%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n500\n79.53\n60.63\n-23.8%\n\n\nrandom\nlog\n<all>\n1000000\ncountry\n1000\n72.62\n40.15\n-44.7%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n10\n125.54\n116.18\n-7.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n25\n124.83\n111.81\n-10.4%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n50\n124.17\n108.26\n-12.8%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n100\n88.58\n100.63\n13.6%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n500\n81.81\n64.23\n-21.5%\n\n\nrandom\nlog\n<all>\n1000000\nrand int\n1000\n74.14\n42.79\n-42.3%\n\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12770242"
        },
        {
            "date": "2009-10-27T09:29:35+0000",
            "content": "kind of funny how it likes 100 over 50 and 500 like it does\n\nI've also noticed this.  It's weird. ",
            "author": "Michael McCandless",
            "id": "comment-12770433"
        },
        {
            "date": "2009-10-27T09:34:51+0000",
            "content": "New patch attached, that un-conflates the comparator & PQ.  I think this patch more accurately separates the comparator from the queue, ie, better matches the approach we'd go with if we did this for \"real\".\n\nAlso, I turned on the BALANCED case in sortBench.py, which also generates 20-segment balanced (same segment size) index for both wiki & random, and runs the tests on those.\n\nFinally, I added query \"2\" for testing, if the index is wikipedia. ",
            "author": "Michael McCandless",
            "id": "comment-12770434"
        },
        {
            "date": "2009-10-27T10:26:13+0000",
            "content": "Small change: add in the same reverseMul that singlePQ must do, to multiPQ, to handle reversed sort. ",
            "author": "Michael McCandless",
            "id": "comment-12770450"
        },
        {
            "date": "2009-10-27T10:26:55+0000",
            "content": "Ugh \u2013 last one was wrong patch.  This one should be right. ",
            "author": "Michael McCandless",
            "id": "comment-12770452"
        },
        {
            "date": "2009-10-27T17:11:48+0000",
            "content": "OK yet more results, this time on 5M doc indexes:\n\nJAVA:\njava version \"1.6.0_14\"\nJava(TM) SE Runtime Environment (build 1.6.0_14-b08)\nJava HotSpot(TM) 64-Bit Server VM (build 14.0-b16, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n1169569\ntitle\n10\n26.80\n28.08\n4.8%\n\n\nwiki\nlog\n1\n1169569\ntitle\n25\n26.52\n27.68\n4.4%\n\n\nwiki\nlog\n1\n1169569\ntitle\n50\n26.37\n26.58\n0.8%\n\n\nwiki\nlog\n1\n1169569\ntitle\n100\n25.91\n26.50\n2.3%\n\n\nwiki\nlog\n1\n1169569\ntitle\n500\n24.28\n23.78\n-2.1%\n\n\nwiki\nlog\n1\n1169569\ntitle\n1000\n22.64\n21.32\n-5.8%\n\n\nwiki\nlog\n2\n1088167\ntitle\n10\n28.23\n29.16\n3.3%\n\n\nwiki\nlog\n2\n1088167\ntitle\n25\n27.90\n28.54\n2.3%\n\n\nwiki\nlog\n2\n1088167\ntitle\n50\n28.04\n28.62\n2.1%\n\n\nwiki\nlog\n2\n1088167\ntitle\n100\n27.57\n27.88\n1.1%\n\n\nwiki\nlog\n2\n1088167\ntitle\n500\n25.85\n25.27\n-2.2%\n\n\nwiki\nlog\n2\n1088167\ntitle\n1000\n23.65\n22.33\n-5.6%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n10\n27.32\n30.47\n11.5%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n25\n26.29\n31.16\n18.5%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n50\n26.64\n30.03\n12.7%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n100\n21.37\n29.67\n38.8%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n500\n21.05\n26.15\n24.2%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n1000\n19.83\n21.97\n10.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n29.00\n32.39\n11.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n28.39\n32.36\n14.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n28.74\n31.90\n11.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n28.44\n31.16\n9.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n28.20\n28.03\n-0.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n19.47\n24.50\n25.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n29.18\n32.15\n10.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n28.66\n31.74\n10.7%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n28.02\n32.00\n14.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n28.68\n31.27\n9.0%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n28.16\n26.66\n-5.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n19.58\n23.03\n17.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n28.75\n29.51\n2.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n28.74\n28.96\n0.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n29.29\n28.63\n-2.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n29.14\n28.38\n-2.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n28.44\n24.59\n-13.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n20.06\n21.71\n8.2%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n10\n25.70\n27.06\n5.3%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n25\n25.91\n27.26\n5.2%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n50\n25.84\n26.63\n3.1%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n100\n25.51\n25.66\n0.6%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n500\n24.39\n22.32\n-8.5%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n1000\n22.66\n18.74\n-17.3%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n10\n27.86\n28.77\n3.3%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n25\n27.46\n28.37\n3.3%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n50\n27.49\n28.05\n2.0%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n100\n27.64\n26.25\n-5.0%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n500\n25.52\n23.26\n-8.9%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n1000\n23.77\n19.69\n-17.2%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n10\n27.13\n31.05\n14.4%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n25\n27.81\n30.47\n9.6%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n50\n27.99\n29.81\n6.5%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n100\n22.00\n29.35\n33.4%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n500\n21.03\n24.12\n14.7%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n1000\n20.43\n19.80\n-3.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n28.16\n32.13\n14.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n28.40\n31.44\n10.7%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n28.13\n31.07\n10.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n27.93\n31.25\n11.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n27.49\n26.91\n-2.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n19.13\n22.69\n18.6%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n27.74\n32.11\n15.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n28.31\n32.15\n13.6%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n27.98\n31.17\n11.4%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n27.66\n30.85\n11.5%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n27.57\n26.25\n-4.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n19.01\n21.27\n11.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n28.16\n28.57\n1.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n28.39\n28.56\n0.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n28.08\n27.84\n-0.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n27.81\n27.23\n-2.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n27.80\n24.01\n-13.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n19.87\n20.14\n1.4%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12770578"
        },
        {
            "date": "2009-10-27T17:31:01+0000",
            "content": "Excellent, good to see that my big-O analysis is holding up on the 5M doc set: as the sub-leading terms drop off and become negligible, any improvement of singlePQ over multiPQ starts to go away entirely.\n\nStill seems to be some statistical fluctuations here and there though (why would 1000 hits ever have better perf for multiPQ vs singlePQ compared to 500 hits?), but I guess that's entropy for you... ",
            "author": "Jake Mannix",
            "id": "comment-12770587"
        },
        {
            "date": "2009-10-27T18:45:15+0000",
            "content": "Hmm disregard those results \u2013 something is wrong. ",
            "author": "Michael McCandless",
            "id": "comment-12770602"
        },
        {
            "date": "2009-10-27T18:51:59+0000",
            "content": "What happened with these ones, Mike? ",
            "author": "Jake Mannix",
            "id": "comment-12770604"
        },
        {
            "date": "2009-10-27T19:05:56+0000",
            "content": "First, my reverseMul changes did not actually compile (I've now added \"ant compile\" to sortBench.py).  Second, the field we are sorting on for int seems to be null / have too many zeros.  I'm still digging on why... ",
            "author": "Michael McCandless",
            "id": "comment-12770611"
        },
        {
            "date": "2009-10-27T19:46:48+0000",
            "content": "OK new rev attached.  In previous tests, ints were restricted to 0..20000.  Now they span the full int range (randomly).  I added \"ant compile\" to sortBench.py.  I improved a bit the merging at the end of multi-PQ, by up-front getting the Comparable (rather than recomputing on every comparison), and I use this to return the topN comparables, which I now print in the logs.\n\nI'll return on my opensolaris box... ",
            "author": "Michael McCandless",
            "id": "comment-12770620"
        },
        {
            "date": "2009-10-27T23:53:13+0000",
            "content": "\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n22.02\n22.99\n4.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n21.45\n22.91\n6.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n21.40\n22.58\n5.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n21.16\n21.93\n3.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n21.27\n17.45\n-18.0%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n20.62\n14.33\n-30.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n18.33\n21.91\n19.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n18.37\n22.20\n20.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n18.51\n22.13\n19.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n20.30\n21.91\n7.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n18.34\n18.69\n1.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n17.83\n15.04\n-15.6%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n18.49\n22.16\n19.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n18.62\n22.28\n19.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n18.30\n21.86\n19.5%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n20.46\n22.08\n7.9%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n18.02\n18.38\n2.0%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n18.28\n14.62\n-20.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n22.04\n23.18\n5.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n21.48\n23.02\n7.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n21.37\n22.96\n7.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n21.47\n22.43\n4.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n21.09\n19.60\n-7.1%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n20.37\n17.58\n-13.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n18.54\n22.49\n21.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n18.50\n22.32\n20.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n18.53\n22.24\n20.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n20.46\n22.43\n9.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n18.59\n20.83\n12.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n18.27\n18.01\n-1.4%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n18.55\n22.50\n21.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n18.48\n22.35\n20.9%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n18.48\n22.24\n20.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n20.73\n22.34\n7.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n18.77\n20.12\n7.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n17.92\n18.44\n2.9%\n\n\n\n ",
            "author": "Jake Mannix",
            "id": "comment-12770708"
        },
        {
            "date": "2009-10-27T23:55:00+0000",
            "content": "Not terribly useful, in that it's MacOS X laptop - \n\n2.4 GHz Intel Core 2 Duo, 4GB 667 MHz DDR2 SDRAM, \n\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02-304)\nJava HotSpot(TM) Client VM (build 1.5.0_19-137, mixed mode, sharing)\n\nbut it's run with the latest patch. ",
            "author": "Jake Mannix",
            "id": "comment-12770709"
        },
        {
            "date": "2009-10-28T10:09:47+0000",
            "content": "Yet another patch.  This one incorporates thread safety fixes (from LUCENE-1994) that were messing up at least the sort-by-country cases by creating many docs with null country.  Also, I noticed we were unfairly penalizing the single PQ test by passing in \"false\" for \"docsScoredInOrder\", whereas the multi-PQ case currently assumes docs are scored in order.  So I changed that param to \"true\" to make the test fair. ",
            "author": "Michael McCandless",
            "id": "comment-12770872"
        },
        {
            "date": "2009-10-28T10:22:52+0000",
            "content": "OK results using last patch:\n\nJAVA:\njava version \"1.6.0_14\"\nJava(TM) SE Runtime Environment (build 1.6.0_14-b08)\nJava HotSpot(TM) 64-Bit Server VM (build 14.0-b16, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n10\n25.87\n25.36\n-2.0%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n25\n24.95\n26.51\n6.3%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n50\n25.27\n25.28\n0.0%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n100\n24.81\n24.25\n-2.3%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n500\n22.91\n21.37\n-6.7%\n\n\nwiki\nbalanced\n1\n1169558\ntitle\n1000\n21.45\n17.73\n-17.3%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n10\n27.45\n26.97\n-1.7%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n25\n26.85\n26.88\n0.1%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n50\n26.66\n26.66\n0.0%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n100\n26.45\n26.27\n-0.7%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n500\n23.53\n21.99\n-6.5%\n\n\nwiki\nbalanced\n2\n1088162\ntitle\n1000\n23.03\n19.38\n-15.8%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n10\n26.75\n27.41\n2.5%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n25\n26.18\n27.74\n6.0%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n50\n24.28\n27.06\n11.4%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n100\n20.82\n26.96\n29.5%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n500\n19.71\n22.41\n13.7%\n\n\nwiki\nbalanced\n<all>\n5000000\ntitle\n1000\n19.65\n18.18\n-7.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n25.75\n27.66\n7.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n24.32\n27.59\n13.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n21.42\n27.19\n26.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n19.30\n26.66\n38.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n17.80\n22.00\n23.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n17.30\n18.12\n4.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n28.63\n28.04\n-2.1%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n25.02\n27.57\n10.2%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n20.50\n26.81\n30.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n19.95\n26.34\n32.0%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n18.43\n22.19\n20.4%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n18.69\n17.54\n-6.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n28.39\n29.46\n3.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n26.97\n27.73\n2.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n29.76\n27.87\n-6.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n28.63\n26.59\n-7.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n27.18\n21.67\n-20.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n25.79\n16.92\n-34.4%\n\n\nwiki\nlog\n1\n1169569\ntitle\n10\n23.98\n23.32\n-2.8%\n\n\nwiki\nlog\n1\n1169569\ntitle\n25\n25.55\n26.53\n3.8%\n\n\nwiki\nlog\n1\n1169569\ntitle\n50\n25.44\n26.04\n2.4%\n\n\nwiki\nlog\n1\n1169569\ntitle\n100\n24.43\n25.92\n6.1%\n\n\nwiki\nlog\n1\n1169569\ntitle\n500\n23.61\n22.77\n-3.6%\n\n\nwiki\nlog\n1\n1169569\ntitle\n1000\n21.88\n20.48\n-6.4%\n\n\nwiki\nlog\n2\n1088167\ntitle\n10\n28.03\n27.48\n-2.0%\n\n\nwiki\nlog\n2\n1088167\ntitle\n25\n27.20\n27.57\n1.4%\n\n\nwiki\nlog\n2\n1088167\ntitle\n50\n27.75\n27.59\n-0.6%\n\n\nwiki\nlog\n2\n1088167\ntitle\n100\n26.77\n26.69\n-0.3%\n\n\nwiki\nlog\n2\n1088167\ntitle\n500\n25.11\n24.21\n-3.6%\n\n\nwiki\nlog\n2\n1088167\ntitle\n1000\n23.27\n21.75\n-6.5%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n10\n26.51\n28.24\n6.5%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n25\n25.22\n27.58\n9.4%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n50\n24.51\n28.05\n14.4%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n100\n20.64\n27.47\n33.1%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n500\n19.69\n24.32\n23.5%\n\n\nwiki\nlog\n<all>\n5000000\ntitle\n1000\n19.30\n20.56\n6.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n25.57\n26.94\n5.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n25.56\n28.12\n10.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n21.33\n27.63\n29.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n18.96\n27.31\n44.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n18.74\n23.72\n26.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n16.88\n20.39\n20.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n28.96\n26.71\n-7.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n24.91\n27.41\n10.0%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n19.81\n27.19\n37.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n20.80\n26.80\n28.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n18.90\n23.63\n25.0%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n18.52\n19.31\n4.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n27.48\n28.12\n2.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n28.81\n27.76\n-3.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n29.60\n28.41\n-4.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n29.16\n26.44\n-9.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n27.81\n21.96\n-21.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n25.61\n18.48\n-27.8%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12770878"
        },
        {
            "date": "2009-10-28T12:55:10+0000",
            "content": "When I tried changing 5000000 to 4999999 I got exceptions... is this expected?\n\n\nRUN: balanced=balanced source=random query=*:* sort=sort_field:int nhits=10\n  log: logs/singlePQ_balanced=20_numHits=10_query=*:*_sort=sort_field:int_source=random\nTraceback (most recent call last):\n  File \"sortBench.py\", line 517, in <module>\n    main()\n  File \"sortBench.py\", line 374, in main\n    run(mode, name)\n  File \"sortBench.py\", line 486, in run\n    singlePQ = r.runOne(s, 'singlePQ_%s' % prefix, INDEX_NUM_DOCS, query, verify=doVerify)\n  File \"sortBench.py\", line 271, in runOne\n    raise RuntimeError('indexNumDocs mismatch: expected %d but got %d' % (indexNumDocs, ndocs))\nRuntimeError: indexNumDocs mismatch: expected 4999999 but got 4999996\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12770919"
        },
        {
            "date": "2009-10-28T13:38:35+0000",
            "content": "I think thats just a limitation of the parallel indexing task? I've seen it not hit the target number exactly due to how it divides the docs between threads. ",
            "author": "Mark Miller",
            "id": "comment-12770931"
        },
        {
            "date": "2009-10-28T14:09:29+0000",
            "content": "One thing that bothers me about multiPQ is the memory usage if you start paging deeper and have many segments. I've seen up to 100 segments in production systems.  100x the memory use isn't pretty.\n\nSo another thought is... 2 queues instead of N queues?\n\n\tsearch segment 1 into queue A\n\tsearch segment 2 into queue B (with possible short circuiting by the smallest value in queueA)\n\tqueue B will have larger values than queue A on average, so merge queue A into queue B (unless B is much smaller?)\n\tsearch segment 3 into queue A, short circuit by smallest in B, then merge B into A, etc\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12770940"
        },
        {
            "date": "2009-10-28T14:19:02+0000",
            "content": "Good catch!\n\nsearch segment 2 into queue B (with possible short circuiting by the smallest value in queueA) \n\nBut this needs the new Comparator API's compareBottom (with which I have no problem! ). Nevertheless, I have still the opinion, that the new Collector API is much better (even it is more complicated). Maybe the implementation behind could be switchable and work different. Even with the new API, we could have more than one PQ. ",
            "author": "Uwe Schindler",
            "id": "comment-12770942"
        },
        {
            "date": "2009-10-28T14:23:01+0000",
            "content": "I think thats just a limitation of the parallel indexing task? I've seen it not hit the target number exactly due to how it divides the docs between threads.\n\nActually that's my bad \u2013 I divide the number by 4.  So long as the number is 0 mod 4 it should work. ",
            "author": "Michael McCandless",
            "id": "comment-12770943"
        },
        {
            "date": "2009-10-28T14:25:57+0000",
            "content": "search segment 2 into queue B (with possible short circuiting by the smallest value in queueA)\n\nWell, we're not doing the short circuit trick on multiPQ right now, are we?  It would certainly speed things up, but requires the API have the convert() method available, which was the big savings on the API side to multiPQ.  If it was available, I think multiPQ (either with N or 2 queues) would perform strictly better than singlePQ, but I didn't suggest this because it seems to negate the cleanliness of the API.\n\nOne thing John mentioned offhand is that perhaps the convert() method could be optional?  If you don't implement it, you don't get to short-circuit using knowledge of previous segments, but if you do, you get maximum performance in the cases where multiPQ performs worse (mid-range hitCount, high numResultsToReturn, and in the numeric sorting case).\n\nI think maybe combining this idea with 2 queues could be the best of all worlds, with best overall speed, only twice the memory of singlePQ, and the simplest API with the addition of one new optional method? ",
            "author": "Jake Mannix",
            "id": "comment-12770945"
        },
        {
            "date": "2009-10-28T14:31:09+0000",
            "content": "I personally think this is a ways from being resolved one way or another... we shouldn't rush it, and we also shouldn't just necessarily \"revert\" to the previous API.  If we so end up switching away from FieldComparator, we should consider it a new change, and make it the best we can. \n\nthe new Collector API is much better (even it is more complicated)\n\nThe power is nice... and it does allow certain optimizations that the old one did not - such as caching a value that's not a trivial lookup by docid.  But I think that when singlePQ does lose, it's perhaps due to the extra indirection overhead of FieldComparator... how else can one explain multiPQ sometimes being faster with integers? ",
            "author": "Yonik Seeley",
            "id": "comment-12770948"
        },
        {
            "date": "2009-10-28T14:33:27+0000",
            "content": "Actually that's my bad - I divide the number by 4. So long as the number is 0 mod 4 it should work.\n\nAh right - the parallel indexing task multiplies up (eg you give the docs per thread, not the total docs), it doesn't divide down - so that doesn't make sense. I was confusing with a different reporting oddity I've seen when using the parallel task - would have to investigate again to remember it correctly though. ",
            "author": "Mark Miller",
            "id": "comment-12770949"
        },
        {
            "date": "2009-10-28T14:46:16+0000",
            "content": "how else can one explain multiPQ sometimes being faster with integers?\n\nRight, I think that's due to the higher constant overhead of single PQ.  In my most recent run, multi PQ is a bit faster when sorting by int only when queue size is 10 or 25. ",
            "author": "Michael McCandless",
            "id": "comment-12770956"
        },
        {
            "date": "2009-10-28T16:27:18+0000",
            "content": "So if we're considering new comparator APIs, and the indirection seems to be slowing things down... one thing to think about is how to eliminate that indirection.\n\nEven thinking about the multiPQ case - why should one need more than a single PQ when dealing with primitives that don't depend on context (i.e. everything except ord).  If the comparator API had a way to set (or return) a primitive value for a single docid, and then those were compared (either directly by the PQ or via a callback), there wouldn't be an issue with reader transitions (because you don't compare id vs id) and hence no need for multiple priority queues.  Avoiding the creation of intermediate Comparable objects also seems desirable.\n\nPerhaps do it how \"score\" is handled now... inlined into Entry?  Should make heap rebalancing faster (fewer callbacks, fewer array lookups). ",
            "author": "Yonik Seeley",
            "id": "comment-12770978"
        },
        {
            "date": "2009-10-28T16:32:37+0000",
            "content": "Linux odin 2.6.28-16-generic #55-Ubuntu SMP Tue Oct 20 19:48:32 UTC 2009 x86_64 GNU/Linux\nJava(TM) SE Runtime Environment (build 1.6.0_14-b08)\njava -Xms2048M -Xmx2048M -Xbatch -server\nPhenom II x4 3GHz (dynamic freq scaling turned off)\n\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n24.50\n32.66\n33.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n24.48\n31.94\n30.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n28.86\n31.79\n10.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n29.04\n28.63\n-1.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n28.08\n23.21\n-17.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n25.20\n19.04\n-24.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n28.34\n26.86\n-5.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n26.24\n26.60\n1.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n18.33\n25.94\n41.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n17.99\n25.77\n43.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n17.24\n21.80\n26.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n16.30\n18.37\n12.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n25.15\n26.85\n6.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n25.46\n26.82\n5.3%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n18.02\n26.04\n44.5%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n18.10\n26.23\n44.9%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n17.63\n22.34\n26.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n17.33\n18.46\n6.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n24.60\n32.69\n32.9%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n29.35\n32.85\n11.9%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n29.25\n32.23\n10.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n29.26\n28.87\n-1.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n28.30\n24.86\n-12.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n25.17\n21.14\n-16.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n25.27\n26.96\n6.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n26.32\n26.95\n2.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n18.28\n26.23\n43.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n18.06\n26.23\n45.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n17.40\n22.79\n31.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n16.45\n19.94\n21.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n25.27\n26.89\n6.4%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n27.13\n26.84\n-1.1%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n26.50\n26.17\n-1.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n18.00\n26.42\n46.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n17.75\n23.08\n30.0%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n17.41\n20.25\n16.3%\n\n\n\n\n\nSame setup, but no -Xbatch (I've found it slower sometimes):\njava -Xms2048M -Xmx2048M -server\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n27.83\n22.38\n-19.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n25.91\n23.05\n-11.0%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n28.72\n22.78\n-20.7%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n28.65\n22.12\n-22.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n28.21\n18.96\n-32.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n25.73\n16.18\n-37.1%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n27.83\n21.81\n-21.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n23.30\n22.30\n-4.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n19.81\n22.05\n11.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n19.70\n22.04\n11.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n18.77\n18.84\n0.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n17.76\n16.21\n-8.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n26.25\n21.85\n-16.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n26.36\n22.30\n-15.4%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n20.04\n22.17\n10.6%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n19.44\n22.12\n13.8%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n19.38\n19.45\n0.4%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n19.00\n16.41\n-13.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n27.89\n22.25\n-20.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n29.01\n23.16\n-20.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n28.79\n22.29\n-22.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n29.51\n22.21\n-24.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n28.43\n19.60\n-31.1%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n25.74\n17.31\n-32.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n27.64\n22.05\n-20.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n24.33\n22.23\n-8.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n19.86\n22.31\n12.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n19.67\n21.99\n11.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n19.07\n19.65\n3.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n17.91\n17.33\n-3.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n26.54\n22.31\n-15.9%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n26.66\n21.77\n-18.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n26.65\n22.33\n-16.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n19.48\n22.08\n13.3%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n19.21\n19.74\n2.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n18.81\n17.70\n-5.9%\n\n\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12770981"
        },
        {
            "date": "2009-10-28T17:03:32+0000",
            "content": "If we do go with a multi-queue approach (anywhere from 2 to N), perhaps there are ways to optimize queue merging too.  It looks like the current code fully pops all the source queues?\n\nIf you're going to insert an element into a different queue, it's a waste to maintain heap order on the source queue, and it's less efficient to start with the smallest elements.  Start with the leaves for the most effective short-circuiting.  We could even optionally prune... if two children aren't competetive, neither will the parent be.\n ",
            "author": "Yonik Seeley",
            "id": "comment-12770996"
        },
        {
            "date": "2009-10-28T17:19:06+0000",
            "content": "That should speed things up, but that's way subleading in complexity.   This is an additive term O(numSegments * numDesiredResults) total operations when done \"slowly\" (as opposed to the best merge, which is O(numDesiredResults * log(numSegments)) ), in comparison to the primary subleading piece for multiPQ, which is O(numSegments * numDesiredResults * log(numDesiredResults) * log(numHitsPerSegment) ), so that's taking a piece of the CPU time which is smaller by a factor of 20-100 already than the total PQ insert time, and reducing it by a further factor of maybe 5-10.  \n\nIf it's easy to code up, sure, why not.  But it's not really \"inner loop\" necessary optimizations anymore, I'd argue. ",
            "author": "Jake Mannix",
            "id": "comment-12771005"
        },
        {
            "date": "2009-10-28T17:27:40+0000",
            "content": "The more I think about it though, the more I'd like to not simply return to compare(int doca, int docb).  Getting the values for the documents to compare is not always a fast operation for custom comparators, so the ability to set/cache that value is a big win in those cases. ",
            "author": "Yonik Seeley",
            "id": "comment-12771008"
        },
        {
            "date": "2009-10-28T17:44:18+0000",
            "content": "But part of the point is that you don't have to get the values - you can have a fast in-memory structure which just encodes their sort-order, right?  This is the whole point of using the ordinal - you pre-sort all of the possible values to get the ordinals, and now arbitrarily complex comparator reduces to int compare at sort time.  In the custom comparators we use, for example, this allows for even sorting by multivalued fields in a custom way, via the simple compare(int doca, int docb) way.\n\n This reminds me: Mike, you switched the compare for ord values from being \"return ordA - ordB\" to being \"return ordA > ordB ? 1 : (ordA == ordB ? 0 : -1)\", on the basis of int overflow at some point, right?  This is only true if we're really sorting by integers, which could overflow - if they're ordinals, then these are both non-negative numbers, and their difference will always be greater than -MAX_INT, so the branching can be avoided in this innermost comparison in this case. ",
            "author": "Jake Mannix",
            "id": "comment-12771017"
        },
        {
            "date": "2009-10-28T18:24:53+0000",
            "content": "But part of the point is that you don't have to get the values - you can have a fast in-memory structure which just encodes their sort-order, right?\n\nFor certain types of comparators... but some custom comparators do more work and more indirect lookups (still fast enough to be a comparator, but certainly slower than just a direct array access).  It would be nice to avoid doing it more than once per id, and this is where FieldComparator is superior.\n ",
            "author": "Yonik Seeley",
            "id": "comment-12771036"
        },
        {
            "date": "2009-10-28T18:31:35+0000",
            "content": "Can you tell me more about this?  What kind of comparator can't pre-create a fixed ordinal list for all the possible values?  I'm sure I've seen this too, but I can't bring one to mind right now. ",
            "author": "Jake Mannix",
            "id": "comment-12771041"
        },
        {
            "date": "2009-10-28T18:51:06+0000",
            "content": "What kind of comparator can't pre-create a fixed ordinal list for all the possible values?\n\nIt's less about \"can't\" and more about there being too many disadvantages to pre-create in many cases.\nSparse representations would fit in this category... the number of documents with a value is small, so you use a hash (like Solr's query elevation component).\n\nSolr's random sort comparator is another - it hashes directly from docid to get the sort value.  Not slow per se, but it's certainly going to be faster to avoid recalculating it on every compare. ",
            "author": "Yonik Seeley",
            "id": "comment-12771054"
        },
        {
            "date": "2009-10-28T19:04:12+0000",
            "content": "\n This reminds me: Mike, you switched the compare for ord values from being \"return ordA - ordB\" to being \"return ordA > ordB ? 1 : (ordA == ordB ? 0 : -1)\", on the basis of int overflow at some point, right? This is only true if we're really sorting by integers, which could overflow - if they're ordinals, then these are both non-negative numbers, and their difference will always be greater than -MAX_INT, so the branching can be avoided in this innermost comparison in this case.\n\nRight, in the latest patch, for ords I just do the subtraction; for arbitrary ints, I do the if statement. ",
            "author": "Michael McCandless",
            "id": "comment-12771060"
        },
        {
            "date": "2009-10-28T23:38:18+0000",
            "content": "> What kind of comparator can't pre-create a fixed ordinal list for all the\n> possible values? I'm sure I've seen this too, but I can't bring one to mind\n> right now.\n\nI think the only time the ordinal list can't be created is when the source\narray contains some value that can't be compared against another value \u2013 e.g.\nsome variant on NULL \u2013 or when the comparison function is broken, e.g. when \na < b and b < c but c > a.\n\nFor current KinoSearch and future Lucy, we pre-build the ord array at index\ntime and mmap it at search time.  (Thanks to mmap, sort caches have virtually\nno impact on IndexReader launch time.) ",
            "author": "Marvin Humphrey",
            "id": "comment-12771201"
        },
        {
            "date": "2009-10-29T08:10:41+0000",
            "content": "One thing that bothers me about multiPQ is the memory usage if you start paging deeper and have many segments. I've seen up to 100 segments in production systems. 100x the memory use isn't pretty. \nThat's 100x the memory only for heaps, plus memory for Comparables - not nice.\n\nWhat kind of comparator can't pre-create a fixed ordinal list for all the possible values?\nAny comparator that has query-dependent ordering. Distance sort (of any kind, be it geo, or just any kind of value being close to your sample) for instance.\n\nI think the only time the ordinal list can't be created is when the source array contains some value that can't be compared against another value - e.g. some variant on NULL - or when the comparison function is broken, e.g. when a < b and b < c but c > a.\nWith such comparison function you're busted anyway - the order of your hits is dependent on segment traversal order for instance. If you sharded your search - it depends on the order your shards responded to meta-search. Ugly. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12771324"
        },
        {
            "date": "2009-10-29T10:30:23+0000",
            "content": "If the comparator API had a way to set (or return) a primitive value for a single docid, and then those were compared (either directly by the PQ or via a callback), there wouldn't be an issue with reader transitions (because you don't compare id vs id) and hence no need for multiple priority queues.\n\nI like this idea; I'll explore it.  Also, your (Yonik's) results showed a very sizable gain for multi-PQ when sorting by int, which is surprising. ",
            "author": "Michael McCandless",
            "id": "comment-12771376"
        },
        {
            "date": "2009-10-29T11:54:39+0000",
            "content": "Also, your (Yonik's) results showed a very sizable gain for multi-PQ when sorting by int, which is surprising.\n\nYeah... I can't help but wondering if we're measuring the edge (i.e. what won't be the bottleneck in typical searches).  *:* is fast and doesn't access the index at all.  Int sorting is also super fast in general, so we're down to mostly measuring the indirection time and overhead of the comparators... that's good when tweaking/optimizing, but you don't necessarily want to make bigger tradeoffs based on the fastest part of the system. ",
            "author": "Yonik Seeley",
            "id": "comment-12771395"
        },
        {
            "date": "2009-10-29T11:59:33+0000",
            "content": "Agreed.  Can you post your table when run on the term queries (eg '1')? ",
            "author": "Michael McCandless",
            "id": "comment-12771397"
        },
        {
            "date": "2009-10-29T12:16:02+0000",
            "content": "Can you post your table when run on the term queries (eg '1')?\n\nWoops, scratch that.  We don't have an int field on the wikipedia index.  Hmm. ",
            "author": "Michael McCandless",
            "id": "comment-12771400"
        },
        {
            "date": "2009-10-29T16:21:01+0000",
            "content": "Here's some more mud to help clear the water   This is with the latest JDK7 - tested twice to be sure, and all results were within .5 percentile points of eachother.\n\nLinux odin 2.6.28-16-generic #55-Ubuntu SMP Tue Oct 20 19:48:32 UTC 2009 x86_64 GNU/Linux\nJava(TM) SE Runtime Environment (build 1.7.0-ea-b74) (Oct 15 2009)\njava -Xms2048M -Xmx2048M -Xbatch -server\nPhenom II x4 3GHz (dynamic freq scaling turned off) \n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n28.02\n18.86\n-32.7%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n27.93\n18.80\n-32.7%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n23.89\n21.77\n-8.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n23.74\n21.21\n-10.7%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n22.92\n17.30\n-24.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n21.99\n14.64\n-33.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n23.63\n20.58\n-12.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n22.74\n20.42\n-10.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n16.88\n21.93\n29.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n19.32\n21.42\n10.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n18.58\n18.14\n-2.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n18.08\n15.25\n-15.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n23.89\n20.70\n-13.4%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n22.59\n20.58\n-8.9%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n16.84\n22.04\n30.9%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n16.68\n21.71\n30.2%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n19.65\n18.60\n-5.3%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n17.70\n15.48\n-12.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n28.31\n18.94\n-33.1%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n23.75\n22.09\n-7.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n23.99\n21.90\n-8.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n23.75\n21.47\n-9.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n22.83\n18.41\n-19.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n21.99\n15.96\n-27.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n22.92\n20.61\n-10.1%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n23.36\n22.27\n-4.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n16.96\n22.12\n30.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n19.61\n21.59\n10.1%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n18.02\n19.03\n5.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n18.54\n16.51\n-10.9%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n24.32\n20.65\n-15.1%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n23.46\n20.72\n-11.7%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n22.71\n20.62\n-9.2%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n16.78\n21.78\n29.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n19.14\n19.22\n0.4%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n17.61\n16.79\n-4.7%\n\n\n\n\n\nSame setup, just w/o -Xbatch\njava -Xms2048M -Xmx2048M -server\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n10\n28.63\n24.16\n-15.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n25\n28.24\n19.51\n-30.9%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n50\n29.24\n19.21\n-34.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n100\n27.42\n20.03\n-27.0%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n500\n26.38\n16.82\n-36.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand int\n1000\n26.34\n14.40\n-45.3%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n10\n27.61\n20.29\n-26.5%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n25\n25.84\n21.80\n-15.6%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n50\n19.47\n21.69\n11.4%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n100\n19.17\n19.40\n1.2%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n500\n18.29\n16.87\n-7.8%\n\n\nrandom\nbalanced\n<all>\n5000000\nrand string\n1000\n17.09\n14.35\n-16.0%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n10\n22.48\n21.42\n-4.7%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n25\n20.86\n21.88\n4.9%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n50\n20.26\n21.67\n7.0%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n100\n18.32\n19.60\n7.0%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n500\n17.93\n17.01\n-5.1%\n\n\nrandom\nbalanced\n<all>\n5000000\ncountry\n1000\n18.92\n14.48\n-23.5%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n28.71\n24.35\n-15.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n28.47\n19.55\n-31.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n28.19\n19.38\n-31.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n27.89\n20.31\n-27.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n25.13\n17.64\n-29.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n26.51\n15.55\n-41.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n10\n27.81\n20.39\n-26.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n25\n25.66\n21.96\n-14.4%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n50\n17.70\n20.17\n14.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n100\n19.28\n19.63\n1.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n500\n18.03\n17.45\n-3.2%\n\n\nrandom\nlog\n<all>\n5000000\nrand string\n1000\n18.84\n15.29\n-18.8%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n10\n22.58\n21.47\n-4.9%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n25\n21.09\n20.36\n-3.5%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n50\n21.03\n21.80\n3.7%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n100\n18.45\n21.38\n15.9%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n500\n17.89\n17.69\n-1.1%\n\n\nrandom\nlog\n<all>\n5000000\ncountry\n1000\n18.93\n15.62\n-17.5%\n\n\n\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12771466"
        },
        {
            "date": "2009-10-30T10:37:33+0000",
            "content": "\n\n\tAdded inlined single PQ approach, for sorting by int, so now\n    sortBench.py runs trunk as baseline and inlined single PQ as\n    \"new\".  It now only does the int sort.  The external API now looks\n    like function queries DocValues (I added a simple\n    IntDocValueSource/IntDocValues, that has one method to get the int\n    for a given docID).\n\n\n\n\n\tAdded random int field when building wiki index; you'll have to\n    remove existing wiki index.  This is so we can test \"more normal\"\n    queries, sorting by int.\n\n\n\n\n\tDropped number of threads during indexing to 1, and seeded the\n    Random, so that we all produce the same index\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12771903"
        },
        {
            "date": "2009-10-30T11:43:57+0000",
            "content": "JAVA:\njava version \"1.6.0_14\"\nJava(TM) SE Runtime Environment (build 1.6.0_14-b08)\nJava HotSpot(TM) 64-Bit Server VM (build 14.0-b16, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\n\n\nSource\nSeg size\nQuery\nTot hits\nSort\nTop N\nQPS old\nQPS new\nPct change\n\n\nwiki\nlog\n1\n1170209\nrand int\n10\n27.08\n27.52\n1.6%\n\n\nwiki\nlog\n1\n1170209\nrand int\n25\n26.95\n27.72\n2.9%\n\n\nwiki\nlog\n1\n1170209\nrand int\n50\n27.54\n27.03\n-1.9%\n\n\nwiki\nlog\n1\n1170209\nrand int\n100\n27.08\n24.19\n-10.7%\n\n\nwiki\nlog\n1\n1170209\nrand int\n500\n26.13\n26.88\n2.9%\n\n\nwiki\nlog\n1\n1170209\nrand int\n1000\n24.30\n25.91\n6.6%\n\n\nwiki\nlog\n2\n1088727\nrand int\n10\n29.66\n29.45\n-0.7%\n\n\nwiki\nlog\n2\n1088727\nrand int\n25\n28.80\n29.47\n2.3%\n\n\nwiki\nlog\n2\n1088727\nrand int\n50\n29.57\n30.21\n2.2%\n\n\nwiki\nlog\n2\n1088727\nrand int\n100\n30.74\n27.30\n-11.2%\n\n\nwiki\nlog\n2\n1088727\nrand int\n500\n28.72\n30.17\n5.0%\n\n\nwiki\nlog\n2\n1088727\nrand int\n1000\n27.20\n29.21\n7.4%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n10\n33.43\n31.09\n-7.0%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n25\n33.24\n31.77\n-4.4%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n50\n32.89\n31.68\n-3.7%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n100\n32.04\n21.33\n-33.4%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n500\n31.25\n30.59\n-2.1%\n\n\nwiki\nlog\n<all>\n5000000\nrand int\n1000\n29.48\n29.72\n0.8%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n10\n32.61\n31.64\n-3.0%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n25\n32.98\n31.79\n-3.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n50\n32.63\n30.89\n-5.3%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n100\n32.08\n21.60\n-32.7%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n500\n30.48\n30.65\n0.6%\n\n\nrandom\nlog\n<all>\n5000000\nrand int\n1000\n28.95\n29.44\n1.7%\n\n\n\n\n\nThis table compares trunk (= old) with the \"inline int value directly into single PQ\" approach (=new).  So, a green result means the inline-single-PQ is faster; red means it's slower.\n\nThe results baffle me.  I would have expected for the 5M hits, with shallow topN, that the diffs would be minor since the sub-leading cost should be in the noise for either approach, and then as we go to fewer hits and deeper topN, that the inline-single-PQ approach would be faster.  And, we still see strangeness at topN=100 where current trunk is always substantially better.  The only difference between these ought to be the constant in front of the net number of inserstions.  Strange! ",
            "author": "Michael McCandless",
            "id": "comment-12771923"
        },
        {
            "date": "2009-11-01T11:08:34+0000",
            "content": "The number of insertions into the queue is miniscule for these tests.\nEG with topN=10, the query \"1\" against the 5M wikipedia index, causes\n110 insertions.\n\nEven at topN=1000 we see only 8053 insertions.\n\nSo, the time difference of these runs is really driven by the \"compare\nto bottom\" check that's done for every hit.\n\nWhat baffles me is even if I take the inline-single-PQ from the last\npatch, and instead of invoking a separate class's\n\"IntDocValues.intValue(doc)\" I look it up straight from the int[] I\nget from FieldCache, I'm still seeing worse performance vs trunk.\n\nI think at this point this test is chasing java ghosts, so, we really\ncan't conclude much.\n\nAlso, I think, if you are sorting by native value per doc, likely the\nfastest way to take \"bottom\" into account is to push the check all the\nway down into the bottom TermScorers that're pulling docIDs from the\nposting lists.  Ie, if your queue has converged, and you know a given\ndoc must have value < 7 (say) to get into the queue, you can check\neach doc's value immediately on pulling it from the posting list and\nskip it if it won't compete (and, if you don't require exact total\nhits count).\n\nFor queries that are otherwise costly, this can save alot of CPU.\nThis is what the source code specialization (LUCENE-1594) does, and it\nresults in excellent gains (if I'm remembering right!). ",
            "author": "Michael McCandless",
            "id": "comment-12772309"
        },
        {
            "date": "2009-11-02T21:53:47+0000",
            "content": "Hi Michael:\n\n    Any plans/decisions on moving forward with multiQ within Lucene? I am planning on making the change locally for my project, but I would rather not duplicate the work if you are planning on doing this within lucene.\n\nThanks\n\n-John ",
            "author": "John Wang",
            "id": "comment-12772710"
        },
        {
            "date": "2009-11-02T22:54:14+0000",
            "content": "Hi John \u2013 it seems unlikely to happen any time too soon.  We're still iterating here, and there are concerns (eg increased memory usage) with the multi PQ approach. ",
            "author": "Michael McCandless",
            "id": "comment-12772735"
        },
        {
            "date": "2009-11-02T23:03:59+0000",
            "content": "The current concern is to do with the memory?  I'm more concerned with the weird \"java ghosts\" that are flying around, sometimes swaying results by 20-40%...  the memory could only be an issue on a setup with hundreds of segments and sorting the top 1000 values (do we really try to optimize for this performance case?).  In the normal case (no more than tens of segments, and the top 10 or 100 hits), we're talking about what, 100-1000 PQ entries? ",
            "author": "Jake Mannix",
            "id": "comment-12772741"
        },
        {
            "date": "2009-11-02T23:35:47+0000",
            "content": "Hi Michael:\n\n    Thanks for the heads up. I will work on it locally then.\n\n    I am a bit confused here with memory, since most users don't go beyond page one, I can't see memory is even a concern here comparing to the amount of memory lucene uses overall. Am I missing something?\n\n-John ",
            "author": "John Wang",
            "id": "comment-12772754"
        },
        {
            "date": "2009-11-02T23:53:49+0000",
            "content": "I just looked at the most recent patch. Every entry in the PQ is an extra int, so even at the very very very rare and extreme case, 100th page(assuming 10 entries per page) and 100 segment index, we are looking at 400k.\nIs this really a concern? I must be missing something...\n\n-John ",
            "author": "John Wang",
            "id": "comment-12772764"
        },
        {
            "date": "2009-11-02T23:58:04+0000",
            "content": "Its not as rare as you think - certainly not 3 verys rare. ",
            "author": "Mark Miller",
            "id": "comment-12772770"
        },
        {
            "date": "2009-11-03T00:00:32+0000",
            "content": "Regarding memory - If I'm not misunderstanding things, you also have to materialize all field values you're sorting on for each doc in each PQ? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12772773"
        },
        {
            "date": "2009-11-03T00:07:08+0000",
            "content": "Yes, but you have to do that with the current method as well. Comes with per segment really. ",
            "author": "Mark Miller",
            "id": "comment-12772776"
        },
        {
            "date": "2009-11-03T00:14:12+0000",
            "content": "Right now DocComparator cheats and stores fields striped, keeping primitives(or in my case any value that can be represented with primitive) - primitive. If the same approach is taken with multiPQs, BOOM! - there goes your API simplicity. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12772779"
        },
        {
            "date": "2009-11-03T00:36:40+0000",
            "content": "Mark:\n\n       100th page at the same time index is at 100 segments? How many very's would you give it?\n\nEarwin:\n\n      Field values are in FieldCache. Not in the PQ. It is the PQ's memory consumption is at question here (If I am not misunderstanding) You only materialize after the merge, which even at the N*Very case, it is only a page worth, which is the same as the singlePQ approach.\n\n-John ",
            "author": "John Wang",
            "id": "comment-12772790"
        },
        {
            "date": "2009-11-03T00:38:50+0000",
            "content": "100th page at the same time index is at 100 segments? How many very's would you give it?\n\nI'm not claiming 100th page with many segments - I have no info on that, and I agree it would be more rare. But it has come to my attention that 100th page is more common than I would have thought. (sorry - I wasn't very clear on that in my last comment - I am just referring to the deep paging - I previously would have thought its more rare than I do now - though even before, its something I wouldnt want to see a huge perf drop on)\n\nIn any case - no one is saying this change won't happen. Just that its not likely to happen soon.\n\nedit\n\nLet me answer the question though - based on my experience with the mergefactors people like to use, and the cost of optimizing, I would say 100 segments deserves no very. At best, it might be semi rare. Mixed with the 100 page req, I'd take it to rare. But thats just me guessing based on my Lucene/Solr experience - so its not worth a whole ton. ",
            "author": "Mark Miller",
            "id": "comment-12772792"
        },
        {
            "date": "2009-11-03T00:49:33+0000",
            "content": "Mark:\n\n       The point of discussion is memory, unless a few hundred K of memory consumption implies a \"huge perf drop\". (I see you are being conservative and using only 1 huge  )\n\n       Even with 100 segment which I am guessing you agree that it is rare, it is 400K, (in this discussion, I am using it as an upper bound, perhaps I should state it more explicitly) and thus my inability to understand that being a memory concern.\n\n       BTW, I am interested the percentage of \"deep paging\" you are seeing. You argue it is not rare, do you have some concrete numbers? The stats I have seen from our production logs and also web search logs when I was working on that, the percentage is very very very very very (5 very's) low. (sharp drop usually is at page 4, let alone page 100)\n\n-John ",
            "author": "John Wang",
            "id": "comment-12772794"
        },
        {
            "date": "2009-11-03T00:55:24+0000",
            "content": "The point of discussion is memory, unless a few hundred K of memory consumption implies a \"huge perf drop\". (I see you are being conservative and using only 1 huge  )\n\nI know, I was purposely avoiding getting into the mem argument and just focusing on how rare the situation is. And whether there is going to be a huge perf drop with queue sizes of 1000, I just don't know. The tests have been changing a lot - which is why I think its a little early to come to final conclusions.\n\nEven with 100 segment which I am guessing you agree that it is rare, it is 400K, (in this discussion, I am using it as an upper bound, perhaps I should state it more explicitly) and thus my inability to understand that being a memory concern.\n\nYes - I do agree its rare.\n\nBTW, I am interested the percentage of \"deep paging\" you are seeing. You argue it is not rare, do you have some concrete numbers? The stats I have seen from our production logs and also web search logs when I was working on that, the percentage is very very very very very (5 very's) low. (sharp drop usually is at page 4, let alone page 100)\n\nI don't have numbers I can share - but this isn't for situations with users paging through an interface (like a web search page) - its users that are using Lucene for other tasks - and there are plenty of those. Lucene is used a lot for websites with users click through 10 results at a time - but its also used in many, many other apps (and I do mean two manys !  ) ",
            "author": "Mark Miller",
            "id": "comment-12772798"
        },
        {
            "date": "2009-11-03T01:03:56+0000",
            "content": "Actually - while I cannot share any current info I have, I'll share an example from my last job. I worked on a system that librarians used to maintain a newspaper archive. The feed for the paper would come in daily and the librarians would \"enhance\" the data - adding keywords, breaking up stories, etc. Then reporters or end users could search this data. Librarians, who I learned are odd in there requirements by nature, insisted on bringing in thousands of results that they could scroll through at a time. This was demanded at paper after paper. So we regularly fed back up to 5000 thousand results at a time with our software (though they'd have preferred no limit - \"what are you talking about ! I want them all!\" - we made them click more buttons for that  ). Thats just one small example, but I know for a fact there are many, many more.\n\nedit \n\nWe also actually ran into many situations were there were lots of segments in this scenario as well - before I knew better, I'd regularly build the indexes with a high merge factor for speed - and then be stuck, unable to optimize because it killed performance and newspapers need to be up pretty much 24/7 - without bringing there server to a crawl - so I would be unable to optimize (this was before you could optimize down to n segments and work slowly over time). Not the greatest example, but a situation I found myself in. ",
            "author": "Mark Miller",
            "id": "comment-12772809"
        },
        {
            "date": "2009-11-03T04:53:31+0000",
            "content": "Another observation, with multiQ approach, seems there would be no need for the set of OutOfOrder*Comparators. Because we would know each Q corresponds to 1 segment, and would not matter if docs come out of order.\nPlease correct me if I am misunderstanding here. ",
            "author": "John Wang",
            "id": "comment-12772877"
        },
        {
            "date": "2009-11-03T10:19:44+0000",
            "content": "though they'd have preferred no limit - \"what are you talking about ! I want them all!\"\nSame here. People searching on a job site don't really care for top 10 vacancies/resumes, they want eeeeeverything! that matched their requirements. \n\nJohn: on the other hand, we here already have code to merge results coming from different shards, with stripes, primitives and whistles to appease GC. Might as well reuse it asis to merge between segments. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12773000"
        },
        {
            "date": "2009-11-03T10:43:16+0000",
            "content": "Another observation, with multiQ approach, seems there would be no need for the set of OutOfOrder*Comparators. \n\nI think it'd still be beneficial to differentiate In vs OutOf order collectors, because even within 1 segment if you know the docIDs arrive in order then the compare bottom is cheaper (need not break ties).\n\nEven with 100 segment which I am guessing you agree that it is rare, it is 400K, \n\nDon't forget that this is multiplied by however many queries are currently in flight.\n\nYonik also raised an important difference of the single PQ API (above: https://issues.apache.org/jira/browse/LUCENE-1997?focusedCommentId=12771008&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12771008), ie, the fact that it references \"slots\" instead of \"docid\" means you can cache something private in your \"slots\" based on previous compare/copy.\n\nSince each approach has distinct advantages, why not offer both (\"simple\" and \"expert\") comparator extensions APIs? ",
            "author": "Michael McCandless",
            "id": "comment-12773012"
        },
        {
            "date": "2009-11-03T18:50:57+0000",
            "content": "Since each approach has distinct advantages, why not offer both (\"simple\" and \"expert\") comparator extensions APIs?\n\n+1 from me on this one, as long as the simpler one is around.  I'll bet we'll find that we regret keeping the \"expert\" one by 3.2 or so though, but I'll take any compromise which gets the simpler API in there.\n\nDon't forget that this is multiplied by however many queries are currently in flight.\n\nSure, so if you're running with 100 queries per second on a single shard (pretty fast!), with 100 segments, and you want to do sorting by value on the top 1000 values (how far down the long tail of extreme cases are we at now?  Do librarians hit their search servers with 100 QPS and have indices poorly built with hundreds of segments and can't take downtime to ever optimize?), we're now talking about 40MB.  \n\nForty megabytes.  On a beefy machine which is supposed to be handling 100QPS across an index big enough to need 100 segments.  How much heap would such a machine already be allocating?  4GB?  6?  More? \n\nWe're talking about less than 1% of the heap is being used by the multiPQ approach in comparison to singlePQ. ",
            "author": "Jake Mannix",
            "id": "comment-12773114"
        },
        {
            "date": "2009-11-03T23:59:43+0000",
            "content": "If this is about ease of use, its pretty easy to return Comparable to the fieldcache, add a Comparable fieldcomparator, and let users that need it (havn't seen the clamoring yet though) just implement getComparable(String). Its not that hard to support that with a single queue either.\n\nIts still my opinion that users that need this are advanced enough to look at the provided impls and figure it out pretty quick. Its not rocket science, and each method impls is generally a couple simple lines of code. ",
            "author": "Mark Miller",
            "id": "comment-12773298"
        },
        {
            "date": "2013-11-30T12:42:45+0000",
            "content": "2013 Old JIRA cleanup ",
            "author": "Erick Erickson",
            "id": "comment-13835675"
        }
    ]
}