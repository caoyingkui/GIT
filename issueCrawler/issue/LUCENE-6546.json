{
    "id": "LUCENE-6546",
    "title": "Test2BPostingsBytes sometimes OOMs even with -Dtests.heapsize=30g",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "general/test"
        ],
        "labels": "",
        "fix_versions": [
            "5.3"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "Here's an example: http://jenkins.sarowe.net/job/Lucene-core-nightly-monster-5.x-Java7/22/console; the seed still OOMs for me when -Dtests.heapsize=60g:\n\n\nant test  -Dtestcase=Test2BPostingsBytes -Dtests.method=test -Dtests.seed=F99606D852DB1420 -Dtests.nightly=true -Dtests.slow=true -Dtests.locale=sr_RS_#Latn -Dtests.timezone=America/Port-au-Prince -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n\n\n\n(Although the repro line doesn't include all of them - Dawid Weiss is this expected? - the above-linked job runs with -Dtests.jvms=4 -Dtests.nightly=true -Dtests.monster=true -Dtests.heapsize=30g.)\n\nI narrowed the problem down to CompressingCodec - any of the 4 subclasses - with maxDocsPerChunk * blockSize less than 16.  The other variable randomized in CompressingCodec.randomInstance() is chunkSize, which doesn't seem to be implicated in the OOMs.\n\nI guess we could suppress CompressingCodec, or just expect OOMs from time to time (the above condition happens fairly rarely).  But I'd rather continue to exercise CompressingCodec as much as possible.\n\nHere are the OOMs I saw with -Dtests.heapsize=30g and -Dtests.codec=CompressingCodec:\n\n\n\n\ncompressionMode\nchunkSize\nmaxDocsPerChunk\nblockSize\n\n\nDUMMY\n3\n2\n1\n\n\nDUMMY\n7\n1\n1\n\n\nFAST_DECOMPRESSION\n10\n9\n1\n\n\nFAST_DECOMPRESSION\n31614\n3\n4\n\n\nFAST\n21879\n1\n9\n\n\nDUMMY\n3\n3\n1\n\n\nHIGH_COMPRESSION\n4167\n10\n1\n\n\nHIGH_COMPRESSION\n12437\n2\n5\n\n\nDUMMY\n4\n3\n2\n\n\nHIGH_COMPRESSION\n3\n3\n2\n\n\nFAST_DECOMPRESSION\n3339\n10\n1\n\n\nDUMMY\n2\n5\n3\n\n\n\n\n\nHere are some that did not OOM:\n\n\n\n\ncompressionMode\nchunkSize\nmaxDocsPerChunk\nblockSize\n\n\nDUMMY\n4\n4\n4\n\n\nDUMMY\n4\n2\n246\n\n\nFAST\n30677\n907\n2\n\n\nFAST_DECOMPRESSION\n15165\n2\n10\n\n\nFAST_DECOMPRESSION\n28633\n67\n1\n\n\nDUMMY\n7\n3\n9\n\n\nDUMMY\n2835\n9\n3\n\n\nDUMMY\n25785\n4\n7",
    "attachments": {
        "LUCENE-6546.patch": "https://issues.apache.org/jira/secure/attachment/12738885/LUCENE-6546.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14580908",
            "author": "Dawid Weiss",
            "date": "2015-06-10T18:20:19+0000",
            "content": "Although the repro line doesn't include all of them - Dawid Weiss is this expected?\n\nThe \"repro line\" is not part of the randomizedtesting package. In fact, RR doesn't print anything at all \u2013 it's the report listeners that should do it. In Lucene the \"repro line\" is put together by a class listener attached on LuceneTestCase:\n\n@Listeners({\n  RunListenerPrintReproduceInfo.class,\n  FailureMarker.class\n})\n\n\n\nIf you look for RunListenerPrintReproduceInfo it just emits (our) properties that (we think) lead to fully reproducible test case. It would be nice to have a \"real\" repro \u2013 full forked JVM command line, including the JVM and any switches attached to it, but it's not there (yet). "
        },
        {
            "id": "comment-14580911",
            "author": "Steve Rowe",
            "date": "2015-06-10T18:21:44+0000",
            "content": "Patch that will skip the test (using LTC.assumeTrue()) when new IndexWriterConfig(null).getCodec() is a subclass of CompressingCodec and maxDocsPerChunk * blockSize is less than 16. "
        },
        {
            "id": "comment-14580919",
            "author": "Steve Rowe",
            "date": "2015-06-10T18:28:09+0000",
            "content": "Thanks for the info Dawid. "
        },
        {
            "id": "comment-14580928",
            "author": "Steve Rowe",
            "date": "2015-06-10T18:33:03+0000",
            "content": "I beasted Test2BPostingsBytes with the attached patch and -Dtests.heapsize=30g -Dtests.codec=Compressing, with the randomization in CompressingCodec.getRandomInstance() modified to produce maxDocsPerChunk * blockSize between 1 and 30, and got no OOMs from 100 runs each under Java7 and Java8 (Debian 8 Linux, OpenJDK 1.7.0_79, Oracle JDK 1.8.0_45).  "
        },
        {
            "id": "comment-14582047",
            "author": "ASF subversion and git services",
            "date": "2015-06-11T15:12:53+0000",
            "content": "Commit 1684915 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1684915 ]\n\nLUCENE-6546: Skip Test2BPostingsBytes to avoid OOM with -Dtests.heapsize=30g when codec is-a CompressingCodec and maxDocsPerChunk * blockSize is less than 16 "
        },
        {
            "id": "comment-14582083",
            "author": "ASF subversion and git services",
            "date": "2015-06-11T15:30:22+0000",
            "content": "Commit 1684919 from Steve Rowe in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1684919 ]\n\nLUCENE-6546: Skip Test2BPostingsBytes to avoid OOM with -Dtests.heapsize=30g when codec is-a CompressingCodec and maxDocsPerChunk * blockSize is less than 16 (merged trunk r1684915) "
        },
        {
            "id": "comment-14713297",
            "author": "Shalin Shekhar Mangar",
            "date": "2015-08-26T13:06:21+0000",
            "content": "Bulk close for 5.3.0 release "
        }
    ]
}