{
    "id": "LUCENE-2522",
    "title": "add simple japanese tokenizer, based on tinysegmenter",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "TinySegmenter (http://www.chasen.org/~taku/software/TinySegmenter/) is a tiny japanese segmenter.\n\nIt was ported to java/lucene by Kohei TAKETA <k-tak@void.in>, \nand is under friendly license terms (BSD, some files explicitly disclaim copyright to the source code, giving a blessing instead)\n\nKoji knows the author, and already contacted about incorporating into lucene:\n\nI've contacted Takeda-san who is the creater of Java version of\nTinySegmenter. He said he is happy if his program is part of Lucene.\nHe is a co-author of my book about Solr published in Japan, BTW. ;-)",
    "attachments": {
        "LUCENE-2522.patch": "https://issues.apache.org/jira/secure/attachment/12448504/LUCENE-2522.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-07-01T16:50:55+0000",
            "content": "here is a really quickly done patch, just to get started (not really for committing)\n\n\n\tconverted their tests to basetokenstream tests,\n\tchanged it to use CharTermAttribute instead of TermAttribute,\n\tadded clearAttributes()\n\tmade class final.\n\tadded solr factory.\n\n\n\nThe code is nice, it is setup to work on unicode codepoints etc, but i think we can improve\nit by using CharArrayMaps for speed and by using lucene's codepoint i/o stuff in CharUtils. ",
            "author": "Robert Muir",
            "id": "comment-12884348"
        },
        {
            "date": "2010-07-02T22:41:08+0000",
            "content": "i refactored the TinySegmenterConstants to use ints/switch statements instead of all the hashmaps.\n\nthis creates a larger .java file, but its a smaller .class, and scoring no longer has to create 24 strings per character ",
            "author": "Robert Muir",
            "id": "comment-12884839"
        },
        {
            "date": "2011-04-23T23:28:12+0000",
            "content": "attached is an updated patch, its still a work in progress (needs some more tests and benchmarking and some other things little fixes).\n\nTheres a general pattern for these segmenters (this one, smartchinese, sen) thats a little tricky, that is they want to really look at sentences to determine how to segment.\n\nSo, I added a base class for this to make writing these segmenters easier, and also to hopefully improve segmentation accuracy. (I would like to switch smartchinese over to it) This class makes it easy to segment sentences with a Sentence BreakIterator... in my opinion it doesnt matter how theoretically good the word tokenization is for these things, if the sentence tokenizer is really bad (I found this issue with both sen and smartchinese).\n\nhope to get it committable soon ",
            "author": "Robert Muir",
            "id": "comment-13023617"
        },
        {
            "date": "2013-07-23T18:44:49+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717066"
        },
        {
            "date": "2014-04-16T12:54:35+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970811"
        }
    ]
}