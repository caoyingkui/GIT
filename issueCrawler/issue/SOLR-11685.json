{
    "id": "SOLR-11685",
    "title": "CollectionsAPIDistributedZkTest.testCollectionsAPI fails regularly with leader mismatch",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I've been noticing lots of failures on Jenkins where the document add get's rejected because of leader conflict and throws an error like \n\n\nClusterState says we are the leader (https://127.0.0.1:38715/solr/awhollynewcollection_0_shard2_replica_n2), but locally we don't think so. Request came from null\n\n\n\nScanning Jenkins logs I see that these failures have increased since Sept 28th and has been failing daily.",
    "attachments": {
        "jenkins_7x_257.log": "https://issues.apache.org/jira/secure/attachment/12899485/jenkins_7x_257.log",
        "solr_master_8983.log": "https://issues.apache.org/jira/secure/attachment/12901440/solr_master_8983.log",
        "solr_master_7574.log": "https://issues.apache.org/jira/secure/attachment/12901441/solr_master_7574.log",
        "jenkins_master_7045.log": "https://issues.apache.org/jira/secure/attachment/12901200/jenkins_master_7045.log"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-11-27T20:42:54+0000",
            "content": "Attaching a failure of one such run. ",
            "author": "Varun Thacker",
            "id": "comment-16267484"
        },
        {
            "date": "2017-12-08T07:08:03+0000",
            "content": "Analysis from jenkins_master_7045.log\n\nL20542: Test CollectionsAPIDistributedZkTest.testCollectionsAPI starts at line 20542\nQuestion: Why is halfcollectionblocker being deleted after this test has started and not before?\nL20563: create collection awhollynewcollection_0 with 4 shards and 1 replica\nL20746: ChaosMonkey monkey: stop jetty! 49379\nL20774: This shoes that the jetty that was shut down has core=awhollynewcollection_0_shard3_replica_n4\nL20781: ChaosMonkey monkey: starting jetty! 49379\nL20859: Exception causing close of session 0x1603371da360011 due to java.io.IOException: ZooKeeperServer not running/Watch limit violations ..\nQuestion : Why are we hitting a watcher limit?\nL20889: Restarting zookeeper\nL20915: An add request comes in \"ClusterState says we are the leader, but locally we don't think so\" for awhollynewcollection_0_shard3_replica_n4\n\n\nPresumably when CloudSolrClient sends the request awhollynewcollection_0_shard3_replica_n4 was the leader of shard3. After the restart it hasn't become leader yet but there are no other replicas. \n\nCloudSolrClient should catch this exception as it's local cache might not be the most updated one, refresh it state and retry the add operation. Today CloudSolrClient retries in requestWithRetryOnStaleState when wasCommError is true . DistributedUpdateProcessor#doDefensiveChecks throws this as a SolrException . We should throw this as another exception on which we can retry the operation ",
            "author": "Varun Thacker",
            "id": "comment-16283140"
        },
        {
            "date": "2017-12-08T07:15:36+0000",
            "content": "\n    if ((isLeader && !localIsLeader) || (isSubShardLeader && !localIsLeader)) {\n      log.error(\"ClusterState says we are the leader, but locally we don't think so\");\n+      if (from == null) {\n+        //if leader=null means the client sent the request. If we aren't the leader (the client has old info) return a exception which the client can +retry on\n+      }\n      throw new SolrException(ErrorCode.SERVICE_UNAVAILABLE,\n          \"ClusterState says we are the leader (\" + zkController.getBaseUrl()\n              + \"/\" + req.getCore().getName() + \"), but locally we don't think so. Request came from \" + from);\n    }\n\n ",
            "author": "Varun Thacker",
            "id": "comment-16283148"
        },
        {
            "date": "2017-12-11T04:28:02+0000",
            "content": "Attaching solr_master*.log which was from some manual testing.\n\nI did some manual testing where I started the cloud example which creates a 2X2 collection\nThen I had a script running which indexes constantly . I then simulated a GC pause on node2 ( 7574 ) by running kill -STOP <node2_pid> && sleep 60 && kill -CONT <node2_pid> . \n\nWe hit two types of race conditions here:\n\n\n\tnode2 thinks it's the leader and rejects a request from node1 ( the actual leader at the time )\n\tnode1 forwards a request when it was a leader . node2 becomes the leader before the update reaches and rejects the request.\n\n\n\nThis is very easy to reproduce when you index constantly and then force a zookeeper connection timeout on the node which has the leader. \n ",
            "author": "Varun Thacker",
            "id": "comment-16285500"
        },
        {
            "date": "2017-12-11T04:31:55+0000",
            "content": "The easiest thing we could do is acknowledge that this race condition could happen , and {[doDefensiveChecks}} could throw a type of exception which the client can retry on?  ",
            "author": "Varun Thacker",
            "id": "comment-16285501"
        }
    ]
}