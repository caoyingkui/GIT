{
    "id": "SOLR-3088",
    "title": "create shard placeholders",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "When creating a new collection, a placeholder for each shard should be created.",
    "attachments": {
        "SOLR-3088.patch": "https://issues.apache.org/jira/secure/attachment/12513957/SOLR-3088.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13198896",
            "date": "2012-02-02T15:58:38+0000",
            "content": "There are a couple of reasons...\n\n\tthe cluster should be self-describing so that one can start up a node and say \"go join this cluster\" and need not know anything else (like the number of shards that is currently passed when starting each node)\n\tright now, one can bring up one shard of a 3 shard cluster and then successfully index a bunch of docs (oops, they all went into shard1)\n\n\n\nI imagine the placeholders should be both in the ZK structure under /collections and in clusterstate "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13203503",
            "date": "2012-02-08T12:04:50+0000",
            "content": "the cluster should be self-describing so that one can start up a node and say \"go join this cluster\"\n\nDo you mean that the new node would automatically discover from the placeholders how many cores it needs to start?\n\nNow everything works just the opposite way: nodes create the cores they are configured to do (solr.xml/CoreAdminHandler) and the overseer just assigns them a shard id as they come in. I assume everything could still work this way when using the placeholders but it sounds a bit awkward.\n\nI imagine the placeholders should be both in the ZK structure under /collections and in clusterstate\n\nWhat did you have in mind that should be stored under /collections?\n\nIn earlier versions I had overseer read the target number of slices from the collection node (/collections/collection1 for example) but that was later removed in favor of the system property.\n\nWhat should be in charge of creating a new collection? Now you can do it for example through CoreAdminHandler by simply adding a core into a collection that does not yet exist, I believe you can do this also through editing solr.xml.\n\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13203641",
            "date": "2012-02-08T14:31:06+0000",
            "content": "I think the only change I'm suggesting is that when a new collection is created via bootstrapping and the -DnumShards=3 parameter is passed,\nthat empty placeholders for shard2 and shard3 be created:\n\n\n{\"collection1\":{\"shard1\":{\n      \"Rogue:8983_solr_\":{\n        \"shard_id\":\"shard1\",\n        \"leader\":\"true\",\n        \"state\":\"active\",\n        \"core\":\"\",\n        \"collection\":\"collection1\",\n        \"node_name\":\"Rogue:8983_solr\",\n        \"base_url\":\"http://Rogue:8983/solr\"}},\n  \"shard2\":{}\n  \"shard3\":{}\n}\n\n\n\nThis allows for appropriate errors to be thrown when shards that would normally be needed are not \"up\".\n\nAll new nodes being brought up can now omit the -DnumShards parameter... and the overseer assigns as it normally would.\nWe should shoot for being able to have pretty dumb nodes on startup that are only told to join a cluster and nothing else... the cluster controls everything.\n\nDo you mean that the new node would automatically discover from the placeholders how many cores it needs to start?\n\nNo... by default it would still only be one core. Although we could have a parameter that would start more than one core.\n\nWhat did you have in mind that should be stored under /collections?\n\nThere could be a placeholder for shard2, but if the only thing that uses it is leader election, and it's always created on demand, then it shouldn't be necessary.\n\nIn earlier versions I had overseer read the target number of slices from the collection node (/collections/collection1 for example) but that was later removed in favor of the system property.\n\nRight - that still sounds correct as it will fit better with custom sharding and with shard splitting (however that will work).\n\nWhat should be in charge of creating a new collection? Now you can do it for example through CoreAdminHandler by simply adding a core into a collection that does not yet exist\n\nWe should eventually have some API to create a new collection without necessitating the creation of new nodes.  Perhaps it's part of the core admin handler?  Perhaps Mark has thoughts on this. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13203741",
            "date": "2012-02-08T16:56:29+0000",
            "content": "My thoughts are in line with what Yonik has said.\n\nI only saw this issue as:\n\ndummy entries get added to the collection state - eg if you start the first node with -DnumShards=3, one real shard entry is added to the collection state and two dummy entries. As Yonik says, now all searches will fail because the 2 dummy shards are not 'up' and the full collection is not available.\n\nAgain, as Yonik notes, a nice side effect is that you don't always have to pass numShards = except perhaps in the future when you want to reshard if that ends up making sense. Is it a little bit like storing the number of shards as you where? Heh - I guess it is - though with the side affect of nodes being able to see a full view of what shards should be in the collection right away (that is, the info is in the clusterstate.json).\n\nI've been leaning towards getting the req to pass numShards every time out anyway having dealt with it for a while now. This seems like a slightly more useful way to do it compared to just storing that value though. As long as the system property would always update it, I never had much of a problem with storing the value actually. I had been imagining you would trigger a re shard by passing a new number - but I'm not even sure if that is how that will work out.\n\nEssentially though, everything would work as it does now - I'm not sure we need to do anything more in this issue?\n "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13203745",
            "date": "2012-02-08T17:03:52+0000",
            "content": "Ok, I think I got it now. Looks like I was perhaps thinking too far ahead... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13203867",
            "date": "2012-02-08T19:21:45+0000",
            "content": "The mental model I've had is that numShards is just an optional helper at collection creation time, and is largely meaningless after that point.  clusterstate is the system of record and contains all of the shards in a collection.\nWhen shards are split in the future there will be a period of time when both the pre-split shard and the resulting post-split shards will coexist and overlap, and numShards as a collection property (rather than just a creation property) doesn't make sense.\nIf one is going to do custom sharding, then one would not use numShards, but simply add shards as needed (an example is shard-by-date, adding a new shard every week). "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13203910",
            "date": "2012-02-08T19:59:55+0000",
            "content": "and numShards as a collection property (rather than just a creation property) doesn't make sense.\n\nYeah I agree. I just got confused when you mentioned (in the original issue text) that something would have to be stored under /collections in zk.\n\nSo to summarize the scope of this issue once more:\n\nInitiating the cluster/collection from the event of starting the first node:\n\nThe first node that joins the cluster will pass the desired slice count (numShards System Property) and that number will be used to generate the placeholders (slices containing 0 shards) for the specific collection in cloudstate.\n\nAfter that everything works pretty much as it is now, with one exception: overseer does not look at the system property for determining how many slices there should be but uses the slice count (the placeholders in the beginning) from cloudstate. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13203926",
            "date": "2012-02-08T20:10:56+0000",
            "content": "with one exception: overseer does not look at the system property for determining how many slices there should be but uses the slice count (the placeholders in the beginning) from cloudstate.\n\n...actually the shard assignment seems to do this already, it looks at the number of existing slices from cloud state. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13203929",
            "date": "2012-02-08T20:14:29+0000",
            "content": "The first node that joins the cluster will pass the desired slice count (numShards System Property) and that number will be used to generate the placeholders\n\nI wouldn't model it as \"first node passes the desired slice count\" though.  I'd model it as \"collection creation has an optional numShards param\".\nCollection creation happens to always coincide with \"first node that joins\" just because we don't yet have a separate create-collection API? "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13203939",
            "date": "2012-02-08T20:20:22+0000",
            "content": "Yeah, I'd imagine that by using the other available way to create new collection (CoreAdminHandler) you'd pass the desired number of slices as an additional parameter to the create action.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13203966",
            "date": "2012-02-08T20:43:31+0000",
            "content": "I've actually been working on that right now because I need it for a test I'm writing.\n\nWhat I realized pretty much immediately was that I needed a way to pass that param down the chain so that the overseer eventually sees it. You should double check what I have done probably. What I found is that it seems only the overseer looks at the numShards sys prop - so it won't matter if its set on other nodes?\n\nAnyway, what I settled for at the moment is, you can set the numShards on the cloud descriptor - its not stored, so you may not always get it back in the future, it's just a startup param. Then num_shards is also registered as part of the core state in zk - again it's only needed once though. This lets us specify numShards per core though - which is kind of what you need to support it in CoreAdminHandler.\n\nBecause the numShard will stay in the core_state, I'm not yet sure how that will work if you end up being able to reshard by starting a core with a different number... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13203977",
            "date": "2012-02-08T20:58:37+0000",
            "content": "To clarify:\n\nI've actually been working on that right now\n\nBy that, I mean making it so that you can pass numShards via the create action with CoreAdminHandler - for a test for SOLR-3108 "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13204519",
            "date": "2012-02-09T13:48:09+0000",
            "content": "Initial patch. Once the changes in SOLR-3108 gets committed this patch needs to be updated to so that the System property is no longer used. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13204531",
            "date": "2012-02-09T14:01:06+0000",
            "content": "Also i think the error message that is displayed when querying a collection with placeholders needs to be improved. it's not that easy to understand what's going on from:\n\n\nCaused by: org.apache.solr.common.SolrException: no servers hosting shard:\n\nno servers hosting shard:\n\nrequest: http://localhost:33529/solr/collection1/select?q=*:*&wt=javabin&version=2\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:433)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:251)\n\tat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n\t... 34 more\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13204546",
            "date": "2012-02-09T14:25:41+0000",
            "content": "Also i think the error message that is displayed when querying a collection with placeholders needs to be improved. \n\n+1 - I saw this a lot during dev and always had to jump into the code again to figure out what the heck it meant (no shard is even listed, so its just super confusing) "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13204608",
            "date": "2012-02-09T16:06:15+0000",
            "content": "an updated patch that applies with current trunk "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13205166",
            "date": "2012-02-10T02:25:26+0000",
            "content": "Thanks Sami! I've committed your latest patch. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13205287",
            "date": "2012-02-10T08:24:30+0000",
            "content": "I saw this a lot during dev and always had to jump into the code again to figure out what the heck it meant (no shard is even listed, so its just super confusing)\n\nI added a new issue for this: SOLR-3118 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13207907",
            "date": "2012-02-14T18:54:01+0000",
            "content": "You done here Sami? Should we resolve this one? "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13208281",
            "date": "2012-02-15T09:02:38+0000",
            "content": "Yeah, this can be resolved. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13654139",
            "date": "2013-05-10T10:34:10+0000",
            "content": "Closed after release. "
        }
    ]
}