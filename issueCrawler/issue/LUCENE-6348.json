{
    "id": "LUCENE-6348",
    "title": "Incorrect results from UAX_URL_EMAIL tokenizer",
    "details": {
        "resolution": "Not A Problem",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "I'm using an analyzer based on the UAX_URL_EMAIL, with a maximum token length of 64 characters. I expect the analyzer to discard any text in the URL beyond those 64 characters, but the actual results yield ordinary terms from the tail-end of the URL.\n\nFor example, \n\n\ncurl -XGET http://localhost:9200/my_index/_analyze?analyzer=uax_url_email_analyzer -d \"hey, check out http://edge.org/conversation/yuval_noah_harari-daniel_kahneman-death-is-optional for some light reading.\"\n\n\n\nThe results look like this:\n\n\n{\n    \"tokens\": [\n        {\n            \"token\": \"hey\",\n            \"start_offset\": 0,\n            \"end_offset\": 3,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 1\n        },\n        {\n            \"token\": \"check\",\n            \"start_offset\": 5,\n            \"end_offset\": 10,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 2\n        },\n        {\n            \"token\": \"out\",\n            \"start_offset\": 11,\n            \"end_offset\": 14,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 3\n        },\n        {\n            \"token\": \"http://edge.org/conversation/yuval_noah_harari-daniel_kahneman-d\",\n            \"start_offset\": 15,\n            \"end_offset\": 79,\n            \"type\": \"<URL>\",\n            \"position\": 4\n        },\n        {\n            \"token\": \"eath\",\n            \"start_offset\": 79,\n            \"end_offset\": 83,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 5\n        },\n        {\n            \"token\": \"is\",\n            \"start_offset\": 84,\n            \"end_offset\": 86,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 6\n        },\n        {\n            \"token\": \"optional\",\n            \"start_offset\": 87,\n            \"end_offset\": 95,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 7\n        },\n        {\n            \"token\": \"for\",\n            \"start_offset\": 96,\n            \"end_offset\": 99,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 8\n        },\n        {\n            \"token\": \"some\",\n            \"start_offset\": 100,\n            \"end_offset\": 104,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 9\n        },\n        {\n            \"token\": \"light\",\n            \"start_offset\": 105,\n            \"end_offset\": 110,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 10\n        },\n        {\n            \"token\": \"reading\",\n            \"start_offset\": 111,\n            \"end_offset\": 118,\n            \"type\": \"<ALPHANUM>\",\n            \"position\": 11\n        }\n    ]\n}\n\n\n\nThe term from the extracted URL is correct, and correctly truncated at 64 characters. But as you can see, the analysis pipeline also creates three spurious terms [ \"eath\", \"is\" \"optional\" ] which come from the discarded portion of the URL.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14350985",
            "author": "Benji Smith",
            "date": "2015-03-06T21:59:23+0000",
            "content": "By the way, in case it helps, this is what my Elasticsearch index creation JSON looks like:\n\n\n{\n  \"settings\" : {\n    \"index\" : {\n      \"number_of_shards\" : 10,\n      \"number_of_replicas\" : 2\n    },\n    \"analysis\" : {\n      \"analyzer\" : {\n        \"uax_url_email_analyzer\" : {\n          \"type\" : \"custom\",\n          \"tokenizer\" : \"uax_url_email_tokenizer\",\n          \"filter\" : [\n            \"standard\", \"lowercase\"\n          ]\n        }\n      },\n      \"tokenizer\" : {\n        \"uax_url_email_tokenizer\" : {\n          \"type\" : \"uax_url_email\",\n          \"max_token_length\" : 64\n        }\n      }\n    }\n  }\n}\n\n "
        },
        {
            "id": "comment-14351090",
            "author": "Steve Rowe",
            "date": "2015-03-06T22:59:02+0000",
            "content": "Hi Benji,\n\nThis is the intended behavior.\n\nBefore LUCENE-5897/LUCENE-5400 were committed in Lucene 4.9.1, tokenization rules could match any length tokens, and ones that were larger than max_token_length would simply be (silently) dropped, not truncated.\n\nFrom Lucene 4.9.1 onward, StandardTokenizer and UAX29URLEmailTokenizer rules are not allowed to match against more than max_token_length characters, so URL prefixes will match, but the non-matched remaining characters of the URL will be subject to all of the other tokenization rules, resulting in behavior like you're seeing.\n\nTo get the behavior you want, increase the max_token_length to the maximum token length you expect to encounter, then add a TruncateTokenFilter, set to truncate tokens to your current max_token_length.  "
        },
        {
            "id": "comment-14351092",
            "author": "Benji Smith",
            "date": "2015-03-06T23:01:44+0000",
            "content": "Gotcha. Thanks for your help! "
        }
    ]
}