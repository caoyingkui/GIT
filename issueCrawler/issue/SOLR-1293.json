{
    "id": "SOLR-1293",
    "title": "Support for large no:of cores and faster loading/unloading of cores",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.2"
        ],
        "components": [
            "multicore"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "Solr , currently ,is not very suitable for a large no:of homogeneous cores where you require fast/frequent loading/unloading of cores . usually a core is required to be loaded just to fire a search query or to just index one document\nThe requirements of such a system are.\n\n\n\tVery efficient loading of cores . Solr cannot afford to read and parse and create Schema, SolrConfig Objects for each core each time the core has to be loaded ( SOLR-919 , SOLR-920)\n\tSTART STOP core . Currently it is only possible to unload a core (SOLR-880)\n\tAutomatic loading of cores . If a core is present and it is not loaded and a request comes for that load it automatically before serving up a request\n\tAs there are a large no:of cores , all the cores cannot be kept loaded always. There has to be an upper limit beyond which we need to unload a few cores (probably the least recently used ones)\n\tAutomatic allotment of dataDir for cores. If the no:of cores is too high al the cores' dataDirs cannot live in the same dir. There is an upper limit on the no:of dirs you can create in a unix dir w/o affecting performance",
    "attachments": {
        "SOLR-1293.patch": "https://issues.apache.org/jira/secure/attachment/12414426/SOLR-1293.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12735013",
            "date": "2009-07-24T10:09:08+0000",
            "content": "The patch is untested. The internal patch we used was on an older svn version . That patch is just merged to trunk and I am submitting that. I plan to test this sometime soon. But for those need this really badly this can be a starting point.\n\nrefer the wiki page for usage details http://wiki.apache.org/solr/LotsOfCores "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12737838",
            "date": "2009-08-01T06:26:26+0000",
            "content": "Do you have any thoughts on handling the situation where each core belongs to a different party and each party has access only to its own core via Solr Admin (i.e. doesn't see all the other cores hosted by the instance)?  Only the privileged administrator user can see and access all cores.\n\nHave you done any work in on this or is this on your TODO? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12737843",
            "date": "2009-08-01T07:09:18+0000",
            "content": "access control is not something we were trying to do in Solr. The Solr farm is setup behind our mail servers and it is never exposed outside.  "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12737844",
            "date": "2009-08-01T07:23:45+0000",
            "content": "OK, thanks.\nWhen you go to your Solr Admin page today, it lists all cores, even if there are 10000 of them? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12737886",
            "date": "2009-08-01T14:17:25+0000",
            "content": "yes. if you use the core admin command status , use verbose=false so that you only get the minimum info "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872454",
            "date": "2010-05-27T22:05:38+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043757",
            "date": "2011-06-03T16:46:54+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106426",
            "date": "2011-09-16T14:51:03+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234639",
            "date": "2012-03-21T18:08:41+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Jose Faria",
            "id": "comment-13409131",
            "date": "2012-07-09T02:20:05+0000",
            "content": "I do not know if this is the right place, but on the wiki http://wiki.apache.org/solr/LotsOfCores there is a note that this feature will be available at 4.0, but since May it was moved to 4.1 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13481296",
            "date": "2012-10-22T10:56:33+0000",
            "content": "Well, I think this JIRA will finally get some action...\n\nJose: \nThe actual availability of any particular feature is best tracked by the actual JIRA ticket. The \"fix version\" is usually the earliest possible fix. Not until the resolution is something like \"fixed\" is the code really in the code line.\n\nAll:\nOK, I'm thinking along these lines. I've started implementation, but wanted to open up the discussion in case I'm going down the wrong path.\n\nAssumption:\n1> For installations with multiple thousands of cores, provision has to me made for some kind of administrative process, probably an RDBMS that really maintains this information.\n\n\nSo here's a brief outline of the approach I'm thinking about.\n1> Add an additional optional parameter to the <cores> entry in solr.xml, LRUCacheSize=#. (what default?)\n2> Implement SOLR-1306, allow a data provider to be specified in solr.xml that gives back core descriptions, something like: <coreDescriptorProvider class=\"com.foo.FooDataProvider\" [attr=\"val\"]/> (don't quite know what attrs we want, if any).\n3> Add two optional attributes to individual <core> entries\n   a> sticky=\"true|false\". Default to true. Any cores marked with this would never be aged out, essentially treat them just as current. \n   b> loadOnStartup=\"true|false\", default to true.\n4> so the process of getting a core would be something like\n   a> check the normal list, just like now. If a core was found, return it.\n   b> Check the LRU list, if a core was found, return it.\n   c> ask the dataprovider (if defined) for the core descriptor. create the core and put it in the LRU list.\n   d> remove any core entries over the LRU limit. Any hints on the right cache to use? There's the Lucene LRUCache, ConcurrentLRUCache, the LRUHashMap in lucene that I can't find in any of the compiled jars....). I've got to close the core as it's removed.... It looks like I can use ConcurrentLRUCache and add a listener to close the core when it's removed from the list.\n\nProcessing-wise, in the usual case this would cost an extra check each time a core was fetched. If <a> above failed, we would have to see if the dataprovider was defined before returning null. I don't think that's onerous, the rest of the costs would only be incurred when a dataprovider did exist.\n\nBut one design decisions here is along these lines. What to do with persistence and stickiness? Specifically, if the coreDescriptorProvider gives us a core from, say, an RDBMS, should we allow that core to be persisted into the solr.xml file if they've set persist=\"true\" in solr.xml? I'm thinking that we can make this all work with maximum flexibility if we allow the coreDataProvider to tell us whether we should persist any core currently loaded....\n\nAnyway, I'll be fleshing this out over the next little while, anybody want to weigh in?\n\nErick\n "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13481368",
            "date": "2012-10-22T13:55:39+0000",
            "content": "an RDBMS\n\nIs a full RDBMS needed? How about a NoSQL approach... like... um... Solr (or raw Lucene) itself? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13481370",
            "date": "2012-10-22T13:59:13+0000",
            "content": "I don't care what's used to store the info. The provider that the user provides cares, but that's the point of getting that info through a custom component, Solr doesn't need to know. Nor should it <G>... \n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13481372",
            "date": "2012-10-22T14:02:34+0000",
            "content": "Rdbms is not required. We are managing that with the xml itself.  Now that we have moved to zookeeper for cloud, we should piggyback on zookeeper for everything "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13481375",
            "date": "2012-10-22T14:07:43+0000",
            "content": "Solr doesn't need to know\n\nTrue, but what store would you propose using in unit tests? I suppose you could develop a \"Mock RDBMS\" which could be even simpler than Solr so unit tests don't need a solr running. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13481380",
            "date": "2012-10-22T14:10:42+0000",
            "content": "If you wish to test the zk persistence feature should we just not use an embedded zk? "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13481386",
            "date": "2012-10-22T14:14:14+0000",
            "content": "piggyback on zookeeper\n\nThat's okay, but zk is optimized for a \"small\" amount of configuration info - 1 MB limit. Is \"large number\" times data per core going to be under 1 MB?\n\nIs \"large number\" supposed to be hundreds, thousands, tens of thousands, hundreds of thousands, millions, ...? I mean, if a web site had millions of users, could they have one loadable core per user? The use case should be more specific about the goals.\n "
        },
        {
            "author": "Andrzej Rusin",
            "id": "comment-13481413",
            "date": "2012-10-22T15:02:10+0000",
            "content": "Whatever would be the storage of the cores info, it would be nice to have some API and/or command line tools for (batch) manipulating the cores; what do you think? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13481428",
            "date": "2012-10-22T15:13:22+0000",
            "content": "Well, I don't think the use-case I'm working on needs an API or command-line tools, so I probably won't be working on it. I'd be glad to commit it in if someone else wanted to do it. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13481536",
            "date": "2012-10-22T17:34:16+0000",
            "content": "Is \"large number\" supposed to be hundreds, thousands, tens of thousands, hundreds of thousands, millions, ...?\n\nI'll be surprised if it ever crosses a few 10000's . But let us say the upper limit sa a 100000 , shouldn't it be simple to keep in ZK?\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13482413",
            "date": "2012-10-23T15:32:42+0000",
            "content": "General comment:\nWe may want the index/core re-opener to remain aware of previous locations (nodes) on which cores were opened for the purposes of reusing any possible OS-level caches that may still exist on those nodes for that core.  For example, if the cluster has nodes 1-100 and core Foo was on nodes 1, 2, and 3 before it was closed, then maybe next time it needs to be opened it would ideally be opened on those 1, 2, and 3 nodes.  Of course, nodes 1, 2, or 3 may no longer be around or may be currently overloaded, or.... in which case alternative nodes need to be picked. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13482423",
            "date": "2012-10-23T15:45:57+0000",
            "content": "Otis:\n\nI'm not sure I understand this. As I'm looking at this particular implementation, all the potential cores (configuration, data files, etc) are already on the particular node, it's just a matter of loading/unloading them. If you're thinking about SolrCloud/ZK, oh my aching head! I guess I'd propose that how this all works with ZK be split off to different tickets all together, too much for me to deal with....\n\nI'm explicitly thinking of this as having no cluster-awareness, it's all local to a single Solr node. Any meta-level coordination on which node a particular query should be routed to is assumed to be out of scope, at least for this version.\n\nThat said, I can certainly see the value in what you're talking about, that's just not the use-case I'm trying to address.... "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13484479",
            "date": "2012-10-25T21:04:13+0000",
            "content": "I've implemented some parts of this (SOLR-880, SOLR-1028), I should be checking them in sometime relatively soon, then on to some other JIRAs related to this one. But I got to thinking that maybe what we really want is two new characteristics for cores, call the loadOnStartup(T|F, default T) and sticky(T|F, default T). \n\nWhat I've done so far conflates the two ideas; things loaded \"lazily\" are assumed to be NOT sticky and there's really no reason to conflate them. Use cases are\n\nLOS=T, STICKY=T - really, what we have now. Pay the penalty on startup for loading the core at startup in exchange for speed later.\n\nLOS=T, STICKY=F - load on startup, but allow the core to be automatically unloaded later. For preloading expected 'hot' cores. Cores are unloaded on an LRU basis. NOTE: a core can be unloaded and then loaded again later if it's referenced.\n\nLOS=F, STICKY=T - Defer loading the core, but once it's loaded, keep it loaded. Get's us started fast, amortizes loading the core. This one I actually expect to be the least useful, but it's a consequence of the others and doesn't cost anything extra to implement coding-wise.\n\nLOS=F, STICKY=F - what I was originally thinking of as \"lazy loading\". Cores get loaded when first referenced, and swapped out on an LRU algorithm.\n\nLooking at what I've done on the two JIRA's mentioned, this is actually not at all difficult, just a matter of putting the CoreConfig in the right list...\n\nSo, if any STICKY=F is found, there's a LRU cache created (actually a LinkedHashMap with removeEldestEntry overridden), with an optional size specified in the <cores...> tag. I'd guess I'll default it to 100 or some such if (and only if) there's at least one STICKY=F defined but no cache size in <cores...>. Of course if the user defined cacheSize in <cores...>, I'd allocate the cache up front.\n\nThoughts? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13484849",
            "date": "2012-10-26T10:51:07+0000",
            "content": "the combination of LOS, STICKY and their defaults looks fine to me  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13485442",
            "date": "2012-10-27T14:05:24+0000",
            "content": "About persistence. And about coreDescriptorProvider in general. If one is supplied, I'm thinking that it will always have first crack at most things having to do with getting a CoreDescriptor. For instance:\n1> When persisting a core, ask the coreDescriptor whether to persist to solr.xml.\n2> When listing cores, give preference to any descriptor the coreDescriptor knows about. I.e. override the ones in any CoreContainer lists with ones from the provider.\n3> ???\n\nThe mind-set here is that, if a CoreDescriptorProvider is present, it should be the arbiter of relevant decisions about that core. We can default to reasonable stuff (e.g. default behavior for CoreDescriptorProvider.shouldPersist(String coreName) is to return false)\n\nI'm seeing one other thing related to persistence, NOT having to do with a CoreDescriptorProvider. Since CoreContainer now has two new params, loadOnStartup=true and swappable=false magically show up in the persisted file if they aren't specified. It would be a bit more aesthetic to only show what was specified by the user, but I'm not sure it's worth any effort, and it appears that this is true for some other properties as well. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13597330",
            "date": "2013-03-08T17:47:16+0000",
            "content": "All the functionality here is part of other JIRAs, e.g. SOLR-4196, SOLR-4478 and the like. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13653751",
            "date": "2013-05-10T10:32:52+0000",
            "content": "Closed after release. "
        }
    ]
}