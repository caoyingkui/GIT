{
    "id": "LUCENE-3846",
    "title": "Fuzzy suggester",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Would be nice to have a suggester that can handle some fuzziness (like spell correction) so that it's able to suggest completions that are \"near\" what you typed.\n\nAs a first go at this, I implemented 1T (ie up to 1 edit, including a transposition), except the first letter must be correct.\n\nBut there is a penalty, ie, the \"corrected\" suggestion needs to have a much higher freq than the \"exact match\" suggestion before it can compete.\n\nStill tons of nocommits, and somehow we should merge this / make it work with analyzing suggester too (LUCENE-3842).",
    "attachments": {
        "LUCENE-3846.patch": "https://issues.apache.org/jira/secure/attachment/12517003/LUCENE-3846.patch",
        "LUCENE-3846_fuzzy_analyzing.patch": "https://issues.apache.org/jira/secure/attachment/12547312/LUCENE-3846_fuzzy_analyzing.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-04T15:18:53+0000",
            "content": "Patch, very much work in progress... there are some hard things to work out still. ",
            "author": "Michael McCandless",
            "id": "comment-13221914"
        },
        {
            "date": "2012-03-04T18:17:47+0000",
            "content": "awesome! FST/A went a long way.\n\nJust a few random toughs, triggered by \"... \"corrected\" suggestion needs to have a much higher freq than the \"exact match\"...\" \n\nFrequency influence is normally slightly more complicated than \"only more popular\", depending on search task user is facing. Only more popular helps if we assume user types it wrong and our suggestions dictionary is always right. But in cases where you have user who types it correctly, and collection contains errors you would cut all documents with \"fuzzy\". \n\nWhat I found works pretty good is considering this problem to be of nearest neighbor type. Namely, \ntask is to find closest matches to the query. Some are more and some less popular. Take for example a case where user types \"black dog\" and our collection contains document \"blaKC dog\", having frequency of blakc much lower than black, \"only more popular\" would miss this document.\n\nWhat works out of the box pretty good is comparing frequency of query word and \"candidate\" to some reasonable cut-off and classifying them to \"HF\"/\"LF\" (high/low frequency) terms. It is based on the fact that typos are normally very seldom (if not, they should be treated as synonyms!). So if user types LF token, probably fuzzy candidate would be HF, and the other way around. \n\nBut as said, it depends what the task is.    \n\n\nNext level for \"fuzzy *\" in Lucene is going into specifying separate costs for Inserts/deletes, swaps and transpositions at character(byte) level and optionally considering position of edit. This brings precision++ if used properly, like in \n\n\t\"inserting/deleting silent h should cost less than other letters (thomas vs thomas)\"\n\t\"Phonetics, swap \"c\" <-> \"k\" is less evil than default\"\n\t\"inserting s at the end... bug vs bugs\"\n\n\n\nApart from that, I see absolutely nothing more one on earth can do better \n\n\nSorry again for just shooting around with \"wish lists\" at you guys, my time-schedule really does not permit any serious work in form of patches.      ",
            "author": "Eks Dev",
            "id": "comment-13221961"
        },
        {
            "date": "2012-03-04T18:17:48+0000",
            "content": "awesome! FST/A went a long way.\n\nJust a few random toughs, triggered by \"... \"corrected\" suggestion needs to have a much higher freq than the \"exact match\"...\" \n\nFrequency influence is normally slightly more complicated than \"only more popular\", depending on search task user is facing. Only more popular helps if we assume user types it wrong and our suggestions dictionary is always right. But in cases where you have user who types it correctly, and collection contains errors you would cut all documents with \"fuzzy\". \n\nWhat I found works pretty good is considering this problem to be of nearest neighbor type. Namely, \ntask is to find closest matches to the query. Some are more and some less popular. Take for example a case where user types \"black dog\" and our collection contains document \"blaKC dog\", having frequency of blakc much lower than black, \"only more popular\" would miss this document.\n\nWhat works out of the box pretty good is comparing frequency of query word and \"candidate\" to some reasonable cut-off and classifying them to \"HF\"/\"LF\" (high/low frequency) terms. It is based on the fact that typos are normally very seldom (if not, they should be treated as synonyms!). So if user types LF token, probably fuzzy candidate would be HF, and the other way around. \n\nBut as said, it depends what the task is.    \n\n\nNext level for \"fuzzy *\" in Lucene is going into specifying separate costs for Inserts/deletes, swaps and transpositions at character(byte) level and optionally considering position of edit. This brings precision++ if used properly, like in \n\n\t\"inserting/deleting silent h should cost less than other letters (thomas vs thomas)\"\n\t\"Phonetics, swap \"c\" <-> \"k\" is less evil than default\"\n\t\"inserting s at the end... bug vs bugs\"\n\n\n\nApart from that, I see absolutely nothing more one on earth can do better \n\n\nSorry again for just shooting around with \"wish lists\" at you guys, my time-schedule really does not permit any serious work in form of patches.      ",
            "author": "Eks Dev",
            "id": "comment-13221962"
        },
        {
            "date": "2012-03-04T18:48:23+0000",
            "content": "\nNext level for \"fuzzy *\" in Lucene is going into specifying separate costs for Inserts/deletes, swaps and transpositions at character(byte) level and optionally considering position of edit. This brings precision++ if used properly, like in \n\nIts probably \"good enough\" for this suggester to allow someone to re-rank their top-N with a StringDistance http://svn.apache.org/repos/asf/lucene/dev/trunk/modules/suggest/src/java/org/apache/lucene/search/spell/StringDistance.java\n\nsuch things are language and domain-specific and i think just adding this pluggability will work pretty well, rather than trying to complicate the actual intersection algorithm (which will ultimately never satisfy everyone anyway).\n\nthe default can be \"internal metric\" which means no re-ranking at all. This is how DirectSpellChecker works for example. ",
            "author": "Robert Muir",
            "id": "comment-13221970"
        },
        {
            "date": "2012-03-04T19:22:52+0000",
            "content": "sure as hell, re-ranking covers most of the cases. If you are not saturating top-N depth, it works just perfect, but if you are saturating top-n, you have to increase depth / number of allowed edits, which in turn hurts performance...  \n\n\n\"rather than trying to complicate the actual intersection algorithm\" \n\nThe logic in intersection algorithm would not have to know anything about the language specifics, it would be defined in cost matrix. But suporting cost matrix per edit operation deep down can be complex. You would simply reduce language/domain parametrization to configuration of costs in matrix\n\n ",
            "author": "Eks Dev",
            "id": "comment-13221974"
        },
        {
            "date": "2012-03-04T19:32:42+0000",
            "content": "\nThe logic in intersection algorithm would not have to know anything about the language specifics, it would be defined in cost matrix. But suporting cost matrix per edit operation deep down can be complex. You would simply reduce language/domain parametrization to configuration of costs in matrix\n\nLike i said, it wont satisfy everyone. Lots of people are going to want ranking thats way more complex than just a cost matrix anyway,\ne.g. works on context, or phonemes, or other things.\n\nSo i think its good to just plug in the re-ranking so people can write whatever StringDistance they want and call it a day.\n\nPersonally i dont think context-free single-character cost-matrixes really help myself, feel free to show me evidence they do  ",
            "author": "Robert Muir",
            "id": "comment-13221978"
        },
        {
            "date": "2012-03-04T19:43:44+0000",
            "content": "This is a nice benefit of the path-based best-first search (in the patch): it's easy to use a custom cost matrix.  The cost can also be context-dependent too (based on past matched characters, though not [easily] future ones).\n\nWe don't need to explore that now, before committing this, but it's nice that we'll have the freedom to do so later.\n\nRe-ranking definitely adds cost since you'll have to pull a bigger N.  I think we'll likely have to somehow combine the cost and \"isFuzzy\" into a single cost, during the search, not after (reranking).  Not sure how to do that yet... ",
            "author": "Michael McCandless",
            "id": "comment-13221985"
        },
        {
            "date": "2012-03-04T21:25:21+0000",
            "content": "\nfeel free to show me evidence they do\n\nEven here they help a lot, do not underestimate error model! (as in noisy channel, see http://norvig.com/spell-correct.html for a nice overview).\n\nExamples, off the top of my head:\nin a case you search for Carin in a set \n{Karin, Marin, Darin}\n, (All valid names, at edit distance one) you would prefer to see Karin as a highest (to the only one) ranked fuzzy suggestion. (close consonants).\n\nOr discount on swap(vowel ,vowel) vs swap(vowel/consonant, consonant). Mistaking one vowel for another is more probable than mistaking two consonants or consonant and vowel (as long as humans type). \n\nBooks, scanned using OCR have no problems with phonetics, but other...\n\nContext is important, in-word context as part of \"error model\" (character level context, like previous character) but even more important is the context from  the \"language model\", that normally dominates. \n\nI could look for some interesting papers in my archives if you are not convinced yet \nThis one is worth reading (http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf), tackles, among other things, exactly this topic. \n\n\nit's easy to use a custom cost matrix. The cost can also be context-dependent too (based on past matched characters, though not [easily] future ones).\n\nGreat to hear that!  \nprefix based context is the only context at sub-word level I ever used. I doubt lookahead brings something.  ",
            "author": "Eks Dev",
            "id": "comment-13222014"
        },
        {
            "date": "2012-03-04T21:31:53+0000",
            "content": "\nI could look for some interesting papers in my archives if you are not convinced yet \n\nnone of that is evidence, those are just examples, I could list examples where\na cost table hurts, too.\n\nEvidence is some actual experiment with results showing some cost table has better results. ",
            "author": "Robert Muir",
            "id": "comment-13222020"
        },
        {
            "date": "2012-03-04T21:40:24+0000",
            "content": "Sure, give me enough annotated data and I can give you \"close to optimal\" cost matrix. There are (rather simple) ways to estimate these costs.  \n\nOr you are trying to argument there is no cost table better than the one filled with ones? ",
            "author": "Eks Dev",
            "id": "comment-13222022"
        },
        {
            "date": "2012-03-04T21:42:59+0000",
            "content": "\nOr you are trying to argument there is no cost table better than the one filled with ones?\n\nYes. there is no evidence to the contrary, so why make things complicated. ",
            "author": "Robert Muir",
            "id": "comment-13222024"
        },
        {
            "date": "2012-03-05T09:01:00+0000",
            "content": "Robert, \nI am not talking from some abstract-theoretical point of view, I made my own experience on  nontrivial Lucene datasets that are unfortunately not for sharing. Having possibility to train cost matrices per edit operation brings a lot, but you may have had another experience (different problems, different data...).  \n\nWithout specifying concrete task (annotated data), there is no notion of \"better\", so this argument simply does not help (\"show me it is better\", \"no you show me all ones matrix  is better than any other\", \"no, no...\"). It is simply about the experience we made in the past, different opinions.   \n\nI personally would not try this argument with molecular biology teams, and tell them their POM and BLOSUM matrices are worthless or to someone in record linkage community (Lucene was used in this context a lot) or ... \n\n ",
            "author": "Eks Dev",
            "id": "comment-13222228"
        },
        {
            "date": "2012-03-05T12:37:07+0000",
            "content": "\nI personally would not try this argument with molecular biology teams, and tell them their POM and BLOSUM matrices are worthless or to someone in record linkage community (Lucene was used in this context a lot) or ... \n\nThats ok, I would \n\nI don't think we should complicate already-complicated things unless there is some clear benefit.\n\nI'm not worried about offending anyone. ",
            "author": "Robert Muir",
            "id": "comment-13222303"
        },
        {
            "date": "2012-03-08T11:14:39+0000",
            "content": "I applied this and fixed some compile errors while reading through the code. I also simplified the build() method and removed some minor things.\n\nbasically updated to trunk ",
            "author": "Simon Willnauer",
            "id": "comment-13225138"
        },
        {
            "date": "2012-03-08T11:23:06+0000",
            "content": "Thanks Simon! ",
            "author": "Michael McCandless",
            "id": "comment-13225144"
        },
        {
            "date": "2012-10-01T23:42:31+0000",
            "content": "here's my hacky Fuzzy+Analyzing prototype.\n\nBut we need to fix intersectPrefixPaths to be able to efficiently intersect transition ranges (e.g. findTargetArc + readNextArc through the range?).\n\nanyway we should see how slow this is compared to mike's: the advantage would be you would still get all the stuff AnalyzingSuggester has... ",
            "author": "Robert Muir",
            "id": "comment-13467331"
        },
        {
            "date": "2012-10-11T16:22:38+0000",
            "content": "here is a patch that adds the missing intersect method and adds several tests derived from the AnalyzingSuggestorTest. The tests all pass at this point but I do get a weird failure if I run the benchmarks. somehow the TopNSearcher runs into a bad state which I can't really figure out.\n\nthis patch has several refactorings in AnalyzingSuggestor mainly to make testing easier in the fuzzy case (encapuslated some stuff into package private methods etc.) Yet there are tons of nocommits but at least we have something working. \n\nRegarding the failure, I see a NoSuchELementException from the \"queue\" in the top N searcher that somehow removed the bottom and tries to pull the last element that doesn't exists. (stacktrace below) Yet, the funky thing is that this doesn't happen if I run this with exactFirst=false but the problem seems to be in the non-exactFirst part (see stacktrace). I use a direct intersection for exactFirst in the fuzzy case so that code is identical to analyzing suggestor since the intersection of the LD automaton doesn't return enough information to tell what is an exact match. \n\nhere is the stacktrace:\n\n\njava.util.NoSuchElementException\n\tat java.util.TreeMap.key(TreeMap.java:1206)\n\tat java.util.TreeMap.lastKey(TreeMap.java:274)\n\tat java.util.TreeSet.last(TreeSet.java:384)\n\tat org.apache.lucene.util.fst.Util$TopNSearcher.addIfCompetitive(Util.java:339)\n\tat org.apache.lucene.util.fst.Util$TopNSearcher.search(Util.java:453)\n\tat org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester.lookup(AnalyzingSuggester.java:581)\n\tat org.apache.lucene.search.suggest.LookupBenchmarkTest$2.call(LookupBenchmarkTest.java:228)\n\tat org.apache.lucene.search.suggest.LookupBenchmarkTest$2.call(LookupBenchmarkTest.java:1)\n\tat org.apache.lucene.search.suggest.LookupBenchmarkTest.measure(LookupBenchmarkTest.java:253)\n\tat org.apache.lucene.search.suggest.LookupBenchmarkTest.runPerformanceTest(LookupBenchmarkTest.java:224)\n\tat org.apache.lucene.search.suggest.LookupBenchmarkTest.testPerformanceOnPrefixes6_9(LookupBenchmarkTest.java:192)\nNOTE: reproduce with: ant test  -Dtestcase=LookupBenchmarkTest -Dtests.method=testPerformanceOnPrefixes6_9 -Dtests.seed=B5BAF2A9592263BC -Dtests.locale=fi_FI -Dtests.timezone=Africa/Lagos -Dtests.file.encoding=UTF-8\nNOTE: test params are: codec=Lucene40: {}, sim=DefaultSimilarity, locale=fi_FI, timezone=Africa/Lagos\nNOTE: Linux 2.6.38-16-generic amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=12,threads=1,free=578809008,total=1539571712\nNOTE: All tests run in this JVM: [LookupBenchmarkTest]\n\n\n\n\nmike if you get a chance it would be great if you could look into that one?! ",
            "author": "Simon Willnauer",
            "id": "comment-13474284"
        },
        {
            "date": "2012-10-11T16:36:36+0000",
            "content": "updated patch - mike conflicted me in the way  ",
            "author": "Simon Willnauer",
            "id": "comment-13474296"
        },
        {
            "date": "2012-10-11T17:04:00+0000",
            "content": "New patch fixing that exc in benchmark: it's a pre-existing bug in how bottom is set ... if the queue has come empty we just have to set bottom to null.\n\nI think we should separately fix this... I'll commit (not sure why WFST/AnalyzingSuggester haven't hit this already).  It only happens w/ exactFirst because this removes one of the competing topN paths from the queue, and then if there aren't enough suggestions remaining the queue empties before we find the topN results... I'll work up a test. ",
            "author": "Michael McCandless",
            "id": "comment-13474320"
        },
        {
            "date": "2012-10-11T17:07:26+0000",
            "content": "Argh, I forgot to svn add files ... i'll make a branch and commit there. ",
            "author": "Michael McCandless",
            "id": "comment-13474324"
        },
        {
            "date": "2012-10-11T17:09:34+0000",
            "content": "OK branch here: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3846\n\nI committed the last patch. ",
            "author": "Michael McCandless",
            "id": "comment-13474327"
        },
        {
            "date": "2012-10-12T15:27:43+0000",
            "content": "just for the record here are my benchmark numbers for the latest branch code:\n\n\nTest class requires enabled assertions, enable globally (-ea) or for Solr/Lucene subpackages only: org.apache.lucene.search.suggest.LookupBenchmarkTest\n-- prefixes: 6-9, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 4650 [+- 12.56], ~kQPS: 11\nAnalyzingSuggester queries: 50001, time[ms]: 444 [+- 1.89], ~kQPS: 113\nJaspellLookup   queries: 50001, time[ms]: 181 [+- 0.96], ~kQPS: 275\nTSTLookup       queries: 50001, time[ms]: 229 [+- 2.35], ~kQPS: 218\nFSTCompletionLookup queries: 50001, time[ms]: 245 [+- 3.54], ~kQPS: 204\nWFSTCompletionLookup queries: 50001, time[ms]: 121 [+- 1.72], ~kQPS: 413\n-- prefixes: 100-200, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 5432 [+- 20.86], ~kQPS: 9\nAnalyzingSuggester queries: 50001, time[ms]: 403 [+- 1.47], ~kQPS: 124\nJaspellLookup   queries: 50001, time[ms]: 129 [+- 1.24], ~kQPS: 389\nTSTLookup       queries: 50001, time[ms]: 68 [+- 4.03], ~kQPS: 739\nFSTCompletionLookup queries: 50001, time[ms]: 254 [+- 2.60], ~kQPS: 197\nWFSTCompletionLookup queries: 50001, time[ms]: 82 [+- 1.03], ~kQPS: 610\n-- construction time\nFuzzySuggester  input: 50001, time[ms]: 450 [+- 1.86]\nAnalyzingSuggester input: 50001, time[ms]: 449 [+- 1.82]\nJaspellLookup   input: 50001, time[ms]: 40 [+- 3.80]\nTSTLookup       input: 50001, time[ms]: 111 [+- 3.33]\nFSTCompletionLookup input: 50001, time[ms]: 213 [+- 4.36]\nWFSTCompletionLookup input: 50001, time[ms]: 156 [+- 2.08]\n-- prefixes: 2-4, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 3571 [+- 12.15], ~kQPS: 14\nAnalyzingSuggester queries: 50001, time[ms]: 997 [+- 5.73], ~kQPS: 50\nJaspellLookup   queries: 50001, time[ms]: 494 [+- 2.25], ~kQPS: 101\nTSTLookup       queries: 50001, time[ms]: 1846 [+- 9.67], ~kQPS: 27\nFSTCompletionLookup queries: 50001, time[ms]: 221 [+- 1.57], ~kQPS: 227\nWFSTCompletionLookup queries: 50001, time[ms]: 457 [+- 9.05], ~kQPS: 109\n-- RAM consumption\nFuzzySuggester  size[B]:      889,138\nAnalyzingSuggester size[B]:      889,138\nJaspellLookup   size[B]:    9,815,128\nTSTLookup       size[B]:    9,858,792\nFSTCompletionLookup size[B]:      466,520\nWFSTCompletionLookup size[B]:      507,640\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-13475072"
        },
        {
            "date": "2012-10-12T15:34:31+0000",
            "content": "here is another benchmark with minPrefix=3 instead of 1 this looks much better: \n\n\n-- prefixes: 6-9, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 2125 [+- 6.38], ~kQPS: 24\nAnalyzingSuggester queries: 50001, time[ms]: 452 [+- 3.61], ~kQPS: 111\nJaspellLookup   queries: 50001, time[ms]: 187 [+- 1.02], ~kQPS: 267\nTSTLookup       queries: 50001, time[ms]: 263 [+- 1.78], ~kQPS: 190\nFSTCompletionLookup queries: 50001, time[ms]: 269 [+- 1.59], ~kQPS: 186\nWFSTCompletionLookup queries: 50001, time[ms]: 121 [+- 0.75], ~kQPS: 414\n-- prefixes: 100-200, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 2778 [+- 16.56], ~kQPS: 18\nAnalyzingSuggester queries: 50001, time[ms]: 414 [+- 1.70], ~kQPS: 121\nJaspellLookup   queries: 50001, time[ms]: 133 [+- 1.85], ~kQPS: 376\nTSTLookup       queries: 50001, time[ms]: 69 [+- 3.41], ~kQPS: 724\nFSTCompletionLookup queries: 50001, time[ms]: 257 [+- 1.79], ~kQPS: 194\nWFSTCompletionLookup queries: 50001, time[ms]: 83 [+- 3.31], ~kQPS: 605\n-- prefixes: 2-4, num: 7, onlyMorePopular: true\nFuzzySuggester  queries: 50001, time[ms]: 1310 [+- 3.30], ~kQPS: 38\nAnalyzingSuggester queries: 50001, time[ms]: 995 [+- 8.03], ~kQPS: 50\nJaspellLookup   queries: 50001, time[ms]: 507 [+- 4.19], ~kQPS: 99\nTSTLookup       queries: 50001, time[ms]: 2148 [+- 16.63], ~kQPS: 23\nFSTCompletionLookup queries: 50001, time[ms]: 223 [+- 2.14], ~kQPS: 224\nWFSTCompletionLookup queries: 50001, time[ms]: 414 [+- 28.44], ~kQPS: 121\n\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-13475077"
        },
        {
            "date": "2012-10-12T15:46:32+0000",
            "content": "This looks really good (especially if it holds up for a larger fst!) Thanks for hacking\nat the intersectPrefixPaths to get us going.\n\nOne caveat: the whole thing is optimized for the case where the \"query analyzer\" \nis very simple, particularly where there are no positionIncrement=0 tokens.\n\nSo if you use a query analyzer with say a synonymsfilter, then you hit the\n\"nocommit: how slow can this be?\". We should maybe benchmark that. if its\nreally horrible we could just throw an exception and document that you cannot\nuse synonyms etc in your query analyzer (use them at index-time instead) to\nprevent performance traps... ",
            "author": "Robert Muir",
            "id": "comment-13475090"
        },
        {
            "date": "2012-10-18T16:05:43+0000",
            "content": "FYI - I cleaned up the code & tests, synced with trunk, removed no-commits, enabled assertions again and documented the expensiveness of complex query analyzers. From my perspective this is good to go but we should run another round of reviews. ",
            "author": "Simon Willnauer",
            "id": "comment-13479098"
        },
        {
            "date": "2012-10-29T15:36:00+0000",
            "content": "I think this is ready!  I'm attaching the reviewable patch w/ diffs on the branch vs trunk ... ",
            "author": "Michael McCandless",
            "id": "comment-13486084"
        },
        {
            "date": "2012-10-29T15:59:46+0000",
            "content": "cool man it looks good. we need a changes entry but from my side this looks good. we can tackle the todos on trunk ",
            "author": "Simon Willnauer",
            "id": "comment-13486102"
        },
        {
            "date": "2012-10-29T16:07:01+0000",
            "content": "I think we might want to beast the tests on this before merging! ",
            "author": "Robert Muir",
            "id": "comment-13486109"
        },
        {
            "date": "2012-10-29T16:58:47+0000",
            "content": "New patch, w/ CHANGES entry, fixing a couple issues Robert spotted (thanks!). ",
            "author": "Michael McCandless",
            "id": "comment-13486148"
        },
        {
            "date": "2013-03-22T16:21:56+0000",
            "content": "[branch_4x commit] Michael McCandless\nhttp://svn.apache.org/viewvc?view=revision&revision=1403780\n\nLUCENE-3846: add new FuzzySuggester ",
            "author": "Commit Tag Bot",
            "id": "comment-13610620"
        },
        {
            "date": "2013-05-10T10:34:16+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654172"
        }
    ]
}