{
    "id": "SOLR-7932",
    "title": "Solr replication relies on timestamps to sync across machines",
    "details": {
        "components": [
            "replication (java)"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Spinning off SOLR-7859, noticed there that wall time recorded as commit data on a commit to check if replication needs to be done. In IndexFetcher, there is this code:\n\n\n      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {\n        //master and slave are already in sync just return\n        LOG.info(\"Slave in sync with master.\");\n        successfulInstall = true;\n        return true;\n      }\n\n\n\nIt appears as if we are checking wall times across machines to check if we are in sync, this could go wrong.\n\nOnce a decision is made to replicate, we do seem to use generations instead, except for this place below checks both generations and timestamps to see if a full copy is needed..\n\n\n      // if the generation of master is older than that of the slave , it means they are not compatible to be copied\n      // then a new index directory to be created and all the files need to be copied\n      boolean isFullCopyNeeded = IndexDeletionPolicyWrapper\n          .getCommitTimestamp(commit) >= latestVersion\n          || commit.getGeneration() >= latestGeneration || forceReplication;",
    "attachments": {
        "SOLR-7932.patch": "https://issues.apache.org/jira/secure/attachment/12750701/SOLR-7932.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-15T15:06:55+0000",
            "author": "Shawn Heisey",
            "content": "I cannot comment on the suggested fix of using generations, but I might have something to add regarding wall clocks on multiple machines.\n\nI think that Solr should provide an implicit system-level handler whose purpose is to return System.currentTimeMillis as quickly as possible.  At certain times, which would include SolrCloud initialization and possibly SolrCloud-related replication requests, a Solr node should compare its own wall clock time with the other relevant node(s) for the particular action, and if a large enough discrepancy is found, a warning should be logged.  In the future, we might upgrade that to an error.  My initial SWAG for an acceptable discrepancy is no more than half a second. ",
            "id": "comment-14698308"
        },
        {
            "date": "2015-08-15T15:14:12+0000",
            "author": "Shawn Heisey",
            "content": "After thinking about it for a few minutes, if we have a \"warn for time discrepancy\" feature, the default acceptable discrepancy should be geared towards a LAN setup (half a second or smaller), and configurable for those who push the limits with something like satellite networking, which has a minimum round-trip (ping) latency of over 600 milliseconds. ",
            "id": "comment-14698311"
        },
        {
            "date": "2015-08-15T18:12:11+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Personally I would rather remove any place relying on absolute timestamps. The feature you describe IMO doesn't belong to Solr, and there are well known mechanisms to keep clocks in sync already (ntpd on Linux for example) ",
            "id": "comment-14698390"
        },
        {
            "date": "2015-08-15T22:27:20+0000",
            "author": "Shawn Heisey",
            "content": "I wasn't suggesting that Solr should be responsible for syncing the clocks.  That's likely not even possible.  We could theoretically compensate for differences if we can detect what the difference is, but I don't think that's our job either.\n\nI was suggesting that Solr should proactively DETECT clock sync problems, and let the user know that there's a problem with their install.  Fixing it is up to the user. ",
            "id": "comment-14698474"
        },
        {
            "date": "2015-08-15T22:33:25+0000",
            "author": "Ramkumar Aiyengar",
            "content": "My point was that we shouldn't have to worry about if the clocks are out of sync.. This (replication) is one place where we do currently rely on clocks being sync, and my question is if it needs to be. We do have a replication bug here regardless of how sync'd up the clocks are, it's a race condition waiting to happen.. ",
            "id": "comment-14698475"
        },
        {
            "date": "2015-08-15T23:07:51+0000",
            "author": "Shawn Heisey",
            "content": "If we don't have (or fix as you're suggesting) any functionality where time skew causes problems, then we don't really need to worry about it.\n\nBut I think we should.  Even if Solr itself doesn't care, a user who is troubleshooting a SorlCloud problem may try to compare the logs on two machines, or make those logs available to people on the user list for help.\n\nThis is probably another in the long list of things I'd like to do, can't get anyone else interested in, and may prove too difficult for me to figure out. ",
            "id": "comment-14698478"
        },
        {
            "date": "2015-08-16T08:53:12+0000",
            "author": "Varun Thacker",
            "content": "\n      boolean isFullCopyNeeded = IndexDeletionPolicyWrapper\n          .getCommitTimestamp(commit) >= latestVersion\n          || commit.getGeneration() >= latestGeneration || forceReplication;\n\n\n\nI think we can just change it to: commit.getGeneration() >= latestGeneration || forceReplication . \nComparing timestamps would have been valid for master slave when the actual index was rsynced and hence the commit timestamp would have been the same. And the new check would not break master slave either since the commit generation would be the same as well.\n\n\n\nif (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {\n        //master and slave are already in sync just return\n        LOG.info(\"Slave in sync with master.\");\n        successfulInstall = true;\n        return true;\n      }\n\n\n\nThis should also check with the generation numbers I guess. This check is only required in the master slave architecture. In cloud mode we would never call IndexFetcher unless we wanted to replicate.  ",
            "id": "comment-14698590"
        },
        {
            "date": "2015-08-16T13:11:27+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Thanks Varun Thacker, that's what I would have thought, good to have confirmation. Also good point regarding all this being applicable only to master-slave replication.\n\nShawn Heisey, I can see why detecting this would be good for log analysis, could you raise a separate ticket for this? I would like to keep this to solve the replication issue. ",
            "id": "comment-14698653"
        },
        {
            "date": "2015-08-16T13:14:02+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Attached is a patch to remove the use of versions. This however fails TestReplicationHandler tests, need to dig into that further.. ",
            "id": "comment-14698657"
        },
        {
            "date": "2015-08-16T14:18:42+0000",
            "author": "Yonik Seeley",
            "content": "There's a ton of history behind the replication stuff - and it's had to change over time due to changes in Lucene's versions / generations.\n\nIt appears as if we are checking wall times across machines to check if we are in sync,\n\nAt least in traditional master-slave replication, that's not the case... the timestamps being compared are all generated on the master (even if they are compared on the slaves?)\n\nAnyway, using index generation is certainly not enough since one can blow away an index and re-create it, reseting the generation to a low number. ",
            "id": "comment-14698683"
        },
        {
            "date": "2015-08-19T09:38:37+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Thanks for the comments Yonik Seeley. With master-slave replication, yes, this is less of a problem (you still have to deal with clock skew though).\n\nThere are two places where the index time is used..\n\n\n\tTo compare if they are equal to skip replication. Unless I am mistaken, the timestamp check is not useful to detect index re-creation in this case.\n\tTo check if full index replication should be forced. I see the use here (though I don't see an easy way you can do this in a cloud without stopping the full cloud, blowing an index on one but not all replicas, and making sure it comes up first)\n\n\n\nI am more concerned really about the first case, as you can lose data if you are unlucky. Do you agree that the timestamp check can be removed there?\n\nFor the second, probably the index creation time is a better thing to check against rather than the last commit time, as it is less subject to skew? I don't know if Lucene even provides a way to know when the index was initially created though. And this could be tackled as a different issue.. ",
            "id": "comment-14702773"
        },
        {
            "date": "2015-08-19T12:02:16+0000",
            "author": "Mark Miller",
            "content": "you still have to deal with clock skew though\n\nWhy? Don't both times come from the master? \n\nDo you agree that the timestamp check can be removed there?\n\nI don't think it can just be removed in either case without better replacement logic. ",
            "id": "comment-14702911"
        },
        {
            "date": "2015-08-19T12:17:43+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Why? Don't both times come from the master?\n\nA clock skew could cause two different commits to have the same time (commit 1 happens at time X, NTP sets the clock back by 200ms. 200ms later, commit 2 happens). It's not exactly what's in this title (i.e. relying on timestamps across machines), and you have to be a lot more unlucky, but you can't rely on wall time even in the same machine.\n\nI don't think it can just be removed in either case without better replacement logic.\n\nHow does the timestamp help currently in the first case? We are anyway using generations immediately following, so won't you be better off comparing generations instead to check if replication can be skipped? ",
            "id": "comment-14702927"
        },
        {
            "date": "2015-08-19T14:10:02+0000",
            "author": "Varun Thacker",
            "content": "If I understand it correctly we don't need the timestamp check in a master-slave setup. The reason being since the index on the slave is coming from the master both timestamp and generation will be the same. So just checking generation will be enough right?\n\nIn cloud mode, commits on different replicas happen at different times so the timestamps would always be different. But this code path with only get invoked during a recovery. So we could remove it for this use case as well right?  ",
            "id": "comment-14703067"
        },
        {
            "date": "2015-08-20T02:29:21+0000",
            "author": "Yonik Seeley",
            "content": "If I understand it correctly we don't need the timestamp check in a master-slave setup. The reason being since the index on the slave is coming from the master both timestamp and generation will be the same. So just checking generation will be enough right?\n\nSomeone can always switch masters, or blow away the index and rebuild from scratch, etc. ",
            "id": "comment-14704185"
        },
        {
            "date": "2015-08-23T18:47:07+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Okay, attached here is a smaller patch which was my biggest concern here with the timestamps to begin with. These two places use timestamps instead of generations, and can lead to data loss. The other place where both are used, worst case you will end up doing a full-copy, not a big deal either way (and I understand the case with master-slave in such a situation).\n\nFirst change:\n\n\n      if (latestVersion == 0L) {\n        if (forceReplication && commit.getGeneration() != 0) {\n          // since we won't get the files for an empty index,\n          // we just clear ours and commit\n          RefCounted<IndexWriter> iw = solrCore.getUpdateHandler().getSolrCoreState().getIndexWriter(solrCore);\n          try {\n            iw.get().deleteAll();\n          } finally {\n            iw.decref();\n          }\n          SolrQueryRequest req = new LocalSolrQueryRequest(solrCore, new ModifiableSolrParams());\n          solrCore.getUpdateHandler().commit(new CommitUpdateCommand(req, false));\n        }\n\n        //there is nothing to be replicated\n        successfulInstall = true;\n        return true;\n      }\n\n\n\nProposing replacing latestVersion with latestGeneration. Is there a case where one would be 0 and the other not?\n\nSecond change:\n\n\n      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {\n        //master and slave are already in sync just return\n        LOG.info(\"Slave in sync with master.\");\n        successfulInstall = true;\n        return true;\n      }\n\n\n\nI am proposing:\n\n\n-      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {\n+      if (!forceReplication && commit.getGeneration() == latestGeneration) {\n\n\n\n\n\tIf generations are not the same, but timestamps are, no harm done really, yes, you will go through replication unnecessarily, but that's it.\n\tWill master-slave result in the same situation? If you blow away the index, I would expect the generations to not match (would be much lesser). Alternatively, we could OR the two checks here as well (i.e. check both versions and generations here), to be absolutely sure we don't skip replication if we shouldn't.\n\n\n\nYonik Seeley, Mark Miller, could you clarify how this change would affect this adversely in such a case? ",
            "id": "comment-14708476"
        },
        {
            "date": "2015-09-14T19:59:53+0000",
            "author": "Mark Miller",
            "content": "I think when in SolrCloud mode perhaps it's just best to always replicate and count on peer sync as the short circuit. If replication is really not needed, most of the work will be skipped properly via filenames and checksums anyway, rather than this sloppy way that may miss a replication in rare cases. ",
            "id": "comment-14744143"
        }
    ]
}