{
    "id": "SOLR-1475",
    "title": "Java-based replication doesn't properly reserve its commit point during backups",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "replication (java)"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The issue title reflects Mark Miller's initial diagnosis of the problem.\n\nHere are my symptoms:\n\nThis is regarding the backup feature of replication, as opposed to replication. Backups seem to work fine on toy indexes. When trying backups out on a copy of my production index (300GB-ish), though, I'm getting FileNotFoundExceptions. These cancel the backup, and delete the snapshot.yyyymmdd* directory. It seems reproducible, in that every time I try to make a backup of my large index it will fail the same way.\n\nThis is Solr r815830. I'm not sure if this is something that would potentially be addressed by SOLR-1458? (That patch is from after r815830.)\n\nFor now I'm not using any event-based backup triggers; instead I'm manually hitting\n\nhttp://master_host:port/solr/replication?command=backup\n\nThis successfully sets off a snapshot, as seen in a thread dump.  However, after a while the snapshot fails. I'll paste in a couple of stack traces below.\n\nI haven't seen any other evidence that my index is corrupt; in particular, searching the index and Java-based replication seem to be working fine, and the Lucene CheckIndex tool did not report any problems with the index.\n\n********************\n\n\nSep 28, 2009 9:32:18 AM org.apache.solr.handler.SnapShooter createSnapshot\nSEVERE: Exception while creating snapshot\njava.io.FileNotFoundException: Source\n'E:\\tomcat\\solrstuff\\solr\\filingcore\\data\\index\\_y0w.fnm' does not\nexist\n       at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:637)\n       at org.apache.commons.io.FileUtils.copyFileToDirectory(FileUtils.java:587)\n       at org.apache.solr.handler.SnapShooter.createSnapshot(SnapShooter.java:83)\n       at org.apache.solr.handler.SnapShooter$1.run(SnapShooter.java:61)\n\nSep 28, 2009 10:39:43 AM org.apache.solr.handler.SnapShooter createSnapshot\nSEVERE: Exception while creating snapshot\njava.io.FileNotFoundException: Source\n'E:\\tomcat\\solrstuff\\solr\\filingcore\\data\\index\\segments_by' does not\nexist\n       at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:637)\n       at org.apache.commons.io.FileUtils.copyFileToDirectory(FileUtils.java:587)\n       at org.apache.solr.handler.SnapShooter.createSnapshot(SnapShooter.java:83)\n       at org.apache.solr.handler.SnapShooter$1.run(SnapShooter.java:61)\n\n\nSep 28, 2009 11:52:08 AM org.apache.solr.handler.SnapShooter createSnapshot\nSEVERE: Exception while creating snapshot\njava.io.FileNotFoundException: Source\n'E:\\tomcat\\solrstuff\\solr\\filingcore\\data\\index\\_yby.nrm' does not\nexist\n       at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:637)\n       at org.apache.commons.io.FileUtils.copyFileToDirectory(FileUtils.java:587)\n       at org.apache.solr.handler.SnapShooter.createSnapshot(SnapShooter.java:83)\n       at org.apache.solr.handler.SnapShooter$1.run(SnapShooter.java:61)",
    "attachments": {
        "SOLR-1475.patch": "https://issues.apache.org/jira/secure/attachment/12420839/SOLR-1475.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-12760736",
            "date": "2009-09-29T22:01:32+0000",
            "content": "So I don't really know the java replication code at all, but here is a patch I was playing with.\n\nIts uncommitable.\n\nIt has a test that was only useful for me to check that things were broke and that were fixed, but it doesn't fail in either case.\nIt has a tiny bit of test code to cause a pause between copying files so i could make it take long enough without using a huge index.\n\nits written by someone that doesn't know the java replication code at all.\n\nbut i think its generally how the fix should/would work and may be useful to someone else.\n\nedit\n\nOh yeah: there is also some unpolished ugliness even with the code thats not throw away test code. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12760747",
            "date": "2009-09-29T22:23:21+0000",
            "content": "Here are the various ways I've thought of doing it:\n1. write your own copy that interleaves short term reservations\n2. asynchronously (in a separate thread) make short term reservations while a different thread is actually doing copyFiles()\n3. do a super-long reservation before invoking copyFiles() and then set it back to a short reservation after done (the problem being that if someone else made a super long reservation, it would be lost).\n\nOn a quick peek, it looks like you opted for #1\n\nOne thought on efficiency is to try and use FileChannel.transferTo() to potentially skip the user-space copy altogether.  We'd probably want to use relatively large block sizes too - 4 to 8MB perhaps? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12760755",
            "date": "2009-09-29T22:40:17+0000",
            "content": "On a quick peek, it looks like you opted for #1\n\nYeah - originally, I was thinking about how it should be done, but when I saw that it was done somewhat this way for transferring to slaves, I just went with it. Since its local though, perhaps another method is better.\n\nOne thought on efficiency is to try and use FileChannel.transferTo() to potentially skip the user-space copy altogether\n\nConsidered that too - a few years ago I measured it as much faster - but I also thought I remembered that stream based copying was faster on large files with Windows. So I just went safe and went with the method it was currently using in this patch.\n\nWe'd probably want to use relatively large block sizes too - 4 to 8MB perhaps?\n\nAgreed - I picked 4 times the buffer size out of a hat. Really should be based on whats reasonable to transfer within about 10 seconds with a good margin of safety. 4-8 sounds pretty good to me. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12761641",
            "date": "2009-10-02T16:51:42+0000",
            "content": "Here is a version that is much closer to commitable.\n\nedit\n\nCouple issues with the test though - \n\n1. in the test, ive still got the teardown commented out - needs to be put back.\n\n2. The wait loop just waits for the snapshot dir to show up - not necessarily the full copy to be done - just happens to finish fast enough on my machine anyway\n\n3. Test doesnt test that the reserve works right - I couldn't find a good clean way to do that without the pause stuff I introduced in the last patch. Tested it works right with that though. This test just tests that the backup is made and is a searchable index with all of the docs. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12761976",
            "date": "2009-10-03T23:39:20+0000",
            "content": "New patch coming soon -\n\ncleans up some of what I mention and fixes what appears to be a little bug - you can't get stats about the backup details unless tempSnapPuller != null - so if you don't do a fetch first, and just ask for a backup, that means you can't get the details on your backup's success and whatnot. So I have moved that out of the if block that prevents it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12762028",
            "date": "2009-10-04T15:33:40+0000",
            "content": "Okay, I think this patch is looking pretty good. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12762031",
            "date": "2009-10-04T16:28:53+0000",
            "content": "Wasnt sure how to add Changes for this - was going to incorporate into the orig issue with: additional bug fixes by ... since replication hasn't been released and I've been pro Changes reading correctly from release to release ...\n\nBut since others have already added new entries for fixes with replication anyway, Ill just add it as  a new bug fix entry. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12762412",
            "date": "2009-10-05T22:13:13+0000",
            "content": "I'll give a bit for someone to review if they want - but should prob commit this soon. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12762513",
            "date": "2009-10-06T05:19:13+0000",
            "content": "hi Mark .  guess we can use the reserve/release methods in the latest patch of SOLR-1458 for this too "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12762640",
            "date": "2009-10-06T14:25:31+0000",
            "content": "Makes sense to me. Should be a more efficient copy as we can do a Channel transfer of the whole file rather than in chunks.\n\nThought about adding that to begin with, but wasn't up to speed on all of this, and thought perhaps the reservations were sticky - so that if something went wrong, when Solr booted back up, it would see the reservation was over and still remove the commit point, whereas, if you reserved it forever, maybe it wouldn't. Obviously not the case though. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763483",
            "date": "2009-10-08T13:01:53+0000",
            "content": "Hmm - while Channel transfer is generally a win to equal with smaller files (ie < a gig) if you use a good sized buffer (about 32MB looking like the sweet spot), streams is looking much better on multi gig files. I think the best method is likely one that chooses one or the other based on the file size. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12763503",
            "date": "2009-10-08T14:27:53+0000",
            "content": "streams is looking much better on multi gig files\n\nGrrr... sun messed this up too?\nRelated: LUCENE-1121 HADOOP-3164\n\nDo you see a CPU consumption difference when using transferTo? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763518",
            "date": "2009-10-08T15:17:22+0000",
            "content": "transfer does use less cpu -\n\nit appears to work like this:\n\nif you just load up the JVM and transfer the 2 gig test file I am using, transfer is actually a bit faster to roughly similar. Generally a bit faster.\n\nbut\n\nif you do a bunch of stream copying first - and then use transfer - its a dog - the more copying first, the worse it appears to be.\n\nBut stream copying does not appear to degrade like that ...\n\nI'm using a 1 gig heap for these tests. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763523",
            "date": "2009-10-08T15:28:24+0000",
            "content": "I see the issue - I read somewhere that you don't want/need to chunk the transfer - but you certainly do. That mostly takes care of it. It still appears a bit slower after a lot of IO, but not really enough to matter and probably worth the lower cpu.\n\nThe windows server results are still scary though - perhaps we should only use transfer on when detecting non windows? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12763525",
            "date": "2009-10-08T15:34:14+0000",
            "content": "interesting... I guess we should be safe and just use the old stream copying for now.\n\nJust out of curiosity though:\nif you do a bunch of stream copying first - and then use transfer\n\ntransferTo to a different file, not the same one, right?\n\n- its a dog - the more copying first, the worse it appears to be.\n\nhmmm, so transferTo causes the write buffers to be flushed or something? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763528",
            "date": "2009-10-08T15:38:02+0000",
            "content": "transferTo to a different file, not the same one, right?\n\nYes - different files first with the stream, then Channel on the 2 gig large file - but it turned out to be because I was transfering the whole thing in one go (some bad advice I read). Chunking by 10 MB removes the issue.\n\nhmmm, so transferTo causes the write buffers to be flushed or something?\n\nCertainly does something you don't want to do  Takes a while for my system to recover after that as well. But like I said, chunking appears to solve. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12763541",
            "date": "2009-10-08T16:00:49+0000",
            "content": "Hmmm, maybe it's trying to set up a DMA for the complete thing or something, and the unflushed write buffers are in the way (and hence need to be flushed). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763571",
            "date": "2009-10-08T17:15:06+0000",
            "content": "Grrr - NM - at first blush it appears that chunking kills the cpu win - makes sense - before it uses cpu at the start, then stops using - I guess now there are lots of starts. Not seeing a win at all anymore. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12763999",
            "date": "2009-10-09T12:54:48+0000",
            "content": "Back to streams copying since I can't seem to win with NIO (thats a familiar story).\n\nI know Noble wanted to switch this to using set/release commit point, but since that patch is not committed yet, its not in here.\n\nI'm vaca today and mon, and I'm not sure how much I'll be coding (headed up to Burlington VT later today), so if someone wants to take this over, that would be great.\n\nOther than the possible switch to the different reserve method, I think it just needs a good review. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12764021",
            "date": "2009-10-09T13:42:02+0000",
            "content": "I'll take it - have a good vacation! "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12764195",
            "date": "2009-10-09T20:53:12+0000",
            "content": "OK, here's my update to Mark's patch... pretty much just using saveCommitPoint instead of doing short duration reserves.  I plan on committing shortly. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12764219",
            "date": "2009-10-09T21:52:52+0000",
            "content": "committed. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775896",
            "date": "2009-11-10T15:52:17+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}