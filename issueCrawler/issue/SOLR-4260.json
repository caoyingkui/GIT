{
    "id": "SOLR-4260",
    "title": "Inconsistent numDocs between leader and replica",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.6.1",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "After wiping all cores and reindexing some 3.3 million docs from Nutch using CloudSolrServer we see inconsistencies between the leader and replica for some shards.\n\nEach core hold about 3.3k documents. For some reason 5 out of 10 shards have a small deviation in then number of documents. The leader and slave deviate for roughly 10-20 documents, not more.\n\nResults hopping ranks in the result set for identical queries got my attention, there were small IDF differences for exactly the same record causing a record to shift positions in the result set. During those tests no records were indexed. Consecutive catch all queries also return different number of numDocs.\n\nWe're running a 10 node test cluster with 10 shards and a replication factor of two and frequently reindex using a fresh build from trunk. I've not seen this issue for quite some time until a few days ago.",
    "attachments": {
        "SOLR-4260.patch": "https://issues.apache.org/jira/secure/attachment/12623488/SOLR-4260.patch",
        "clusterstate.png": "https://issues.apache.org/jira/secure/attachment/12612153/clusterstate.png",
        "192.168.20.104-replica2.png": "https://issues.apache.org/jira/secure/attachment/12612154/192.168.20.104-replica2.png",
        "demo_shard1_replicas_out_of_sync.tgz": "https://issues.apache.org/jira/secure/attachment/12622006/demo_shard1_replicas_out_of_sync.tgz",
        "192.168.20.102-replica1.png": "https://issues.apache.org/jira/secure/attachment/12612155/192.168.20.102-replica1.png"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13544077",
            "date": "2013-01-04T17:59:08+0000",
            "content": "For some reason 5 out of 10 shards have a small deviation in then number of documents. \n\nnumDocs or maxDoc?\nYou can expect variations in maxDoc (due to the same document being added more than once in recovery scenarios).  numDocs should be identical of course.\n\nOh, and that variation in maxDoc will cause small differences in IDF too. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13544078",
            "date": "2013-01-04T17:59:18+0000",
            "content": "What's fresh? We fixed a couple issue for this within the last couple weeks. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13544079",
            "date": "2013-01-04T18:00:17+0000",
            "content": "Looks like fresh means from today? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13544081",
            "date": "2013-01-04T18:01:59+0000",
            "content": "Yonik, i explicitly look at numDocs. Shard_B has Num Docs: 335986, Max Doc: 336079 on one node and Num Docs: 335976 Max Doc: 336091 on the other. \n\nMark: yes, this is today, 5.0.0.2013.01.04.15.31.51 "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13546000",
            "date": "2013-01-07T16:15:49+0000",
            "content": "I tried to reproduce this twice with today's check out of trunk manner but failed to consistently reproduce it. I did see a small deviation while no data was coming in and no recovery was reported to be in progress, in the end all replicates where in sync and had, accoring to Luke, identical numDocs.\n\nI'll keep an eye on this in the next couple of days.\n\nbtw: in this test cluster we use docCount and not maxDoc for our IDF calculation. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546027",
            "date": "2013-01-07T16:49:06+0000",
            "content": "Thanks - would be good to make sure this is okay. FYI, seeing things off for a short time is somewhat expected - there is some 'eventual' consistency here - but it should quickly consistent itself up - under heavy bulk indexing is when you would be most likely to have the most \"eventualness\". "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13546196",
            "date": "2013-01-07T19:41:55+0000",
            "content": "Yes, we see that behaviour all the time but it is expected indeed. The problem with with this issue is that it was not a dozen seconds or a few minutes but it was consistently wrong for almost an hour, then i wrote this issue and left the office  "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13555980",
            "date": "2013-01-17T08:31:28+0000",
            "content": "I've got it again. This time numDocs is consistent but some facet counts are not consistent between leader and replica. Here are two facet counts for one node:\n\n\n   <lst name=\"domain\">\n      <int name=\"domain_a\">238620</int>\n      <int name=\"domain_b\">218</int>\n\n\n\nand the other:\n\n\n   <lst name=\"domain\">\n      <int name=\"domain_a\">238621</int>\n      <int name=\"domain_b\">217</int>\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13556261",
            "date": "2013-01-17T15:16:34+0000",
            "content": "Not sure I'd say it's 'it' again yet - numdocs and facet counts are quite different.\n\nInteresting though - does that persist after a hard commit / open new searcher? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13556315",
            "date": "2013-01-17T15:37:35+0000",
            "content": "You're right, it is different, my bad. A hard commit is issues automatically once in a while and the issue persists and after a manual hard commit as well.\n\nInterestingly, we see that the docCounts returned by CollectionStatistics.docCount() is inconsistent between leader and replica for each shard. As Yonik said, it's normal when using maxDoc but we don't use maxDoc in this set up, docCount should be correct. Since it isn't, our IDF is sometimes skewed, causing docs to jump position in the result set.\n "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13557211",
            "date": "2013-01-18T13:53:12+0000",
            "content": "I've removed domain_b from the index and as i expected the numDocs is now inconsistent indeed. By coincidence the what was missing in one replica from domain_a was replaced by an extra doc from domain_b and vice versa.\n\nThe collection of a couple of million records has one replica that's missing one document. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13558794",
            "date": "2013-01-21T14:30:45+0000",
            "content": "Here's the debug output of the same query executed on both the leader and the replica. This set up uses and overridden BM25Similarity that returns docCount() for IDF instead of maxDoc. In this case both have the equal number of documents in the index so something else doesn't seem right. Facet counts add up, both leader and replicaa have the same number of documents per domain.\n\n\n43.960983 = (MATCH) sum of:\n  43.960983 = (MATCH) max plus 0.35 times others of:\n    29.059849 = (MATCH) weight(title_nl:amsterdam^6.4 in 14437) [], result of:\n      29.059849 = score(doc=14437,freq=3.0 = termFreq=3.0\n), product of:\n        6.4 = boost\n        2.889473 = idf(docFreq=18368, docCount=330335)\n        1.5714288 = tfNorm, computed from:\n          3.0 = termFreq=3.0\n          1.2 = parameter k1\n          0.0 = parameter b (norms omitted for field)\n    4.651832 = (MATCH) weight(content_nl:amsterdam^1.6 in 14437) [], result of:\n      4.651832 = score(doc=14437,freq=1.0 = termFreq=1.0\n), product of:\n        1.6 = boost\n        2.9073951 = idf(docFreq=18039, docCount=330285)\n        1.0 = tfNorm, computed from:\n          1.0 = termFreq=1.0\n          1.2 = parameter k1\n          0.0 = parameter b (norms omitted for field)\n    32.161896 = (MATCH) weight(url:amsterdam^3.64 in 14437) [], result of:\n      32.161896 = score(doc=14437,freq=2.0 = termFreq=2.0\n), product of:\n        3.64 = boost\n        6.328843 = idf(docFreq=608, docCount=341068)\n        1.396098 = tfNorm, computed from:\n          2.0 = termFreq=2.0\n          1.2 = parameter k1\n          0.75 = parameter b\n          4.227131 = avgFieldLength\n          4.0 = fieldLength\n\n\n\n\n45.993042 = (MATCH) sum of:\n  45.993042 = (MATCH) max plus 0.35 times others of:\n    28.35725 = (MATCH) weight(title_nl:amsterdam^6.4 in 170479) [], result of:\n      28.35725 = score(doc=170479,freq=3.0 = termFreq=3.0\n), product of:\n        6.4 = boost\n        2.8196125 = idf(docFreq=16736, docCount=280676)\n        1.5714288 = tfNorm, computed from:\n          3.0 = termFreq=3.0\n          1.2 = parameter k1\n          0.0 = parameter b (norms omitted for field)\n    4.577688 = (MATCH) weight(content_nl:amsterdam^1.6 in 170479) [], result of:\n      4.577688 = score(doc=170479,freq=1.0 = termFreq=1.0\n), product of:\n        1.6 = boost\n        2.8610551 = idf(docFreq=16054, docCount=280631)\n        1.0 = tfNorm, computed from:\n          1.0 = termFreq=1.0\n          1.2 = parameter k1\n          0.0 = parameter b (norms omitted for field)\n    34.465813 = (MATCH) weight(url:amsterdam^3.64 in 170479) [], result of:\n      34.465813 = score(doc=170479,freq=2.0 = termFreq=2.0\n), product of:\n        3.64 = boost\n        6.798851 = idf(docFreq=323, docCount=290119)\n        1.3926809 = tfNorm, computed from:\n          2.0 = termFreq=2.0\n          1.2 = parameter k1\n          0.75 = parameter b\n          4.189095 = avgFieldLength\n          4.0 = fieldLength\n\n\n\nIt's clear that not only docCount is different but also docFreq while both should be equal on the leader and replica. This makes a mess of the final score!\n\nAnyone else here that has seen this issue? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13558910",
            "date": "2013-01-21T17:03:02+0000",
            "content": "Ok, this is not a SolrCloud issue, i can also reproduce this in stand-alone and multi core set ups. This is also not a problem of BM25 since TFIDF has the same problem. Neither docCount vs. maxCount seems to be the problem.\n\nI now have two identical cores set up and index the same data to both, no problem, everything is very consistent. Then i'll reindex the same data again to only one of the two cores and then the trouble starts. There is a small variation in maxDoc which is expected but there is also a variation in docFreq which is very unexpected, docFreq must not change at all if i reindex the same data.\n\nHere's an debug snippet of the first core that did not receive reindexed data:\n\n\n910.47974 = (MATCH) sum of:\n  910.47974 = (MATCH) max plus 0.35 times others of:\n    793.99835 = (MATCH) weight(title_en:groningen^6.4 in 5132) [], result of:\n      793.99835 = score(doc=5132,freq=1.0 = termFreq=1.0\n), product of:\n        71.28527 = queryWeight, product of:\n          6.4 = boost\n          11.138323 = idf(docFreq=1, maxDocs=50588)\n          1.0 = queryNorm\n        11.138323 = fieldWeight in 5132, product of:\n          1.0 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          11.138323 = idf(docFreq=1, maxDocs=50588)\n          1.0 = fieldNorm(doc=5132)\n    312.06528 = (MATCH) weight(content_en:groningen^1.6 in 5132) [], result of:\n      312.06528 = score(doc=5132,freq=1.0 = termFreq=1.0\n), product of:\n        17.172573 = queryWeight, product of:\n          1.6 = boost\n          10.732858 = idf(docFreq=2, maxDocs=50588)\n          1.0 = queryNorm\n        18.172308 = fieldWeight in 5132, product of:\n          1.6931472 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          10.732858 = idf(docFreq=2, maxDocs=50588)\n          1.0 = fieldNorm(doc=5132)\n    20.73867 = (MATCH) weight(domain_grams:groningen^3.7 in 5132) [], result of:\n      20.73867 = score(doc=5132,freq=1.0 = termFreq=1.0\n), product of:\n        26.48697 = queryWeight, product of:\n          3.7 = boost\n          7.158641 = idf(docFreq=106, maxDocs=50588)\n          1.0 = queryNorm\n        0.7829763 = fieldWeight in 5132, product of:\n          1.0 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          7.158641 = idf(docFreq=106, maxDocs=50588)\n          0.109375 = fieldNorm(doc=5132)\n\n\n\nHere's the debug of the same doc on the core which i reindexed the same data to:\n\n\n928.31537 = (MATCH) sum of:\n  928.31537 = (MATCH) max plus 0.35 times others of:\n    815.29694 = (MATCH) weight(title_en:groningen^6.4 in 31881) [], result of:\n      815.29694 = score(doc=31881,freq=1.0 = termFreq=1.0\n), product of:\n        72.23504 = queryWeight, product of:\n          6.4 = boost\n          11.286724 = idf(docFreq=1, maxDocs=58681)\n          1.0 = queryNorm\n        11.286724 = fieldWeight in 31881, product of:\n          1.0 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          11.286724 = idf(docFreq=1, maxDocs=58681)\n          1.0 = fieldNorm(doc=31881)\n    304.0185 = (MATCH) weight(content_en:groningen^1.6 in 31881) [], result of:\n      304.0185 = score(doc=31881,freq=1.0 = termFreq=1.0\n), product of:\n        16.949724 = queryWeight, product of:\n          1.6 = boost\n          10.593577 = idf(docFreq=3, maxDocs=58681)\n          1.0 = queryNorm\n        17.936485 = fieldWeight in 31881, product of:\n          1.6931472 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          10.593577 = idf(docFreq=3, maxDocs=58681)\n          1.0 = fieldNorm(doc=31881)\n    18.891369 = (MATCH) weight(domain_grams:groningen^3.7 in 31881) [], result of:\n      18.891369 = score(doc=31881,freq=1.0 = termFreq=1.0\n), product of:\n        25.279795 = queryWeight, product of:\n          3.7 = boost\n          6.832377 = idf(docFreq=171, maxDocs=58681)\n          1.0 = queryNorm\n        0.7472912 = fieldWeight in 31881, product of:\n          1.0 = tf(freq=1.0), with freq of:\n            1.0 = termFreq=1.0\n          6.832377 = idf(docFreq=171, maxDocs=58681)\n          0.109375 = fieldNorm(doc=31881)\n\n\n\nAs you can see, docFreq has changed but the number of documents is still the same. Since i now suspect the merging of segments has something to do with it i'll send an optimize command to the node that i reindexed data to.\n\nAfter optimizing (or forcing all segments to be merged) i get the same debug as i had for the first node that i didn't reindex to!\n\n "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13558920",
            "date": "2013-01-21T17:12:16+0000",
            "content": "I updated the title to reflect the new issue. Can anyone confirm that with trunk they see different values for CollectionStatistics.docCount and docFreq?\n\nI can confirm that the variations disappear after optimizing one of our SolrCloud clusters. The only time results swap places is when the score is identical and docID is used to score. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13558929",
            "date": "2013-01-21T17:28:39+0000",
            "content": "There is a small variation in maxDoc which is expected but there is also a variation in docFreq which is very unexpected, docFreq must not change at all if i reindex the same data.\n\nUnfortunately, deletions don't change index statistics like docFreq (this has been the case since the first version of Lucene).  This means that reindexing a document can artificially increase the docFreq until the deletion is really removed via merging/optimize. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13558999",
            "date": "2013-01-21T19:14:55+0000",
            "content": "Of course, you're right! I got distracted by that fact and in the process renamed this issue while i shouldn't. The inconsistency between leader and replica on one shard is still here.\n\nI'll rename it back and raise the docFreq and docCount issue on the list. Sorry for the mess  "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13581189",
            "date": "2013-02-19T10:16:44+0000",
            "content": "Here's the index information for two cores of the same shard, running on different nodes.\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n\n<lst name=\"responseHeader\">\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">1</int>\n</lst>\n<lst name=\"index\">\n  <int name=\"numDocs\">117744</int>\n  <int name=\"maxDoc\">118160</int>\n  <int name=\"deletedDocs\">416</int>\n  <long name=\"version\">3802</long>\n  <int name=\"segmentCount\">15</int>\n  <bool name=\"current\">true</bool>\n  <bool name=\"hasDeletions\">true</bool>\n  <str name=\"directory\">org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.lucene.store.MMapDirectory@/opt/solr/cores/shard_h/data/index.20130211094737738 lockFactory=org.apache.lucene.store.NativeFSLockFactory@2ca7563d; maxCacheMB=48.0 maxMergeSizeMB=4.0)</str>\n  <lst name=\"userData\">\n    <str name=\"commitTimeMSec\">1361265544970</str>\n  </lst>\n  <date name=\"lastModified\">2013-02-19T09:19:04.97Z</date>\n</lst>\n</response>\n\n\n\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n\n<lst name=\"responseHeader\">\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">0</int>\n</lst>\n<lst name=\"index\">\n  <int name=\"numDocs\">117767</int>\n  <int name=\"maxDoc\">118181</int>\n  <int name=\"deletedDocs\">414</int>\n  <long name=\"version\">3772</long>\n  <int name=\"segmentCount\">13</int>\n  <bool name=\"current\">true</bool>\n  <bool name=\"hasDeletions\">true</bool>\n  <str name=\"directory\">org.apache.lucene.store.NRTCachingDirectory:NRTCachingDirectory(org.apache.lucene.store.MMapDirectory@/opt/solr/cores/shard_h/data/index.20130211105622621 lockFactory=org.apache.lucene.store.NativeFSLockFactory@684b4388; maxCacheMB=48.0 maxMergeSizeMB=4.0)</str>\n  <lst name=\"userData\">\n    <str name=\"commitTimeMSec\">1361265544937</str>\n  </lst>\n  <date name=\"lastModified\">2013-02-19T09:19:04.937Z</date>\n</lst>\n</response>\n\n\n\nWe send updates/deletes to the cluster every 10-15 minutes. The shard will not become synchronized, unless i remove the index of one of the nodes. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13581942",
            "date": "2013-02-20T04:46:17+0000",
            "content": "No interesting exceptions in the logs? Perhaps dial them up to warn and run? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13582030",
            "date": "2013-02-20T08:30:32+0000",
            "content": "Nothing peculiar in the WARN logs. We don't log INFO usually unless something is really broken, that's too much data. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13712297",
            "date": "2013-07-18T13:11:12+0000",
            "content": "FYI: we're still seeing major inconsistencies, facet counts are off and when inspecting leaders and replica's we notice not all are in sync. This is on yesterday's trunk and with an empty index. There were no node failures during indexing. Shard_b's stats for example:\n\nnode 2 shard b\n\nLast Modified:    about a minute ago\nNum Docs:    158964\nMax Doc:    158964\nDeleted Docs:    0\nVersion:    4479\nSegment Count:    1\n\n\n\nnode 3 shard b\n\nLast Modified:    2 minutes ago\nNum Docs:    158298\nMax Doc:    158298\nDeleted Docs:    0\nVersion:    2886\n\n\n\nSize and versions are also different. Cluster is optimized/forceMerged but doesn't change the facts as expected. At least one other shard also has differences in its two replica's, i haven't manually checked the others. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13712300",
            "date": "2013-07-18T13:17:28+0000",
            "content": "See anything in the logs about zk expirations? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13712314",
            "date": "2013-07-18T13:21:01+0000",
            "content": "I've already restarted the job and enabled logging! It's going to take a while  "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13712400",
            "date": "2013-07-18T15:12:07+0000",
            "content": "Alright, nothing looks like zookeeper expirations i grepped expirations in the error log but there's nothing there. This indexing session did not produce so many inconsistencies as the previous one; there is only 1 shard of which one replica has 2 more documents. It won't fix itself.\n\nDuring indexing there were, as usual, error such as autocommit causing a searcher too many and time outs talking to other nodes.\n\nOnly 2 nodes report a Stopping Recovery For of which one node actually has a replica of the inconsistent core. The other shard is seems fine, both replica's have the same numDocs. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13813827",
            "date": "2013-11-05T10:31:33+0000",
            "content": "Hi, I hit this bug with solr 4.5.1\n\nreplica 1:\n\nlastModified:20 minutes ago\nversion:80616\nnumDocs:6072661\nmaxDoc:6072841\ndeletedDocs:180\n\nreplica 2 (leader)\n\nlastModified:20 minutes ago\nversion:77595\nnumDocs:6072575\nmaxDoc:6072771\ndeletedDocs:196\n\nI don't know when this happened, therefore I have no time frame to find in log valuable information on logs. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13813850",
            "date": "2013-11-05T11:11:17+0000",
            "content": "I attached some screenshots\n\nThe shard is the shard11:\n\n1 - clusterstate: this screenshot shows replica2 192.168.20.104 as the leader\n2 - the replica 2 has lower gen that replica1 and is the leader, is this correct? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13820205",
            "date": "2013-11-12T15:53:46+0000",
            "content": "I can confirm as well that is issue still exists. Since yesterday one of a shard's replica has one document less than it should have. Solr doesn't notice this and makes no attempt in recovering this issue. Around the time when i noticed it first we were shutting down and restarting nodes, it's likely that at that time some documents got indexed as well.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13821300",
            "date": "2013-11-13T13:15:29+0000",
            "content": "Yeah, I know that this can still happen - unfortunately, the debugging and testing required to make continued improvements requires a lot of time, so I don't personally know when I can work on hardening it. Currently, if shards eventually get out of whack, the best you can do is trigger a new recovery against the leader.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13821304",
            "date": "2013-11-13T13:21:58+0000",
            "content": "I have done a bit of work on making it easier to turn on some debug logging that I use for this type of thing recently. I also have some specific local Jenkins jobs I run on dedicated hardware to help track down problems - I have been collecting a lot of logs over the last few weeks. There is a lot more that needs to be done though. I'm hoping to start a wiki page on how I have gone about tracking this type of thing down in the past so that perhaps it's easier for others to get involved. Hopefully Yonik can add any of his useful tricks to that as well. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13821375",
            "date": "2013-11-13T14:26:54+0000",
            "content": " Currently, if shards eventually get out of whack, the best you can do is trigger a new recovery against the leader.\n\nWhat happen when the leader is the shard with less docs? Is the replication done in the right way? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13821421",
            "date": "2013-11-13T15:31:08+0000",
            "content": "What happen when the leader is the shard with less docs? Is the replication done in the right way?\n\nIt really depends on how careful you are trying to be and what you can count on - you don't know if the leader is behind if deletes are involved. The safest thing to do is to stop the cluster and start it again - that triggers a process that tries to pick the most up to date replica and trades up to 100 updates or so among each other if some are on some replicas and not others.\n\nIf you are sure the leader is simply behind, you can just bounce it and let a replica take over as leader. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13823195",
            "date": "2013-11-15T01:36:30+0000",
            "content": "We're seeing the same thing, running v4.5.0.\n\nMark, if you don't mind clarifying...\n\n\nThe safest thing to do is to stop the cluster and start it again - that triggers a process that tries to pick the most up to date replica and trades up to 100 updates or so among each other if some are on some replicas and not others.\nWhat if the difference is greater than 100? Is there any other way to figure out who is the \"truth\" and force that state onto the other replicas by doing a full sync?\n\n\nIf you are sure the leader is simply behind, you can just bounce it and let a replica take over as leader.\nNewbie question: Why would the leader be behind? Aren't all updates sent to the leader first and then the leader distribute it to the replicas? Also, I was under the impression that this update call is synchronous, so once an update request returns successfully to the client, why would any replica be behind?\n "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13823517",
            "date": "2013-11-15T09:54:00+0000",
            "content": "Jessica, \n\nIn some point of the process the leader can be downgraded to replica, the other replica whit less document will become the leader, in this case, the older leader (after the recovery) can be updated as usual and you get the leader behind the replica if the recovery doesn't fix the desviation. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13823652",
            "date": "2013-11-15T13:27:57+0000",
            "content": "What if the difference is greater than 100? Is there any other way to figure out who is the \"truth\" and force that state onto the other replicas by doing a full sync?\n\nThat is basically what should happen - everyone in the leader line will \"try\" and become the leader by trying to peer sync with everyone else - either they will be ahead of everyone else and the sync will succeed or they will be behind by less than 100 updates and trade and the sync will succeed. If the sync fails, the next guy in line tries. Eventually the most up to date guy should succeed and he forces everyone else to match him. That is the idea anyway.\n\nNewbie question: Why would the leader be behind? \n\nZooKeeper session timeouts (due to load, gc, whatever) can cause the leader to be bumped.\n\nYou mainly only expect this stuff to happen if nodes go down (and perhaps come back) or session expirations.\n\nUnfortunately, for a while between 4.4 and 4.5, a couple of our important tests stopped working and I think a couple problems were introduced. I hope to have more time to look into it soon. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13823834",
            "date": "2013-11-15T17:31:08+0000",
            "content": "Thanks Yago and Mark. I really appreciate you guys spending time to answer my questions--it's definitely helping me understand more.\n\nI understand how leader roles can change, but not how replicas can be behind--I thought the updates are synchronously distributed (i.e. solrj's request doesn't return successfully until an update has been distributed to all replicas). Is this not the case?\n\nIf any replica can fall behind, and it can be elected leader without having caught up, wouldn't we possibly end up in the following situation (where let's pretend docs are represented by monotonically increasing numbers):\n\nOld Leader: 1 2 3 4 5 (GC)\nNew Leader:  1 2 3 .. (elected) 6 7\n\nIn this case, who's considered the most up-to-date guy? Would they figure out among themselves that Old Leader is missing 6 7 but the New Leader is missing 4 5? If so, how do they do the right \"merge\" if the difference is greater than 100 and they have to resort to full sync? "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13823867",
            "date": "2013-11-15T18:00:07+0000",
            "content": "I thought the updates are synchronously distributed\n\nMy knowledge about how replication is done is very limited, for me replication is a distributed HTTP requests to all replicas, if all responses return the code 200, then the insertion was successful. I don't know if internally the 200 is returned when the document is written on tlog or in the open segment.\n\nUp-to-date in this case is none, you have your data compromised, you can't guarantee wich is the correct replica, the logic could be pick the replica with more docs and make a new replica using it, but still can know without check one by one if you have all data. An extreme case can be do a full reindex of the data (if you can). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13823871",
            "date": "2013-11-15T18:01:50+0000",
            "content": "I understand how leader roles can change, but not how replicas can be behind--I thought the updates are synchronously distributed (i.e. solrj's request doesn't return successfully until an update has been distributed to all replicas). Is this not the case?\n\nThey could get behind because of a bug generally \n "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13823887",
            "date": "2013-11-15T18:12:39+0000",
            "content": "Mark, \n\nI can confirm that I had session expirations in my logs in some point of time. My index rate is high and some times my boxes are under some \"pressure\".\n\nMy problem is that I don't know how deal with the situation. I'm using a non java client and I don't know how I can do debug or the tools that I can use to give some information to help debug this issue. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13823894",
            "date": "2013-11-15T18:16:33+0000",
            "content": "\nThey could get behind because of a bug generally \n\nI see, so as designed, the scenario I described shouldn't really happen, because New Leader wouldn't have been missing 4 5?\n\nBTW, for our case, turns out the version numbers across the replicas matched even though numDocs didn't. That seems to suggest that there\u2019s at least an issue somewhere in the replication flow where it\u2019s possible to update the version but not have the matching documents. Not sure if this is a useful piece of information to you but thought I'd mention. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13823905",
            "date": "2013-11-15T18:24:41+0000",
            "content": "I think of two failure scenarios:\n\n1. A replica goes down. Design solution: it comes back and uses peer sync or replication to catch up.\n\n2. A leader goes down. It might have been in the middle of sending updates and somehow a couple didn't make it to a replica. Design solution: that leader peer sync dance I talk about above.\n\nBasically, no one should ever be behind by more than 100 docs (a leader sends updates to replicas in parallel), and new leaders should always end up up to date. Obviously a bit more hardening to do though. Could also use more targeted testing - we count  a lot on the chaosmonkey tests for this (those are the tests that stopped working correctly for a while).\n\nA wrinkle is that zookeeper session timeouts also trigger the same thing as if the node had died - it comes back when the session is reestablished. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13823922",
            "date": "2013-11-15T18:40:11+0000",
            "content": "Basically, no one should ever be behind by more than 100 docs\n\nOf course if they are over 100 updates behind, they won't successfully sync and become leader, someone else will, and the behind node will be asked to catch up to the leader via replication. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13823959",
            "date": "2013-11-15T19:07:27+0000",
            "content": "My problem is that I don't know how deal with the situation. I'm using a non java client and I don't know how I can do debug or the tools that I can use to give some information to help debug this issue.\n\nI'm hoping to put together a guide on debugging some of the tests for this sort of thing soon. Perhaps some of that will also be useful for debugging a live installation. I sure could use the help - I have a lot on my plate at the moment.  "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13824270",
            "date": "2013-11-16T00:25:41+0000",
            "content": "Looking at the code a bit, I realized that the scenario I described can in fact happen if the Old Leader dies (or somehow becomes unreachable, for example due to tripping the kernel SYN flood detection, as ours did), because looks like during runLeaderProcess(), the sync that's run is called with cantReachIsSuccess=true. Since the New Leader can't reach Old Leader, it won't find out about 4 5 (assuming no other replicas have it either), but will successfully \"sync\" and become the new leader. This can be remedied if the \"// TODO: optionally fail if n replicas are not reached...\" on DistributedUpdateProcessor.doFinish() is implemented so that at least another replica must have 4 5 before the request would have been ack'd to the user, but of course if New Leader can't reach this other replica either then it's not much help.\n\nI feel like in general the code may be trying too hard to find a new leader to take over, thereby compromising data consistency. This is probably the right thing to do for many, if not most, search solutions. However, if Solr is indeed moving toward being a possible NoSql solution or for use cases where reindexing the entire corpus is extremely expensive, then maybe a more consistent mode can be implemented where user can choose to trade availability for consistency. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13824284",
            "date": "2013-11-16T00:44:07+0000",
            "content": "Since the New Leader can't reach Old Leader, it won't find out about 4 5 (assuming no other replicas have it either)\n\nThis shouldn't be the case, because those updates will only have been ack'd if each replica received them. And if they were not ack'd a success, we don't care if we keep them - we just want to get consistent.\n\nI feel like in general the code may be trying too hard to find a new leader to take over\n\nA further protection is that a node will not become leader unless it's last state was active.\n\nI'm not convinced it's too loose - I do know that the impl could use additional love and tests. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13824295",
            "date": "2013-11-16T00:56:07+0000",
            "content": "\nThis shouldn't be the case, because those updates will only have been ack'd if each replica received them.\n\nThat's what I thought too, but doesn't seem to be the case in the code. If you take a look at DistributedUpdateProcessor.doFinish(),\n\n\n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) \nUnknown macro: {        rsp.setException(errors.get(0).e);      } \n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n\nIt then starts a thread to urge the replica to recover, but if that fails, it just completely gives up. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13824299",
            "date": "2013-11-16T01:01:59+0000",
            "content": "Right - but that's just impl, not design. The idea is that, since we add locally first, there is not much reason it should fail on a replica - unless that replica has crashed or lost connectivity or something really bad. In that case, it will have to reconnect to zk and recover or restart and recover. Just in case, as a precaution, we try and tell it to recover - then if it's still got connectivity or it was an intermittent problem, it won't run around acting active. I think I have a note about perhaps doing more retries in background threads for that recovery request, but I've never gotten to it.\n\nIf you are finding a scenario that eludes that, we should strengthen the impl. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13824921",
            "date": "2013-11-17T18:20:11+0000",
            "content": "This could be related to SOLR-5397 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13825107",
            "date": "2013-11-18T04:38:04+0000",
            "content": "Would love if you guys could try with 4.6 and report back. SOLR-5397 was introduced when we fixed a similar issue, so that has really been an issue for a few releases.\n "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13826350",
            "date": "2013-11-19T09:45:28+0000",
            "content": "I updated our machines to include SOLR-5397. Everything works fine now, it may take quite some time before we can say it is fixed  "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13826356",
            "date": "2013-11-19T10:00:20+0000",
            "content": "It's safe upgrade from 4.5.1 to 4.6?. I have docValues and I read that it's not linear upgraded and I can't reindex the data. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13826529",
            "date": "2013-11-19T13:58:53+0000",
            "content": "According to the wiki, it depends on the doc values impl you are using - the default one will upgrade fine. Others require that you forceMerge your index to rewrite it with the default and then upgrade, then I guess you can forceMerge back to that impl. Honestly, I have not had a chance to play with doc values yet though. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13826533",
            "date": "2013-11-19T14:10:55+0000",
            "content": "I'm using  <codecFactory class=\"solr.SchemaCodecFactory\"/> to enable per-field DocValues formats.\n\nI think that this aspect about docValues it doesn't  explained on wiki in a proper way. There is no example how we can do the switch to default, do the forceMerge and switch back to the original implementation.\n\nIf I can't have the security that all will work fine,  I can't do the upgrade. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13826535",
            "date": "2013-11-19T14:15:11+0000",
            "content": "Should probably bring it up on the user list - we need someone like Robert Muir to weigh in. I assume it all works the same way - you merge each field to the default impl and then back to what they were. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13834760",
            "date": "2013-11-28T11:58:59+0000",
            "content": "I've got some bad news, it happened again on one of our clusters using a build of november 19th.Three replica's went out of sync. "
        },
        {
            "author": "Rafa\u0142 Ku\u0107",
            "id": "comment-13835368",
            "date": "2013-11-29T13:40:24+0000",
            "content": "Happened to me two, collection with two four shards, each having a single replica. The replicas were our of sync. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13835436",
            "date": "2013-11-29T16:21:17+0000",
            "content": "What's the exact version / checkout? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13835883",
            "date": "2013-11-30T22:00:52+0000",
            "content": "Markus Jelsma, hopefully that's SOLR-5516 then. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13836414",
            "date": "2013-12-02T10:54:10+0000",
            "content": "I'll check it out! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13841162",
            "date": "2013-12-06T10:42:00+0000",
            "content": "I'm sorry, i've got three replica's having one document less than the leader. We're on a december, 3th build. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-13841870",
            "date": "2013-12-06T23:16:42+0000",
            "content": "Replicas are still losing docs in Solr 4.6 .\n\nI'm wondering if we can't have a pair (version, numDocs) to track the increments of docs between versions. Also we can save the last 10 tlogs in each replica as backups after be commited and make a diff to see what is missing in case the replicas are out of sync, replay the transaction and avoid a not synchronized replica and a full-recovery that probably will be heaviest that make the diff.\n\nIt's only and idea and of course find the bug must be the priority.\n\nThis issue compromisse Solr to be \"the main\" storage. If re-index data is not possible, we can't guarantee that no data is missing,  and worse, we lost the data forever . "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13841896",
            "date": "2013-12-06T23:37:14+0000",
            "content": "I've fixed some things since 4.6 - I only had time to focus on the leader not going down case for 4.6, I spent a bunch more time on this case after 4.6 was released. Unfortunately, I think there are a couple of issues at play here - some of the new changes makes existing holes easier to spot and the chaos monkey tests where accidentally disabled for some time, so small issues may have crept in.\n\nI think the remaining issue is mostly around SOLR-5516. Need to come up with a better idea than a really long wait though - but if someone wants to help test, putting in a long wait and stressing this would be useful to see if it is indeed the main remaining issue.\n\nI recently put in a lot of time improving the situation and I need to focus on other things for a bit, but that I'll keep coming back to this as I can. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13845459",
            "date": "2013-12-11T14:58:46+0000",
            "content": "I have some cycles to work on this issue over the next couple of days. I'm starting by trying to reproduce it in my environment. Please let me know of any tasks that I can help out on (beyond the long wait stuff you mentioned above).  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13845847",
            "date": "2013-12-11T23:35:00+0000",
            "content": "I don't have fix yet, but I wanted to post an update here to get some feedback on what I'm seeing ...\n\nI have a simple SolrCloud configuration setup locally: 1 collection named \"cloud\" with 1 shard and replicationFactor 2, i.e. here's what I use to create it:\ncurl \"http://localhost:8984/solr/admin/collections?action=CREATE&name=cloud&replicationFactor=$REPFACT&numShards=1&collection.configName=cloud\"\n\nThe collection gets distributed on two nodes: cloud84:8984 and cloud85:8985 with cloud84 being assigned the leader.\n\nHere's an outline of the process I used to get my collection out-of-sync during indexing:\n\n1) start indexing docs using CloudSolrServer in SolrJ - direct updates go to the leader and replica remains in sync for as long as I let this process run\n2) kill -9 the process for the replica cloud85\n3) let indexing continue against cloud84 for a few seconds (just to get the leader and replica out-of-sync once I bring the replica back online)\n4) kill -9 the process for the leader cloud84 ... indexing halts of course as there are no running servers\n5) start the replica cloud85 but do not start the previous leader cloud84\n\nHere are some key log messages as cloud85 - the replica - fires up ... my annotations of the log messages are prefixed by [TJP >>\n\n2013-12-11 11:43:22,076 [main-EventThread] INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 1)\n2013-12-11 11:43:23,370 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.ShardLeaderElectionContext  - Waiting until we see more replicas up for shard shard1: total=2 found=1 timeoutin=139841\n\n[TJP >> This looks good and is expected because cloud85 was not the leader before it died, so it should not immediately assume it is the leader until it sees more replicas\n\n6) now start the previous leader cloud84 ...\n\nHere are some key log messages from cloud85 as the previous leader cloud84 is coming up ... \n\n2013-12-11 11:43:24,085 [main-EventThread] INFO  common.cloud.ZkStateReader  - Updating live nodes... (2)\n2013-12-11 11:43:24,136 [main-EventThread] INFO  solr.cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2013-12-11 11:43:24,137 [Thread-13] INFO  common.cloud.ZkStateReader  - Updating cloud state from ZooKeeper... \n2013-12-11 11:43:24,138 [Thread-13] INFO  solr.cloud.Overseer  - Update state numShards=1 message=\n{\n  \"operation\":\"state\",\n  \"state\":\"down\",\n  \"base_url\":\"http://cloud84:8984/solr\",\n  \"core\":\"cloud_shard1_replica2\",\n  \"roles\":null,\n  \"node_name\":\"cloud84:8984_solr\",\n  \"shard\":\"shard1\",\n  \"shard_range\":null,\n  \"shard_state\":\"active\",\n  \"shard_parent\":null,\n  \"collection\":\"cloud\",\n  \"numShards\":\"1\",\n  \"core_node_name\":\"core_node1\"}\n\n[TJP >> state of cloud84 looks correct as it is still initializing ...\n\n2013-12-11 11:43:24,140 [main-EventThread] INFO  solr.cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2013-12-11 11:43:24,141 [main-EventThread] INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 2)\n\n2013-12-11 11:43:25,878 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.ShardLeaderElectionContext  - Enough replicas found to continue.\n\n[TJP >> hmmmm ... cloud84 is listed in /live_nodes but it isn't \"active\" yet or even recovering (see state above - it's currently \"down\") ... My thinking here is that the ShardLeaderElectionContext needs to take the state of the replica into account before deciding it should continue.\n\n\n2013-12-11 11:43:25,878 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.ShardLeaderElectionContext  - I may be the new leader - try and sync\n2013-12-11 11:43:25,878 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.SyncStrategy  - Sync replicas to http://cloud85:8985/solr/cloud_shard1_replica1/\n2013-12-11 11:43:25,880 [coreLoadExecutor-3-thread-1] INFO  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=http://cloud85:8985/solr START replicas=http://cloud84:8984/solr/cloud_shard1_replica2/ nUpdates=100\n2013-12-11 11:43:25,936 [coreLoadExecutor-3-thread-1] WARN  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=http://cloud85:8985/solr  couldn't connect to http://cloud84:8984/solr/cloud_shard1_replica2/, counting as success\n\n\n[TJP >> whoops! of course it couldn't connect to cloud84 as it's still initializing ...\n\n\n2013-12-11 11:43:25,936 [coreLoadExecutor-3-thread-1] INFO  solr.update.PeerSync  - PeerSync: core=cloud_shard1_replica1 url=http://cloud85:8985/solr DONE. sync succeeded\n2013-12-11 11:43:25,937 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.SyncStrategy  - Sync Success - now sync replicas to me\n2013-12-11 11:43:25,937 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.SyncStrategy  - http://cloud85:8985/solr/cloud_shard1_replica1/: try and ask http://cloud84:8984/solr/cloud_shard1_replica2/ to sync\n2013-12-11 11:43:25,938 [coreLoadExecutor-3-thread-1] ERROR solr.cloud.SyncStrategy  - Sync request error: org.apache.solr.client.solrj.SolrServerException: Server refused connection at: http://cloud84:8984/solr/cloud_shard1_replica2\n\n[TJP >> ayep, cloud84 is still initializing so it can't respond to you Mr. Impatient cloud85!\n\n2013-12-11 11:43:25,939 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.SyncStrategy  - http://cloud85:8985/solr/cloud_shard1_replica1/: Sync failed - asking replica (http://cloud84:8984/solr/cloud_shard1_replica2/) to recover.\n2013-12-11 11:43:25,940 [coreLoadExecutor-3-thread-1] INFO  solr.cloud.ShardLeaderElectionContext  - I am the new leader: http://cloud85:8985/solr/cloud_shard1_replica1/ shard1\n\n[TJP >> oh no! the collection is now out-of-sync ... my test harness periodically polls the replicas for their doc counts and at this point, we ended up with:\nshard1: {\n\thttp://cloud85:8985/solr/cloud_shard1_replica1/ = 300800 LEADER\n\thttp://cloud84:8984/solr/cloud_shard1_replica2/ = 447600 diff:146800  <-- this should be the real leader!\n}\n\nWhich of course is expected because cloud85 should NOT be the leader\n\nSo all that is interesting, but how to fix???\n\nMy first idea was to go tackle the decision making process ShardLeaderElectionContext uses to decide if it has enough replicas to continue. \n\nIt's easy enough to do something like the following:\n        int notDownCount = 0;\n        Map<String,Replica> replicasMap = slices.getReplicasMap();\n        for (Replica replica : replicasMap.values()) {\n          ZkCoreNodeProps replicaCoreProps = new ZkCoreNodeProps(replica);\n          String replicaState = replicaCoreProps.getState();\n          log.warn(\">>>> State of replica \"replica.getName()\" is \"replicaState\" <<<<\");\n          if (\"active\".equals(replicaState) || \"recovering\".equals(replicaState)) \n{\n            ++notDownCount;\n          }\n        }\n\nWas thinking I could use the notDownCount to make a better decision, but then I ran into another issue related to replica state being stale. In my cluster, if I have /clusterstate.json:\n\n{\"cloud\":{\n    \"shards\":{\"shard1\":{\n        \"range\":\"80000000-7fffffff\",\n        \"state\":\"active\",\n        \"replicas\":{\n          \"core_node1\":\n{\n            \"state\":\"active\",\n            \"base_url\":\"http://cloud84:8984/solr\",\n            \"core\":\"cloud_shard1_replica2\",\n            \"node_name\":\"cloud84:8984_solr\",\n            \"leader\":\"true\"}\n,\n          \"core_node2\":{\n            \"state\":\"active\",\n            \"base_url\":\"http://cloud85:8985/solr\",\n            \"core\":\"cloud_shard1_replica1\",\n            \"node_name\":\"cloud85:8985_solr\"}}}},\n    \"maxShardsPerNode\":\"1\",\n    \"router\":\n{\"name\":\"compositeId\"}\n,\n    \"replicationFactor\":\"2\"}}\n\nIf I kill the process using kill -9 PID for the Solr running on 8985 (the replica), core_node2's state remains \"active\" in /clusterstate.json\n\nWhen tailing the log on core_node1, I do see one notification coming in the watcher setup by ZkStateReader from ZooKeeper about live nodes having changed:\n2013-12-11 15:42:46,010 [main-EventThread] INFO  common.cloud.ZkStateReader  - Updating live nodes... (1)\n\nSo after killing the process, /live_nodes is updated to only have one node, but /clusterstate.json still thinks there are 2 healthy replicas for shard1, instead of just 1.\n\nOf course, if I restart 8985, then it goes through a series of state changes until it is marked active again, which looks correct.\n\nBottom line ... it seems there is something in SolrCloud that does not update a replica's state when the node is killed. If a change to /live_nodes doesn't trigger a refresh of replica state, what does?\n\nI'm seeing this stale replica state issue in Solr 4.6.0 and in revision 1550300 of branch_4x - the latest from svn.\n\nNot having a fresh state of a replica prevents my idea for fixing ShardLeaderElectionContext's decision making process. I'm also curious about the decision to register a node under /live_nodes before it is fully initialized, but maybe that is a discussion for another time.\n\nIn any case, I wanted to get some feedback on my findings before moving forward with a solution. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13845887",
            "date": "2013-12-12T00:13:41+0000",
            "content": "A lot there! I'll respond to most of it later.\n\nAs far as the stale state, that is expected. You cannot tell the state just from clusterstate.json - it is a mix of clusterstate.json and the live_nodes list. If the livenode for anything in clusterstate.json is missing, it's considered not up. This is just currently by design - without live_nodes, you don't know the state. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13845895",
            "date": "2013-12-12T00:20:08+0000",
            "content": "ok - cool ... just wanted to make sure that \"stale\" situation was expected ...\n\nthe more I dig into ShardLeaderElectionContext's decision making process, I think looking at state won't work because both are in the \"down\" state while this is happening. I think some determination of is the node \"reachable\" so that PeerSync can get good information from it is what needs to be factored into ShardLeaderElectionContext. Or maybe there is another state \"trying to figure out my role in the world as I come back up\"  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13846572",
            "date": "2013-12-12T18:44:03+0000",
            "content": "The other issue is expected as well. It's the safety mechanism - we don't let you just start one node and let it becomes leader - ideally you want all replicas to be involved in the election to prevent data loss. You have to be explicit if you want to have this work with no wait. It might be nice if we added a startup sys prop that caused it not to wait on first startup.  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13846593",
            "date": "2013-12-12T19:01:46+0000",
            "content": "Agreed on the wait being necessary (which I actually annotated in the comment above). The crux of the issue here is that the replica (cloud85) can't sync with the previous leader (cloud84) because they are waiting on each other; much like a dead-lock. Eventually, they both give up and one wins; unfortunately in my test case, cloud85 wins which leads to the shard being out-of-sync because the wrong leader is selected in this scenario (cloud84 should have been selected). \n\nI'm continuing to dig into this but have come to the conclusion that tweaking the waitForReplicasToComeUp process is a dead end and it's working as well as it can. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13846694",
            "date": "2013-12-12T20:19:23+0000",
            "content": "because they are waiting on each other;\n\nThat doesn't make sense to me - the wait should be until all the replicas for a shard are up - so what exactly are they both waiting on? If they are both waiting, there should be enough replicas up to continue... "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13846755",
            "date": "2013-12-12T21:25:46+0000",
            "content": "I'm sorry for being unclear; \"waiting\" was probably the wrong term ... and they definitely continue right on down the path of selecting the wrong leader. \n\nHere's what I know so far, which admittedly isn't much:\n\nAs cloud85 (replica before it crashed) is initializing, it enters the wait process in ShardLeaderElectionContext#waitForReplicasToComeUp; this is expected and a good thing.\n\nSome short amount of time in the future, cloud84 (leader before it crashed) begins initializing and gets to a point where it adds itself as a possible leader for the shard (by creating a znode under /collections/cloud/leaders_elect/shard1/election), which leads to cloud85 being able to return from waitForReplicasToComeUp and try to determine who should be the leader.\n\ncloud85 then tries to run the SyncStrategy, which can never work because in this scenario the Jetty HTTP listener is not active yet on either node, so all replication work that uses HTTP requests fails on both nodes ... PeerSync treats these failures as indicators that the other replicas in the shard are unavailable (or whatever) and assumes success. Here's the log message:\n\n2013-12-11 11:43:25,936 [coreLoadExecutor-3-thread-1] WARN solr.update.PeerSync - PeerSync: core=cloud_shard1_replica1 url=http://cloud85:8985/solr couldn't connect to http://cloud84:8984/solr/cloud_shard1_replica2/, counting as success\n\nThe Jetty HTTP listener doesn't start accepting connections until long after this process has completed and already selected the wrong leader.\n\nFrom what I can see, we seem to have a leader recovery process that is based partly on HTTP requests to the other nodes, but the HTTP listener on those nodes isn't active yet. We need a leader recovery process that doesn't rely on HTTP requests. Perhaps, leader recovery for a shard w/o a current leader may need to work differently than leader election in a shard that has replicas that can respond to HTTP requests? All of what I'm seeing makes perfect sense for leader election when there are active replicas and the current leader fails.\n\nAll this aside, I'm not asserting that this is the only cause for the out-of-sync issues reported in this ticket, but it definitely seems like it could happen in a real cluster. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13846775",
            "date": "2013-12-12T21:42:50+0000",
            "content": "Ah, thanks for the explanation.  I think we should roll that specific issue into a new JIRA issue.  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13846812",
            "date": "2013-12-12T22:09:46+0000",
            "content": "Mark -> https://issues.apache.org/jira/browse/SOLR-5552 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13855273",
            "date": "2013-12-22T20:57:17+0000",
            "content": "SOLR-5552 investigation has also led to SOLR-5569 and SOLR-5568  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13856002",
            "date": "2013-12-23T23:05:41+0000",
            "content": "Found another interesting case that may or may not be valid depending on whether we think HTTP requests between a leader and replica can fail even if the ZooKeeper session on the replica does not drop?\n\nSpecifically, what I'm seeing is that if an update request between the leader and replica fails, but the replica doesn't lose it's session with ZK, then the replica can get out-of-sync with the leader. In a real network partition, the ZK connection would also likely be lost and the replica would get marked as down. So as long as the HTTP connection timeout between the leader and replica exceeds the ZK client timeout, the replica would probably recover correctly, rendering this test case invalid. So maybe the main question here is whether we think it's possible for HTTP requests between a leader and replica to fail even though the ZooKeeper connection stays alive?\n\nHere's the steps I used to reproduce this case (all using revision 1553150 in branch_4x):\n\n> STEP 1: Setup a collection named \u201ccloud\u201d containing 1 shard and 2 replicas on hosts: cloud84 (127.0.0.1:8984) and cloud85 (127.0.0.1:8985)\n\nSOLR_TOP=/home/ec2-user/branch_4x/solr\n$SOLR_TOP/cloud84/cloud-scripts/zkcli.sh -zkhost $ZK_HOST -cmd upconfig -confdir $SOLR_TOP/cloud84/solr/cloud/conf -confname cloud\nAPI=http://localhost:8984/solr/admin/collections\ncurl -v \"$API?action=CREATE&name=cloud&replicationFactor=2&numShards=1&collection.configName=cloud\"\n\nReplica on cloud84 is elected as the initial leader. /clusterstate.json looks like:\n\n{\"cloud\":{\n    \"shards\":{\"shard1\":{\n        \"range\":\"80000000-7fffffff\",\n        \"state\":\"active\",\n        \"replicas\":{\n          \"core_node1\":\n{\n            \"state\":\"active\",\n            \"base_url\":\"http://cloud84:8984/solr\",\n            \"core\":\"cloud_shard1_replica1\",\n            \"node_name\":\"cloud84:8984_solr\",\n            \"leader\":\"true\"}\n,\n          \"core_node2\":{\n            \"state\":\"active\",\n            \"base_url\":\"http://cloud85:8985/solr\",\n            \"core\":\"cloud_shard1_replica2\",\n            \"node_name\":\"cloud85:8985_solr\"}}}},\n    \"maxShardsPerNode\":\"1\",\n    \"router\":\n{\"name\":\"compositeId\"}\n,\n    \"replicationFactor\":\"2\"}}\n\n\n> STEP 2: Simulate network partition\n\nsudo iptables -I INPUT 1 -i lo -p tcp --sport 8985 -j DROP; sudo iptables -I INPUT 2 -i lo -p tcp --dport 8985 -j DROP\n\nVarious ways to do this, but to keep it simple, I'm just dropping inbound traffic on localhost to port 8985.\n\n> STEP 3: Send document with ID \u201cdoc1\u201d to leader on cloud84\n\ncurl \"http://localhost:8984/solr/cloud/update\" -H 'Content-type:application/xml' \\\n  --data-binary '<add><doc><field name=\"id\">doc1</field><field name=\"foo_s\">bar</field></doc></add>'\n\nThe update request takes some time because the replica is down but ultimately succeeds on the leader. In the logs on the leader, we have (some stack trace lines removed for clarity):\n\n2013-12-23 10:59:33,688 [updateExecutor-1-thread-1] ERROR solr.update.StreamingSolrServers  - error\norg.apache.http.conn.HttpHostConnectException: Connection to http://cloud85:8985 refused\n        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:190)\n        ...\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer$Runner.run(ConcurrentUpdateSolrServer.java:232)\n        ...\nCaused by: java.net.ConnectException: Connection timed out\n        ...\n2013-12-23 10:59:33,695 [qtp1073932139-16] INFO  update.processor.LogUpdateProcessor  - [cloud_shard1_replica1] webapp=/solr path=/update params={} \n{add=[doc1 (1455228778490888192)]}\n 0 63256\n2013-12-23 10:59:33,702 [updateExecutor-1-thread-2] INFO  update.processor.DistributedUpdateProcessor  - try and ask http://cloud85:8985/solr to recover\n2013-12-23 10:59:48,718 [updateExecutor-1-thread-2] ERROR update.processor.DistributedUpdateProcessor  - http://cloud85:8985/solr: Could not tell a replica to recover:org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://cloud85:8985/solr\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:507)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor$1.run(DistributedUpdateProcessor.java:657)\n        ...\nCaused by: org.apache.http.conn.ConnectTimeoutException: Connect to cloud85:8985 timed out\n        ...\n\nOf course these log messages are expected. The key is that the leader accepted the update and now has one doc with ID \"doc1\"\n\n> STEP 4: Heal the network partition\n\nsudo service iptables restart (undoes the DROP rules we added above)\n\n> STEP 5: Send document with ID \u201cdoc2\u201d to leader on cloud84\n\ncurl \"http://localhost:8984/solr/cloud/update\" -H 'Content-type:application/xml' \\\n  --data-binary '<add><doc><field name=\"id\">doc2</field><field name=\"foo_s\">bar</field></doc></add>'\n\nOf course this time the update gets sent successfully to replica ... here are some log messages ...\n\nfrom the log on cloud84:\n2013-12-23 11:00:46,982 [qtp1073932139-18] INFO  update.processor.LogUpdateProcessor  - [cloud_shard1_replica1] webapp=/solr path=/update params={} \n{add=[doc2 (1455228921389776896)]} 0 162\n\nfrom the log on cloud85 (out-of-sync replica):\n2013-12-23 10:47:26,363 [main-EventThread] INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 2)\n\n...\n\n*2013-12-23 11:00:46,979 [qtp2124890785-12] INFO  update.processor.LogUpdateProcessor  - [cloud_shard1_replica2] webapp=/solr path=/update params={distrib.from=http://cloud84:8984/solr/cloud_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2} {add=[doc2 (1455228921389776896)]}\n 0 142*\n\nNotice that there is no logged activity on cloud85 between 10:47 and 11:00\n\n> STEP 6: Commit updates\n\ncurl \"http://localhost:8984/solr/cloud/update\" -H 'Content-type:application/xml' --data-binary \"<commit waitSearcher=\\\"true\\\"/>\"\n\n> STEP 7: Send non-distributed queries to each replica\n\ncurl \"http://localhost:8984/solr/cloud/select?q=foo_s:bar&rows=0&wt=json&distrib=false\"\n\n{\"responseHeader\":{\"status\":0,\"QTime\":1,\"params\":{\"q\":\"foo_s:bar\",\"distrib\":\"false\",\"wt\":\"json\",\"rows\":\"0\"}},\"response\":{\"numFound\":2,\"start\":0,\"docs\":[]}}\n\ncurl \"http://localhost:8985/solr/cloud/select?q=foo_s:bar&rows=0&wt=json&distrib=false\"\n\n{\"responseHeader\":{\"status\":0,\"QTime\":1,\"params\":{\"q\":\"foo_s:bar\",\"distrib\":\"false\",\"wt\":\"json\",\"rows\":\"0\"}},\"response\":{\"numFound\":1,\"start\":0,\"docs\":[]}}\n\nObserve that the leader has 2 docs and the replica on cloud85 only has 1, but should have 2.\n\nFrom what I can tell, the replica that missed some updates because of a temporary network partition doesn't get any notification that it missed some documents. In other words, the out-of-sync replica doesn't know it's out-of-sync and it's state in ZooKeeper is active. As you can see from the log messages I posted in step 3 above, the leader tried to tell the replica to recovery, but due to the network partition, that request got dropped too.\n\nI'm wondering if the leader should send a state version tracking ID along with each update request so that a replica can detect that it's view of state was stale? I could see the process working as follows:\n\n1. Shard leader now keeps track of a Slice state version tracking identifier that gets sent with every update request\n2. Leader tries to send an update request (including the state version ID) to a replica and send fails\n3. Leader updates the state version to a different value\n4. Leader sends another update request to the replica; request includes the updated version ID; replica accepts the request but realizes its state version ID is out-of-date from what the leader sent\n5. Replica enters recovery "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13856009",
            "date": "2013-12-23T23:14:47+0000",
            "content": "Yeah, that's currently expected. We don't expect the case where you can talk to ZooKeeper but not your replicas to be common, so we kind of punted on this scenario for the first phase.\n\nSome related JIRA issues:\n\nSOLR-5482\nSOLR-5450\nSOLR-5495 \t\n\nI think we should do all that, but the key is really, in this case, we need to pass the order to recover through ZooKeeper to the partitioned off replica. With an eventually consistent model, it can be off for a short time, but it needs to recover in a timely manner.\n\nI think this is the right solution because the replica is sure to either get the information to recover from ZooKeeper or lose it's connection to ZooKeeper in which case it will have to recover anyway. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13856013",
            "date": "2013-12-23T23:17:51+0000",
            "content": "so we kind of punted\n\nThe other thing to note is that if you restart the shard or that node or the cluster, you should be able to do it without losing any data. It will recover from the leader when everything else is working correctly. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13856052",
            "date": "2013-12-24T00:36:47+0000",
            "content": "Thanks Mark, I suspected my test case was a little cherry picked ... something interesting happened when I also severed the connection between the replica and ZK (ie. same test as above but I also dropped the ZK connection on the replica).\n\n2013-12-23 15:39:57,170 [main-EventThread] INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None\n2013-12-23 15:39:57,170 [main-EventThread] INFO  common.cloud.ConnectionManager  - zkClient has disconnected\n\n>>> fixed the connection between replica and ZK here <<<\n\n2013-12-23 15:40:45,579 [main-EventThread] INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:Expired type:None path:null path:null type:None\n2013-12-23 15:40:45,579 [main-EventThread] INFO  common.cloud.ConnectionManager  - Our previous ZooKeeper session was expired. Attempting to reconnect to recover relationship with ZooKeeper...\n2013-12-23 15:40:45,580 [main-EventThread] INFO  common.cloud.DefaultConnectionStrategy  - Connection expired - starting a new one...\n2013-12-23 15:40:45,586 [main-EventThread] INFO  common.cloud.ConnectionManager  - Waiting for client to connect to ZooKeeper\n2013-12-23 15:40:45,595 [main-EventThread] INFO  common.cloud.ConnectionManager  - Watcher org.apache.solr.common.cloud.ConnectionManager@4f857c62 name:ZooKeeperConnection Watcher:ec2-54-197-0-103.compute-1.amazonaws.com:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None\n2013-12-23 15:40:45,595 [main-EventThread] INFO  common.cloud.ConnectionManager  - Client is connected to ZooKeeper\n2013-12-23 15:40:45,595 [main-EventThread] INFO  common.cloud.ConnectionManager  - Connection with ZooKeeper reestablished.\n2013-12-23 15:40:45,596 [main-EventThread] WARN  solr.cloud.RecoveryStrategy  - Stopping recovery for zkNodeName=core_node3core=cloud_shard1_replica3\n2013-12-23 15:40:45,597 [main-EventThread] INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=down\n2013-12-23 15:40:45,597 [main-EventThread] INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2013-12-23 15:40:45,905 [qtp2124890785-14] INFO  handler.admin.CoreAdminHandler  - It has been requested that we recover\n2013-12-23 15:40:45,906 [qtp2124890785-14] INFO  solr.servlet.SolrDispatchFilter  - [admin] webapp=null path=/admin/cores params=\n{action=REQUESTRECOVERY&core=cloud_shard1_replica3&wt=javabin&version=2}\n status=0 QTime=2 \n2013-12-23 15:40:45,909 [Thread-17] INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering\n2013-12-23 15:40:45,909 [Thread-17] INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2013-12-23 15:40:45,920 [Thread-17] INFO  solr.update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2013-12-23 15:40:45,921 [RecoveryThread] INFO  solr.cloud.RecoveryStrategy  - Starting recovery process.  core=cloud_shard1_replica3 recoveringAfterStartup=false\n2013-12-23 15:40:45,924 [RecoveryThread] INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering\n2013-12-23 15:40:45,924 [RecoveryThread] INFO  solr.cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2013-12-23 15:40:48,613 [qtp2124890785-15] INFO  solr.core.SolrCore  - [cloud_shard1_replica3] webapp=/solr path=/select params=\n{q=foo_s:bar&distrib=false&wt=json&rows=0}\n hits=0 status=0 QTime=1 \n2013-12-23 15:42:42,770 [qtp2124890785-13] INFO  solr.core.SolrCore  - [cloud_shard1_replica3] webapp=/solr path=/select params=\n{q=foo_s:bar&distrib=false&wt=json&rows=0}\n hits=0 status=0 QTime=1 \n2013-12-23 15:42:45,650 [main-EventThread] ERROR solr.cloud.ZkController  - There was a problem making a request to the leader:org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: I was asked to wait on state down for cloud86:8986_solr but I still do not see the requested state. I see state: recovering live:false\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)\n\tat org.apache.solr.cloud.ZkController.waitForLeaderToSeeDownState(ZkController.java:1434)\n\tat org.apache.solr.cloud.ZkController.registerAllCoresAsDown(ZkController.java:347)\n\tat org.apache.solr.cloud.ZkController.access$100(ZkController.java:85)\n\tat org.apache.solr.cloud.ZkController$1.command(ZkController.java:225)\n\tat org.apache.solr.common.cloud.ConnectionManager$1.update(ConnectionManager.java:118)\n\tat org.apache.solr.common.cloud.DefaultConnectionStrategy.reconnect(DefaultConnectionStrategy.java:56)\n\tat org.apache.solr.common.cloud.ConnectionManager.process(ConnectionManager.java:93)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)\n\n2013-12-23 15:42:45,963 [RecoveryThread] ERROR solr.cloud.RecoveryStrategy  - Error while trying to recover. core=cloud_shard1_replica3:org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: I was asked to wait on state recovering for cloud86:8986_solr but I still do not see the requested state. I see state: recovering live:false\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:495)\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:199)\n\tat org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:224)\n\tat org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:371)\n\tat org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:247)\n\n2013-12-23 15:42:45,964 [RecoveryThread] ERROR solr.cloud.RecoveryStrategy  - Recovery failed - trying again... (0) core=cloud_shard1_replica3\n2013-12-23 15:42:45,964 [RecoveryThread] INFO  solr.cloud.RecoveryStrategy  - Wait 2.0 seconds before trying to recover again (1)\n2013-12-23 15:42:47,964 [RecoveryThread] INFO  solr.cloud.ZkController  - publishing core=cloud_shard1_replica3 state=recovering "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13856078",
            "date": "2013-12-24T01:57:47+0000",
            "content": "That's interesting. The logging makes it look like it's not creating it's new ephemeral live node for some reason...or the leader is not getting an updated view of the live node... "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13864141",
            "date": "2014-01-07T12:00:43+0000",
            "content": "Ok, I followed all the great work here and in related tickets and yesterday i had the time to rebuild Solr and check for this issue. I hadn't seen it yesterday but it is right in front of me again, using a fresh build from January 6th.\n\nLeader has Num Docs: 379659\nReplica has Num Docs: 379661 "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13865724",
            "date": "2014-01-08T18:32:25+0000",
            "content": "While doing some other testing of SolrCloud (branch4x - 4.7-SNAPSHOT rev. 1556055), I hit this issue and here's the kicker ... there were no errors in my replica's log, the tlogs are identical, and there was no significant GC activity during the time where the replica got out of sync with the leader. I'm attaching the data directories (index + tlog) for both replicas (demo_shard1_replica1 [leader], and demo_shard1_replica2) and their log files. When I do a doc-by-doc comparison of the two indexes, here's the result:\n\n>> finished querying replica1, found 33537 documents (33537)\n>> finished querying replica2, found 33528 documents\nDoc [82995] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82995</field><field name=\"string_s\">test</field><field name=\"int_i\">-274468088</field><field name=\"float_f\">0.90338105</field><field name=\"double_d\">0.6949391474539932</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668206518274</field></doc>\nDoc [82997] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82997</field><field name=\"string_s\">test</field><field name=\"int_i\">301737117</field><field name=\"float_f\">0.6746266</field><field name=\"double_d\">0.26034065188918565</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668206518276</field></doc>\nDoc [82996] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82996</field><field name=\"string_s\">test</field><field name=\"int_i\">-1768315588</field><field name=\"float_f\">0.6641093</field><field name=\"double_d\">0.23708033183534993</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668206518275</field></doc>\nDoc [82991] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82991</field><field name=\"string_s\">test</field><field name=\"int_i\">-2057280061</field><field name=\"float_f\">0.27617514</field><field name=\"double_d\">0.7885214691953506</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668206518273</field></doc>\nDoc [82987] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82987</field><field name=\"string_s\">test</field><field name=\"int_i\">1051456320</field><field name=\"float_f\">0.51863414</field><field name=\"double_d\">0.7881255443862878</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668206518272</field></doc>\nDoc [82986] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82986</field><field name=\"string_s\">test</field><field name=\"int_i\">-1356807889</field><field name=\"float_f\">0.2762279</field><field name=\"double_d\">0.003657816979820372</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668205469699</field></doc>\nDoc [82984] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82984</field><field name=\"string_s\">test</field><field name=\"int_i\">732678870</field><field name=\"float_f\">0.31199205</field><field name=\"double_d\">0.9848865821766198</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668205469698</field></doc>\nDoc [82970] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82970</field><field name=\"string_s\">test</field><field name=\"int_i\">283693979</field><field name=\"float_f\">0.6119651</field><field name=\"double_d\">0.04142006867388914</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668205469696</field></doc>\nDoc [82973] not found in replica2: <doc boost=\"1.0\"><field name=\"id\">82973</field><field name=\"string_s\">test</field><field name=\"int_i\">1343103920</field><field name=\"float_f\">0.5855809</field><field name=\"double_d\">0.6575904716584224</field><field name=\"text_en\">this is a test</field><field name=\"version\">1456683668205469697</field></doc>\n\nNo amount of committing or reloading of these cores helps. Also, restarting replica2 doesn't lead to it being in-sync either, most likely because the tlog is identical to the leader? Here's the log messages on replica2 after restarting it.\n\n2014-01-08 13:28:20,112 [searcherExecutor-5-thread-1] INFO  solr.core.SolrCore  - [demo_shard1_replica2] Registered new searcher Searcher@4345de8a main\n{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}\n2014-01-08 13:28:21,298 [RecoveryThread] INFO  solr.cloud.RecoveryStrategy  - Attempting to PeerSync from http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/ core=demo_shard1_replica2 - recoveringAfterStartup=true\n2014-01-08 13:28:21,302 [RecoveryThread] INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr START replicas=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/ nUpdates=100\n2014-01-08 13:28:21,330 [RecoveryThread] INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr  Received 99 versions from ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/\n2014-01-08 13:28:21,331 [RecoveryThread] INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr  Our versions are newer. ourLowThreshold=1456683689417113603 otherHigh=1456683689602711553\n2014-01-08 13:28:21,331 [RecoveryThread] INFO  solr.update.PeerSync  - PeerSync: core=demo_shard1_replica2 url=http://ec2-54-209-97-145.compute-1.amazonaws.com:8984/solr DONE. sync succeeded\n2014-01-08 13:28:21,332 [RecoveryThread] INFO  solr.update.UpdateHandler  - start commit\n{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n2014-01-08 13:28:21,332 [RecoveryThread] INFO  solr.update.LoggingInfoStream  - [DW][RecoveryThread]: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false\n2014-01-08 13:28:21,333 [RecoveryThread] INFO  solr.update.UpdateHandler  - No uncommitted changes. Skipping IW.commit.\n2014-01-08 13:28:21,334 [RecoveryThread] INFO  solr.update.LoggingInfoStream  - [DW][RecoveryThread]: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false\n2014-01-08 13:28:21,334 [RecoveryThread] INFO  solr.update.LoggingInfoStream  - [IW][RecoveryThread]: nrtIsCurrent: infoVersion matches: true; DW changes: false; BD changes: false\n2014-01-08 13:28:21,335 [RecoveryThread] INFO  solr.update.LoggingInfoStream  - [DW][RecoveryThread]: anyChanges? numDocsInRam=0 deletes=false hasTickets:false pendingChangesInFullFlush: false\n2014-01-08 13:28:21,335 [RecoveryThread] INFO  solr.search.SolrIndexSearcher  - Opening Searcher@5fc2a9d main\n2014-01-08 13:28:21,338 [searcherExecutor-5-thread-1] INFO  solr.core.SolrCore  - QuerySenderListener sending requests to Searcher@5fc2a9d main\n{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}\n2014-01-08 13:28:21,338 [searcherExecutor-5-thread-1] INFO  solr.core.SolrCore  - QuerySenderListener done.\n2014-01-08 13:28:21,338 [searcherExecutor-5-thread-1] INFO  solr.core.SolrCore  - [demo_shard1_replica2] Registered new searcher Searcher@5fc2a9d main\n{StandardDirectoryReader(segments_e:38:nrt _d(4.7):C26791 _e(4.7):C3356 _f(4.7):C3381)}\n2014-01-08 13:28:21,339 [RecoveryThread] INFO  solr.update.UpdateHandler  - end_commit_flush\n2014-01-08 13:28:21,339 [RecoveryThread] INFO  solr.cloud.RecoveryStrategy  - PeerSync Recovery was successful - registering as Active. core=demo_shard1_replica2\n2014-01-08 13:28:21,339 [RecoveryThread] INFO  solr.cloud.ZkController  - publishing core=demo_shard1_replica2 state=active\n2014-01-08 13:28:21,370 [main-EventThread] INFO  common.cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 6)\n\n\nThus, it would seem there might be some code that's outright losing documents (almost feels like a last batch not flushed error but more subtle as it's not easy to reproduce this all the time)\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13865742",
            "date": "2014-01-08T18:50:40+0000",
            "content": "I've noticed something like this too - but nothing i could reproduce easily. I imagine it's likely an issue in SolrCmdDistributor. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13865744",
            "date": "2014-01-08T18:51:31+0000",
            "content": "Although that doesn't really jive with the tran logs being identical...hmm... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13865769",
            "date": "2014-01-08T19:19:04+0000",
            "content": "No, wait, it could jive. We only check the last 99 docs on peer sync - if bunch of docs just didn't show up well before that, it wouldn't be detected by peer sync. I still think SolrCmdDistributor is the first place to look. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13865963",
            "date": "2014-01-08T22:06:55+0000",
            "content": "Still digging into it ... I'm curious why a batch of 34 adds on the leader gets processed as several sub-batches on the replica? Here's what I'm seeing the logs around the documents that are missing from the replica. Basically, there are 34 docs on the leader and only 25 processed in 4 separate batches (from my counting of the logs) on the replica. Why wouldn't it just be one for one? The docs are all roughly the same size ... and what's breaking it up? Having trouble seeing that in the logs / code \n\nOn the leader:\n\n2014-01-08 12:23:21,501 [qtp604104855-17] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica1] webapp=/solr path=/update params=\n{wt=javabin&version=2} {add=[82900 (1456683668174012416), 82901 (1456683668181352448), 82903 (1456683668181352449), 82904 (1456683668181352450), 82912 (1456683668187643904), 82913 (1456683668188692480), 82914 (1456683668188692481), 82916 (1456683668188692482), 82917 (1456683668188692483), 82918 (1456683668188692484), ... (34 adds)]} 0 34\n\n>>>> NOT ALL OF THE 34 DOCS MENTIONED ABOVE MAKE IT TO THE REPLICA <<<<<\n\n2014-01-08 12:23:21,600 [qtp604104855-17] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica1] webapp=/solr path=/update params={wt=javabin&version=2}\n \n{add=[83002 (1456683668280967168), 83005 (1456683668286210048), 83008 (1456683668286210049), 83011 (1456683668286210050), 83012 (1456683668286210051), 83013 (1456683668287258624), 83018 (1456683668287258625), 83019 (1456683668289355776), 83023 (1456683668289355777), 83024 (1456683668289355778), ... (43 adds)]}\n 0 32\n\n\nOn the replica:\n\n2014-01-08 12:23:21,126 [qtp604104855-22] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica2] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[82900 (1456683668174012416), 82901 (1456683668181352448), 82903 (1456683668181352449)]}\n 0 1\n\n2014-01-08 12:23:21,134 [qtp604104855-22] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica2] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[82904 (1456683668181352450), 82912 (1456683668187643904), 82913 (1456683668188692480), 82914 (1456683668188692481), 82916 (1456683668188692482), 82917 (1456683668188692483), 82918 (1456683668188692484), 82919 (1456683668188692485), 82922 (1456683668188692486)]}\n 0 2\n\n2014-01-08 12:23:21,139 [qtp604104855-22] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica2] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[82923 (1456683668188692487), 82926 (1456683668190789632), 82928 (1456683668190789633), 82932 (1456683668190789634), 82939 (1456683668192886784), 82945 (1456683668192886785), 82946 (1456683668192886786), 82947 (1456683668193935360), 82952 (1456683668193935361), 82962 (1456683668193935362), ... (12 adds)]}\n 0 3\n\n2014-01-08 12:23:21,144 [qtp604104855-22] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica2] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[82967 (1456683668199178240)]}\n 0 0\n\n\n>>>> 9 Docs Missing here <<<<\n\n2014-01-08 12:23:21,227 [qtp604104855-22] INFO  update.processor.LogUpdateProcessor  - \n[demo_shard1_replica2] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-209-223-12.compute-1.amazonaws.com:8984/solr/demo_shard1_replica1/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[83002 (1456683668280967168), 83005 (1456683668286210048), 83008 (1456683668286210049), 83011 (1456683668286210050), 83012 (1456683668286210051), 83013 (1456683668287258624)]}\n 0 2\n\n\nNote the add log message starting with doc ID 83002 is just included here for context to show where the leader / replica got out of sync. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13865984",
            "date": "2014-01-08T22:29:04+0000",
            "content": "\nBasically, there are 34 docs on the leader and only 25 processed in 4 separate batches (from my counting of the logs) on the replica. Why wouldn't it just be one for one? The docs are all roughly the same size ... and what's breaking it up? \n\nConcurrentUpdateSolrServer?  If another doc doesn't come in quickly enough (250ms by default), it ends the batch.\nI thought there used to be a doc count limit too or something... but after a quick scan, I'm not seeing it. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13866031",
            "date": "2014-01-08T23:15:45+0000",
            "content": "Cuss on CUSS  Thanks, I sometimes forget that the client-side batch gets broken into individual AddUpdateCommands when sending on to the replicas. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13869134",
            "date": "2014-01-12T20:27:20+0000",
            "content": "In this case there is no wait due to the massive penalty it puts on doc per request speed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13869137",
            "date": "2014-01-12T20:38:16+0000",
            "content": "SOLR-5625: Add to testing for SolrCmdDistributor "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13869916",
            "date": "2014-01-13T20:12:05+0000",
            "content": "Make sense about not waiting because of the penalty now that I've had a chance to get into the details of that code.\n\nI spent a lot of time on Friday and over the weekend trying to track down the docs getting dropped. Unfortunately have not been able to track down the source of the issue yet. I'm fairly certain the issue happens before docs get submitted to CUSS, meaning that the lost docs never seemed to hit the queue in ConcurrentUpdateSolrServer. My original thinking was that given the complex nature of CUSS, there might be some sort of race condition but after having added a log of what hit the queue, it seems that the documents that get lost never hit the queue. Not to mention that the actual use of CUSS is mostly single-threaded because StreamingSolrServers construct them with a threadCount of 1.\n\nAs a side note, one thing I noticed while is that direct updates don't necessarily hit the correct core initially when a Solr node hosts more than one shard per collection. In other words, if host X had shard1 and shard3 of collection foo, then some update requests would hit shard1 on host X when they should go to shard3 on the same host; shard1 correctly forwards them on but it's still an extra hop. Of course that is probably not a big deal in production as it would be rare to host multiple shards of the same collection in the same Solr host, unless they are over-sharding.\n\nIn terms of this issue, here's what I'm seeing:\n\nAssume a SolrCloud environment with shard1 having replicas on host A and B; A is the current leader\nclient sends direct update request to shard1 on host A containing 3 docs (1,2,3) (for example)\nbatch from client gets broken up into individual docs (during request parsing)\ndocs 1,2,3 get indexed on host A (the leader)\ndocs 1 and 2 get queued into CUSS and sent on to the replica on host B (sometimes in the same request, sometimes in separate requests)\ndoc 3 never makes it and from what I can tell, never hits the queue\n\nThis may be anecdotal but from what I can tell, it's always docs on the end of a batch and not in the middle. Meaning that I haven't seen a case where 1 and 3 make it and 2 not ... maybe useful, maybe not. The only other thing I'll mention is it does seem timing / race condition related as it's almost impossible to reproduce this on my Mac when running 2 shards across 2 nodes but much easier to trigger if I ramp up to say 8 shards on 2 nodes, i.e. the busier my CPU is, the easier it is to see docs getting dropped.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13870068",
            "date": "2014-01-13T22:26:14+0000",
            "content": "How many threads are you using to load docs? How large are the batches? "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13870197",
            "date": "2014-01-14T00:49:16+0000",
            "content": "Oddly enough, just 1 indexing thread on the client side and batches of around 30-40 docs per shard (ie I set my batch size so that direct updates send about 30-40 per shard to the leaders from the client side). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13870858",
            "date": "2014-01-14T16:22:18+0000",
            "content": "Markus Jelsma, are you loading docs via the bulk methods or cuss or what?\n\nTimothy Potter, I think I'm seeing your issue. Have not gotten to the bottom of it yet, but if I am seeing the same thing, it seems those docs are being setup to send to 0 replicas. Trying to figure out why/how. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13870859",
            "date": "2014-01-14T16:23:08+0000",
            "content": "FYI, I also had to overshard to see anything. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13870865",
            "date": "2014-01-14T16:28:09+0000",
            "content": "Mark - We use CloudSolrServer and send batches of around 380 documents from Nutch. I am not sure what actual implementation we get back when connecting. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13870868",
            "date": "2014-01-14T16:33:02+0000",
            "content": "I also think i'm seeing this happening right now with a trunk build of yesterday. I am slowly indexing few hundred docs every few minutes for quite some time for fixing a Nutch issue. Looks like i can restart it because replica's are already out of sync  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13870984",
            "date": "2014-01-14T18:11:43+0000",
            "content": "Markus Jelsma, are you indexing to an oversharded cluster? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13871878",
            "date": "2014-01-15T09:59:46+0000",
            "content": "Mark, no, each node holds a single JVM and single core. We have 5 leaders and five replica's, so ten nodes. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13872666",
            "date": "2014-01-15T22:07:20+0000",
            "content": "Well the affects I was seeing related to having a control collection with a core named collection1 and another collection called collection1. Over shard, and that causes some similar looking effects.\n\nI've addressed that and will see if ramping up my tests can spot anything - so far cannot replicate in a test though. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13873303",
            "date": "2014-01-16T11:56:06+0000",
            "content": "Did something crucial change recently? Since at least the 13th, maybe earlier, indexing small segments from Nutch in several cycles (few hundred per cycle), some shards get out of sync really quick! I did lots of tests before that but didn't see it happening before.\n\nOk, someting did change. I reverted back to a build of the 6th and everything is fine. I can run the index process from Nutch for many cycles and more data. No shard is going out of sync with itself. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13873334",
            "date": "2014-01-16T12:59:55+0000",
            "content": "Correction: it happens on a build of the 6th as well, although it doens't look that bad as when index to a 13th build. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13873457",
            "date": "2014-01-16T14:56:03+0000",
            "content": "Seems autocommit has something to do with triggering the problem, at least in my case.\n\n\n\t13th build without autocommit: out of sync very soon\n\t13th build with autocommit: out of sync after a while\n\t6th build without autocommit: out of sync after a while\n\t6th build with autocommit: out of sync after many more documents\n\n "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13873638",
            "date": "2014-01-16T17:29:33+0000",
            "content": "The commit behavior is interesting. I'm seeing docs flushing from the leader to replica following a manual hard commit issued long after indexing has stopped. That means somewhere along the way docs are buffered and waiting for an event to flush them to the replica. I haven't figured out just yet where the buffering is occurring but I'm trying to track it down.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873657",
            "date": "2014-01-16T17:48:09+0000",
            "content": "I spent some time a while back trying to find a fault in ConcurrentSolrServer#blockUntilFinished - didn't uncover anything yet though. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13873788",
            "date": "2014-01-16T19:10:35+0000",
            "content": "I'm betting it's something in the streaming. This afternoon I'm going to put some debugging in to see if the docs being flushed by the commit were already written to the stream. My bet is that they were, and that the commit is pushing them all the way through to the replica.  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13873841",
            "date": "2014-01-16T19:53:10+0000",
            "content": "I was able to reproduce this issue on EC2 without any over-sharding (on latest rev on branch_4x) ... basically 6 Solr nodes with 3 shards and RF=2, i.e. each replica gets its own Solr instance. Here's the output from my client app that traps the inconsistency:\n\n>>>>>>\nFound 1 shards with mis-matched doc counts.\nAt January 16, 2014 12:18:08 PM MST\nshard2: {\n\thttp://ec2-54-236-245-61.compute-1.amazonaws.com:8985/solr/test_shard2_replica2/ = 62984 LEADER\n\thttp://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/ = 62980 diff:4\n}\nDetails:\nshard2\n>> finished querying leader, found 62984 documents (62984)\n>> finished querying http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test_shard2_replica1/, found 62980 documents\nDoc [182866] not found in replica: <doc boost=\"1.0\"><field name=\"id\">182866</field><field name=\"string_s\">test</field><field name=\"int_i\">-1257345242</field><field name=\"float_f\">0.92657363</field><field name=\"double_d\">0.5259114828332452</field><field name=\"text_en\">this is a test</field><field name=\"version\">1457415570117885953</field></doc>\nDoc [182859] not found in replica: <doc boost=\"1.0\"><field name=\"id\">182859</field><field name=\"string_s\">test</field><field name=\"int_i\">991366909</field><field name=\"float_f\">0.5311716</field><field name=\"double_d\">0.10846350752086309</field><field name=\"text_en\">this is a test</field><field name=\"version\">1457415570117885952</field></doc>\nDoc [182872] not found in replica: <doc boost=\"1.0\"><field name=\"id\">182872</field><field name=\"string_s\">test</field><field name=\"int_i\">824512897</field><field name=\"float_f\">0.830366</field><field name=\"double_d\">0.6560223698806142</field><field name=\"text_en\">this is a test</field><field name=\"version\">1457415570117885954</field></doc>\nDoc [182876] not found in replica: <doc boost=\"1.0\"><field name=\"id\">182876</field><field name=\"string_s\">test</field><field name=\"int_i\">-1657831473</field><field name=\"float_f\">0.4877965</field><field name=\"double_d\">0.9214420679315872</field><field name=\"text_en\">this is a test</field><field name=\"version\">1457415570117885955</field></doc>\nSending hard commit after mis-match and then will wait for user to handle it ...\n<<<<<<\n\nSo four missing docs: 182866, 182859, 182872, 182876\n\nNow I'm thinking this might be in the ConcurrentUpdateSolrServer logic. I added some detailed logging to show when JavabinLoader unmarshals a doc and when it is offered on the CUSS queue (to be sent to the replica). On the leader, here's the log around some messages that were lost:\n\n2014-01-16 14:16:37,534 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182857\n2014-01-16 14:16:37,534 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182857\n/////////////////////////////////////\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182859\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182859\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182866\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182866\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182872\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182872\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182876\n2014-01-16 14:16:37,552 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182876\n2014-01-16 14:16:37,558 [qtp417447538-17] INFO  update.processor.LogUpdateProcessor  - [test_shard2_replica2] webapp=/solr path=/update params=\n{wt=javabin&version=2}\n \n{add=[182704 (1457415570048679936), 182710 (1457415570049728512), 182711 (1457415570049728513), 182717 (1457415570056019968), 182720 (1457415570056019969), 182722 (1457415570057068544), 182723 (1457415570057068545), 182724 (1457415570058117120), 182730 (1457415570058117121), 182735 (1457415570059165696), ... (61 adds)]}\n 0 72\n/////////////////////////////////////\n2014-01-16 14:16:37,764 [qtp417447538-17] INFO  handler.loader.JavabinLoader  - test_shard2_replica2 add: 182880\n2014-01-16 14:16:37,764 [qtp417447538-17] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test_shard2_replica2 queued: 182880\n\n\nAs you can see, the leader received doc with ID:182859 at 2014-01-16 14:16:37,552 and the queued it on the CUSS queue to be sent to the replica. On the replica, the log shows it receiving 182857 and then 182880 ... the 4 missing docs (182866, 182859, 182872, 182876) were definitely queued in CUSS on the leader. I've checked the logs on all the other replicas and the docs didn't go there either.\n\n\n2014-01-16 14:16:37,292 [qtp417447538-14] INFO  handler.loader.JavabinLoader  - test_shard2_replica1 add: 182857\n2014-01-16 14:16:37,293 [qtp417447538-14] INFO  update.processor.LogUpdateProcessor  - [test_shard2_replica1] webapp=/solr path=/update params=\n{distrib.from=http://ec2-54-236-245-61.compute-1.amazonaws.com:8985/solr/test_shard2_replica2/&update.distrib=FROMLEADER&wt=javabin&version=2}\n \n{add=[182841 (1457415570096914432), 182842 (1457415570096914433), 182843 (1457415570096914434), 182844 (1457415570096914435), 182846 (1457415570097963008), 182848 (1457415570097963009), 182850 (1457415570099011584), 182854 (1457415570099011585), 182857 (1457415570099011586)]}\n 0 2\n2014-01-16 14:16:37,521 [qtp417447538-14] INFO  handler.loader.JavabinLoader  - test_shard2_replica1 add: 182880\n\n\nSo it seems like a \"batch\" of docs queued on the leader just got missed ... "
        },
        {
            "author": "Shikhar Bhushan",
            "id": "comment-13873851",
            "date": "2014-01-16T19:58:58+0000",
            "content": "This may be unrelated - I have not done much digging or looked at the full context, but was just looking at CUSS out of curiosity.\n\nWhy do we flush() the OutputStream, but then write() on stuff like ending tags? Shouldn't the flush be after all those writes()'s?\n\nhttps://github.com/apache/lucene-solr/blob/lucene_solr_4_6/solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java#L205 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873868",
            "date": "2014-01-16T20:04:41+0000",
            "content": "I've checked the logs on all the other replicas and the docs didn't go there either.\n\nSo strange - it would be a different CUSS instance used for each server.... "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13873874",
            "date": "2014-01-16T20:11:36+0000",
            "content": "That was a blind alley, a faulty test was causing the effect I described above. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873882",
            "date": "2014-01-16T20:17:38+0000",
            "content": "I have various theories, but without a test that fails, it's hard to test out anything - so I've been putting most of my efforts into a unit test that can get this, but it's been surprisingly difficult for me to trigger in a test. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13873888",
            "date": "2014-01-16T20:25:22+0000",
            "content": "So strange - it would be a different CUSS instance used for each server....\n\nright, I was just mentioning that I did check to make sure there wasn't a bug in the routing logic or anything like that but I now see that was silly because it wouldn't be able to go to the other replicas because the message was on the correct queue \n\nagreed on the need for a unit test to reproduce this and am working on the same. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13873951",
            "date": "2014-01-16T21:15:09+0000",
            "content": "Added some more logging on the leader ... as a bit of context, the replica received doc with ID 41029 and then 41041 and didn't receive 41033 and 41038 in between ... here's the log on the leader of activity between 41029 and then 41041.\n\n2014-01-16 16:03:02,523 [updateExecutor-1-thread-1] INFO  solrj.impl.ConcurrentUpdateSolrServer  - sent docs to http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1 , 41003, 41\n005, 41007, 41010, 41014, 41015, 41026, 41029\n2014-01-16 16:03:02,527 [qtp417447538-16] INFO  handler.loader.JavabinLoader  - test3_shard3_replica2 add: 41033\n2014-01-16 16:03:02,527 [qtp417447538-16] INFO  update.processor.DistributedUpdateProcessor  - doLocalAdd 41033\n2014-01-16 16:03:02,527 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test3_shard3_replica2 queued (to: http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1): 41033\n2014-01-16 16:03:02,528 [qtp417447538-16] INFO  handler.loader.JavabinLoader  - test3_shard3_replica2 add: 41038\n2014-01-16 16:03:02,528 [qtp417447538-16] INFO  update.processor.DistributedUpdateProcessor  - doLocalAdd 41038\n2014-01-16 16:03:02,528 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - test3_shard3_replica2 queued (to: http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1): 41038\n2014-01-16 16:03:02,559 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - blockUntilFinished starting http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1\n2014-01-16 16:03:02,559 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - blockUntilFinished is done for http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1\n2014-01-16 16:03:02,559 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - shutting down CUSS for http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1\n2014-01-16 16:03:02,559 [qtp417447538-16] INFO  solrj.impl.ConcurrentUpdateSolrServer  - shut down CUSS for http://ec2-107-21-55-0.compute-1.amazonaws.com:8985/solr/test3_shard3_replica1\n\nNot quite sure what this means but I think you're hunch about blockUntilFinished being involved is getting warmer "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873964",
            "date": "2014-01-16T21:21:12+0000",
            "content": "For a long time, I've wanted to try putting in a check that the queue is empty as well for blockUntilFinished when we use it in this case - I just need a test that sees this so I can check if it works \n\nWithout that, it seems there is a window where we can bail before we are done sending everything in the queue. Shutdown doesn't help much, because it can't even wait for the executor to shutdown in this case. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873988",
            "date": "2014-01-16T21:39:17+0000",
            "content": "Patch attached that does the above. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874031",
            "date": "2014-01-16T22:09:12+0000",
            "content": "That's all I have come up with so far - though I'm not even completely sold on it. Because we are using CUSS with a single thread, all the previous doc adds should have hit the request method and so a Runner should be going for them if necessary.\n\nIt's all pretty tricky logic to understand clearly though. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13874098",
            "date": "2014-01-16T22:49:44+0000",
            "content": "So far so good, Mark! I applied the patch to latest rev of branch_4x and have indexed about 3M docs without hitting the issue, before the patch, I would see this issue within a few minutes. So jury is still out and I'll keep stress testing it, but looks promising. Nice work! "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-13874120",
            "date": "2014-01-16T23:09:12+0000",
            "content": "Did another couple of million docs in an oversharded env. 24 replicas on 6 nodes (m1.mediums so I didn't want to overload them too much) ... still looking good. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13874194",
            "date": "2014-01-17T00:10:09+0000",
            "content": "I installed the patch and ran it. \n\nI'm getting some intermittent null pointers:\n\n1578995 [qtp433857665-17] ERROR org.apache.solr.servlet.SolrDispatchFilter  \u2013 null:java.lang.NullPointerException\n\tat org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.blockUntilFinished(ConcurrentUpdateSolrServer.java:401)\n\tat org.apache.solr.update.StreamingSolrServers.blockUntilFinished(StreamingSolrServers.java:99)\n\tat org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:69)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doFinish(DistributedUpdateProcessor.java:606)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.finish(DistributedUpdateProcessor.java:1449)\n\tat org.apache.solr.update.processor.LogUpdateProcessor.finish(LogUpdateProcessorFactory.java:179)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:83)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1915)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:764)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:418)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:203)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419) "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13874204",
            "date": "2014-01-17T00:22:52+0000",
            "content": "In the code snippet below it looks like this line is the culprit:\n\n\nif ((runner == null && queue.isEmpty()) || scheduler.isTerminated())\n\n\n\n\n public synchronized void blockUntilFinished(boolean waitForEmptyQueue) {\n    lock = new CountDownLatch(1);\n    try {\n      // Wait until no runners are running\n      for (;;) {\n        Runner runner;\n        synchronized (runners) {\n          runner = runners.peek();\n        }\n        if (waitForEmptyQueue) {\n          if ((runner == null && queue.isEmpty()) || scheduler.isTerminated())\n            break;\n        } else {\n          if (runner == null || scheduler.isTerminated())\n            break;\n        }\n        runner.runnerLock.lock();\n        runner.runnerLock.unlock();\n      }\n    } finally {\n      lock.countDown();\n      lock = null;\n    }\n  }\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874270",
            "date": "2014-01-17T01:24:52+0000",
            "content": "Strange Joel - queue and scheduler are both final and set in the constructor. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13874294",
            "date": "2014-01-17T01:43:19+0000",
            "content": "It's actually the runner that is null:\n\nrunner.runnerLock.lock();\n\n\n\nThe conditions in this statement have changed and I think made it possible for the null pointer to appear.\n\nif ((runner == null && queue.isEmpty()) || scheduler.isTerminated())\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874300",
            "date": "2014-01-17T01:48:43+0000",
            "content": "Commit 1558978 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558978 ]\n\nSOLR-4260: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874301",
            "date": "2014-01-17T01:49:28+0000",
            "content": "Well, this is important for 4.6.1 - given Potter's feedback, in it goes. Please help test and review this guys. Especially around this possible NPE. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874302",
            "date": "2014-01-17T01:50:58+0000",
            "content": "Commit 1558979 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558979 ]\n\nSOLR-4260: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874303",
            "date": "2014-01-17T01:51:09+0000",
            "content": "Commit 1558980 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558980 ]\n\nSOLR-4260: Add name to CHANGES "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874305",
            "date": "2014-01-17T01:52:10+0000",
            "content": "Commit 1558981 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558981 ]\n\nSOLR-4260: Add name to CHANGES "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874311",
            "date": "2014-01-17T01:59:45+0000",
            "content": "The conditions in this statement have changed and I think made it possible for the null pointer to appear.\n\nAh, nice - thanks. I had already made some changes so couldn't line up the src lines - thought you meant the line that was the culprit was the one that the NPE came from.\n\nI'll take a closer look. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874314",
            "date": "2014-01-17T02:00:58+0000",
            "content": "Commit 1558982 from Mark Miller in branch 'dev/branches/lucene_solr_4_6'\n[ https://svn.apache.org/r1558982 ]\n\nSOLR-4260: ConcurrentUpdateSolrServer#blockUntilFinished can return before all previously added updates have finished. This could cause distributed updates meant for replicas to be lost. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874317",
            "date": "2014-01-17T02:03:07+0000",
            "content": "Commit 1558983 from Mark Miller in branch 'dev/branches/lucene_solr_4_6'\n[ https://svn.apache.org/r1558983 ]\n\nSOLR-4260: Add name to CHANGES "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874320",
            "date": "2014-01-17T02:06:32+0000",
            "content": "Commit 1558985 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558985 ]\n\nSOLR-4260: Guard against NPE. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874321",
            "date": "2014-01-17T02:08:01+0000",
            "content": "Commit 1558986 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558986 ]\n\nSOLR-4260: Guard against NPE. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874323",
            "date": "2014-01-17T02:10:17+0000",
            "content": "Commit 1558988 from Mark Miller in branch 'dev/branches/lucene_solr_4_6'\n[ https://svn.apache.org/r1558988 ]\n\nSOLR-4260: Guard against NPE. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874340",
            "date": "2014-01-17T02:28:39+0000",
            "content": "ChaosMonkeyNothingIsSafeTest is exposing an issue now with ConcurrentUpdateSolrServer - it looks like it's getting stuck in blockUntilFinished because the queue is not empty and no runners are being spawned to empty it.\n\nIt may be that NPE that would occurred before in this case just kept the docs from being lost 'silently', and this is closer to the actual bug? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874345",
            "date": "2014-01-17T02:36:27+0000",
            "content": "Commit 1558996 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1558996 ]\n\nSOLR-4260: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874348",
            "date": "2014-01-17T02:37:36+0000",
            "content": "Commit 1558997 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558997 ]\n\nSOLR-4260: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874351",
            "date": "2014-01-17T02:40:30+0000",
            "content": "Committed something for that.\n\nAs a separate issue, it seems to me that CUSS#shutdown should probably call blockUntilFinished as it's first order of business. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874352",
            "date": "2014-01-17T02:40:58+0000",
            "content": "Commit 1558998 from Mark Miller in branch 'dev/branches/lucene_solr_4_6'\n[ https://svn.apache.org/r1558998 ]\n\nSOLR-4260: If in blockUntilFinished and there are no Runners running and the queue is not empty, start a new Runner. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874364",
            "date": "2014-01-17T02:55:48+0000",
            "content": "This is a fine fix for SolrCloud, especially for 4.6.1 - but there may be a better general fix hidden still - what seems to happen is that we have docs that enter the queue that don't spawn a runner. The current fix means docs can be added that will sit in the queue until you call blockUntilFinished. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13874434",
            "date": "2014-01-17T04:59:06+0000",
            "content": "This might be old news by now, but I noticed it while updating my test system, so I'm reporting it.\n\nThe lucene_solr_4_6 branch fails to compile with these fixes committed.  One of the changes removes the import for RemoteSolrException from SolrCmdDistributor, but the doRetries method still uses this exception.  That method is very different in 4.6 than it is in branch_4x.  Everything's good on branch_4x.  Re-adding the import fixes the problem, but the discrepancy between the two branches needs some investigation.\n\nThe specific code that fails to compile with the removed import seems to have been initially added to trunk by revision 1545464 (2013/11/25) and removed from trunk by revision 1546670 (2013/11/29).  It was then re-added to lucene_solr_4_6 by revision 1554122 (2013/12/29). "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13874667",
            "date": "2014-01-17T11:10:37+0000",
            "content": "I believe the whole building now knows i cannot reproduce the problem! "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13874686",
            "date": "2014-01-17T11:24:52+0000",
            "content": "What a great hunt, guys! Thanks a lot! "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-13874835",
            "date": "2014-01-17T14:45:19+0000",
            "content": "Ok, just had two clean test runs with trunk. The NPE is no longer occurring and the leaders and replicas are in sync. Running through some more stress tests this morning, but so far so good.\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13874839",
            "date": "2014-01-17T14:54:40+0000",
            "content": "Commit 1559125 from Mark Miller in branch 'dev/branches/lucene_solr_4_6'\n[ https://svn.apache.org/r1559125 ]\n\nSOLR-4260: Bring back import still used on 4.6 branch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13874841",
            "date": "2014-01-17T15:03:26+0000",
            "content": "Thanks Shawn - fixed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13875755",
            "date": "2014-01-18T21:57:41+0000",
            "content": "Thanks everyone. I'll make a new JIRA issue to properly fix this. I'm not sure we should remove this logic, it's a good failsafe, but ideally, we don't want to run out of runners when there are still updates in the queue. Calling blockUntilFinished is not supposed to be required to make sure the queue is emptied. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13888951",
            "date": "2014-02-02T15:19:33+0000",
            "content": "Calling this done for 4.6.1. Let's open a new issue for anything further. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13897716",
            "date": "2014-02-11T10:39:36+0000",
            "content": "ignore apparently one node did not receive the update. forgive my stupidity "
        },
        {
            "author": "Hari Sekhon",
            "id": "comment-14496055",
            "date": "2015-04-15T11:15:16+0000",
            "content": "I've seen discrepancies between leader and followers of much higher numbers on newer versions of Solr than in this ticket when running on HDFS, it might be a separate issue, raised as SOLR-7395. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14499853",
            "date": "2015-04-17T13:51:17+0000",
            "content": "This ticket addressed specific issues - please open a new ticket for any further reports. "
        }
    ]
}