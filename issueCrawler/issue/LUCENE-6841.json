{
    "id": "LUCENE-6841",
    "title": "LZ4 compression using too much CPU time",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "5.3.1",
        "components": [
            "core/codecs"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Bug"
    },
    "description": "I am using Lucene for search indexing, including storing a large number of small fields, and some larger plain text fields, and searching using both exact matches and analyzed queries.\n\nLZ4 (specifically the decompress method) is using nearly exactly 50% of the application's CPU time.\n\nIt seems to me that LZ4 is inappropriate for my use case. I note that I can choose BEST_SPEED or BEST_COMPRESSION.\n\nWould it be palatable to add a NO_COMPRESSION option, or some way to pick and choose which fields get compressed? Perhaps a minimum length of a field could be specified before it's compressed? I'm not sure if that's possible.\n\nIf this approach, or similar is palatable, I would be happy to contribute a patch (or to consume and test a patch).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14960523",
            "author": "Adrien Grand",
            "date": "2015-10-16T11:21:10+0000",
            "content": "This issue is a bit tricky, because we have some users wishing that stored fields compression was more aggressive, and other users wishing that compression was less aggressive. But on the other hand, we want to support as few options as possible in order to keep backward compatibility manageable (we already have 2 and certainly want to avoid 3).\n\nAre you sure that the issue is with lz4 and not I/O? The method that performs decompression performs I/O and actual decompression at the same time, so from the output of a profiler it could easily look like the issue is with lz4 while it actually is with I/O.\n\nAlso what kind of workload do you have (number of docs, size of individual docs, number of docs retrieved per page)? Any chance that your index fits in memory entirely? "
        },
        {
            "id": "comment-14960553",
            "author": "Uwe Schindler",
            "date": "2015-10-16T11:48:59+0000",
            "content": "or some way to pick and choose which fields get compressed? Perhaps a minimum length of a field could be specified before it's compressed? I'm not sure if that's possible.\n\nThe compression is in blocks and not per field. To read a stored field it has to load and decompress the whole block, containing multiple fields (and maybe also multiple documents). If you do this for many small fields where you are only interested in some of them you may use the wrong type of storage. Since Lucene 4 there is an alternative, column-based store called docvalues. If you want to use data fields during scoring or to read sequentially a single field for all documents, then it is better to use column-based docvalues (which can be numeric, too - e.g. useful as scoring factors).\n\nStored fields use-case is to load few documents as part of search result display of like 10 or 20 top-ranking documents. They are not made for processing millions of documents like to retrieve scoring factors. "
        },
        {
            "id": "comment-14961541",
            "author": "Karl von Randow",
            "date": "2015-10-16T23:41:49+0000",
            "content": "Thank you both very much for your prompt responses.\n\nMy search index on disk is 5.2G. My heap is larger than that and the box memory is larger than that, so possibly the entire index can fit in memory. Could you point me to something to try to verify this?\n\nNumber of docs is in the low millions. Each doc is often quite small, 10-20 fields with only one containing more than a single line of text.\n\nMy profiling was only of threads that were in the RUNNABLE state, so I don't believe that this includes IO waiting time. So I do suspect it is actually LZ4ing rather than IO.\n\nI feel like to proceed I may need to do a run without compression to compare the CPU usage. This will I presume require rebuilding the search index\u2026 there is an upgrade process isn't there for converting between codecs? Perhaps I could use that to speed the process? "
        }
    ]
}