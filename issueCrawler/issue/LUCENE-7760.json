{
    "id": "LUCENE-7760",
    "title": "StandardAnalyzer/Tokenizer.setMaxTokenLength's javadocs are lying",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Closed",
        "type": "Bug",
        "components": [],
        "fix_versions": [
            "6.6",
            "7.0"
        ]
    },
    "description": "The javadocs claim that too-long tokens are discarded, but in fact they are simply chopped up.  The following test case unexpectedly passes:\n\n\n  public void testMaxTokenLengthNonDefault() throws Exception {\n    StandardAnalyzer a = new StandardAnalyzer();\n    a.setMaxTokenLength(5);\n    assertAnalyzesTo(a, \"ab cd toolong xy z\", new String[]{\"ab\", \"cd\", \"toolo\", \"ng\", \"xy\", \"z\"});\n    a.close();\n  }\n\n\n\nWe should at least fix the javadocs ...\n\n(I hit this because I was trying to also add setMaxTokenLength to EnglishAnalyzer).",
    "attachments": {
        "LUCENE-7760.patch": "https://issues.apache.org/jira/secure/attachment/12861642/LUCENE-7760.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15949082",
            "date": "2017-03-30T13:41:37+0000",
            "content": "+1\n\nFrom http://mail-archives.apache.org/mod_mbox/lucene-java-user/201611.mbox/%3c36FCFD77-D873-4757-9D16-E89016F169E6@gmail.com%3e, where I most recently responded to a user question about the situation - this should be useful as a seed for javadoc fixes:\n\n\nThe behavior you mention is an intentional change from the behavior in Lucene 4.9.0 and earlier,\nwhen tokens longer than maxTokenLenth were silently ignored: see LUCENE-5897[1] and LUCENE-5400[2].\n\nThe new behavior is as follows: Token matching rules are no longer allowed to match against\ninput char sequences longer than maxTokenLength.  If a rule that would match a sequence longer\nthan maxTokenLength, but also matches at maxTokenLength chars or fewer, and has the highest\npriority among all other rules matching at this length, and no other rule matches more chars,\nthen a token will be emitted for that rule at the matching length.  And then the rule-matching\niteration simply continues from that point as normal.  If the same rule matches against the\nremainder of the sequence that the first rule would have matched if maxTokenLength were longer,\nthen another token at the matched length will be emitted, and so on. \n\nNote that this can result in effectively splitting the sequence at maxTokenLength intervals\nas you noted.\n\nYou can fix the problem by setting maxTokenLength higher - this has the side effect of growing\nthe buffer and not causing unwanted token splitting.  If this results in tokens larger than\nyou would like, you can remove them with LengthFilter.\n\nFYI there is discussion on LUCENE-5897 about separating buffer size from maxTokenLength, starting\nhere: <https://issues.apache.org/jira/browse/LUCENE-5897?focusedCommentId=14105729&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14105729>\n- ultimately I decided that few people would benefit from the increased configuration complexity.\n\n[1] https://issues.apache.org/jira/browse/LUCENE-5897\n[2] https://issues.apache.org/jira/browse/LUCENE-5400\n\n ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-15950622",
            "date": "2017-03-31T09:52:42+0000",
            "content": "OK thanks for the history here Steve Rowe ... I will fixup the javadocs based on this. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15952839",
            "date": "2017-04-02T19:54:43+0000",
            "content": "Patch, improving the javadocs and adding a couple test cases ... ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15952874",
            "date": "2017-04-02T22:25:31+0000",
            "content": "+1\n\nTests look good.  I like your simplification of my explanation (\"[long tokens] are chopped up at [maxTokenLength] and emitted as multiple tokens\"); a more precise description about rule matching, including the possibility of emitted tokens not being exactly maxTokenLength, is not likely to help many people. ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-15952875",
            "date": "2017-04-02T22:27:41+0000",
            "content": "Oh, I forgot to mention: UAX29URLEmailTokenizer has the same issue, and would benefit from the same javadoc fix (and tests). ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-15953403",
            "date": "2017-04-03T12:41:25+0000",
            "content": "Thanks Steve Rowe; I'll do the same for UAX29URLEmailTokenizer. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15953975",
            "date": "2017-04-03T18:41:17+0000",
            "content": "Another iteration, also fixing UAX29URLEmailTokenizer.  I also carried over the max token length check from StandardAnalyzer. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15954346",
            "date": "2017-04-03T23:46:21+0000",
            "content": "+1, thanks Mike.\n\nAnother iteration, also fixing UAX29URLEmailTokenizer. I also carried over the max token length check from StandardAnalyzer.\n\nCool, that check was from LUCENE-6682, where I should have also applied it to UAX29URLEmailTokenizer... ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-15964862",
            "date": "2017-04-11T19:39:02+0000",
            "content": "Commit 9ed722f5655639dd572853df5a5a14130323cf0f in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9ed722f ]\n\nLUCENE-7760: improve setMaxTokenLength javadocs for StandardAnalyzer/Tokenizer and UAX29URLEmailAnalyzer/Tokenizer ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-15964865",
            "date": "2017-04-11T19:41:13+0000",
            "content": "Commit 740d96767b37aac31f1e99ed1bab301f5e915f3a in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=740d967 ]\n\nLUCENE-7760: improve setMaxTokenLength javadocs for StandardAnalyzer/Tokenizer and UAX29URLEmailAnalyzer/Tokenizer ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-15964866",
            "date": "2017-04-11T19:41:31+0000",
            "content": "Thanks Steve Rowe. ",
            "author": "Michael McCandless"
        }
    ]
}