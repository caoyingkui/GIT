{
    "id": "LUCENE-7179",
    "title": "GeoPoint and LatLonPoint test data should quantize once",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Bug"
    },
    "description": "LatLonPoint and GeoPointField tests pre quantizes test data to ensure consistency with indexed (encoded) data. The pre quantized data then becomes indexed, undergoing another quantization. To guarantee numerical stability this should be changed such that the test data is quantized after indexing.",
    "attachments": {
        "LUCENE-7179.patch": "https://issues.apache.org/jira/secure/attachment/12797101/LUCENE-7179.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15226307",
            "author": "Robert Muir",
            "date": "2016-04-05T14:04:21+0000",
            "content": "stability is already separately tested, e.g.:\n\nhttps://github.com/apache/lucene-solr/blob/master/lucene/sandbox/src/test/org/apache/lucene/document/TestLatLonPoint.java#L109\n\nI think this change would make some of these tests very complex or impossible, as e.g. some of them put the value in a stored field / array /or similar for comparisons.\n\nIt is ok two quantize twice or even 10,000 times or there is a bug. "
        },
        {
            "id": "comment-15226311",
            "author": "Karl Wright",
            "date": "2016-04-05T14:05:54+0000",
            "content": "+1 to that.  You can't create numerical instability because you do the same exact operation multiple times. "
        },
        {
            "id": "comment-15226352",
            "author": "Nicholas Knize",
            "date": "2016-04-05T14:26:19+0000",
            "content": "This is not an issue for existing LatLonPoint or GeoPointField because:\n\n1. LatLonPoint uses 0x1L << 32 / 360.0 and handles overflow for 90.0 180.0 by calling Math.nextDown.\n2. GeoPointField uses 31 bits and tolerance 1e-6. So overflow is not a problem.\n\nWhen switching to a morton encoding which maps -90.0, -180.0 to 0L and 90.0, 180.0 to 0xFFFFF... round off error becomes a problem when points are quantized more than once.  I will add the LUCENE-7165 patch along with a test to show this.   "
        },
        {
            "id": "comment-15226429",
            "author": "Robert Muir",
            "date": "2016-04-05T15:13:41+0000",
            "content": "But that is because the encoding is not good here: its unstable. I think if we are changing it to use 64-bits, then we should also fix the current rounding and overflow issues too?\n "
        },
        {
            "id": "comment-15226503",
            "author": "Nicholas Knize",
            "date": "2016-04-05T15:50:45+0000",
            "content": "I think if we are changing it to use 64-bits, then we should also fix the current rounding and overflow issues too?\n\nIn the Lucene-7165 patch the encoding tolerance is changed to be consistent 1e-7. Indeed the instability occurs > 1e-7. You can see the issue present itself in scaleLon when the 32bit quantization is \"truncated\" to a long. Overflow is not a problem because the positive values are mapped using (0x1L << 32) - 1 so 360.0 correctly maps to 0xFFFFFFFF / 4294967295. Not sure what you mean by \"fix the rounding issue\"? "
        },
        {
            "id": "comment-15226509",
            "author": "Nicholas Knize",
            "date": "2016-04-05T15:54:03+0000",
            "content": "I'm sure this will be discussed more. Nevertheless, here's a quick patch that only quantizes the test data after its indexed so its comparing apples/apples. "
        },
        {
            "id": "comment-15226633",
            "author": "Robert Muir",
            "date": "2016-04-05T16:50:58+0000",
            "content": "When I say rounding issues, I mean the direction the users value will \"go\" in quantized space. Today GeoPoint truncates with a (long) cast, which goes in the direction of zero. Because of the subtraction it does, it mostly works (simply because supposedly numbers are always positive), but its buggy for some double values: I tried to port LatLonPoints \"rounds down\" test and it fails.\n\nHaving consistency with how we round things, having stability in the encoding, dealing with the maximum representable value, these are not fun things to do, but I think they are necessary for our own sanity at least. They can also make some things efficient, for example being able to do a bounding box query without having to do two-phase confirmation or actually any decoding at all (like LatLonPoint).\n\nIf we are gonna have this quantization in lucene, then we need to make it sane, test the hell out of it, and then take advantage of it too for faster queries / simpler processing in integer space, rather than it just being a constant pain. Otherwise I don't think we should do quantization! "
        },
        {
            "id": "comment-15226699",
            "author": "Nicholas Knize",
            "date": "2016-04-05T17:21:14+0000",
            "content": "I don't disagree with the \"pain\" points. But you have to remember that GeoPointField works by way of a quad tree represented in unsigned long space. This isn't \"quantization\" for memory/disk purposes, its a dimensionality reduction technique. GeoPointTermsEnum relations simply reduce to a bunch of prefix masking and bit operations. The fact that the space filling curve is represented as a 64 bit long is only for bit operation simplicity. I could change it to a bigger bit space and make it closer to lossless, it just makes the enum code harrier.\n\nI tried to port LatLonPoints \"rounds down\" test and it fails\n\nIf you're referring to TestEncodingUtils.testEncodeDecodeRoundsDown it passes fine with the LUCENE-7164 64 bit space change. It won't pass if you change the GeoPoint encoding to use Math.round But again... all of these inconsistencies are occurring within the expected accepted TOLERANCE so they shouldn't be a surprise. Its the same as casting a double to a float and back and expecting numerical stability.\n\nits buggy for some double values\n\n?? Not sure I follow. Its not lossless if that's what you mean? But that's also a known limitation for using 32bit unsigned space. \n\nOtherwise I don't think we should do quantization!\n\nIts needed for GeoPointField. But if we don't want to handle the dimensional reduction limitations we can remove this approach altogether. Noting that we haven't even begun to tap into its optimization potential. "
        },
        {
            "id": "comment-15226716",
            "author": "Nicholas Knize",
            "date": "2016-04-05T17:29:16+0000",
            "content": "Oh, I do think the \"quantization\" issue (which I still think \"encoding\" is more accurate) is separate from this issue. While I agree we \"shouldn't care\" that its being encoded more than once, its already being encoded in the indexer. So it seems more appropriate to just encode the raw test data for consistent comparisons and not before adding the point to the document?  "
        },
        {
            "id": "comment-15226801",
            "author": "Robert Muir",
            "date": "2016-04-05T18:09:29+0000",
            "content": "\nBut you have to remember that GeoPointField works by way of a quad tree represented in unsigned long space. This isn't \"quantization\" for memory/disk purposes, its a dimensionality reduction technique. \n\nThis is unrelated to what I am talking about. I am only talking about the truncation of the data provided by the user. \n\nAll encoding around this stuff needs to be stable: I think I cannot compromise on this! we can encode/decode 1 time or 10,000 times and expect the same result. If we don't handle the rounding issues and overflow issues, then encoded integers aren't sorted in a meaningful way in integer or binary space, which means things are buggy.  "
        },
        {
            "id": "comment-15226809",
            "author": "Robert Muir",
            "date": "2016-04-05T18:12:01+0000",
            "content": "\nIf you're referring to TestEncodingUtils.testEncodeDecodeRoundsDown it passes fine with the LUCENE-7164 64 bit space change.\n\nNo I am not: that test is relatively worthless because it does not test, and will never test the values that are buggy. If you fix it to look like LatLonPoint's, it will fail, and it will fail with the new encoding on LUCENE-7164 too. "
        },
        {
            "id": "comment-15226848",
            "author": "Robert Muir",
            "date": "2016-04-05T18:24:22+0000",
            "content": "\nIts needed for GeoPointField.\n\nNot true: we can take float instead. And maybe this is a good idea if we are not going to handle this stuff properly. Then no data from the user is lost: 64 bits in, 64 bits out.\n\nBut today it is 128 bits in, 64 bits out. My problem is exactly how this lossiness occurs. It must be something we can reason about. For example with LatLonPoint, for any 64-bit double from the user, we know we encode an integer that that is always <= to that value (not willy nilly rounding), we know that sort order is completely preserved in integer space, and that there is no overflow. This is really all I am asking for. "
        },
        {
            "id": "comment-15226859",
            "author": "Michael McCandless",
            "date": "2016-04-05T18:29:58+0000",
            "content": "+1 for using stable encode/decode, meaning:\n\n\n  decode(encode(x)) == decode(encode(decode(encode(x))))\n\n\n\nfor all (in-bounds) values of x. "
        },
        {
            "id": "comment-15226868",
            "author": "Nicholas Knize",
            "date": "2016-04-05T18:34:37+0000",
            "content": "that test is relatively worthless because it does not test, and will never test the values that are buggy\n\nI'm going to remove it if its worthless. LatLonPoint version should be the standard if the TestEncodingUtils version omits important cases.\n\nThis is unrelated to what I am talking about. I am only talking about the truncation of the data provided by the user.\n\nhmm. I miss the distinction then. The user provides lat lon values as 64 bit double precision. For GeoPointField these floats are binned into unsigned 32 bit integer space so that all values will be \"sorted\" (along the single dimension integer number line) correctly. Because its a 32 bit space, though, data truncation is inevitable. \"close\" values will be binned together. Similarly, LatLonPoint encodes these in signed 32 bit space and NumericUtils is left to handle the signed bit for comparisons. But, again, because its 32 bit space truncation is inevitable (e.g., 180.0 has to be stepped down). So in both cases there's data loss. We just handle them differently. \n\nI think I cannot compromise on this!\n\nThe current patch isn't asking for a compromise. Its up for discussion and investigation for everyone to have a look. There's likely a simple solution but I'm working a few things in tandem. So if something comes to someone else's mind real quick I'm super happy to have the collaboration. I'll get back to this one in a sec and check the LatLonPoint failing test case. "
        },
        {
            "id": "comment-15226888",
            "author": "Nicholas Knize",
            "date": "2016-04-05T18:41:39+0000",
            "content": "b.q. Not true: we can take float instead. \n\nI'm not talking about float vs. double. I'm talking about encoding to 64 bit unsigned space. (this is why I dislike the term \"quantization\" it's only part of the GeoPointField encoding). Encoding to 64 bit unsigned space is required for GeoPointField so that GeoPointTermsEnum can continue to operate using bit operations. Unless I change the size of the bit space.\n "
        },
        {
            "id": "comment-15226899",
            "author": "Robert Muir",
            "date": "2016-04-05T18:45:19+0000",
            "content": "\nBecause its a 32 bit space, though, data truncation is inevitable.\n\nNo, we are choosing to truncate the user's data. This is something to be taken seriously. \n\nI care very much about the details about exactly how this truncation happens, that is what I keep bringing up on this issue:\n\n\tstability\n\trounding\n\toverflow\n\n\n\nGeoPoint has a much slower bounding box query than LatLonPoint because it can't take advantage of exactly how its truncation happens for these silly reasons. If these were fixed, this query would be faster.\n\nI just commented on the port of distance sort (LUCENE-7180) about how important integer space bounding box is to reduce cpu in compareBottom.\n\nAnd you can see speedups in LUCENE-7177 for GeoPoint's polygon queries which are based on operations in integer space (i had to incorporate significant hair to accomodate the current untamed quantization).\n\nSo there is 3 use cases right now on the table for why we should fix this: faster bounding box, sorting, polygon queries. And code can still be simple and the quantization \"effect\" is easier to reason about: e.g. fixing rounding means the 'double' we treat it as is always the closest double in our integer space that is <= the user's value, instead of \"rounded half-hazardly in an unknown direction\". It just requires we pay attention and fix the bugs and write good tests. "
        },
        {
            "id": "comment-15226976",
            "author": "Nicholas Knize",
            "date": "2016-04-05T19:33:50+0000",
            "content": "Unless there's ideas for lossless compression of 64 bit signed double precision to 32 bit unsigned space I'm not sure this discussion will be any more productive. So I'll get back to investigating other approaches. In the meantime I vote the API refactor all doubles to floats. At least then its explicit to the user that lat/lon values are stored in 32 bits.\n\n "
        },
        {
            "id": "comment-15226999",
            "author": "Robert Muir",
            "date": "2016-04-05T19:48:47+0000",
            "content": "\nBut if we don't want to handle the dimensional reduction limitations we can remove this approach altogether. Noting that we haven't even begun to tap into its optimization potential.\n\nRight, I think we should try to improve it here to work the best it can in one dimensional space. Working better on a z-order curve means doing work in integer space! I have an example of what this looks like on LUCENE-7177. Look in the code for all my comments about the encoding & rounding issues i worked around.\n\nIf we can reason well about the encoding and quantization, we can explore adapting that patch's 1D grid to work in the same z-order space, or other options, or we can stick with using rectangles. Even if we stick with rectangles, in my opinion integers and byte[] are easier for many operations, so its worth the trouble. "
        },
        {
            "id": "comment-15227028",
            "author": "Robert Muir",
            "date": "2016-04-05T19:59:31+0000",
            "content": "\nUnless there's ideas for lossless compression of 64 bit signed double precision to 32 bit unsigned space I'm not sure this discussion will be any more productive. \n\nWe don't need it to be lossless, just well-defined. If i were to try, my first stab would be:\n\n\tUse same quantization methods as LatLonPoint to quantize each component from a 64-bit double to a 32-bit integer.\n\tRun NumericUtils on each component so it sorts correctly.\n\tInterleave those and treat it in your head like its a unsigned 64-bit long.\n\n\n "
        }
    ]
}