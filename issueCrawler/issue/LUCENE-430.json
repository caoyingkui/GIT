{
    "id": "LUCENE-430",
    "title": "Reducing buffer sizes for TermDocs.",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "From java-dev: \n\nOn Friday 09 September 2005 00:34, Doug Cutting wrote: \n> Paul Elschot wrote: \n> > I suppose one of these cases are when many terms are used in a query.  \n> > Would it be easily possible to make the buffer size for a term iterator \n> > depend on the numbers of documents to be iterated? \n> > Many terms only occur in a few documents, so this could be a  \n> > nice win on total buffer size for the many terms case. \n>  \n> This would not be too difficult. \n>  \n> Look in SegmentTermDocs.java. \u00a0The buffer may be allocated when the  \n> parent's stream is first cloned, but clone() won't allocate a buffer if  \n> the source hasn't had a buffer allocated yet, and nothing should perform  \n> i/o directly on the parent's freqStream, so in practice a buffer should  \n> not be allocated until the first read is performed on the clone. \n\nI tried delaying the buffer allocation in BufferedIndexInput by \nusing this clone() method: \n\n\u00a0 public Object clone() \n{ \n\u00a0 \u00a0 BufferedIndexInput clone = (BufferedIndexInput)super.clone(); \n\u00a0 \u00a0 clone.buffer = null; \n\u00a0 \u00a0 clone.bufferLength = 0; \n\u00a0 \u00a0 clone.bufferPosition = 0; \n\u00a0 \u00a0 clone.bufferStart = getFilePointer();  \n\u00a0 \u00a0 return clone; \n\u00a0 }\n \n\nWith this all term document iterators seem to be empty, no \nquery in the test cases gives any results, for example TestDemo \nand TestBoolean2. \nAs far as I can see, this delaying should work, but it doesn't and \nI have no idea why. \n\nEnd of quote from java-dev. \n\nDoug replied that at a glance this clone method looks good. \nWithout this delayed buffer allocation, a reduced buffer size \nfor TermDocs cannot be implemented easily.",
    "attachments": {
        "lucene-430.patch": "https://issues.apache.org/jira/secure/attachment/12358050/lucene-430.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-05-24T01:16:39+0000",
            "content": "I'm attaching a patch that is slightly different from the patch Paul submitted. In refill() it calls seekInternal(bufferStart) in case the buffer is null. The reason is that after a clone the value of bufferStart might be different from the actual file pointer. This causes some test cases to fail with the original patch because refill() reads the data to buffer from the wrong position.\n\nWith this version all test cases pass. ",
            "author": "Michael Busch",
            "id": "comment-12498447"
        },
        {
            "date": "2007-05-24T07:32:30+0000",
            "content": "Are you also going to try and make the buffer size dependent on the number of docs that contain the term?\n\nThe current patch still uses BUFFERSIZE only. ",
            "author": "Paul Elschot",
            "id": "comment-12498523"
        },
        {
            "date": "2007-05-24T14:31:47+0000",
            "content": "Paul,\n\nyou are right, the patch I submitted just avoids copying the buffer. I was thinking about adding a method setBufferSize() to BufferedIndexInput. Please see my comment about this in LUCENE-888. ",
            "author": "Michael Busch",
            "id": "comment-12498659"
        },
        {
            "date": "2007-05-24T19:43:44+0000",
            "content": "OK, I committed this patch that delays the allocation of the buffer after a clone. I'll leave this issue open. After LUCENE-888 is committed I will attach a patch here that adjusts the buffer size of the freq stream in SegmentTermDocs according to the document frequency. ",
            "author": "Michael Busch",
            "id": "comment-12498792"
        },
        {
            "date": "2008-05-23T20:41:41+0000",
            "content": "Paul, have you experimented with changing the buffer sizes and how it affects search\nperformance?  ",
            "author": "Michael Busch",
            "id": "comment-12599500"
        },
        {
            "date": "2008-05-23T21:48:12+0000",
            "content": "I wish I could have done that, sorry. I left this one after I failed to change to buffer size and reported it. Time flies, fortunately. ",
            "author": "Paul Elschot",
            "id": "comment-12599516"
        },
        {
            "date": "2008-05-23T23:02:35+0000",
            "content": "\nI left this one after I failed to change to buffer size and reported it.\n\nThis change was committed a while ago, so if you have some time maybe\nyou could run some performance experiments? I somehow doubt that this\nwill improve performance significantly, but maybe I'm wrong.\n\nI'm going to unassign this for now, because I probably won't have time to\nrun the performance experiments anytime soon. Maybe after I finished \nworking on the other issues assigned to me I'll get back to this one. ",
            "author": "Michael Busch",
            "id": "comment-12599535"
        },
        {
            "date": "2011-01-26T13:59:51+0000",
            "content": "Michael B. committed the patch long time ago and the issue remained open for benchmark results. I doubt if they'd matter now. ",
            "author": "Shai Erera",
            "id": "comment-12987009"
        }
    ]
}